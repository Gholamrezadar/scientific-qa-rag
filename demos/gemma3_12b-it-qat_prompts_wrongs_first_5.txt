Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Coffee ring effect\n\nIn physics, a "coffee ring" is a pattern left by a puddle of particle-laden liquid after it evaporates. The phenomenon is named for the characteristic ring-like deposit along the perimeter of a spill of coffee. It is also commonly seen after spilling red wine. The mechanism behind the formation of these and similar rings is known as the coffee ring effect or in some instances, the coffee stain effect, or simply ring stain.\n== Flow mechanism ==\nThe coffee-ring pattern originates from the capillary flow induced by the evaporation of the drop: liquid evaporating from the edge is replenished by liquid from the interior. The resulting current can carry nearly all the dispersed material to the edge. As a function of time, this process exhibits a "rush-hour" effect, that is, a rapid acceleration of the flow towards the edge at the final stage of the drying process.\nEvaporation induces a Marangoni flow inside a droplet.  The flow, if strong, redistributes particles back to the center of the droplet.  Thus, for particles to accumulate at the edges, the liquid must have a weak Marangoni flow, or something must occur to disrupt the flow.  For example, surfactants can be added to reduce the liquid\'s surface tension gradient, disrupting the induced flow.  Water has a weak Marangoni flow to begin with, which is then reduced significantly by natural surfactants.\nInteraction of the particles suspended in a droplet with the free surface of the droplet is important in creating a coffee ring. "When the drop evaporates, the free surface collapses and traps the suspended particles ... eventually all the particles are captured by the free surface and stay there for the rest of their trip towards the edge of the drop." This result means that surfactants can be used to manipulate the motion of the solute particles by changing the surface tension of the drop, rather than trying to control the bulk flow inside the drop. A number of unique morphologies of the deposited particles can result.  For example, an enantiopure poly (isocyanate) derivative has been shown to form ordered arrays of squashed donut structures.\n== Suppression ==\nThe coffee-ring pattern is detrimental when uniform application of a dried deposit is required, such as in printed electronics. It can be suppressed by adding elongated particles, such as cellulose fibers, to the spherical particles that cause the coffee-ring effect. The size and weight fraction of added particles may be smaller than those of the primary ones.\nIt is also reported that controlling flow inside a droplet is a powerful way to generate a uniform film; for example, by harnessing solutal Marangoni flows occurring during evaporation.\nMixtures of low boiling point and high boiling point solvents were shown to suppress the coffee ring effect, changing the shape of a deposited solute from a ring-like to a dot-like shape.\nControl of the substrate temperature was shown to be an effective way to suppress the coffee ring formed by droplets of water-based PEDOT:PSS solution. On a heated hydrophilic or hydrophobic substrate, a thinner ring with an inner deposit forms, which is attributed to Marangoni convection.\nControl of the substrate wetting properties on slippery surfaces can prevent the pinning of the drop contact line, which will, therefore, suppress the coffee ring effect by reducing the number of particles deposited at the contact line. Drops on superhydrophobic or liquid impregnated surfaces are less likely to have a pinned contact line and will suppress ring formation. Drops with an oil ring formed at the drop contact line have high mobility and can avoid the ring formation on hydrophobic surfaces.\nAlternating voltage electrowetting may suppress coffee stains without the need to add surface-active materials. Reverse particle motion may also reduce the coffee-ring effect because of the capillary force near the contact line. The reversal takes place when the capillary force prevails over the outward coffee-ring flow by the geometric constraints.\n== Determinants of size and pattern ==\nThe lower-limit size of a coffee ring depends on the time scale competition between the liquid evaporation and the movement of suspended particles.  When the liquid evaporates much faster than the particle movement near a three-phase contact line, a coffee ring cannot be formed successfully.  Instead, these particles will disperse uniformly on a surface upon complete liquid evaporation.  For suspended particles of size 100 nm, the minimum diameter of the coffee ring structure is found to be 10 Î¼m, or about 10 times smaller than the width of human hair.  The shape of particles in the liquid is responsible for coffee ring effect. On porous substrates, the competition among infiltration, particle motion and evaporation of the solvent governs the final deposition morphology.\nThe pH of the solution of the drop influences the final deposit pattern. The transition between these patterns is explained by considering how DLVO interactions such as the electrostatic and Van der Waals forces modify the particle deposition process.\n== Applications ==\nThe coffee ring effect is utilized in convective deposition by researchers wanting to order particles on a substrate using capillary-driven assembly, replacing a stationary droplet with an advancing meniscus drawn across the substrate.  This process differs from dip-coating in that evaporation drives flow along the substrate as opposed to gravity.\nConvective deposition can control particle orientation, resulting in the formation of crystalline monolayer films from nonspherical particles such as hemispherical, dimer, and dumbbell shaped particles. Orientation is afforded by the system trying to reach a state of maximum packing of the particles in the thin meniscus layer over which evaporation occurs. They showed that tuning the volume fraction of particles in solution will control the specific location along the varying meniscus thickness at which assembly occurs. Particles will align with their long axis in- or out-of-plane depending on whether or not their longer dimension of the particle was equal to the thickness of the wetting layer at the meniscus location. Such thickness transitions were established with spherical particles as well. It was later shown that convective assembly could control particle orientation in assembling multi-layers, resulting in long-range 3D colloidal crystals from dumbbell shaped particles. These finds were attractive for the self-assembled of colloidal crystal films for applications such as photonics. Recent advances have increased the application of coffee-ring assembly from colloidal particles to organized patterns of inorganic crystals.\n== References ==', 'Peierls bracket\n\nIn theoretical physics, the Peierls bracket is an equivalent description of the Poisson bracket. It can be defined directly from the action and does not require the canonical coordinates and their canonical momenta to be defined in advance.\nThe bracket\n{\\displaystyle [A,B]}\nis defined as\n{\\displaystyle D_{A}(B)-D_{B}(A)}\nas the difference between some kind of action of one quantity on the other, minus the flipped term.\nIn quantum mechanics, the Peierls bracket becomes a commutator i.e. a Lie bracket.\n== References ==\nThis article incorporates material from the Citizendium article "Peierls bracket", which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License but not under the GFDL.\nPeierls, R. "The Commutation Laws of Relativistic Field Theory,"\nProc. R. Soc. Lond. August 21, 1952 214 1117 143-157.', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by LeÃ³ SzilÃ¡rd, and later by LÃ©on Brillouin.  SzilÃ¡rd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidtâs paradox.\nJohn Earman and John D. Norton have argued that SzilÃ¡rd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Crystallography', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  Ä§Ïk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, Î¸kâ , Î¸k, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the Î¸k, and the Fock space is built by applying all products of creation operators Î¸kâ  to |0â©.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at Ï = 0, so that the vacuum minimizing the Hamiltonian satisfies â¨Ïâ© = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  Ï = v .  This occurs for example, if V(Ï) = gÏ4 â 2m2Ï2 with g > 0 and m2 > 0, for which the minimum energy is found at v = Â±m/âg. The value of v in one of these vacua may be considered as condensate of the field Ï. Canonical quantization then can be carried out for the shifted field  Ï(x,t) â v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an Ä§-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over Ä§ of the commutator  [A, B]  expressed in the phase space formulation is iÄ§{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  Ä§-deformations are highly nonuniqueâquantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  Ä§ â 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an Ä§-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional SchrÃ¶dinger Hilbert space. If the chosen variables are complex, we get something like the SegalâBargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', 'Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word ÎºÏÏÏÏÎ±Î»Î»Î¿Ï (krÃºstallos; "clear ice, rock-crystal"), and Î³ÏÎ¬ÏÎµÎ¹Î½ (grÃ¡phein; "to write"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well new developments in these fields.\n== History and timeline ==\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the DavissonâGermer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n== Methodology ==\nCrystallographic methods depend mainly on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. Crystallographers often explicitly state the type of beam used, as in the terms X-ray diffraction, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.\nX-rays interact with the spatial distribution of electrons in the sample.\nNeutrons are scattered by the atomic nuclei through the strong nuclear forces, but in addition the magnetic moment of neutrons is non-zero, so they are also scattered by magnetic fields. When neutrons are scattered from hydrogen-containing materials, they produce diffraction patterns with high noise levels, which can sometimes be resolved by substituting deuterium for hydrogen.\nElectrons are charged particles and therefore interact with the total charge distribution of both the atomic nuclei and the electrons of the sample.:\u200aChpt 4\nIt is hard to focus x-rays or neutrons, but since electrons are charged they can be focused and are used in electron microscope to produce magnified images. There are many ways that transmission electron microscopy and related techniques such as scanning transmission electron microscopy, high-resolution electron microscopy can be used to obtain images with in many cases atomic resolution from which crystallographic information can be obtained. There are also other methods such as low-energy electron diffraction, low-energy electron microscopy and reflection high-energy electron diffraction which can be used to obtain crystallographic information about surfaces.\n== Applications in various areas ==\n=== Materials science ===\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Most materials do not occur as a single crystal, but are poly-crystalline in nature (they exist as an aggregate of small crystals with different orientations). As such, powder diffraction techniques, which take diffraction patterns of samples with a large number of crystals, play an important role in structural determination.\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements. Crystallographic studies help elucidate the relationship between a material\'s structure and its properties, aiding in developing new materials with tailored characteristics. This understanding is crucial in various fields, including metallurgy, geology, and materials science. Advancements in crystallographic techniques, such as electron diffraction and X-ray crystallography, continue to expand our understanding of material behavior at the atomic level.\nIn another example, iron transforms from a body-centered cubic (bcc) structure called ferrite to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material\'s properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which structures are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory.\n=== Biology ===\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. The double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol, Pymol or VMD can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron diffraction has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n== Notation ==\nCoordinates in square brackets such as [100] denote a direction vector (in real space).\nCoordinates in angle brackets or chevrons such as <100> denote a family of directions which are related by symmetry operations. In the cubic crystal system for example, <100> would mean [100], [010], [001] or the negative of any of those directions.\nMiller indices in parentheses such as (100) denote a plane of the crystal structure, and regular repetitions of that plane with a particular spacing. In the cubic system, the normal to the (hkl) plane is the direction [hkl], but in lower-symmetry cases, the normal to (hkl) is not parallel to [hkl].\nIndices in curly brackets or braces such as {100} denote a family of planes and their normals. In cubic materials the symmetry makes them equivalent, just as the way angle brackets denote a family of directions. In non-cubic materials, <hkl> is not necessarily perpendicular to {hkl}.\n== Reference literature ==\nThe International Tables for Crystallography is an eight-book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure through x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and do not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:', 'Jordy, W. H. (1952). Henry Adams: Scientific Historian. New Haven. ISBN 978-0-685-26683-0. {{cite book}}: ISBN / Date incompatibility (help)\nKhan, Salman. "Maxwell\'s Demon". Archived from the original on 2010-03-17.\nMaroney, O. J. E. (2009) ""Information Processing and Thermodynamic Entropy" The Stanford Encyclopedia of Philosophy (Autumn 2009 Edition)\nMaxwell, J. C. (1871). Theory of Heat. London, New York [etc.] Longmans, Green., reprinted (2001) New York: Dover, ISBN 0-486-41735-2\nNorton, J. (2005). "Eaters of the lotus: Landauer\'s principle and the return of Maxwell\'s demon" (PDF). Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics. 36 (2): 375â411. Bibcode:2005SHPMP..36..375N. CiteSeerX 10.1.1.468.3017. doi:10.1016/j.shpsb.2004.12.002. S2CID 21104635. Archived (PDF) from the original on 2006-09-01.\nRaizen, Mark G. (2011) "Demons, Entropy, and the Quest for Absolute Zero", Scientific American, March, pp54-59\nReaney, Patricia. "Scientists build nanomachine", Reuters, February 1, 2007\nRubi, J Miguel, "Does Nature Break the Second Law of Thermodynamics?"; Scientific American, October 2008 :\nSplasho (2008) â Historical development of Maxwell\'s demon\nWeiss, Peter. "Breaking the Law â Can quantum mechanics + thermodynamics = perpetual motion?", Science News, October 7, 2000', "Maxwell's demon", 'A time zone is an area which observes a uniform standard time for legal, commercial and social purposes. Time zones tend to follow the boundaries between countries and their subdivisions instead of strictly following longitude, because it is convenient for areas in frequent communication to keep the same time.\nEach time zone is defined by a standard offset from Coordinated Universal Time (UTC). The offsets range from UTCâ12:00 to UTC+14:00, and are usually a whole number of hours, but a few zones are offset by an additional 30 or 45 minutes, such as in India and Nepal. Some areas in a time zone may use a different offset for part of the year, typically one hour ahead during spring and summer, a practice known as daylight saving time (DST).\n== List of UTC offsets ==\nIn the table below, the locations that use daylight saving time (DST) are listed in their UTC offset when DST is not in effect. When DST is in effect, approximately during spring and summer, their UTC offset is increased by one hour (except for Lord Howe Island, where it is increased by 30 minutes). For example, during the DST period California observes UTCâ07:00 and the United Kingdom observes UTC+01:00.\n== History ==\nThe apparent position of the Sun in the sky, and thus solar time, varies by location due to the spherical shape of the Earth. This variation corresponds to four minutes of time for every degree of longitude, so for example when it is solar noon in London, it is about 10 minutes before solar noon in Bristol, which is about 2.5 degrees to the west.\nThe Royal Observatory, Greenwich, founded in 1675, established Greenwich Mean Time (GMT), the mean solar time at that location, as an aid to mariners to determine longitude at sea, providing a standard reference time while each location in England kept a different time.\n=== Railway time ===\nIn the 19th century, as transportation and telecommunications improved, it became increasingly inconvenient for each location to observe its own solar time. In November 1840, the British Great Western Railway started using GMT kept by portable chronometers. This practice was soon followed by other railway companies in Great Britain and became known as railway time.\nAround August 23, 1852, time signals were first transmitted by telegraph from the Royal Observatory. By 1855, 98% of Great Britain\'s public clocks were using GMT, but it was not made the island\'s legal time until August 2, 1880. Some British clocks from this period have two minute hands, one for the local time and one for GMT.\nOn November 2, 1868, the British Colony of New Zealand officially adopted a standard time to be observed throughout the colony. It was based on longitude 172Â°30â² east of Greenwich, that is 11 hours 30 minutes ahead of GMT. This standard was known as New Zealand Mean Time.\nTimekeeping on North American railroads in the 19th century was complex. Each railroad used its own standard time, usually based on the local time of its headquarters or most important terminus, and the railroad\'s train schedules were published using its own time. Some junctions served by several railroads had a clock for each railroad, each showing a different time.  Because of this a number of accidents occurred when trains from different companies using the same tracks mistimed their passings.\nAround 1863, Charles F. Dowd proposed a system of hourly standard time zones for North American railroads, although he published nothing on the matter at that time and did not consult railroad officials until 1869. In 1870 he proposed four ideal time zones having northâsouth borders, the first centered on Washington, D.C., but by 1872 the first was centered on meridian 75Â° west of Greenwich, with natural borders such as sections of the Appalachian Mountains. Dowd\'s system was never accepted by North American railroads.\nChief meteorologist at the United States Weather Bureau Cleveland Abbe divided the United States into four standard time zones for consistency among the weather stations. In 1879, he published a paper titled Report on Standard Time. In 1883, he convinced North American railroad companies to adopt his time-zone system. In 1884, Britain, which had already adopted its own standard time system for England, Scotland, and Wales, helped gather international consent for global time. In time, the American government, influenced in part by Abbe\'s 1879 paper, adopted the time-zone system.\nIt was a version proposed by William F. Allen, the editor of the Traveler\'s Official Railway Guide. The borders of its time zones ran through railroad stations, often in major cities. For example, the border between its Eastern and Central time zones ran through Detroit, Buffalo, Pittsburgh, Atlanta, and Charleston. It was inaugurated on Sunday, November 18, 1883, also called "The Day of Two Noons", when each railroad station clock was reset as standard-time noon was reached within each time zone.\nThe North American zones were named Intercolonial, Eastern, Central, Mountain, and Pacific. Within a year 85% of all cities with populations over 10,000 (about 200 cities) were using standard time. A notable exception was Detroit (located about halfway between the meridians of Eastern and Central time), which kept local time until 1900, then tried Central Standard Time, local mean time, and Eastern Standard Time (EST) before a May 1915 ordinance settled on EST and was ratified by popular vote in August 1916. The confusion of times came to an end when standard time zones were formally adopted by the U.S. Congress in the Standard Time Act of March 19, 1918.\n=== Worldwide time zones ===\nItalian mathematician Quirico Filopanti introduced the idea of a worldwide system of time zones in his book Miranda!, published in 1858. He proposed 24 hourly time zones, which he called "longitudinal days", the first centred on the meridian of Rome. He also proposed a universal time to be used in astronomy and telegraphy. However, his book attracted no attention until long after his death.\nScottish-born Canadian Sir Sandford Fleming proposed a worldwide system of time zones in 1876 - see Sandford Fleming Â§ Inventor of worldwide standard time. The proposal divided the world into twenty-four time zones labeled A-Y (skipping J), each one covering 15 degrees of longitude. All clocks within each zone would be set to the same time as the others, but differed by one hour from those in the neighboring zones. He advocated his system at several international conferences, including the International Meridian Conference, where it received some consideration. The system has not been directly adopted, but some maps divide the world into 24 time zones and assign letters to them, similarly to Fleming\'s system.\nBy about 1900, almost all inhabited places on Earth had adopted a standard time zone, but only some of them used an hourly offset from GMT. Many applied the time at a local astronomical observatory to an entire country, without any reference to GMT. It took many decades before all time zones were based on some standard offset from GMT or Coordinated Universal Time (UTC). By 1929, the majority of countries had adopted hourly time zones, though some countries such as Iran, India, Myanmar and parts of Australia had time zones with a 30-minute offset. Nepal was the last country to adopt a standard offset, shifting slightly to UTC+05:45 in 1986.\nAll nations currently use standard time zones for secular purposes, but not all of them apply the concept as originally conceived. Several countries and subdivisions use half-hour or quarter-hour deviations from standard time. Some countries, such as China and India, use a single time zone even though the extent of their territory far exceeds the ideal 15Â° of longitude for one hour; other countries, such as Spain and Argentina, use standard hour-based offsets, but not necessarily those that would be determined by their geographical location. The consequences, in some areas, can affect the lives of local citizens, and in extreme cases contribute to larger political issues, such as in the western reaches of China. In Russia, which has 11 time zones, two time zones were removed in 2010 and reinstated in 2014.\n== Notation ==\n=== ISO 8601 ===\nISO 8601 is a standard established by the International Organization for Standardization defining methods of representing dates and times in textual form, including specifications for representing time zones.\nIf a time is in Coordinated Universal Time (UTC), a "Z" is added directly after the time without a separating space. "Z" is the zone designator for the zero UTC offset. "09:30 UTC" is therefore represented as "09:30Z" or "0930Z". Likewise, "14:45:15 UTC" is written as "14:45:15Z" or "144515Z". UTC time is also known as "Zulu" time, since "Zulu" is a phonetic alphabet code word for the letter "Z".\nOffsets from UTC are written in the format Â±hh:mm, Â±hhmm, or Â±hh (either hours ahead or behind UTC). For example, if the time being described is one hour ahead of UTC (such as the time in Germany during the winter), the zone designator would be "+01:00", "+0100", or simply "+01". This numeric representation of time zones is appended to local times in the same way that alphabetic time zone abbreviations (or "Z", as above) are appended. The offset from UTC changes with daylight saving time, e.g. a time offset in Chicago, which is in the North American Central Time Zone, is "â06:00" for the winter (Central Standard Time) and "â05:00" for the summer (Central Daylight Time).\n=== Abbreviations ===\nTime zones are often represented by alphabetic abbreviations such as "EST", "WST", and "CST", but these are not part of the international time and date standard ISO 8601. Such designations can be ambiguous; for example, "CST" can mean (North American) Central Standard Time (UTCâ06:00), Cuba Standard Time (UTCâ05:00) and China Standard Time (UTC+08:00), and it is also a widely used variant of ACST (Australian Central Standard Time, UTC+09:30).']

Question: What is a "coffee ring" in physics?

Choices:
Choice A) A type of coffee that is made by boiling coffee grounds in water.
Choice B) A pattern left by a particle-laden liquid after it is spilled, named for the characteristic ring-like deposit along the perimeter of a spill of coffee or red wine.
Choice C) A type of coffee that is made by mixing instant coffee with hot water.
Choice D) A type of coffee that is made by pouring hot water over coffee grounds in a filter.
Choice E) A pattern left by a particle-laden liquid after it evaporates, named for the characteristic ring-like deposit along the perimeter of a spill of coffee or red wine.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Peierls bracket\n\nIn theoretical physics, the Peierls bracket is an equivalent description of the Poisson bracket. It can be defined directly from the action and does not require the canonical coordinates and their canonical momenta to be defined in advance.\nThe bracket\n{\\displaystyle [A,B]}\nis defined as\n{\\displaystyle D_{A}(B)-D_{B}(A)}\nas the difference between some kind of action of one quantity on the other, minus the flipped term.\nIn quantum mechanics, the Peierls bracket becomes a commutator i.e. a Lie bracket.\n== References ==\nThis article incorporates material from the Citizendium article "Peierls bracket", which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License but not under the GFDL.\nPeierls, R. "The Commutation Laws of Relativistic Field Theory,"\nProc. R. Soc. Lond. August 21, 1952 214 1117 143-157.', 'Canonical quantization', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  Ä§Ïk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, Î¸kâ , Î¸k, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the Î¸k, and the Fock space is built by applying all products of creation operators Î¸kâ  to |0â©.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at Ï = 0, so that the vacuum minimizing the Hamiltonian satisfies â¨Ïâ© = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  Ï = v .  This occurs for example, if V(Ï) = gÏ4 â 2m2Ï2 with g > 0 and m2 > 0, for which the minimum energy is found at v = Â±m/âg. The value of v in one of these vacua may be considered as condensate of the field Ï. Canonical quantization then can be carried out for the shifted field  Ï(x,t) â v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an Ä§-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over Ä§ of the commutator  [A, B]  expressed in the phase space formulation is iÄ§{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  Ä§-deformations are highly nonuniqueâquantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  Ä§ â 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an Ä§-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional SchrÃ¶dinger Hilbert space. If the chosen variables are complex, we get something like the SegalâBargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by LeÃ³ SzilÃ¡rd, and later by LÃ©on Brillouin.  SzilÃ¡rd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidtâs paradox.\nJohn Earman and John D. Norton have argued that SzilÃ¡rd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', 'Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word ÎºÏÏÏÏÎ±Î»Î»Î¿Ï (krÃºstallos; "clear ice, rock-crystal"), and Î³ÏÎ¬ÏÎµÎ¹Î½ (grÃ¡phein; "to write"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well new developments in these fields.\n== History and timeline ==\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the DavissonâGermer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n== Methodology ==\nCrystallographic methods depend mainly on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. Crystallographers often explicitly state the type of beam used, as in the terms X-ray diffraction, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.\nX-rays interact with the spatial distribution of electrons in the sample.\nNeutrons are scattered by the atomic nuclei through the strong nuclear forces, but in addition the magnetic moment of neutrons is non-zero, so they are also scattered by magnetic fields. When neutrons are scattered from hydrogen-containing materials, they produce diffraction patterns with high noise levels, which can sometimes be resolved by substituting deuterium for hydrogen.\nElectrons are charged particles and therefore interact with the total charge distribution of both the atomic nuclei and the electrons of the sample.:\u200aChpt 4\nIt is hard to focus x-rays or neutrons, but since electrons are charged they can be focused and are used in electron microscope to produce magnified images. There are many ways that transmission electron microscopy and related techniques such as scanning transmission electron microscopy, high-resolution electron microscopy can be used to obtain images with in many cases atomic resolution from which crystallographic information can be obtained. There are also other methods such as low-energy electron diffraction, low-energy electron microscopy and reflection high-energy electron diffraction which can be used to obtain crystallographic information about surfaces.\n== Applications in various areas ==\n=== Materials science ===\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Most materials do not occur as a single crystal, but are poly-crystalline in nature (they exist as an aggregate of small crystals with different orientations). As such, powder diffraction techniques, which take diffraction patterns of samples with a large number of crystals, play an important role in structural determination.\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements. Crystallographic studies help elucidate the relationship between a material\'s structure and its properties, aiding in developing new materials with tailored characteristics. This understanding is crucial in various fields, including metallurgy, geology, and materials science. Advancements in crystallographic techniques, such as electron diffraction and X-ray crystallography, continue to expand our understanding of material behavior at the atomic level.\nIn another example, iron transforms from a body-centered cubic (bcc) structure called ferrite to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material\'s properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which structures are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory.\n=== Biology ===\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. The double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol, Pymol or VMD can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron diffraction has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n== Notation ==\nCoordinates in square brackets such as [100] denote a direction vector (in real space).\nCoordinates in angle brackets or chevrons such as <100> denote a family of directions which are related by symmetry operations. In the cubic crystal system for example, <100> would mean [100], [010], [001] or the negative of any of those directions.\nMiller indices in parentheses such as (100) denote a plane of the crystal structure, and regular repetitions of that plane with a particular spacing. In the cubic system, the normal to the (hkl) plane is the direction [hkl], but in lower-symmetry cases, the normal to (hkl) is not parallel to [hkl].\nIndices in curly brackets or braces such as {100} denote a family of planes and their normals. In cubic materials the symmetry makes them equivalent, just as the way angle brackets denote a family of directions. In non-cubic materials, <hkl> is not necessarily perpendicular to {hkl}.\n== Reference literature ==\nThe International Tables for Crystallography is an eight-book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure through x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and do not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:', 'Crystallography', 'Jordy, W. H. (1952). Henry Adams: Scientific Historian. New Haven. ISBN 978-0-685-26683-0. {{cite book}}: ISBN / Date incompatibility (help)\nKhan, Salman. "Maxwell\'s Demon". Archived from the original on 2010-03-17.\nMaroney, O. J. E. (2009) ""Information Processing and Thermodynamic Entropy" The Stanford Encyclopedia of Philosophy (Autumn 2009 Edition)\nMaxwell, J. C. (1871). Theory of Heat. London, New York [etc.] Longmans, Green., reprinted (2001) New York: Dover, ISBN 0-486-41735-2\nNorton, J. (2005). "Eaters of the lotus: Landauer\'s principle and the return of Maxwell\'s demon" (PDF). Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics. 36 (2): 375â411. Bibcode:2005SHPMP..36..375N. CiteSeerX 10.1.1.468.3017. doi:10.1016/j.shpsb.2004.12.002. S2CID 21104635. Archived (PDF) from the original on 2006-09-01.\nRaizen, Mark G. (2011) "Demons, Entropy, and the Quest for Absolute Zero", Scientific American, March, pp54-59\nReaney, Patricia. "Scientists build nanomachine", Reuters, February 1, 2007\nRubi, J Miguel, "Does Nature Break the Second Law of Thermodynamics?"; Scientific American, October 2008 :\nSplasho (2008) â Historical development of Maxwell\'s demon\nWeiss, Peter. "Breaking the Law â Can quantum mechanics + thermodynamics = perpetual motion?", Science News, October 7, 2000', 'Coffee ring effect\n\nIn physics, a "coffee ring" is a pattern left by a puddle of particle-laden liquid after it evaporates. The phenomenon is named for the characteristic ring-like deposit along the perimeter of a spill of coffee. It is also commonly seen after spilling red wine. The mechanism behind the formation of these and similar rings is known as the coffee ring effect or in some instances, the coffee stain effect, or simply ring stain.\n== Flow mechanism ==\nThe coffee-ring pattern originates from the capillary flow induced by the evaporation of the drop: liquid evaporating from the edge is replenished by liquid from the interior. The resulting current can carry nearly all the dispersed material to the edge. As a function of time, this process exhibits a "rush-hour" effect, that is, a rapid acceleration of the flow towards the edge at the final stage of the drying process.\nEvaporation induces a Marangoni flow inside a droplet.  The flow, if strong, redistributes particles back to the center of the droplet.  Thus, for particles to accumulate at the edges, the liquid must have a weak Marangoni flow, or something must occur to disrupt the flow.  For example, surfactants can be added to reduce the liquid\'s surface tension gradient, disrupting the induced flow.  Water has a weak Marangoni flow to begin with, which is then reduced significantly by natural surfactants.\nInteraction of the particles suspended in a droplet with the free surface of the droplet is important in creating a coffee ring. "When the drop evaporates, the free surface collapses and traps the suspended particles ... eventually all the particles are captured by the free surface and stay there for the rest of their trip towards the edge of the drop." This result means that surfactants can be used to manipulate the motion of the solute particles by changing the surface tension of the drop, rather than trying to control the bulk flow inside the drop. A number of unique morphologies of the deposited particles can result.  For example, an enantiopure poly (isocyanate) derivative has been shown to form ordered arrays of squashed donut structures.\n== Suppression ==\nThe coffee-ring pattern is detrimental when uniform application of a dried deposit is required, such as in printed electronics. It can be suppressed by adding elongated particles, such as cellulose fibers, to the spherical particles that cause the coffee-ring effect. The size and weight fraction of added particles may be smaller than those of the primary ones.\nIt is also reported that controlling flow inside a droplet is a powerful way to generate a uniform film; for example, by harnessing solutal Marangoni flows occurring during evaporation.\nMixtures of low boiling point and high boiling point solvents were shown to suppress the coffee ring effect, changing the shape of a deposited solute from a ring-like to a dot-like shape.\nControl of the substrate temperature was shown to be an effective way to suppress the coffee ring formed by droplets of water-based PEDOT:PSS solution. On a heated hydrophilic or hydrophobic substrate, a thinner ring with an inner deposit forms, which is attributed to Marangoni convection.\nControl of the substrate wetting properties on slippery surfaces can prevent the pinning of the drop contact line, which will, therefore, suppress the coffee ring effect by reducing the number of particles deposited at the contact line. Drops on superhydrophobic or liquid impregnated surfaces are less likely to have a pinned contact line and will suppress ring formation. Drops with an oil ring formed at the drop contact line have high mobility and can avoid the ring formation on hydrophobic surfaces.\nAlternating voltage electrowetting may suppress coffee stains without the need to add surface-active materials. Reverse particle motion may also reduce the coffee-ring effect because of the capillary force near the contact line. The reversal takes place when the capillary force prevails over the outward coffee-ring flow by the geometric constraints.\n== Determinants of size and pattern ==\nThe lower-limit size of a coffee ring depends on the time scale competition between the liquid evaporation and the movement of suspended particles.  When the liquid evaporates much faster than the particle movement near a three-phase contact line, a coffee ring cannot be formed successfully.  Instead, these particles will disperse uniformly on a surface upon complete liquid evaporation.  For suspended particles of size 100 nm, the minimum diameter of the coffee ring structure is found to be 10 Î¼m, or about 10 times smaller than the width of human hair.  The shape of particles in the liquid is responsible for coffee ring effect. On porous substrates, the competition among infiltration, particle motion and evaporation of the solvent governs the final deposition morphology.\nThe pH of the solution of the drop influences the final deposit pattern. The transition between these patterns is explained by considering how DLVO interactions such as the electrostatic and Van der Waals forces modify the particle deposition process.\n== Applications ==\nThe coffee ring effect is utilized in convective deposition by researchers wanting to order particles on a substrate using capillary-driven assembly, replacing a stationary droplet with an advancing meniscus drawn across the substrate.  This process differs from dip-coating in that evaporation drives flow along the substrate as opposed to gravity.\nConvective deposition can control particle orientation, resulting in the formation of crystalline monolayer films from nonspherical particles such as hemispherical, dimer, and dumbbell shaped particles. Orientation is afforded by the system trying to reach a state of maximum packing of the particles in the thin meniscus layer over which evaporation occurs. They showed that tuning the volume fraction of particles in solution will control the specific location along the varying meniscus thickness at which assembly occurs. Particles will align with their long axis in- or out-of-plane depending on whether or not their longer dimension of the particle was equal to the thickness of the wetting layer at the meniscus location. Such thickness transitions were established with spherical particles as well. It was later shown that convective assembly could control particle orientation in assembling multi-layers, resulting in long-range 3D colloidal crystals from dumbbell shaped particles. These finds were attractive for the self-assembled of colloidal crystal films for applications such as photonics. Recent advances have increased the application of coffee-ring assembly from colloidal particles to organized patterns of inorganic crystals.\n== References ==', 'A time zone is an area which observes a uniform standard time for legal, commercial and social purposes. Time zones tend to follow the boundaries between countries and their subdivisions instead of strictly following longitude, because it is convenient for areas in frequent communication to keep the same time.\nEach time zone is defined by a standard offset from Coordinated Universal Time (UTC). The offsets range from UTCâ12:00 to UTC+14:00, and are usually a whole number of hours, but a few zones are offset by an additional 30 or 45 minutes, such as in India and Nepal. Some areas in a time zone may use a different offset for part of the year, typically one hour ahead during spring and summer, a practice known as daylight saving time (DST).\n== List of UTC offsets ==\nIn the table below, the locations that use daylight saving time (DST) are listed in their UTC offset when DST is not in effect. When DST is in effect, approximately during spring and summer, their UTC offset is increased by one hour (except for Lord Howe Island, where it is increased by 30 minutes). For example, during the DST period California observes UTCâ07:00 and the United Kingdom observes UTC+01:00.\n== History ==\nThe apparent position of the Sun in the sky, and thus solar time, varies by location due to the spherical shape of the Earth. This variation corresponds to four minutes of time for every degree of longitude, so for example when it is solar noon in London, it is about 10 minutes before solar noon in Bristol, which is about 2.5 degrees to the west.\nThe Royal Observatory, Greenwich, founded in 1675, established Greenwich Mean Time (GMT), the mean solar time at that location, as an aid to mariners to determine longitude at sea, providing a standard reference time while each location in England kept a different time.\n=== Railway time ===\nIn the 19th century, as transportation and telecommunications improved, it became increasingly inconvenient for each location to observe its own solar time. In November 1840, the British Great Western Railway started using GMT kept by portable chronometers. This practice was soon followed by other railway companies in Great Britain and became known as railway time.\nAround August 23, 1852, time signals were first transmitted by telegraph from the Royal Observatory. By 1855, 98% of Great Britain\'s public clocks were using GMT, but it was not made the island\'s legal time until August 2, 1880. Some British clocks from this period have two minute hands, one for the local time and one for GMT.\nOn November 2, 1868, the British Colony of New Zealand officially adopted a standard time to be observed throughout the colony. It was based on longitude 172Â°30â² east of Greenwich, that is 11 hours 30 minutes ahead of GMT. This standard was known as New Zealand Mean Time.\nTimekeeping on North American railroads in the 19th century was complex. Each railroad used its own standard time, usually based on the local time of its headquarters or most important terminus, and the railroad\'s train schedules were published using its own time. Some junctions served by several railroads had a clock for each railroad, each showing a different time.  Because of this a number of accidents occurred when trains from different companies using the same tracks mistimed their passings.\nAround 1863, Charles F. Dowd proposed a system of hourly standard time zones for North American railroads, although he published nothing on the matter at that time and did not consult railroad officials until 1869. In 1870 he proposed four ideal time zones having northâsouth borders, the first centered on Washington, D.C., but by 1872 the first was centered on meridian 75Â° west of Greenwich, with natural borders such as sections of the Appalachian Mountains. Dowd\'s system was never accepted by North American railroads.\nChief meteorologist at the United States Weather Bureau Cleveland Abbe divided the United States into four standard time zones for consistency among the weather stations. In 1879, he published a paper titled Report on Standard Time. In 1883, he convinced North American railroad companies to adopt his time-zone system. In 1884, Britain, which had already adopted its own standard time system for England, Scotland, and Wales, helped gather international consent for global time. In time, the American government, influenced in part by Abbe\'s 1879 paper, adopted the time-zone system.\nIt was a version proposed by William F. Allen, the editor of the Traveler\'s Official Railway Guide. The borders of its time zones ran through railroad stations, often in major cities. For example, the border between its Eastern and Central time zones ran through Detroit, Buffalo, Pittsburgh, Atlanta, and Charleston. It was inaugurated on Sunday, November 18, 1883, also called "The Day of Two Noons", when each railroad station clock was reset as standard-time noon was reached within each time zone.\nThe North American zones were named Intercolonial, Eastern, Central, Mountain, and Pacific. Within a year 85% of all cities with populations over 10,000 (about 200 cities) were using standard time. A notable exception was Detroit (located about halfway between the meridians of Eastern and Central time), which kept local time until 1900, then tried Central Standard Time, local mean time, and Eastern Standard Time (EST) before a May 1915 ordinance settled on EST and was ratified by popular vote in August 1916. The confusion of times came to an end when standard time zones were formally adopted by the U.S. Congress in the Standard Time Act of March 19, 1918.\n=== Worldwide time zones ===\nItalian mathematician Quirico Filopanti introduced the idea of a worldwide system of time zones in his book Miranda!, published in 1858. He proposed 24 hourly time zones, which he called "longitudinal days", the first centred on the meridian of Rome. He also proposed a universal time to be used in astronomy and telegraphy. However, his book attracted no attention until long after his death.\nScottish-born Canadian Sir Sandford Fleming proposed a worldwide system of time zones in 1876 - see Sandford Fleming Â§ Inventor of worldwide standard time. The proposal divided the world into twenty-four time zones labeled A-Y (skipping J), each one covering 15 degrees of longitude. All clocks within each zone would be set to the same time as the others, but differed by one hour from those in the neighboring zones. He advocated his system at several international conferences, including the International Meridian Conference, where it received some consideration. The system has not been directly adopted, but some maps divide the world into 24 time zones and assign letters to them, similarly to Fleming\'s system.\nBy about 1900, almost all inhabited places on Earth had adopted a standard time zone, but only some of them used an hourly offset from GMT. Many applied the time at a local astronomical observatory to an entire country, without any reference to GMT. It took many decades before all time zones were based on some standard offset from GMT or Coordinated Universal Time (UTC). By 1929, the majority of countries had adopted hourly time zones, though some countries such as Iran, India, Myanmar and parts of Australia had time zones with a 30-minute offset. Nepal was the last country to adopt a standard offset, shifting slightly to UTC+05:45 in 1986.\nAll nations currently use standard time zones for secular purposes, but not all of them apply the concept as originally conceived. Several countries and subdivisions use half-hour or quarter-hour deviations from standard time. Some countries, such as China and India, use a single time zone even though the extent of their territory far exceeds the ideal 15Â° of longitude for one hour; other countries, such as Spain and Argentina, use standard hour-based offsets, but not necessarily those that would be determined by their geographical location. The consequences, in some areas, can affect the lives of local citizens, and in extreme cases contribute to larger political issues, such as in the western reaches of China. In Russia, which has 11 time zones, two time zones were removed in 2010 and reinstated in 2014.\n== Notation ==\n=== ISO 8601 ===\nISO 8601 is a standard established by the International Organization for Standardization defining methods of representing dates and times in textual form, including specifications for representing time zones.\nIf a time is in Coordinated Universal Time (UTC), a "Z" is added directly after the time without a separating space. "Z" is the zone designator for the zero UTC offset. "09:30 UTC" is therefore represented as "09:30Z" or "0930Z". Likewise, "14:45:15 UTC" is written as "14:45:15Z" or "144515Z". UTC time is also known as "Zulu" time, since "Zulu" is a phonetic alphabet code word for the letter "Z".\nOffsets from UTC are written in the format Â±hh:mm, Â±hhmm, or Â±hh (either hours ahead or behind UTC). For example, if the time being described is one hour ahead of UTC (such as the time in Germany during the winter), the zone designator would be "+01:00", "+0100", or simply "+01". This numeric representation of time zones is appended to local times in the same way that alphabetic time zone abbreviations (or "Z", as above) are appended. The offset from UTC changes with daylight saving time, e.g. a time offset in Chicago, which is in the North American Central Time Zone, is "â06:00" for the winter (Central Standard Time) and "â05:00" for the summer (Central Daylight Time).\n=== Abbreviations ===\nTime zones are often represented by alphabetic abbreviations such as "EST", "WST", and "CST", but these are not part of the international time and date standard ISO 8601. Such designations can be ambiguous; for example, "CST" can mean (North American) Central Standard Time (UTCâ06:00), Cuba Standard Time (UTCâ05:00) and China Standard Time (UTC+08:00), and it is also a widely used variant of ACST (Australian Central Standard Time, UTC+09:30).']

Question: What is the Peierls bracket in canonical quantization?

Choices:
Choice A) The Peierls bracket is a mathematical symbol used to represent the Poisson algebra in the canonical quantization method.
Choice B) The Peierls bracket is a mathematical tool used to generate the Hamiltonian in the canonical quantization method.
Choice C) The Peierls bracket is a Poisson bracket derived from the action in the canonical quantization method that converts the quotient algebra into a Poisson algebra.
Choice D) The Peierls bracket is a mathematical symbol used to represent the quotient algebra in the canonical quantization method.
Choice E) The Peierls bracket is a mathematical tool used to generate the Euler-Lagrange equations in the canonical quantization method.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Coordinated Universal Time', "In the graph of DUT1 above, the excess of LOD above the nominal 86,400 s corresponds to the downward slope of the graph between vertical segments. (The slope became shallower in the 1980s, 2000s and late 2010s to 2020s because of slight accelerations of Earth's rotation temporarily shortening the day.) Vertical position on the graph corresponds to the accumulation of this difference over time, and the vertical segments correspond to leap seconds introduced to match this accumulated difference. Leap seconds are timed to keep DUT1 within the vertical range depicted by the adjacent graph. The frequency of leap seconds therefore corresponds to the slope of the diagonal graph segments, and thus to the excess LOD. Time periods when the slope reverses direction (slopes upwards, not the vertical segments) are times when the excess LOD is negative, that is, when the LOD is below 86,400 s.\n== Future ==\nAs the Earth's rotation continues to slow, positive leap seconds will be required more frequently. The long-term rate of change of LOD is approximately +1.7 ms per century. At the end of the 21st century, LOD will be roughly 86,400.004 s, requiring leap seconds every 250 days. Over several centuries, the frequency of leap seconds will become problematic. A change in the trend of the UT1 â UTC values was seen beginning around June 2019 in which instead of slowing down (with leap seconds to keep the difference between UT1 and UTC less than 0.9 seconds) the Earth's rotation has sped up, causing this difference to increase. If the trend continues, a negative leap second may be required, which has not been used before. This may not be needed until 2025.\nSome time in the 22nd century, two leap seconds will be required every year. The current practice of only allowing leap seconds in June and December will be insufficient to maintain a difference of less than 1 second, and it might be decided to introduce leap seconds in March and September. In the 25th century, four leap seconds are projected to be required every year, so the current quarterly options would be insufficient.\nIn April 2001, Rob Seaman of the National Optical Astronomy Observatory proposed that leap seconds be allowed to be added monthly rather than twice yearly.\nIn 2022 a resolution was adopted by the General Conference on Weights and Measures to redefine UTC and abolish leap seconds, but keep the civil second constant and equal to the SI second, so that sundials would slowly get further and further out of sync with civil time. The leap seconds will be eliminated by 2035. The resolution does not break the connection between UTC and UT1, but increases the maximum allowable difference. The details of what the maximum difference will be and how corrections will be implemented is left for future discussions. This will result in a shift of the sun's movements relative to civil time, with the difference increasing quadratically with time (i.e., proportional to elapsed centuries squared). This is analogous to the shift of seasons relative to the yearly calendar that results from the calendar year not precisely matching the tropical year length. This would be a change in civil timekeeping, and would have a slow effect at first, but becoming drastic over several centuries. UTC (and TAI) would be more and more ahead of UT; it would coincide with local mean time along a meridian drifting eastward faster and faster. Thus, the time system will lose its fixed connection to the geographic coordinates based on the IERS meridian. The difference between UTC and UT would reach 0.5 hours after the year 2600 and 6.5 hours around 4600.\nITU-R Study Group 7 and Working Party 7A were unable to reach consensus on whether to advance the proposal to the 2012 Radiocommunications Assembly; the chairman of Study Group 7 elected to advance the question to the 2012 Radiocommunications Assembly (20 January 2012), but consideration of the proposal was postponed by the ITU until the World Radio Conference in 2015. This conference, in turn, considered the question, but no permanent decision was reached; it only chose to engage in further study with the goal of reconsideration in 2023.\nA proposed alternative to the leap second is the leap hour or leap minute, which requires changes only once every few centuries.\nITU World Radiocommunication Conference 2023 (WRC-23), which was held in Dubai (United Arab Emirates) from 20 November to 15 December 2023 formally recognised the Resolution 4 of the 27th CGPM (2022) which decides that the maximum value for the difference (UT1-UTC) will be increased in, or before, 2035.\n== See also ==\nCoordinated Lunar Time\nCoordinated Mars Time â Proposed approaches to tracking date and time on the planet MarsPages displaying short descriptions of redirect targets (MTC)\nEphemeris time â Time standard used in astronomical ephemerides\nIERS Reference Meridian â International prime meridian used for GPS and other systems\nISO 8601 â International standards for dates and times\nITU-R â One of the three sectors of the ITU\nList of UTC timing centers â Recognized maintainers of atomic clocks from which UTC is calculated\nTerrestrial Time â Time standard for astronomical observations from the Earth\nUniversal Time â Time standard based on the slowing rotation of the Earth\nWorld Radiocommunication Conference â ConventionPages displaying short descriptions with no spaces\n== References ==\n=== Notes ===\n=== Citations ===\n=== General and cited sources ===\n== External links ==\nCurrent UTC time\nDefinition of Coordinated Universal Time in German law â ZeitG Â§1 (3)\nInternational Earth Rotation Service; list of differences between TAI and UTC from 1961 to present\nW3C Specification about UTC Date and Time and RFC 3339, based on ISO 8601\nStandard of time definition: UTC, GPS, LORAN and TAI\nWhat is in a name? On the term Coordinated Universal Time at the Wayback Machine (archived 6 November 2013)", 'Time zone', 'Time zone', 'On electronic devices which only allow the time zone to be configured using maps or city names, UTC can be selected indirectly by selecting cities such as Accra in Ghana or ReykjavÃ­k in Iceland as they are always on UTC and do not currently use daylight saving time (which Greenwich and London do, and so could be a source of error).\n=== Daylight saving time ===\nUTC does not change with a change of seasons, but local time or civil time may change if a time zone jurisdiction observes daylight saving time (summer time). For example, local time on the east coast of the United States is five hours behind UTC during winter, but four hours behind while daylight saving is observed there.\n== History ==\nIn 1928, the term Universal Time (UT) was introduced by the International Astronomical Union to refer to GMT, with the day starting at midnight. Until the 1950s, broadcast time signals were based on UT, and hence on the rotation of the Earth.\nIn 1955, the caesium atomic clock was invented. This provided a form of timekeeping that was both more stable and more convenient than astronomical observations. In 1956, the U.S. National Bureau of Standards and U.S. Naval Observatory started to develop atomic frequency time scales; by 1959, these time scales were used in generating the WWV time signals, named for the shortwave radio station that broadcasts them. In 1960, the U.S. Naval Observatory, the Royal Greenwich Observatory, and the UK National Physical Laboratory coordinated their radio broadcasts so that time steps and frequency changes were coordinated, and the resulting time scale was informally referred to as "Coordinated Universal Time".\nIn a controversial decision, the frequency of the signals was initially set to match the rate of UT, but then kept at the same frequency by the use of atomic clocks and deliberately allowed to drift away from UT. When the divergence grew significantly, the signal was phase shifted (stepped) by 20 ms to bring it back into agreement with UT. Twenty-nine such steps were used before 1960.\nIn 1958, data was published linking the frequency for the caesium transition, newly established, with the ephemeris second. The ephemeris second is a unit in the system of time that, when used as the independent variable in the laws of motion that govern the movement of the planets and moons in the Solar System, enables the laws of motion to accurately predict the observed positions of Solar System bodies. Within the limits of observable accuracy, ephemeris seconds are of constant length, as are atomic seconds. This publication allowed a value to be chosen for the length of the atomic second that would accord with the celestial laws of motion.\nThe coordination of time and frequency transmissions around the world began on 1 January 1960. UTC was first officially adopted in 1963 as CCIR Recommendation 374, Standard-Frequency and Time-Signal Emissions, and "UTC" became the official abbreviation of Coordinated Universal Time in 1967.\nIn 1961, the Bureau International de l\'Heure began coordinating the UTC process internationally (but the name Coordinated Universal Time was not formally adopted by the International Astronomical Union until 1967). From then on, there were time steps every few months, and frequency changes at the end of each year. The jumps increased in size to 0.1 seconds. This UTC was intended to permit a very close approximation to UT2.\nIn 1967, the SI second was redefined in terms of the frequency supplied by a caesium atomic clock. The length of second so defined was practically equal to the second of ephemeris time. This was the frequency that had been provisionally used in TAI since 1958. It was soon decided that having two types of second with different lengths, namely the UTC second and the SI second used in TAI, was a bad idea. It was thought better for time signals to maintain a consistent frequency, and that this frequency should match the SI second. Thus it would be necessary to rely on time steps alone to maintain the approximation of UT. This was tried experimentally in a service known as "Stepped Atomic Time" (SAT), which ticked at the same rate as TAI and used jumps of 0.2 seconds to stay synchronised with UT2.\nThere was also dissatisfaction with the frequent jumps in UTC (and SAT). In 1968, Louis Essen, the inventor of the caesium atomic clock, and G. M. R. Winkler both independently proposed that steps should be of 1 second only. to simplify future adjustments. This system was eventually approved as leap seconds in a new UTC in 1970 and implemented in 1972, along with the idea of maintaining the UTC second equal to the TAI second. This CCIR Recommendation 460 "stated that (a) carrier frequencies and time intervals should be maintained constant and should correspond to the definition of the SI second; (b) step adjustments, when necessary, should be exactly 1 s to maintain approximate agreement with Universal Time (UT); and (c) standard signals should contain information on the difference between UTC and UT."\nAs an intermediate step at the end of 1971, there was a final irregular jump of exactly 0.107758 TAI seconds, making the total of all the small time steps and frequency shifts in UTC or TAI during 1958â1971 exactly ten seconds, so that 1 January 1972 00:00:00 UTC was 1 January 1972 00:00:10 TAI exactly, and a whole number of seconds thereafter. At the same time, the tick rate of UTC was changed to exactly match TAI. UTC also started to track UT1 rather than UT2. Some time signals started to broadcast the DUT1 correction (UT1 â UTC) for applications requiring a closer approximation of UT1 than UTC now provided.\nThe current version of UTC is defined by International Telecommunication Union Recommendation (ITU-R TF.460-6), Standard-frequency and time-signal emissions, and is based on International Atomic Time (TAI) with leap seconds added at irregular intervals to compensate for the accumulated difference between TAI and time measured by Earth\'s rotation. Leap seconds are inserted as necessary to keep UTC within 0.9 seconds of the UT1 variant of universal time. See the "Current number of leap seconds" section for the number of leap seconds inserted to date.\n=== Current number of leap seconds ===\nThe first leap second occurred on 30 June 1972. Since then, leap seconds have occurred on average about once every 19 months, always on 30 June or 31 December. As of July 2022, there have been 27 leap seconds in total, all positive, putting UTC 37 seconds behind TAI.\nA study published in March 2024 in Nature concluded that accelerated melting of ice in Greenland and Antarctica due to climate change has decreased Earth\'s rotational velocity, affecting UTC adjustments and causing problems for computer networks that rely on UTC.\n== Rationale ==\nEarth\'s rotational speed is very slowly decreasing because of tidal deceleration; this increases the length of the mean solar day. The length of the SI second was calibrated on the basis of the second of ephemeris time and can now be seen to have a relationship with the mean solar day observed between 1750 and 1892, analysed by Simon Newcomb. As a result, the SI second is close to \u20601/86400\u2060 of a mean solar day in the midâ19th century. In earlier centuries, the mean solar day was shorter than 86,400 SI seconds, and in more recent centuries it is longer than 86,400 seconds. Near the end of the 20th century, the length of the mean solar day (also known simply as "length of day" or "LOD") was approximately 86,400.0013 s. For this reason, UT is now "slower" than TAI by the difference (or "excess" LOD) of 1.3 ms/day.\nThe excess of the LOD over the nominal 86,400 s accumulates over time, causing the UTC day, initially synchronised with the mean sun, to become desynchronised and run ahead of it. Near the end of the 20th century, with the LOD at 1.3 ms above the nominal value, UTC ran faster than UT by 1.3 ms per day, getting a second ahead roughly every 800 days. Thus, leap seconds were inserted at approximately this interval, retarding UTC to keep it synchronised in the long term. The actual rotational period varies on unpredictable factors such as tectonic motion and has to be observed, rather than computed.\nJust as adding a leap day every four years does not mean the year is getting longer by one day every four years, the insertion of a leap second every 800 days does not indicate that the mean solar day is getting longer by a second every 800 days. It will take about 50,000 years for a mean solar day to lengthen by one second (at a rate of 2 ms per century). This rate fluctuates within the range of 1.7â2.3 ms/cy. While the rate due to tidal friction alone is about 2.3 ms/cy, the uplift of Canada and Scandinavia by several metres since the last ice age has temporarily reduced this to 1.7 ms/cy over the last 2,700 years. The correct reason for leap seconds, then, is not the current difference between actual and nominal LOD, but rather the accumulation of this difference over a period of time: Near the end of the 20th century, this difference was about \u20601/800\u2060 of a second per day; therefore, after about 800 days, it accumulated to 1 second (and a leap second was then added).', '==== Microsoft Windows ====\nWindows-based computer systems prior to Windows 95 and Windows NT used local time, but Windows 95 and later, and Windows NT, base system time on UTC. They allow a program to fetch the system time as UTC, represented as a year, month, day, hour, minute, second, and millisecond; Windows 95 and later, and Windows NT 3.5 and later, also allow the system time to be fetched as a count of 100 ns units since 1601-01-01 00:00:00 UTC. The system registry contains time zone information that includes the offset from UTC and rules that indicate the start and end dates for daylight saving in each zone. Interaction with the user normally uses local time, and application software is able to calculate the time in various zones. Terminal Servers allow remote computers to redirect their time zone settings to the Terminal Server so that users see the correct time for their time zone in their desktop/application sessions. Terminal Services uses the server base time on the Terminal Server and the client time zone information to calculate the time in the session.\n=== Programming languages ===\n==== Java ====\nWhile most application software will use the underlying operating system for time zone and daylight saving time rule information, the Java Platform, from version 1.3.1, has maintained its own database of time zone and daylight saving time rule information. This database is updated whenever time zone or daylight saving time rules change. Oracle provides an updater tool for this purpose.\nAs an alternative to the information bundled with the Java Platform, programmers may choose to use the Joda-Time library. This library includes its own data based on the IANA time zone database.\nAs of Java 8 there is a new date and time API that can help with converting times.\n==== JavaScript ====\nTraditionally, there was very little in the way of time zone support for JavaScript. Essentially the programmer had to extract the UTC offset by instantiating a time object, getting a GMT time from it, and differencing the two. This does not provide a solution for more complex daylight saving variations, such as divergent DST directions between northern and southern hemispheres.\nECMA-402, the standard on Internationalization API for JavaScript, provides ways of formatting Time Zones. However, due to size constraint, some implementations or distributions do not include it.\n==== Perl ====\nThe DateTime object in Perl supports all entries in the IANA time zone database and includes the ability to get, set and convert between time zones.\n==== PHP ====\nThe DateTime objects and related functions have been compiled into the PHP core since 5.2. This includes the ability to get and set the default script time zone, and DateTime is aware of its own time zone internally. PHP.net provides extensive documentation on this. As noted there, the most current time zone database can be implemented via the PECL timezonedb.\n==== Python ====\nThe standard module datetime included with Python stores and operates on the time zone information class tzinfo. The third party pytz module provides access to the full IANA time zone database. Negated time zone offset in seconds is stored time.timezone and time.altzone attributes. From Python 3.9, the zoneinfo module introduces timezone management without need for third party module.\n==== Smalltalk ====\nEach Smalltalk dialect comes with its own built-in classes for dates, times and timestamps, only a few of which implement the DateAndTime and Duration classes as specified by the ANSI Smalltalk Standard. VisualWorks provides a TimeZone class that supports up to two annually recurring offset transitions, which are assumed to apply to all years (same behavior as Windows time zones). Squeak provides a Timezone class that does not support any offset transitions. Dolphin Smalltalk does not support time zones at all.\nFor full support of the tz database (zoneinfo) in a Smalltalk application (including support for any number of annually recurring offset transitions, and support for different intra-year offset transition rules in different years) the third-party, open-source, ANSI-Smalltalk-compliant Chronos Date/Time Library is available for use with any of the following Smalltalk dialects: VisualWorks, Squeak, Gemstone, or Dolphin.\n== Time in outer space ==\nOrbiting spacecraft may experience many sunrises and sunsets, or none, in a 24-hour period. Therefore, it is not possible to calibrate the time with respect to the Sun and still respect a 24-hour sleep/wake cycle. A common practice for space exploration is to use the Earth-based time of the launch site or mission control, synchronizing the sleeping cycles of the crew and controllers. The International Space Station normally uses Greenwich Mean Time (GMT).\nTimekeeping on Mars can be more complex, since the planet has a solar day of approximately 24 hours and 40 minutes, known as a sol. Earth controllers for some Mars missions have synchronized their sleep/wake cycles with the Martian day, when specifically solar-powered rover activity occurs.\n== See also ==\nJet lag\nLists of time zones\nMetric time\nTime by country\nTime in Europe\nAbolition of time zones â Replacing time zones with UTC\nWorld clock â Clock that displays the times in various locations around the globe\nInternational Date Line â Imaginary line that demarcates the change of one calendar day to the next\n== Notes ==\n== References ==\n== Sources ==\nAsimov, Isaac (1964). "Abbe, Cleveland". Asimov\'s Biographical Encyclopedia of Science and Technology: The Living Stories of More than 1000 Great Scientists from the Age of Greece to the Space Age. Garden City, NY: Doubleday & Company, Inc. pp. 343â344. LCCN 64016199.\nDebus, Allen G., ed. (1968). "Abbe, Cleveland". World Who\'s Who in Science: A Biographical Dictionary of Notable Scientists from Antiquity to the Present (1st ed.). Chicago, IL: A. N. Marquis Company. ISBN 0-8379-1001-3. LCCN 68056149.\n== Further reading ==\nBiswas, Soutik (February 12, 2019). "How India\'s single time zone is hurting its people". BBC News. Retrieved February 12, 2019.\nMaulik Jagnani, economist at Cornell University (January 15, 2019). "Poor Sleep: Sunset Time and Human Capital Production" (Job Market Paper). Retrieved April 28, 2025.\n"Time Bandits: The countries rebelling against GMT" (Video). BBC News. August 14, 2015. Retrieved February 12, 2019.\n"How time zones confused the world". BBC News. August 7, 2015. Retrieved February 12, 2019.\nLane, Megan (May 10, 2011). "How does a country change its time zone?". BBC News. Retrieved February 12, 2019.\n"A brief history of time zones" (Video). BBC News. March 24, 2011. Retrieved February 12, 2019.\nThe Time Zone Information Format (TZif). doi:10.17487/RFC8536. RFC 8536.\n== External links ==\nMedia related to Time zones at Wikimedia Commons', '==== Microsoft Windows ====\nWindows-based computer systems prior to Windows 95 and Windows NT used local time, but Windows 95 and later, and Windows NT, base system time on UTC. They allow a program to fetch the system time as UTC, represented as a year, month, day, hour, minute, second, and millisecond; Windows 95 and later, and Windows NT 3.5 and later, also allow the system time to be fetched as a count of 100 ns units since 1601-01-01 00:00:00 UTC. The system registry contains time zone information that includes the offset from UTC and rules that indicate the start and end dates for daylight saving in each zone. Interaction with the user normally uses local time, and application software is able to calculate the time in various zones. Terminal Servers allow remote computers to redirect their time zone settings to the Terminal Server so that users see the correct time for their time zone in their desktop/application sessions. Terminal Services uses the server base time on the Terminal Server and the client time zone information to calculate the time in the session.\n=== Programming languages ===\n==== Java ====\nWhile most application software will use the underlying operating system for time zone and daylight saving time rule information, the Java Platform, from version 1.3.1, has maintained its own database of time zone and daylight saving time rule information. This database is updated whenever time zone or daylight saving time rules change. Oracle provides an updater tool for this purpose.\nAs an alternative to the information bundled with the Java Platform, programmers may choose to use the Joda-Time library. This library includes its own data based on the IANA time zone database.\nAs of Java 8 there is a new date and time API that can help with converting times.\n==== JavaScript ====\nTraditionally, there was very little in the way of time zone support for JavaScript. Essentially the programmer had to extract the UTC offset by instantiating a time object, getting a GMT time from it, and differencing the two. This does not provide a solution for more complex daylight saving variations, such as divergent DST directions between northern and southern hemispheres.\nECMA-402, the standard on Internationalization API for JavaScript, provides ways of formatting Time Zones. However, due to size constraint, some implementations or distributions do not include it.\n==== Perl ====\nThe DateTime object in Perl supports all entries in the IANA time zone database and includes the ability to get, set and convert between time zones.\n==== PHP ====\nThe DateTime objects and related functions have been compiled into the PHP core since 5.2. This includes the ability to get and set the default script time zone, and DateTime is aware of its own time zone internally. PHP.net provides extensive documentation on this. As noted there, the most current time zone database can be implemented via the PECL timezonedb.\n==== Python ====\nThe standard module datetime included with Python stores and operates on the time zone information class tzinfo. The third party pytz module provides access to the full IANA time zone database. Negated time zone offset in seconds is stored time.timezone and time.altzone attributes. From Python 3.9, the zoneinfo module introduces timezone management without need for third party module.\n==== Smalltalk ====\nEach Smalltalk dialect comes with its own built-in classes for dates, times and timestamps, only a few of which implement the DateAndTime and Duration classes as specified by the ANSI Smalltalk Standard. VisualWorks provides a TimeZone class that supports up to two annually recurring offset transitions, which are assumed to apply to all years (same behavior as Windows time zones). Squeak provides a Timezone class that does not support any offset transitions. Dolphin Smalltalk does not support time zones at all.\nFor full support of the tz database (zoneinfo) in a Smalltalk application (including support for any number of annually recurring offset transitions, and support for different intra-year offset transition rules in different years) the third-party, open-source, ANSI-Smalltalk-compliant Chronos Date/Time Library is available for use with any of the following Smalltalk dialects: VisualWorks, Squeak, Gemstone, or Dolphin.\n== Time in outer space ==\nOrbiting spacecraft may experience many sunrises and sunsets, or none, in a 24-hour period. Therefore, it is not possible to calibrate the time with respect to the Sun and still respect a 24-hour sleep/wake cycle. A common practice for space exploration is to use the Earth-based time of the launch site or mission control, synchronizing the sleeping cycles of the crew and controllers. The International Space Station normally uses Greenwich Mean Time (GMT).\nTimekeeping on Mars can be more complex, since the planet has a solar day of approximately 24 hours and 40 minutes, known as a sol. Earth controllers for some Mars missions have synchronized their sleep/wake cycles with the Martian day, when specifically solar-powered rover activity occurs.\n== See also ==\nJet lag\nLists of time zones\nMetric time\nTime by country\nTime in Europe\nAbolition of time zones â Replacing time zones with UTC\nWorld clock â Clock that displays the times in various locations around the globe\nInternational Date Line â Imaginary line that demarcates the change of one calendar day to the next\n== Notes ==\n== References ==\n== Sources ==\nAsimov, Isaac (1964). "Abbe, Cleveland". Asimov\'s Biographical Encyclopedia of Science and Technology: The Living Stories of More than 1000 Great Scientists from the Age of Greece to the Space Age. Garden City, NY: Doubleday & Company, Inc. pp. 343â344. LCCN 64016199.\nDebus, Allen G., ed. (1968). "Abbe, Cleveland". World Who\'s Who in Science: A Biographical Dictionary of Notable Scientists from Antiquity to the Present (1st ed.). Chicago, IL: A. N. Marquis Company. ISBN 0-8379-1001-3. LCCN 68056149.\n== Further reading ==\nBiswas, Soutik (February 12, 2019). "How India\'s single time zone is hurting its people". BBC News. Retrieved February 12, 2019.\nMaulik Jagnani, economist at Cornell University (January 15, 2019). "Poor Sleep: Sunset Time and Human Capital Production" (Job Market Paper). Retrieved April 28, 2025.\n"Time Bandits: The countries rebelling against GMT" (Video). BBC News. August 14, 2015. Retrieved February 12, 2019.\n"How time zones confused the world". BBC News. August 7, 2015. Retrieved February 12, 2019.\nLane, Megan (May 10, 2011). "How does a country change its time zone?". BBC News. Retrieved February 12, 2019.\n"A brief history of time zones" (Video). BBC News. March 24, 2011. Retrieved February 12, 2019.\nThe Time Zone Information Format (TZif). doi:10.17487/RFC8536. RFC 8536.\n== External links ==\nMedia related to Time zones at Wikimedia Commons', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', 'Jordy, W. H. (1952). Henry Adams: Scientific Historian. New Haven. ISBN 978-0-685-26683-0. {{cite book}}: ISBN / Date incompatibility (help)\nKhan, Salman. "Maxwell\'s Demon". Archived from the original on 2010-03-17.\nMaroney, O. J. E. (2009) ""Information Processing and Thermodynamic Entropy" The Stanford Encyclopedia of Philosophy (Autumn 2009 Edition)\nMaxwell, J. C. (1871). Theory of Heat. London, New York [etc.] Longmans, Green., reprinted (2001) New York: Dover, ISBN 0-486-41735-2\nNorton, J. (2005). "Eaters of the lotus: Landauer\'s principle and the return of Maxwell\'s demon" (PDF). Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics. 36 (2): 375â411. Bibcode:2005SHPMP..36..375N. CiteSeerX 10.1.1.468.3017. doi:10.1016/j.shpsb.2004.12.002. S2CID 21104635. Archived (PDF) from the original on 2006-09-01.\nRaizen, Mark G. (2011) "Demons, Entropy, and the Quest for Absolute Zero", Scientific American, March, pp54-59\nReaney, Patricia. "Scientists build nanomachine", Reuters, February 1, 2007\nRubi, J Miguel, "Does Nature Break the Second Law of Thermodynamics?"; Scientific American, October 2008 :\nSplasho (2008) â Historical development of Maxwell\'s demon\nWeiss, Peter. "Breaking the Law â Can quantum mechanics + thermodynamics = perpetual motion?", Science News, October 7, 2000', 'A time zone is an area which observes a uniform standard time for legal, commercial and social purposes. Time zones tend to follow the boundaries between countries and their subdivisions instead of strictly following longitude, because it is convenient for areas in frequent communication to keep the same time.\nEach time zone is defined by a standard offset from Coordinated Universal Time (UTC). The offsets range from UTCâ12:00 to UTC+14:00, and are usually a whole number of hours, but a few zones are offset by an additional 30 or 45 minutes, such as in India and Nepal. Some areas in a time zone may use a different offset for part of the year, typically one hour ahead during spring and summer, a practice known as daylight saving time (DST).\n== List of UTC offsets ==\nIn the table below, the locations that use daylight saving time (DST) are listed in their UTC offset when DST is not in effect. When DST is in effect, approximately during spring and summer, their UTC offset is increased by one hour (except for Lord Howe Island, where it is increased by 30 minutes). For example, during the DST period California observes UTCâ07:00 and the United Kingdom observes UTC+01:00.\n== History ==\nThe apparent position of the Sun in the sky, and thus solar time, varies by location due to the spherical shape of the Earth. This variation corresponds to four minutes of time for every degree of longitude, so for example when it is solar noon in London, it is about 10 minutes before solar noon in Bristol, which is about 2.5 degrees to the west.\nThe Royal Observatory, Greenwich, founded in 1675, established Greenwich Mean Time (GMT), the mean solar time at that location, as an aid to mariners to determine longitude at sea, providing a standard reference time while each location in England kept a different time.\n=== Railway time ===\nIn the 19th century, as transportation and telecommunications improved, it became increasingly inconvenient for each location to observe its own solar time. In November 1840, the British Great Western Railway started using GMT kept by portable chronometers. This practice was soon followed by other railway companies in Great Britain and became known as railway time.\nAround August 23, 1852, time signals were first transmitted by telegraph from the Royal Observatory. By 1855, 98% of Great Britain\'s public clocks were using GMT, but it was not made the island\'s legal time until August 2, 1880. Some British clocks from this period have two minute hands, one for the local time and one for GMT.\nOn November 2, 1868, the British Colony of New Zealand officially adopted a standard time to be observed throughout the colony. It was based on longitude 172Â°30â² east of Greenwich, that is 11 hours 30 minutes ahead of GMT. This standard was known as New Zealand Mean Time.\nTimekeeping on North American railroads in the 19th century was complex. Each railroad used its own standard time, usually based on the local time of its headquarters or most important terminus, and the railroad\'s train schedules were published using its own time. Some junctions served by several railroads had a clock for each railroad, each showing a different time.  Because of this a number of accidents occurred when trains from different companies using the same tracks mistimed their passings.\nAround 1863, Charles F. Dowd proposed a system of hourly standard time zones for North American railroads, although he published nothing on the matter at that time and did not consult railroad officials until 1869. In 1870 he proposed four ideal time zones having northâsouth borders, the first centered on Washington, D.C., but by 1872 the first was centered on meridian 75Â° west of Greenwich, with natural borders such as sections of the Appalachian Mountains. Dowd\'s system was never accepted by North American railroads.\nChief meteorologist at the United States Weather Bureau Cleveland Abbe divided the United States into four standard time zones for consistency among the weather stations. In 1879, he published a paper titled Report on Standard Time. In 1883, he convinced North American railroad companies to adopt his time-zone system. In 1884, Britain, which had already adopted its own standard time system for England, Scotland, and Wales, helped gather international consent for global time. In time, the American government, influenced in part by Abbe\'s 1879 paper, adopted the time-zone system.\nIt was a version proposed by William F. Allen, the editor of the Traveler\'s Official Railway Guide. The borders of its time zones ran through railroad stations, often in major cities. For example, the border between its Eastern and Central time zones ran through Detroit, Buffalo, Pittsburgh, Atlanta, and Charleston. It was inaugurated on Sunday, November 18, 1883, also called "The Day of Two Noons", when each railroad station clock was reset as standard-time noon was reached within each time zone.\nThe North American zones were named Intercolonial, Eastern, Central, Mountain, and Pacific. Within a year 85% of all cities with populations over 10,000 (about 200 cities) were using standard time. A notable exception was Detroit (located about halfway between the meridians of Eastern and Central time), which kept local time until 1900, then tried Central Standard Time, local mean time, and Eastern Standard Time (EST) before a May 1915 ordinance settled on EST and was ratified by popular vote in August 1916. The confusion of times came to an end when standard time zones were formally adopted by the U.S. Congress in the Standard Time Act of March 19, 1918.\n=== Worldwide time zones ===\nItalian mathematician Quirico Filopanti introduced the idea of a worldwide system of time zones in his book Miranda!, published in 1858. He proposed 24 hourly time zones, which he called "longitudinal days", the first centred on the meridian of Rome. He also proposed a universal time to be used in astronomy and telegraphy. However, his book attracted no attention until long after his death.\nScottish-born Canadian Sir Sandford Fleming proposed a worldwide system of time zones in 1876 - see Sandford Fleming Â§ Inventor of worldwide standard time. The proposal divided the world into twenty-four time zones labeled A-Y (skipping J), each one covering 15 degrees of longitude. All clocks within each zone would be set to the same time as the others, but differed by one hour from those in the neighboring zones. He advocated his system at several international conferences, including the International Meridian Conference, where it received some consideration. The system has not been directly adopted, but some maps divide the world into 24 time zones and assign letters to them, similarly to Fleming\'s system.\nBy about 1900, almost all inhabited places on Earth had adopted a standard time zone, but only some of them used an hourly offset from GMT. Many applied the time at a local astronomical observatory to an entire country, without any reference to GMT. It took many decades before all time zones were based on some standard offset from GMT or Coordinated Universal Time (UTC). By 1929, the majority of countries had adopted hourly time zones, though some countries such as Iran, India, Myanmar and parts of Australia had time zones with a 30-minute offset. Nepal was the last country to adopt a standard offset, shifting slightly to UTC+05:45 in 1986.\nAll nations currently use standard time zones for secular purposes, but not all of them apply the concept as originally conceived. Several countries and subdivisions use half-hour or quarter-hour deviations from standard time. Some countries, such as China and India, use a single time zone even though the extent of their territory far exceeds the ideal 15Â° of longitude for one hour; other countries, such as Spain and Argentina, use standard hour-based offsets, but not necessarily those that would be determined by their geographical location. The consequences, in some areas, can affect the lives of local citizens, and in extreme cases contribute to larger political issues, such as in the western reaches of China. In Russia, which has 11 time zones, two time zones were removed in 2010 and reinstated in 2014.\n== Notation ==\n=== ISO 8601 ===\nISO 8601 is a standard established by the International Organization for Standardization defining methods of representing dates and times in textual form, including specifications for representing time zones.\nIf a time is in Coordinated Universal Time (UTC), a "Z" is added directly after the time without a separating space. "Z" is the zone designator for the zero UTC offset. "09:30 UTC" is therefore represented as "09:30Z" or "0930Z". Likewise, "14:45:15 UTC" is written as "14:45:15Z" or "144515Z". UTC time is also known as "Zulu" time, since "Zulu" is a phonetic alphabet code word for the letter "Z".\nOffsets from UTC are written in the format Â±hh:mm, Â±hhmm, or Â±hh (either hours ahead or behind UTC). For example, if the time being described is one hour ahead of UTC (such as the time in Germany during the winter), the zone designator would be "+01:00", "+0100", or simply "+01". This numeric representation of time zones is appended to local times in the same way that alphabetic time zone abbreviations (or "Z", as above) are appended. The offset from UTC changes with daylight saving time, e.g. a time offset in Chicago, which is in the North American Central Time Zone, is "â06:00" for the winter (Central Standard Time) and "â05:00" for the summer (Central Daylight Time).\n=== Abbreviations ===\nTime zones are often represented by alphabetic abbreviations such as "EST", "WST", and "CST", but these are not part of the international time and date standard ISO 8601. Such designations can be ambiguous; for example, "CST" can mean (North American) Central Standard Time (UTCâ06:00), Cuba Standard Time (UTCâ05:00) and China Standard Time (UTC+08:00), and it is also a widely used variant of ACST (Australian Central Standard Time, UTC+09:30).']

Question: What is the relationship between Coordinated Universal Time (UTC) and Universal Time (UT1)?

Choices:
Choice A) UTC and Universal Time (UT1) are identical time scales that are used interchangeably in science and engineering.
Choice B) UTC is a time scale that is completely independent of Universal Time (UT1). UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the "leap second".
Choice C) UTC is an atomic time scale designed to approximate Universal Time (UT1), but it differs from UT1 by a non-integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the "leap second".
Choice D) UTC is an atomic time scale designed to approximate Universal Time (UT1), but it differs from UT1 by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the "leap second".
Choice E) UTC is a time scale that is based on the irregularities in Earth's rotation and is completely independent of Universal Time (UT1).

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ["Maxwell's demon", 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by LeÃ³ SzilÃ¡rd, and later by LÃ©on Brillouin.  SzilÃ¡rd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidtâs paradox.\nJohn Earman and John D. Norton have argued that SzilÃ¡rd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Jordy, W. H. (1952). Henry Adams: Scientific Historian. New Haven. ISBN 978-0-685-26683-0. {{cite book}}: ISBN / Date incompatibility (help)\nKhan, Salman. "Maxwell\'s Demon". Archived from the original on 2010-03-17.\nMaroney, O. J. E. (2009) ""Information Processing and Thermodynamic Entropy" The Stanford Encyclopedia of Philosophy (Autumn 2009 Edition)\nMaxwell, J. C. (1871). Theory of Heat. London, New York [etc.] Longmans, Green., reprinted (2001) New York: Dover, ISBN 0-486-41735-2\nNorton, J. (2005). "Eaters of the lotus: Landauer\'s principle and the return of Maxwell\'s demon" (PDF). Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics. 36 (2): 375â411. Bibcode:2005SHPMP..36..375N. CiteSeerX 10.1.1.468.3017. doi:10.1016/j.shpsb.2004.12.002. S2CID 21104635. Archived (PDF) from the original on 2006-09-01.\nRaizen, Mark G. (2011) "Demons, Entropy, and the Quest for Absolute Zero", Scientific American, March, pp54-59\nReaney, Patricia. "Scientists build nanomachine", Reuters, February 1, 2007\nRubi, J Miguel, "Does Nature Break the Second Law of Thermodynamics?"; Scientific American, October 2008 :\nSplasho (2008) â Historical development of Maxwell\'s demon\nWeiss, Peter. "Breaking the Law â Can quantum mechanics + thermodynamics = perpetual motion?", Science News, October 7, 2000', 'Peierls bracket\n\nIn theoretical physics, the Peierls bracket is an equivalent description of the Poisson bracket. It can be defined directly from the action and does not require the canonical coordinates and their canonical momenta to be defined in advance.\nThe bracket\n{\\displaystyle [A,B]}\nis defined as\n{\\displaystyle D_{A}(B)-D_{B}(A)}\nas the difference between some kind of action of one quantity on the other, minus the flipped term.\nIn quantum mechanics, the Peierls bracket becomes a commutator i.e. a Lie bracket.\n== References ==\nThis article incorporates material from the Citizendium article "Peierls bracket", which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License but not under the GFDL.\nPeierls, R. "The Commutation Laws of Relativistic Field Theory,"\nProc. R. Soc. Lond. August 21, 1952 214 1117 143-157.', 'Coffee ring effect\n\nIn physics, a "coffee ring" is a pattern left by a puddle of particle-laden liquid after it evaporates. The phenomenon is named for the characteristic ring-like deposit along the perimeter of a spill of coffee. It is also commonly seen after spilling red wine. The mechanism behind the formation of these and similar rings is known as the coffee ring effect or in some instances, the coffee stain effect, or simply ring stain.\n== Flow mechanism ==\nThe coffee-ring pattern originates from the capillary flow induced by the evaporation of the drop: liquid evaporating from the edge is replenished by liquid from the interior. The resulting current can carry nearly all the dispersed material to the edge. As a function of time, this process exhibits a "rush-hour" effect, that is, a rapid acceleration of the flow towards the edge at the final stage of the drying process.\nEvaporation induces a Marangoni flow inside a droplet.  The flow, if strong, redistributes particles back to the center of the droplet.  Thus, for particles to accumulate at the edges, the liquid must have a weak Marangoni flow, or something must occur to disrupt the flow.  For example, surfactants can be added to reduce the liquid\'s surface tension gradient, disrupting the induced flow.  Water has a weak Marangoni flow to begin with, which is then reduced significantly by natural surfactants.\nInteraction of the particles suspended in a droplet with the free surface of the droplet is important in creating a coffee ring. "When the drop evaporates, the free surface collapses and traps the suspended particles ... eventually all the particles are captured by the free surface and stay there for the rest of their trip towards the edge of the drop." This result means that surfactants can be used to manipulate the motion of the solute particles by changing the surface tension of the drop, rather than trying to control the bulk flow inside the drop. A number of unique morphologies of the deposited particles can result.  For example, an enantiopure poly (isocyanate) derivative has been shown to form ordered arrays of squashed donut structures.\n== Suppression ==\nThe coffee-ring pattern is detrimental when uniform application of a dried deposit is required, such as in printed electronics. It can be suppressed by adding elongated particles, such as cellulose fibers, to the spherical particles that cause the coffee-ring effect. The size and weight fraction of added particles may be smaller than those of the primary ones.\nIt is also reported that controlling flow inside a droplet is a powerful way to generate a uniform film; for example, by harnessing solutal Marangoni flows occurring during evaporation.\nMixtures of low boiling point and high boiling point solvents were shown to suppress the coffee ring effect, changing the shape of a deposited solute from a ring-like to a dot-like shape.\nControl of the substrate temperature was shown to be an effective way to suppress the coffee ring formed by droplets of water-based PEDOT:PSS solution. On a heated hydrophilic or hydrophobic substrate, a thinner ring with an inner deposit forms, which is attributed to Marangoni convection.\nControl of the substrate wetting properties on slippery surfaces can prevent the pinning of the drop contact line, which will, therefore, suppress the coffee ring effect by reducing the number of particles deposited at the contact line. Drops on superhydrophobic or liquid impregnated surfaces are less likely to have a pinned contact line and will suppress ring formation. Drops with an oil ring formed at the drop contact line have high mobility and can avoid the ring formation on hydrophobic surfaces.\nAlternating voltage electrowetting may suppress coffee stains without the need to add surface-active materials. Reverse particle motion may also reduce the coffee-ring effect because of the capillary force near the contact line. The reversal takes place when the capillary force prevails over the outward coffee-ring flow by the geometric constraints.\n== Determinants of size and pattern ==\nThe lower-limit size of a coffee ring depends on the time scale competition between the liquid evaporation and the movement of suspended particles.  When the liquid evaporates much faster than the particle movement near a three-phase contact line, a coffee ring cannot be formed successfully.  Instead, these particles will disperse uniformly on a surface upon complete liquid evaporation.  For suspended particles of size 100 nm, the minimum diameter of the coffee ring structure is found to be 10 Î¼m, or about 10 times smaller than the width of human hair.  The shape of particles in the liquid is responsible for coffee ring effect. On porous substrates, the competition among infiltration, particle motion and evaporation of the solvent governs the final deposition morphology.\nThe pH of the solution of the drop influences the final deposit pattern. The transition between these patterns is explained by considering how DLVO interactions such as the electrostatic and Van der Waals forces modify the particle deposition process.\n== Applications ==\nThe coffee ring effect is utilized in convective deposition by researchers wanting to order particles on a substrate using capillary-driven assembly, replacing a stationary droplet with an advancing meniscus drawn across the substrate.  This process differs from dip-coating in that evaporation drives flow along the substrate as opposed to gravity.\nConvective deposition can control particle orientation, resulting in the formation of crystalline monolayer films from nonspherical particles such as hemispherical, dimer, and dumbbell shaped particles. Orientation is afforded by the system trying to reach a state of maximum packing of the particles in the thin meniscus layer over which evaporation occurs. They showed that tuning the volume fraction of particles in solution will control the specific location along the varying meniscus thickness at which assembly occurs. Particles will align with their long axis in- or out-of-plane depending on whether or not their longer dimension of the particle was equal to the thickness of the wetting layer at the meniscus location. Such thickness transitions were established with spherical particles as well. It was later shown that convective assembly could control particle orientation in assembling multi-layers, resulting in long-range 3D colloidal crystals from dumbbell shaped particles. These finds were attractive for the self-assembled of colloidal crystal films for applications such as photonics. Recent advances have increased the application of coffee-ring assembly from colloidal particles to organized patterns of inorganic crystals.\n== References ==', 'Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word ÎºÏÏÏÏÎ±Î»Î»Î¿Ï (krÃºstallos; "clear ice, rock-crystal"), and Î³ÏÎ¬ÏÎµÎ¹Î½ (grÃ¡phein; "to write"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well new developments in these fields.\n== History and timeline ==\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the DavissonâGermer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n== Methodology ==\nCrystallographic methods depend mainly on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. Crystallographers often explicitly state the type of beam used, as in the terms X-ray diffraction, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.\nX-rays interact with the spatial distribution of electrons in the sample.\nNeutrons are scattered by the atomic nuclei through the strong nuclear forces, but in addition the magnetic moment of neutrons is non-zero, so they are also scattered by magnetic fields. When neutrons are scattered from hydrogen-containing materials, they produce diffraction patterns with high noise levels, which can sometimes be resolved by substituting deuterium for hydrogen.\nElectrons are charged particles and therefore interact with the total charge distribution of both the atomic nuclei and the electrons of the sample.:\u200aChpt 4\nIt is hard to focus x-rays or neutrons, but since electrons are charged they can be focused and are used in electron microscope to produce magnified images. There are many ways that transmission electron microscopy and related techniques such as scanning transmission electron microscopy, high-resolution electron microscopy can be used to obtain images with in many cases atomic resolution from which crystallographic information can be obtained. There are also other methods such as low-energy electron diffraction, low-energy electron microscopy and reflection high-energy electron diffraction which can be used to obtain crystallographic information about surfaces.\n== Applications in various areas ==\n=== Materials science ===\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Most materials do not occur as a single crystal, but are poly-crystalline in nature (they exist as an aggregate of small crystals with different orientations). As such, powder diffraction techniques, which take diffraction patterns of samples with a large number of crystals, play an important role in structural determination.\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements. Crystallographic studies help elucidate the relationship between a material\'s structure and its properties, aiding in developing new materials with tailored characteristics. This understanding is crucial in various fields, including metallurgy, geology, and materials science. Advancements in crystallographic techniques, such as electron diffraction and X-ray crystallography, continue to expand our understanding of material behavior at the atomic level.\nIn another example, iron transforms from a body-centered cubic (bcc) structure called ferrite to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material\'s properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which structures are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory.\n=== Biology ===\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. The double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol, Pymol or VMD can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron diffraction has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n== Notation ==\nCoordinates in square brackets such as [100] denote a direction vector (in real space).\nCoordinates in angle brackets or chevrons such as <100> denote a family of directions which are related by symmetry operations. In the cubic crystal system for example, <100> would mean [100], [010], [001] or the negative of any of those directions.\nMiller indices in parentheses such as (100) denote a plane of the crystal structure, and regular repetitions of that plane with a particular spacing. In the cubic system, the normal to the (hkl) plane is the direction [hkl], but in lower-symmetry cases, the normal to (hkl) is not parallel to [hkl].\nIndices in curly brackets or braces such as {100} denote a family of planes and their normals. In cubic materials the symmetry makes them equivalent, just as the way angle brackets denote a family of directions. In non-cubic materials, <hkl> is not necessarily perpendicular to {hkl}.\n== Reference literature ==\nThe International Tables for Crystallography is an eight-book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure through x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and do not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  Ä§Ïk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, Î¸kâ , Î¸k, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the Î¸k, and the Fock space is built by applying all products of creation operators Î¸kâ  to |0â©.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at Ï = 0, so that the vacuum minimizing the Hamiltonian satisfies â¨Ïâ© = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  Ï = v .  This occurs for example, if V(Ï) = gÏ4 â 2m2Ï2 with g > 0 and m2 > 0, for which the minimum energy is found at v = Â±m/âg. The value of v in one of these vacua may be considered as condensate of the field Ï. Canonical quantization then can be carried out for the shifted field  Ï(x,t) â v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an Ä§-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over Ä§ of the commutator  [A, B]  expressed in the phase space formulation is iÄ§{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  Ä§-deformations are highly nonuniqueâquantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  Ä§ â 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an Ä§-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional SchrÃ¶dinger Hilbert space. If the chosen variables are complex, we get something like the SegalâBargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', 'Crystallography', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', 'A time zone is an area which observes a uniform standard time for legal, commercial and social purposes. Time zones tend to follow the boundaries between countries and their subdivisions instead of strictly following longitude, because it is convenient for areas in frequent communication to keep the same time.\nEach time zone is defined by a standard offset from Coordinated Universal Time (UTC). The offsets range from UTCâ12:00 to UTC+14:00, and are usually a whole number of hours, but a few zones are offset by an additional 30 or 45 minutes, such as in India and Nepal. Some areas in a time zone may use a different offset for part of the year, typically one hour ahead during spring and summer, a practice known as daylight saving time (DST).\n== List of UTC offsets ==\nIn the table below, the locations that use daylight saving time (DST) are listed in their UTC offset when DST is not in effect. When DST is in effect, approximately during spring and summer, their UTC offset is increased by one hour (except for Lord Howe Island, where it is increased by 30 minutes). For example, during the DST period California observes UTCâ07:00 and the United Kingdom observes UTC+01:00.\n== History ==\nThe apparent position of the Sun in the sky, and thus solar time, varies by location due to the spherical shape of the Earth. This variation corresponds to four minutes of time for every degree of longitude, so for example when it is solar noon in London, it is about 10 minutes before solar noon in Bristol, which is about 2.5 degrees to the west.\nThe Royal Observatory, Greenwich, founded in 1675, established Greenwich Mean Time (GMT), the mean solar time at that location, as an aid to mariners to determine longitude at sea, providing a standard reference time while each location in England kept a different time.\n=== Railway time ===\nIn the 19th century, as transportation and telecommunications improved, it became increasingly inconvenient for each location to observe its own solar time. In November 1840, the British Great Western Railway started using GMT kept by portable chronometers. This practice was soon followed by other railway companies in Great Britain and became known as railway time.\nAround August 23, 1852, time signals were first transmitted by telegraph from the Royal Observatory. By 1855, 98% of Great Britain\'s public clocks were using GMT, but it was not made the island\'s legal time until August 2, 1880. Some British clocks from this period have two minute hands, one for the local time and one for GMT.\nOn November 2, 1868, the British Colony of New Zealand officially adopted a standard time to be observed throughout the colony. It was based on longitude 172Â°30â² east of Greenwich, that is 11 hours 30 minutes ahead of GMT. This standard was known as New Zealand Mean Time.\nTimekeeping on North American railroads in the 19th century was complex. Each railroad used its own standard time, usually based on the local time of its headquarters or most important terminus, and the railroad\'s train schedules were published using its own time. Some junctions served by several railroads had a clock for each railroad, each showing a different time.  Because of this a number of accidents occurred when trains from different companies using the same tracks mistimed their passings.\nAround 1863, Charles F. Dowd proposed a system of hourly standard time zones for North American railroads, although he published nothing on the matter at that time and did not consult railroad officials until 1869. In 1870 he proposed four ideal time zones having northâsouth borders, the first centered on Washington, D.C., but by 1872 the first was centered on meridian 75Â° west of Greenwich, with natural borders such as sections of the Appalachian Mountains. Dowd\'s system was never accepted by North American railroads.\nChief meteorologist at the United States Weather Bureau Cleveland Abbe divided the United States into four standard time zones for consistency among the weather stations. In 1879, he published a paper titled Report on Standard Time. In 1883, he convinced North American railroad companies to adopt his time-zone system. In 1884, Britain, which had already adopted its own standard time system for England, Scotland, and Wales, helped gather international consent for global time. In time, the American government, influenced in part by Abbe\'s 1879 paper, adopted the time-zone system.\nIt was a version proposed by William F. Allen, the editor of the Traveler\'s Official Railway Guide. The borders of its time zones ran through railroad stations, often in major cities. For example, the border between its Eastern and Central time zones ran through Detroit, Buffalo, Pittsburgh, Atlanta, and Charleston. It was inaugurated on Sunday, November 18, 1883, also called "The Day of Two Noons", when each railroad station clock was reset as standard-time noon was reached within each time zone.\nThe North American zones were named Intercolonial, Eastern, Central, Mountain, and Pacific. Within a year 85% of all cities with populations over 10,000 (about 200 cities) were using standard time. A notable exception was Detroit (located about halfway between the meridians of Eastern and Central time), which kept local time until 1900, then tried Central Standard Time, local mean time, and Eastern Standard Time (EST) before a May 1915 ordinance settled on EST and was ratified by popular vote in August 1916. The confusion of times came to an end when standard time zones were formally adopted by the U.S. Congress in the Standard Time Act of March 19, 1918.\n=== Worldwide time zones ===\nItalian mathematician Quirico Filopanti introduced the idea of a worldwide system of time zones in his book Miranda!, published in 1858. He proposed 24 hourly time zones, which he called "longitudinal days", the first centred on the meridian of Rome. He also proposed a universal time to be used in astronomy and telegraphy. However, his book attracted no attention until long after his death.\nScottish-born Canadian Sir Sandford Fleming proposed a worldwide system of time zones in 1876 - see Sandford Fleming Â§ Inventor of worldwide standard time. The proposal divided the world into twenty-four time zones labeled A-Y (skipping J), each one covering 15 degrees of longitude. All clocks within each zone would be set to the same time as the others, but differed by one hour from those in the neighboring zones. He advocated his system at several international conferences, including the International Meridian Conference, where it received some consideration. The system has not been directly adopted, but some maps divide the world into 24 time zones and assign letters to them, similarly to Fleming\'s system.\nBy about 1900, almost all inhabited places on Earth had adopted a standard time zone, but only some of them used an hourly offset from GMT. Many applied the time at a local astronomical observatory to an entire country, without any reference to GMT. It took many decades before all time zones were based on some standard offset from GMT or Coordinated Universal Time (UTC). By 1929, the majority of countries had adopted hourly time zones, though some countries such as Iran, India, Myanmar and parts of Australia had time zones with a 30-minute offset. Nepal was the last country to adopt a standard offset, shifting slightly to UTC+05:45 in 1986.\nAll nations currently use standard time zones for secular purposes, but not all of them apply the concept as originally conceived. Several countries and subdivisions use half-hour or quarter-hour deviations from standard time. Some countries, such as China and India, use a single time zone even though the extent of their territory far exceeds the ideal 15Â° of longitude for one hour; other countries, such as Spain and Argentina, use standard hour-based offsets, but not necessarily those that would be determined by their geographical location. The consequences, in some areas, can affect the lives of local citizens, and in extreme cases contribute to larger political issues, such as in the western reaches of China. In Russia, which has 11 time zones, two time zones were removed in 2010 and reinstated in 2014.\n== Notation ==\n=== ISO 8601 ===\nISO 8601 is a standard established by the International Organization for Standardization defining methods of representing dates and times in textual form, including specifications for representing time zones.\nIf a time is in Coordinated Universal Time (UTC), a "Z" is added directly after the time without a separating space. "Z" is the zone designator for the zero UTC offset. "09:30 UTC" is therefore represented as "09:30Z" or "0930Z". Likewise, "14:45:15 UTC" is written as "14:45:15Z" or "144515Z". UTC time is also known as "Zulu" time, since "Zulu" is a phonetic alphabet code word for the letter "Z".\nOffsets from UTC are written in the format Â±hh:mm, Â±hhmm, or Â±hh (either hours ahead or behind UTC). For example, if the time being described is one hour ahead of UTC (such as the time in Germany during the winter), the zone designator would be "+01:00", "+0100", or simply "+01". This numeric representation of time zones is appended to local times in the same way that alphabetic time zone abbreviations (or "Z", as above) are appended. The offset from UTC changes with daylight saving time, e.g. a time offset in Chicago, which is in the North American Central Time Zone, is "â06:00" for the winter (Central Standard Time) and "â05:00" for the summer (Central Daylight Time).\n=== Abbreviations ===\nTime zones are often represented by alphabetic abbreviations such as "EST", "WST", and "CST", but these are not part of the international time and date standard ISO 8601. Such designations can be ambiguous; for example, "CST" can mean (North American) Central Standard Time (UTCâ06:00), Cuba Standard Time (UTCâ05:00) and China Standard Time (UTC+08:00), and it is also a widely used variant of ACST (Australian Central Standard Time, UTC+09:30).']

Question: What is the Maxwell's Demon thought experiment?

Choices:
Choice A) A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with different gases at equal temperatures. The demon selectively allows molecules to pass from one side to the other, causing an increase in temperature in one part and a decrease in temperature in the other, contrary to the second law of thermodynamics.
Choice B) A thought experiment in which a demon guards a macroscopic trapdoor in a wall separating two parts of a container filled with different gases at different temperatures. The demon selectively allows molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, in accordance with the second law of thermodynamics.
Choice C) A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with the same gas at equal temperatures. The demon selectively allows faster-than-average molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, contrary to the second law of thermodynamics.
Choice D) A thought experiment in which a demon guards a macroscopic trapdoor in a wall separating two parts of a container filled with the same gas at equal temperatures. The demon selectively allows faster-than-average molecules to pass from one side to the other, causing an increase in temperature in one part and a decrease in temperature in the other, contrary to the second law of thermodynamics.
Choice E) A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with the same gas at different temperatures. The demon selectively allows slower-than-average molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, in accordance with the second law of thermodynamics.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Crystallography', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', 'Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word ÎºÏÏÏÏÎ±Î»Î»Î¿Ï (krÃºstallos; "clear ice, rock-crystal"), and Î³ÏÎ¬ÏÎµÎ¹Î½ (grÃ¡phein; "to write"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well new developments in these fields.\n== History and timeline ==\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the DavissonâGermer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n== Methodology ==\nCrystallographic methods depend mainly on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. Crystallographers often explicitly state the type of beam used, as in the terms X-ray diffraction, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.\nX-rays interact with the spatial distribution of electrons in the sample.\nNeutrons are scattered by the atomic nuclei through the strong nuclear forces, but in addition the magnetic moment of neutrons is non-zero, so they are also scattered by magnetic fields. When neutrons are scattered from hydrogen-containing materials, they produce diffraction patterns with high noise levels, which can sometimes be resolved by substituting deuterium for hydrogen.\nElectrons are charged particles and therefore interact with the total charge distribution of both the atomic nuclei and the electrons of the sample.:\u200aChpt 4\nIt is hard to focus x-rays or neutrons, but since electrons are charged they can be focused and are used in electron microscope to produce magnified images. There are many ways that transmission electron microscopy and related techniques such as scanning transmission electron microscopy, high-resolution electron microscopy can be used to obtain images with in many cases atomic resolution from which crystallographic information can be obtained. There are also other methods such as low-energy electron diffraction, low-energy electron microscopy and reflection high-energy electron diffraction which can be used to obtain crystallographic information about surfaces.\n== Applications in various areas ==\n=== Materials science ===\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Most materials do not occur as a single crystal, but are poly-crystalline in nature (they exist as an aggregate of small crystals with different orientations). As such, powder diffraction techniques, which take diffraction patterns of samples with a large number of crystals, play an important role in structural determination.\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements. Crystallographic studies help elucidate the relationship between a material\'s structure and its properties, aiding in developing new materials with tailored characteristics. This understanding is crucial in various fields, including metallurgy, geology, and materials science. Advancements in crystallographic techniques, such as electron diffraction and X-ray crystallography, continue to expand our understanding of material behavior at the atomic level.\nIn another example, iron transforms from a body-centered cubic (bcc) structure called ferrite to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material\'s properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which structures are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory.\n=== Biology ===\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. The double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol, Pymol or VMD can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron diffraction has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n== Notation ==\nCoordinates in square brackets such as [100] denote a direction vector (in real space).\nCoordinates in angle brackets or chevrons such as <100> denote a family of directions which are related by symmetry operations. In the cubic crystal system for example, <100> would mean [100], [010], [001] or the negative of any of those directions.\nMiller indices in parentheses such as (100) denote a plane of the crystal structure, and regular repetitions of that plane with a particular spacing. In the cubic system, the normal to the (hkl) plane is the direction [hkl], but in lower-symmetry cases, the normal to (hkl) is not parallel to [hkl].\nIndices in curly brackets or braces such as {100} denote a family of planes and their normals. In cubic materials the symmetry makes them equivalent, just as the way angle brackets denote a family of directions. In non-cubic materials, <hkl> is not necessarily perpendicular to {hkl}.\n== Reference literature ==\nThe International Tables for Crystallography is an eight-book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure through x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and do not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:', 'Coffee ring effect\n\nIn physics, a "coffee ring" is a pattern left by a puddle of particle-laden liquid after it evaporates. The phenomenon is named for the characteristic ring-like deposit along the perimeter of a spill of coffee. It is also commonly seen after spilling red wine. The mechanism behind the formation of these and similar rings is known as the coffee ring effect or in some instances, the coffee stain effect, or simply ring stain.\n== Flow mechanism ==\nThe coffee-ring pattern originates from the capillary flow induced by the evaporation of the drop: liquid evaporating from the edge is replenished by liquid from the interior. The resulting current can carry nearly all the dispersed material to the edge. As a function of time, this process exhibits a "rush-hour" effect, that is, a rapid acceleration of the flow towards the edge at the final stage of the drying process.\nEvaporation induces a Marangoni flow inside a droplet.  The flow, if strong, redistributes particles back to the center of the droplet.  Thus, for particles to accumulate at the edges, the liquid must have a weak Marangoni flow, or something must occur to disrupt the flow.  For example, surfactants can be added to reduce the liquid\'s surface tension gradient, disrupting the induced flow.  Water has a weak Marangoni flow to begin with, which is then reduced significantly by natural surfactants.\nInteraction of the particles suspended in a droplet with the free surface of the droplet is important in creating a coffee ring. "When the drop evaporates, the free surface collapses and traps the suspended particles ... eventually all the particles are captured by the free surface and stay there for the rest of their trip towards the edge of the drop." This result means that surfactants can be used to manipulate the motion of the solute particles by changing the surface tension of the drop, rather than trying to control the bulk flow inside the drop. A number of unique morphologies of the deposited particles can result.  For example, an enantiopure poly (isocyanate) derivative has been shown to form ordered arrays of squashed donut structures.\n== Suppression ==\nThe coffee-ring pattern is detrimental when uniform application of a dried deposit is required, such as in printed electronics. It can be suppressed by adding elongated particles, such as cellulose fibers, to the spherical particles that cause the coffee-ring effect. The size and weight fraction of added particles may be smaller than those of the primary ones.\nIt is also reported that controlling flow inside a droplet is a powerful way to generate a uniform film; for example, by harnessing solutal Marangoni flows occurring during evaporation.\nMixtures of low boiling point and high boiling point solvents were shown to suppress the coffee ring effect, changing the shape of a deposited solute from a ring-like to a dot-like shape.\nControl of the substrate temperature was shown to be an effective way to suppress the coffee ring formed by droplets of water-based PEDOT:PSS solution. On a heated hydrophilic or hydrophobic substrate, a thinner ring with an inner deposit forms, which is attributed to Marangoni convection.\nControl of the substrate wetting properties on slippery surfaces can prevent the pinning of the drop contact line, which will, therefore, suppress the coffee ring effect by reducing the number of particles deposited at the contact line. Drops on superhydrophobic or liquid impregnated surfaces are less likely to have a pinned contact line and will suppress ring formation. Drops with an oil ring formed at the drop contact line have high mobility and can avoid the ring formation on hydrophobic surfaces.\nAlternating voltage electrowetting may suppress coffee stains without the need to add surface-active materials. Reverse particle motion may also reduce the coffee-ring effect because of the capillary force near the contact line. The reversal takes place when the capillary force prevails over the outward coffee-ring flow by the geometric constraints.\n== Determinants of size and pattern ==\nThe lower-limit size of a coffee ring depends on the time scale competition between the liquid evaporation and the movement of suspended particles.  When the liquid evaporates much faster than the particle movement near a three-phase contact line, a coffee ring cannot be formed successfully.  Instead, these particles will disperse uniformly on a surface upon complete liquid evaporation.  For suspended particles of size 100 nm, the minimum diameter of the coffee ring structure is found to be 10 Î¼m, or about 10 times smaller than the width of human hair.  The shape of particles in the liquid is responsible for coffee ring effect. On porous substrates, the competition among infiltration, particle motion and evaporation of the solvent governs the final deposition morphology.\nThe pH of the solution of the drop influences the final deposit pattern. The transition between these patterns is explained by considering how DLVO interactions such as the electrostatic and Van der Waals forces modify the particle deposition process.\n== Applications ==\nThe coffee ring effect is utilized in convective deposition by researchers wanting to order particles on a substrate using capillary-driven assembly, replacing a stationary droplet with an advancing meniscus drawn across the substrate.  This process differs from dip-coating in that evaporation drives flow along the substrate as opposed to gravity.\nConvective deposition can control particle orientation, resulting in the formation of crystalline monolayer films from nonspherical particles such as hemispherical, dimer, and dumbbell shaped particles. Orientation is afforded by the system trying to reach a state of maximum packing of the particles in the thin meniscus layer over which evaporation occurs. They showed that tuning the volume fraction of particles in solution will control the specific location along the varying meniscus thickness at which assembly occurs. Particles will align with their long axis in- or out-of-plane depending on whether or not their longer dimension of the particle was equal to the thickness of the wetting layer at the meniscus location. Such thickness transitions were established with spherical particles as well. It was later shown that convective assembly could control particle orientation in assembling multi-layers, resulting in long-range 3D colloidal crystals from dumbbell shaped particles. These finds were attractive for the self-assembled of colloidal crystal films for applications such as photonics. Recent advances have increased the application of coffee-ring assembly from colloidal particles to organized patterns of inorganic crystals.\n== References ==', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  Ä§Ïk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, Î¸kâ , Î¸k, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the Î¸k, and the Fock space is built by applying all products of creation operators Î¸kâ  to |0â©.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at Ï = 0, so that the vacuum minimizing the Hamiltonian satisfies â¨Ïâ© = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  Ï = v .  This occurs for example, if V(Ï) = gÏ4 â 2m2Ï2 with g > 0 and m2 > 0, for which the minimum energy is found at v = Â±m/âg. The value of v in one of these vacua may be considered as condensate of the field Ï. Canonical quantization then can be carried out for the shifted field  Ï(x,t) â v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an Ä§-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over Ä§ of the commutator  [A, B]  expressed in the phase space formulation is iÄ§{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  Ä§-deformations are highly nonuniqueâquantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  Ä§ â 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an Ä§-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional SchrÃ¶dinger Hilbert space. If the chosen variables are complex, we get something like the SegalâBargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by LeÃ³ SzilÃ¡rd, and later by LÃ©on Brillouin.  SzilÃ¡rd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidtâs paradox.\nJohn Earman and John D. Norton have argued that SzilÃ¡rd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Jordy, W. H. (1952). Henry Adams: Scientific Historian. New Haven. ISBN 978-0-685-26683-0. {{cite book}}: ISBN / Date incompatibility (help)\nKhan, Salman. "Maxwell\'s Demon". Archived from the original on 2010-03-17.\nMaroney, O. J. E. (2009) ""Information Processing and Thermodynamic Entropy" The Stanford Encyclopedia of Philosophy (Autumn 2009 Edition)\nMaxwell, J. C. (1871). Theory of Heat. London, New York [etc.] Longmans, Green., reprinted (2001) New York: Dover, ISBN 0-486-41735-2\nNorton, J. (2005). "Eaters of the lotus: Landauer\'s principle and the return of Maxwell\'s demon" (PDF). Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics. 36 (2): 375â411. Bibcode:2005SHPMP..36..375N. CiteSeerX 10.1.1.468.3017. doi:10.1016/j.shpsb.2004.12.002. S2CID 21104635. Archived (PDF) from the original on 2006-09-01.\nRaizen, Mark G. (2011) "Demons, Entropy, and the Quest for Absolute Zero", Scientific American, March, pp54-59\nReaney, Patricia. "Scientists build nanomachine", Reuters, February 1, 2007\nRubi, J Miguel, "Does Nature Break the Second Law of Thermodynamics?"; Scientific American, October 2008 :\nSplasho (2008) â Historical development of Maxwell\'s demon\nWeiss, Peter. "Breaking the Law â Can quantum mechanics + thermodynamics = perpetual motion?", Science News, October 7, 2000', 'Peierls bracket\n\nIn theoretical physics, the Peierls bracket is an equivalent description of the Poisson bracket. It can be defined directly from the action and does not require the canonical coordinates and their canonical momenta to be defined in advance.\nThe bracket\n{\\displaystyle [A,B]}\nis defined as\n{\\displaystyle D_{A}(B)-D_{B}(A)}\nas the difference between some kind of action of one quantity on the other, minus the flipped term.\nIn quantum mechanics, the Peierls bracket becomes a commutator i.e. a Lie bracket.\n== References ==\nThis article incorporates material from the Citizendium article "Peierls bracket", which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License but not under the GFDL.\nPeierls, R. "The Commutation Laws of Relativistic Field Theory,"\nProc. R. Soc. Lond. August 21, 1952 214 1117 143-157.', "In the graph of DUT1 above, the excess of LOD above the nominal 86,400 s corresponds to the downward slope of the graph between vertical segments. (The slope became shallower in the 1980s, 2000s and late 2010s to 2020s because of slight accelerations of Earth's rotation temporarily shortening the day.) Vertical position on the graph corresponds to the accumulation of this difference over time, and the vertical segments correspond to leap seconds introduced to match this accumulated difference. Leap seconds are timed to keep DUT1 within the vertical range depicted by the adjacent graph. The frequency of leap seconds therefore corresponds to the slope of the diagonal graph segments, and thus to the excess LOD. Time periods when the slope reverses direction (slopes upwards, not the vertical segments) are times when the excess LOD is negative, that is, when the LOD is below 86,400 s.\n== Future ==\nAs the Earth's rotation continues to slow, positive leap seconds will be required more frequently. The long-term rate of change of LOD is approximately +1.7 ms per century. At the end of the 21st century, LOD will be roughly 86,400.004 s, requiring leap seconds every 250 days. Over several centuries, the frequency of leap seconds will become problematic. A change in the trend of the UT1 â UTC values was seen beginning around June 2019 in which instead of slowing down (with leap seconds to keep the difference between UT1 and UTC less than 0.9 seconds) the Earth's rotation has sped up, causing this difference to increase. If the trend continues, a negative leap second may be required, which has not been used before. This may not be needed until 2025.\nSome time in the 22nd century, two leap seconds will be required every year. The current practice of only allowing leap seconds in June and December will be insufficient to maintain a difference of less than 1 second, and it might be decided to introduce leap seconds in March and September. In the 25th century, four leap seconds are projected to be required every year, so the current quarterly options would be insufficient.\nIn April 2001, Rob Seaman of the National Optical Astronomy Observatory proposed that leap seconds be allowed to be added monthly rather than twice yearly.\nIn 2022 a resolution was adopted by the General Conference on Weights and Measures to redefine UTC and abolish leap seconds, but keep the civil second constant and equal to the SI second, so that sundials would slowly get further and further out of sync with civil time. The leap seconds will be eliminated by 2035. The resolution does not break the connection between UTC and UT1, but increases the maximum allowable difference. The details of what the maximum difference will be and how corrections will be implemented is left for future discussions. This will result in a shift of the sun's movements relative to civil time, with the difference increasing quadratically with time (i.e., proportional to elapsed centuries squared). This is analogous to the shift of seasons relative to the yearly calendar that results from the calendar year not precisely matching the tropical year length. This would be a change in civil timekeeping, and would have a slow effect at first, but becoming drastic over several centuries. UTC (and TAI) would be more and more ahead of UT; it would coincide with local mean time along a meridian drifting eastward faster and faster. Thus, the time system will lose its fixed connection to the geographic coordinates based on the IERS meridian. The difference between UTC and UT would reach 0.5 hours after the year 2600 and 6.5 hours around 4600.\nITU-R Study Group 7 and Working Party 7A were unable to reach consensus on whether to advance the proposal to the 2012 Radiocommunications Assembly; the chairman of Study Group 7 elected to advance the question to the 2012 Radiocommunications Assembly (20 January 2012), but consideration of the proposal was postponed by the ITU until the World Radio Conference in 2015. This conference, in turn, considered the question, but no permanent decision was reached; it only chose to engage in further study with the goal of reconsideration in 2023.\nA proposed alternative to the leap second is the leap hour or leap minute, which requires changes only once every few centuries.\nITU World Radiocommunication Conference 2023 (WRC-23), which was held in Dubai (United Arab Emirates) from 20 November to 15 December 2023 formally recognised the Resolution 4 of the 27th CGPM (2022) which decides that the maximum value for the difference (UT1-UTC) will be increased in, or before, 2035.\n== See also ==\nCoordinated Lunar Time\nCoordinated Mars Time â Proposed approaches to tracking date and time on the planet MarsPages displaying short descriptions of redirect targets (MTC)\nEphemeris time â Time standard used in astronomical ephemerides\nIERS Reference Meridian â International prime meridian used for GPS and other systems\nISO 8601 â International standards for dates and times\nITU-R â One of the three sectors of the ITU\nList of UTC timing centers â Recognized maintainers of atomic clocks from which UTC is calculated\nTerrestrial Time â Time standard for astronomical observations from the Earth\nUniversal Time â Time standard based on the slowing rotation of the Earth\nWorld Radiocommunication Conference â ConventionPages displaying short descriptions with no spaces\n== References ==\n=== Notes ===\n=== Citations ===\n=== General and cited sources ===\n== External links ==\nCurrent UTC time\nDefinition of Coordinated Universal Time in German law â ZeitG Â§1 (3)\nInternational Earth Rotation Service; list of differences between TAI and UTC from 1961 to present\nW3C Specification about UTC Date and Time and RFC 3339, based on ISO 8601\nStandard of time definition: UTC, GPS, LORAN and TAI\nWhat is in a name? On the term Coordinated Universal Time at the Wayback Machine (archived 6 November 2013)", 'On electronic devices which only allow the time zone to be configured using maps or city names, UTC can be selected indirectly by selecting cities such as Accra in Ghana or ReykjavÃ­k in Iceland as they are always on UTC and do not currently use daylight saving time (which Greenwich and London do, and so could be a source of error).\n=== Daylight saving time ===\nUTC does not change with a change of seasons, but local time or civil time may change if a time zone jurisdiction observes daylight saving time (summer time). For example, local time on the east coast of the United States is five hours behind UTC during winter, but four hours behind while daylight saving is observed there.\n== History ==\nIn 1928, the term Universal Time (UT) was introduced by the International Astronomical Union to refer to GMT, with the day starting at midnight. Until the 1950s, broadcast time signals were based on UT, and hence on the rotation of the Earth.\nIn 1955, the caesium atomic clock was invented. This provided a form of timekeeping that was both more stable and more convenient than astronomical observations. In 1956, the U.S. National Bureau of Standards and U.S. Naval Observatory started to develop atomic frequency time scales; by 1959, these time scales were used in generating the WWV time signals, named for the shortwave radio station that broadcasts them. In 1960, the U.S. Naval Observatory, the Royal Greenwich Observatory, and the UK National Physical Laboratory coordinated their radio broadcasts so that time steps and frequency changes were coordinated, and the resulting time scale was informally referred to as "Coordinated Universal Time".\nIn a controversial decision, the frequency of the signals was initially set to match the rate of UT, but then kept at the same frequency by the use of atomic clocks and deliberately allowed to drift away from UT. When the divergence grew significantly, the signal was phase shifted (stepped) by 20 ms to bring it back into agreement with UT. Twenty-nine such steps were used before 1960.\nIn 1958, data was published linking the frequency for the caesium transition, newly established, with the ephemeris second. The ephemeris second is a unit in the system of time that, when used as the independent variable in the laws of motion that govern the movement of the planets and moons in the Solar System, enables the laws of motion to accurately predict the observed positions of Solar System bodies. Within the limits of observable accuracy, ephemeris seconds are of constant length, as are atomic seconds. This publication allowed a value to be chosen for the length of the atomic second that would accord with the celestial laws of motion.\nThe coordination of time and frequency transmissions around the world began on 1 January 1960. UTC was first officially adopted in 1963 as CCIR Recommendation 374, Standard-Frequency and Time-Signal Emissions, and "UTC" became the official abbreviation of Coordinated Universal Time in 1967.\nIn 1961, the Bureau International de l\'Heure began coordinating the UTC process internationally (but the name Coordinated Universal Time was not formally adopted by the International Astronomical Union until 1967). From then on, there were time steps every few months, and frequency changes at the end of each year. The jumps increased in size to 0.1 seconds. This UTC was intended to permit a very close approximation to UT2.\nIn 1967, the SI second was redefined in terms of the frequency supplied by a caesium atomic clock. The length of second so defined was practically equal to the second of ephemeris time. This was the frequency that had been provisionally used in TAI since 1958. It was soon decided that having two types of second with different lengths, namely the UTC second and the SI second used in TAI, was a bad idea. It was thought better for time signals to maintain a consistent frequency, and that this frequency should match the SI second. Thus it would be necessary to rely on time steps alone to maintain the approximation of UT. This was tried experimentally in a service known as "Stepped Atomic Time" (SAT), which ticked at the same rate as TAI and used jumps of 0.2 seconds to stay synchronised with UT2.\nThere was also dissatisfaction with the frequent jumps in UTC (and SAT). In 1968, Louis Essen, the inventor of the caesium atomic clock, and G. M. R. Winkler both independently proposed that steps should be of 1 second only. to simplify future adjustments. This system was eventually approved as leap seconds in a new UTC in 1970 and implemented in 1972, along with the idea of maintaining the UTC second equal to the TAI second. This CCIR Recommendation 460 "stated that (a) carrier frequencies and time intervals should be maintained constant and should correspond to the definition of the SI second; (b) step adjustments, when necessary, should be exactly 1 s to maintain approximate agreement with Universal Time (UT); and (c) standard signals should contain information on the difference between UTC and UT."\nAs an intermediate step at the end of 1971, there was a final irregular jump of exactly 0.107758 TAI seconds, making the total of all the small time steps and frequency shifts in UTC or TAI during 1958â1971 exactly ten seconds, so that 1 January 1972 00:00:00 UTC was 1 January 1972 00:00:10 TAI exactly, and a whole number of seconds thereafter. At the same time, the tick rate of UTC was changed to exactly match TAI. UTC also started to track UT1 rather than UT2. Some time signals started to broadcast the DUT1 correction (UT1 â UTC) for applications requiring a closer approximation of UT1 than UTC now provided.\nThe current version of UTC is defined by International Telecommunication Union Recommendation (ITU-R TF.460-6), Standard-frequency and time-signal emissions, and is based on International Atomic Time (TAI) with leap seconds added at irregular intervals to compensate for the accumulated difference between TAI and time measured by Earth\'s rotation. Leap seconds are inserted as necessary to keep UTC within 0.9 seconds of the UT1 variant of universal time. See the "Current number of leap seconds" section for the number of leap seconds inserted to date.\n=== Current number of leap seconds ===\nThe first leap second occurred on 30 June 1972. Since then, leap seconds have occurred on average about once every 19 months, always on 30 June or 31 December. As of July 2022, there have been 27 leap seconds in total, all positive, putting UTC 37 seconds behind TAI.\nA study published in March 2024 in Nature concluded that accelerated melting of ice in Greenland and Antarctica due to climate change has decreased Earth\'s rotational velocity, affecting UTC adjustments and causing problems for computer networks that rely on UTC.\n== Rationale ==\nEarth\'s rotational speed is very slowly decreasing because of tidal deceleration; this increases the length of the mean solar day. The length of the SI second was calibrated on the basis of the second of ephemeris time and can now be seen to have a relationship with the mean solar day observed between 1750 and 1892, analysed by Simon Newcomb. As a result, the SI second is close to \u20601/86400\u2060 of a mean solar day in the midâ19th century. In earlier centuries, the mean solar day was shorter than 86,400 SI seconds, and in more recent centuries it is longer than 86,400 seconds. Near the end of the 20th century, the length of the mean solar day (also known simply as "length of day" or "LOD") was approximately 86,400.0013 s. For this reason, UT is now "slower" than TAI by the difference (or "excess" LOD) of 1.3 ms/day.\nThe excess of the LOD over the nominal 86,400 s accumulates over time, causing the UTC day, initially synchronised with the mean sun, to become desynchronised and run ahead of it. Near the end of the 20th century, with the LOD at 1.3 ms above the nominal value, UTC ran faster than UT by 1.3 ms per day, getting a second ahead roughly every 800 days. Thus, leap seconds were inserted at approximately this interval, retarding UTC to keep it synchronised in the long term. The actual rotational period varies on unpredictable factors such as tectonic motion and has to be observed, rather than computed.\nJust as adding a leap day every four years does not mean the year is getting longer by one day every four years, the insertion of a leap second every 800 days does not indicate that the mean solar day is getting longer by a second every 800 days. It will take about 50,000 years for a mean solar day to lengthen by one second (at a rate of 2 ms per century). This rate fluctuates within the range of 1.7â2.3 ms/cy. While the rate due to tidal friction alone is about 2.3 ms/cy, the uplift of Canada and Scandinavia by several metres since the last ice age has temporarily reduced this to 1.7 ms/cy over the last 2,700 years. The correct reason for leap seconds, then, is not the current difference between actual and nominal LOD, but rather the accumulation of this difference over a period of time: Near the end of the 20th century, this difference was about \u20601/800\u2060 of a second per day; therefore, after about 800 days, it accumulated to 1 second (and a leap second was then added).']

Question: What are the four qualitative levels of crystallinity described by geologists?

Choices:
Choice A) Holocrystalline, hypocrystalline, hypercrystalline, and holohyaline
Choice B) Holocrystalline, hypocrystalline, hypohyaline, and holohyaline
Choice C) Holocrystalline, hypohyaline, hypercrystalline, and holohyaline
Choice D) Holocrystalline, hypocrystalline, hypercrystalline, and hyperhyaline
Choice E) Holocrystalline, hypocrystalline, hypohyaline, and hyperhyaline

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
