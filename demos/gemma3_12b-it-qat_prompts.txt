Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['The occurrence of convection in the outer envelope of a main sequence star depends on the star\'s mass. Stars with several times the mass of the Sun have a convection zone deep within the interior and a radiative zone in the outer layers. Smaller stars such as the Sun are just the opposite, with the convective zone located in the outer layers. Red dwarf stars with less than 0.4 M☉ are convective throughout, which prevents the accumulation of a helium core. For most stars the convective zones will vary over time as the star ages and the constitution of the interior is modified.\nThe photosphere is that portion of a star that is visible to an observer. This is the layer at which the plasma of the star becomes transparent to photons of light. From here, the energy generated at the core becomes free to propagate into space. It is within the photosphere that sun spots, regions of lower than average temperature, appear.\nAbove the level of the photosphere is the stellar atmosphere. In a main sequence star such as the Sun, the lowest level of the atmosphere, just above the photosphere, is the thin chromosphere region, where spicules appear and stellar flares begin. Above this is the transition region, where the temperature rapidly increases within a distance of only 100 km (62 mi). Beyond this is the corona, a volume of super-heated plasma that can extend outward to several million kilometres. The existence of a corona appears to be dependent on a convective zone in the outer layers of the star. Despite its high temperature, the corona emits very little light, due to its low gas density. The corona region of the Sun is normally only visible during a solar eclipse.\nFrom the corona, a stellar wind of plasma particles expands outward from the star, until it interacts with the interstellar medium. For the Sun, the influence of its solar wind extends throughout a bubble-shaped region called the heliosphere.\n== Nuclear fusion reaction pathways ==\nWhen nuclei fuse, the mass of the fused product is less than the mass of the original parts. This lost mass is converted to electromagnetic energy, according to the mass–energy equivalence relationship\n{\\displaystyle E=mc^{2}}\n. A variety of nuclear fusion reactions take place in the cores of stars, that depend upon their mass and composition.\nThe hydrogen fusion process is temperature-sensitive, so a moderate increase in the core temperature will result in a significant increase in the fusion rate. As a result, the core temperature of main sequence stars only varies from 4 million kelvin for a small M-class star to 40 million kelvin for a massive O-class star.\nIn the Sun, with a 16-million-kelvin core, hydrogen fuses to form helium in the proton–proton chain reaction:\n41H → 22H + 2e+ + 2νe(2 x 0.4 MeV)\n2e+ + 2e− → 2γ (2 x 1.0 MeV)\n21H + 22H → 23He + 2γ (2 x 5.5 MeV)\n23He → 4He + 21H (12.9 MeV)\nThere are a couple other paths, in which 3He and 4He combine to form 7Be, which eventually (with the addition of another proton) yields two 4He, a gain of one.\nAll these reactions result in the overall reaction:\n41H → 4He + 2γ + 2νe (26.7 MeV)\nwhere γ is a gamma ray photon, νe is a neutrino, and H and He are isotopes of hydrogen and helium, respectively. The energy released by this reaction is in millions of electron volts. Each individual reaction produces only a tiny amount of energy, but because enormous numbers of these reactions occur constantly, they produce all the energy necessary to sustain the star\'s radiation output. In comparison, the combustion of two hydrogen gas molecules with one oxygen gas molecule releases only 5.7 eV.\nIn more massive stars, helium is produced in a cycle of reactions catalyzed by carbon called the carbon-nitrogen-oxygen cycle.\nIn evolved stars with cores at 100 million kelvin and masses between 0.5 and 10 M☉, helium can be transformed into carbon in the triple-alpha process that uses the intermediate element beryllium:\n4He + 4He + 92 keV → 8*Be\n4He + 8*Be + 67 keV → 12*C\n12*C → 12C + γ + 7.4 MeV\nFor an overall reaction of:\n34He → 12C + γ + 7.2 MeV\nIn massive stars, heavier elements can be burned in a contracting core through the neon-burning process and oxygen-burning process. The final stage in the stellar nucleosynthesis process is the silicon-burning process that results in the production of the stable isotope iron-56. Any further fusion would be an endothermic process that consumes energy, and so further energy can only be produced through gravitational collapse.\n== See also ==\nList of proper names of stars\nOutline of astronomy\nSidereal time\nStar clocks\nStar count\nStars in fiction\n== References ==\n== External links ==\n"How To Decipher Classification Codes". Astronomical Society of South Australia. Retrieved 20 August 2010.\nKaler, James. "Portraits of Stars and their Constellations". University of Illinois. Retrieved 20 August 2010.\nPickover, Cliff (2001). The Stars of Heaven. Oxford University Press. ISBN 978-0-19-514874-9.\nPrialnick, Dina; et al. (2001). "Stars: Stellar Atmospheres, Structure, & Evolution". University of St. Andrews. Archived from the original on 11 February 2021. Retrieved 20 August 2010.\n"Query star by identifier, coordinates or reference code". SIMBAD. Centre de Données astronomiques de Strasbourg. Retrieved 20 August 2010.', 'Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.', '=== Spirals ===\nSpiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept.\nSpiral galaxies consist of a rotating disk of stars and interstellar medium, along with a central bulge of generally older stars. Extending outward from the bulge are relatively bright arms. In the Hubble classification scheme, spiral galaxies are listed as type S, followed by a letter (a, b, or c) which indicates the degree of tightness of the spiral arms and the size of the central bulge. An Sa galaxy has tightly wound, poorly defined arms and possesses a relatively large core region. At the other extreme, an Sc galaxy has open, well-defined arms and a small core region. A galaxy with poorly defined arms is sometimes referred to as a flocculent spiral galaxy; in contrast to the grand design spiral galaxy that has prominent and well-defined spiral arms. The speed in which a galaxy rotates is thought to correlate with the flatness of the disc as some spiral galaxies have thick bulges, while others are thin and dense.\nIn spiral galaxies, the spiral arms do have the shape of approximate logarithmic spirals, a pattern that can be theoretically shown to result from a disturbance in a uniformly rotating mass of stars. Like the stars, the spiral arms rotate around the center, but they do so with constant angular velocity. The spiral arms are thought to be areas of high-density matter, or "density waves". As stars move through an arm, the space velocity of each stellar system is modified by the gravitational force of the higher density. (The velocity returns to normal after the stars depart on the other side of the arm.) This effect is akin to a "wave" of slowdowns moving along a highway full of moving cars. The arms are visible because the high density facilitates star formation, and therefore they harbor many bright and young stars.\n==== Barred spiral galaxy ====\nA majority of spiral galaxies, including the Milky Way galaxy, have a linear, bar-shaped band of stars that extends outward to either side of the core, then merges into the spiral arm structure. In the Hubble classification scheme, these are designated by an SB, followed by a lower-case letter (a, b or c) which indicates the form of the spiral arms (in the same manner as the categorization of normal spiral galaxies). Bars are thought to be temporary structures that can occur as a result of a density wave radiating outward from the core, or else due to a tidal interaction with another galaxy. Many barred spiral galaxies are active, possibly as a result of gas being channeled into the core along the arms.\nOur own galaxy, the Milky Way, is a large disk-shaped barred-spiral galaxy about 30 kiloparsecs in diameter and a kiloparsec thick. It contains about two hundred billion (2×1011) stars and has a total mass of about six hundred billion (6×1011) times the mass of the Sun.\n==== Super-luminous spiral ====\nRecently, researchers described galaxies called super-luminous spirals. They are very large with an upward diameter of 437,000 light-years (compared to the Milky Way\'s 87,400 light-year diameter). With a mass of 340 billion solar masses, they generate a significant amount of ultraviolet and mid-infrared light. They are thought to have an increased star formation rate around 30 times faster than the Milky Way.\n=== Other morphologies ===\nPeculiar galaxies are galactic formations that develop unusual properties due to tidal interactions with other galaxies.\nA ring galaxy has a ring-like structure of stars and interstellar medium surrounding a bare core. A ring galaxy is thought to occur when a smaller galaxy passes through the core of a spiral galaxy. Such an event may have affected the Andromeda Galaxy, as it displays a multi-ring-like structure when viewed in infrared radiation.\nA lenticular galaxy is an intermediate form that has properties of both elliptical and spiral galaxies. These are categorized as Hubble type S0, and they possess ill-defined spiral arms with an elliptical halo of stars (barred lenticular galaxies receive Hubble classification SB0).\nIrregular galaxies are galaxies that can not be readily classified into an elliptical or spiral morphology.\nAn Irr-I galaxy has some structure but does not align cleanly with the Hubble classification scheme.\nIrr-II galaxies do not possess any structure that resembles a Hubble classification, and may have been disrupted. Nearby examples of (dwarf) irregular galaxies include the Magellanic Clouds.\nA dark or "ultra diffuse" galaxy is an extremely-low-luminosity galaxy. It may be the same size as the Milky Way, but have a visible star count only one percent of the Milky Way\'s. Multiple mechanisms for producing this type of galaxy have been proposed, and it is possible that different dark galaxies formed by different means. One candidate explanation for the low luminosity is that the galaxy lost its star-forming gas at an early stage, resulting in old stellar populations.\n=== Dwarfs ===\nDespite the prominence of large elliptical and spiral galaxies, most galaxies are dwarf galaxies. They are relatively small when compared with other galactic formations, being about one hundredth the size of the Milky Way, with only a few billion stars. Blue compact dwarf galaxies contains large clusters of young, hot, massive stars. Ultra-compact dwarf galaxies have been discovered that are only 100 parsecs across.\nMany dwarf galaxies may orbit a single larger galaxy; the Milky Way has at least a dozen such satellites, with an estimated 300–500 yet to be discovered.\nMost of the information we have about dwarf galaxies come from observations of the local group, containing two spiral galaxies, the Milky Way and Andromeda, and many dwarf galaxies. These dwarf galaxies are classified as either irregular or dwarf elliptical/dwarf spheroidal galaxies.\nA study of 27 Milky Way neighbors found that in all dwarf galaxies, the central mass is approximately 10 million solar masses, regardless of whether it has thousands or millions of stars. This suggests that galaxies are largely formed by dark matter, and that the minimum size may indicate a form of warm dark matter incapable of gravitational coalescence on a smaller scale.\n== Variants ==\n=== Interacting ===\nInteractions between galaxies are relatively frequent, and they can play an important role in galactic evolution. Near misses between galaxies result in warping distortions due to tidal interactions, and may cause some exchange of gas and dust.\nCollisions occur when two galaxies pass directly through each other and have sufficient relative momentum not to merge. The stars of interacting galaxies usually do not collide, but the gas and dust within the two forms interacts, sometimes triggering star formation. A collision can severely distort the galaxies\' shapes, forming bars, rings or tail-like structures.\nAt the extreme of interactions are galactic mergers, where the galaxies\' relative momentums are insufficient to allow them to pass through each other. Instead, they gradually merge to form a single, larger galaxy. Mergers can result in significant changes to the galaxies\' original morphology. If one of the galaxies is much more massive than the other, the result is known as cannibalism, where the more massive larger galaxy remains relatively undisturbed, and the smaller one is torn apart. The Milky Way galaxy is currently in the process of cannibalizing the Sagittarius Dwarf Elliptical Galaxy and the Canis Major Dwarf Galaxy.\n=== Starburst ===\nStars are created within galaxies from a reserve of cold gas that forms giant molecular clouds. Some galaxies have been observed to form stars at an exceptional rate, which is known as a starburst. If they continue to do so, they would consume their reserve of gas in a time span less than the galaxy\'s lifespan. Hence starburst activity usually lasts only about ten million years, a relatively brief period in a galaxy\'s history. Starburst galaxies were more common during the universe\'s early history, but still contribute an estimated 15% to total star production.\nStarburst galaxies are characterized by dusty concentrations of gas and the appearance of newly formed stars, including massive stars that ionize the surrounding clouds to create H II regions. These stars produce supernova explosions, creating expanding remnants that interact powerfully with the surrounding gas. These outbursts trigger a chain reaction of star-building that spreads throughout the gaseous region. Only when the available gas is nearly consumed or dispersed does the activity end.\nStarbursts are often associated with merging or interacting galaxies. The prototype example of such a starburst-forming interaction is M82, which experienced a close encounter with the larger M81. Irregular galaxies often exhibit spaced knots of starburst activity.\n=== Radio galaxy ===\nA radio galaxy is a galaxy with giant regions of radio emission extending well beyond its visible structure. These energetic radio lobes are powered by jets from its active galactic nucleus. Radio galaxies are classified according to their Fanaroff–Riley classification. The FR I class have lower radio luminosity and exhibit structures which are more elongated; the FR II class are higher radio luminosity. The correlation of radio luminosity and structure suggests that the sources in these two types of galaxies may differ.', 'Cosmology', '=== Evolution ===\nOnce stars begin to form, emit radiation, and in some cases explode, the process of galaxy formation becomes very complex, involving interactions between the forces of gravity, radiation, and thermal energy. Many details are still poorly understood.\nWithin a billion years of a galaxy\'s formation, key structures begin to appear. Globular clusters, the central supermassive black hole, and a galactic bulge of metal-poor Population II stars form. The creation of a supermassive black hole appears to play a key role in actively regulating the growth of galaxies by limiting the total amount of additional matter added. During this early epoch, galaxies undergo a major burst of star formation.\nDuring the following two billion years, the accumulated matter settles into a galactic disc. A galaxy will continue to absorb infalling material from high-velocity clouds and dwarf galaxies throughout its life. This matter is mostly hydrogen and helium. The cycle of stellar birth and death slowly increases the abundance of heavy elements, eventually allowing the formation of planets.\nStar formation rates in galaxies depend upon their local environment. Isolated \'void\' galaxies have highest rate per stellar mass, with \'field\' galaxies associated with spiral galaxies having lower rates and galaxies in dense cluster having the lowest rates.\nThe evolution of galaxies can be significantly affected by interactions and collisions. Mergers of galaxies were common during the early epoch, and the majority of galaxies were peculiar in morphology. Given the distances between the stars, the great majority of stellar systems in colliding galaxies will be unaffected. However, gravitational stripping of the interstellar gas and dust that makes up the spiral arms produces a long train of stars known as tidal tails. Examples of these formations can be seen in NGC 4676 or the Antennae Galaxies.\nThe Milky Way galaxy and the nearby Andromeda Galaxy are moving toward each other at about 130 km/s, and—depending upon the lateral movements—the two might collide in about five to six billion years. Although the Milky Way has never collided with a galaxy as large as Andromeda before, it has collided and merged with other galaxies in the past. Cosmological simulations indicate that, 11 billion years ago, it merged with a particularly large galaxy that has been labeled the Kraken.\nSuch large-scale interactions are rare. As time passes, mergers of two systems of equal size become less common. Most bright galaxies have remained fundamentally unchanged for the last few billion years, and the net rate of star formation probably also peaked about ten billion years ago.\n=== Future trends ===\nSpiral galaxies, like the Milky Way, produce new generations of stars as long as they have dense molecular clouds of interstellar hydrogen in their spiral arms. Elliptical galaxies are largely devoid of this gas, and so form few new stars. The supply of star-forming material is finite; once stars have converted the available supply of hydrogen into heavier elements, new star formation will come to an end.\nThe current era of star formation is expected to continue for up to one hundred billion years, and then the "stellar age" will wind down after about ten trillion to one hundred trillion years (1013–1014 years), as the smallest, longest-lived stars in the visible universe, tiny red dwarfs, begin to fade. At the end of the stellar age, galaxies will be composed of compact objects: brown dwarfs, white dwarfs that are cooling or cold ("black dwarfs"), neutron stars, and black holes. Eventually, as a result of gravitational relaxation, all stars will either fall into central supermassive black holes or be flung into intergalactic space as a result of collisions.\n== Gallery ==\n== See also ==\n== Notes ==\n== References ==\n== Bibliography ==\n== External links ==\nNASA/IPAC Extragalactic Database (NED)\nNED Redshift-Independent Distances\nGalaxies on In Our Time at the BBC\nAn Atlas of The Universe\nGalaxies – Information and amateur observations\nGalaxy Zoo – citizen science galaxy classification project\n"A Flight Through the Universe, by the Sloan Digital Sky Survey" – animated video from Berkeley Lab', 'A star is a luminous spheroid of plasma held together by self-gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye at night; their immense distances from Earth make them appear as fixed points of light. The most prominent stars have been categorised into constellations and asterisms, and many of the brightest stars have proper names. Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations. The observable universe contains an estimated 1022 to 1024 stars. Only about 4,000 of these stars are visible to the naked eye—all within the Milky Way galaxy.\nA star\'s life begins with the gravitational collapse of a gaseous nebula of material largely comprising hydrogen, helium, and trace heavier elements. Its total mass mainly determines its evolution and eventual fate. A star shines for most of its active life due to the thermonuclear fusion of hydrogen into helium in its core. This process releases energy that traverses the star\'s interior and radiates into outer space. At the end of a star\'s lifetime, fusion ceases and its core becomes a stellar remnant: a white dwarf, a neutron star, or—if it is sufficiently massive—a black hole.\nStellar nucleosynthesis in stars or their remnants creates almost all naturally occurring chemical elements heavier than lithium. Stellar mass loss or supernova explosions return chemically enriched material to the interstellar medium. These elements are then recycled into new stars. Astronomers can determine stellar properties—including mass, age, metallicity (chemical composition), variability, distance, and motion through space—by carrying out observations of a star\'s apparent brightness, spectrum, and changes in its position in the sky over time.\nStars can form orbital systems with other astronomical objects, as in planetary systems and star systems with two or more stars. When two such stars orbit closely, their gravitational interaction can significantly impact their evolution. Stars can form part of a much larger gravitationally bound structure, such as a star cluster or a galaxy.\n== Etymology ==\nThe word "star" ultimately derives from the Proto-Indo-European root "h₂stḗr" also meaning star, but further analyzable as h₂eh₁s- ("to burn", also the source of the word "ash") + -tēr (agentive suffix). Compare Latin stella, Greek aster, German Stern. Some scholars believe the word is a borrowing from Akkadian "istar" (Venus). "Star" is cognate (shares the same root) with the following words: asterisk, asteroid, astral, constellation, Esther.\n== Observation history ==\nHistorically, stars have been important to civilizations throughout the world. They have been part of religious practices, divination rituals, mythology, used for celestial navigation and orientation, to mark the passage of seasons, and to define calendars.\nEarly astronomers recognized a difference between "fixed stars", whose position on the celestial sphere does not change, and "wandering stars" (planets), which move noticeably relative to the fixed stars over days or weeks. Many ancient astronomers believed that the stars were permanently affixed to a heavenly sphere and that they were immutable. By convention, astronomers grouped prominent stars into asterisms and constellations and used them to track the motions of the planets and the inferred position of the Sun. The motion of the Sun against the background stars (and the horizon) was used to create calendars, which could be used to regulate agricultural practices. The Gregorian calendar, currently used nearly everywhere in the world, is a solar calendar based on the angle of the Earth\'s rotational axis relative to its local star, the Sun.\nThe oldest accurately dated star chart was the result of ancient Egyptian astronomy in 1534 BC. The earliest known star catalogues were compiled by the ancient Babylonian astronomers of Mesopotamia in the late 2nd millennium BC, during the Kassite Period (c.\u20091531 BC – c.\u20091155 BC).\nThe first star catalogue in Greek astronomy was created by Aristillus in approximately 300 BC, with the help of Timocharis. The star catalog of Hipparchus (2nd century BC) included 1,020 stars, and was used to assemble Ptolemy\'s star catalogue. Hipparchus is known for the discovery of the first recorded nova (new star). Many of the constellations and star names in use today derive from Greek astronomy.\nDespite the apparent immutability of the heavens, Chinese astronomers were aware that new stars could appear. In 185 AD, they were the first to observe and write about a supernova, now known as SN 185. The brightest stellar event in recorded history was the SN 1006 supernova, which was observed in 1006 and written about by the Egyptian astronomer Ali ibn Ridwan and several Chinese astronomers. The SN 1054 supernova, which gave birth to the Crab Nebula, was also observed by Chinese and Islamic astronomers.\nMedieval Islamic astronomers gave Arabic names to many stars that are still used today and they invented numerous astronomical instruments that could compute the positions of the stars. They built the first large observatory research institutes, mainly to produce Zij star catalogues. Among these, the Book of Fixed Stars (964) was written by the Persian astronomer Abd al-Rahman al-Sufi, who observed a number of stars, star clusters (including the Omicron Velorum and Brocchi\'s Clusters) and galaxies (including the Andromeda Galaxy). According to A. Zahoor, in the 11th century, the Persian polymath scholar Abu Rayhan Biruni described the Milky Way galaxy as a multitude of fragments having the properties of nebulous stars, and gave the latitudes of various stars during a lunar eclipse in 1019.\nAccording to Josep Puig, the Andalusian astronomer Ibn Bajjah proposed that the Milky Way was made up of many stars that almost touched one another and appeared to be a continuous image due to the effect of refraction from sublunary material, citing his observation of the conjunction of Jupiter and Mars on 500 AH (1106/1107 AD) as evidence.\nEarly European astronomers such as Tycho Brahe identified new stars in the night sky (later termed novae), suggesting that the heavens were not immutable. In 1584, Giordano Bruno suggested that the stars were like the Sun, and may have other planets, possibly even Earth-like, in orbit around them, an idea that had been suggested earlier by the ancient Greek philosophers, Democritus and Epicurus, and by medieval Islamic cosmologists such as Fakhr al-Din al-Razi. By the following century, the idea of the stars being the same as the Sun was reaching a consensus among astronomers. To explain why these stars exerted no net gravitational pull on the Solar System, Isaac Newton suggested that the stars were equally distributed in every direction, an idea prompted by the theologian Richard Bentley.\nThe Italian astronomer Geminiano Montanari recorded observing variations in luminosity of the star Algol in 1667. Edmond Halley published the first measurements of the proper motion of a pair of nearby "fixed" stars, demonstrating that they had changed positions since the time of the ancient Greek astronomers Ptolemy and Hipparchus.\nWilliam Herschel was the first astronomer to attempt to determine the distribution of stars in the sky. During the 1780s, he established a series of gauges in 600 directions and counted the stars observed along each line of sight. From this, he deduced that the number of stars steadily increased toward one side of the sky, in the direction of the Milky Way core. His son John Herschel repeated this study in the southern hemisphere and found a corresponding increase in the same direction. In addition to his other accomplishments, William Herschel is noted for his discovery that some stars do not merely lie along the same line of sight, but are physical companions that form binary star systems.\nThe science of stellar spectroscopy was pioneered by Joseph von Fraunhofer and Angelo Secchi. By comparing the spectra of stars such as Sirius to the Sun, they found differences in the strength and number of their absorption lines—the dark lines in stellar spectra caused by the atmosphere\'s absorption of specific frequencies. In 1865, Secchi began classifying stars into spectral types. The modern version of the stellar classification scheme was developed by Annie J. Cannon during the early 1900s.\nThe first direct measurement of the distance to a star (61 Cygni at 11.4 light-years) was made in 1838 by Friedrich Bessel using the parallax technique. Parallax measurements demonstrated the vast separation of the stars in the heavens. Observation of double stars gained increasing importance during the 19th century. In 1834, Friedrich Bessel observed changes in the proper motion of the star Sirius and inferred a hidden companion. Edward Pickering discovered the first spectroscopic binary in 1899 when he observed the periodic splitting of the spectral lines of the star Mizar in a 104-day period. Detailed observations of many binary star systems were collected by astronomers such as Friedrich Georg Wilhelm von Struve and S. W. Burnham, allowing the masses of stars to be determined from computation of orbital elements. The first solution to the problem of deriving an orbit of binary stars from telescope observations was made by Felix Savary in 1827.\nThe twentieth century saw increasingly rapid advances in the scientific study of stars. The photograph became a valuable astronomical tool. Karl Schwarzschild discovered that the color of a star and, hence, its temperature, could be determined by comparing the visual magnitude against the photographic magnitude. The development of the photoelectric photometer allowed precise measurements of magnitude at multiple wavelength intervals. In 1921 Albert A. Michelson made the first measurements of a stellar diameter using an interferometer on the Hooker telescope at Mount Wilson Observatory.', 'Stellar evolution is the process by which a star changes over the course of time. Depending on the mass of the star, its lifetime can range from a few million years for the most massive to trillions of years for the least massive, which is considerably longer than the current age of the universe. The table shows the lifetimes of stars as a function of their masses. All stars are formed from collapsing clouds of gas and dust, often called nebulae or molecular clouds.  Over the course of millions of years, these protostars settle down into a state of equilibrium, becoming what is known as a main-sequence star.\nNuclear fusion powers a star for most of its existence. Initially the energy is generated by the fusion of hydrogen atoms at the core of the main-sequence star. Later, as the preponderance of atoms at the core becomes helium, stars like the Sun begin to fuse hydrogen along a spherical shell surrounding the core. This process causes the star to gradually grow in size, passing through the subgiant stage until it reaches the red-giant phase. Stars with at least half the mass of the Sun can also begin to generate energy through the fusion of helium at their core, whereas more-massive stars can fuse heavier elements along a series of concentric shells. Once a star like the Sun has exhausted its nuclear fuel, its core collapses into a dense white dwarf and the outer layers are expelled as a planetary nebula. Stars with around ten or more times the mass of the Sun can explode in a supernova as their inert iron cores collapse into an extremely dense neutron star or black hole. Although the universe is not old enough for any of the smallest red dwarfs to have reached the end of their existence, stellar models suggest they will slowly become brighter and hotter before running out of hydrogen fuel and becoming low-mass white dwarfs.\nStellar evolution is not studied by observing the life of a single star, as most stellar changes occur too slowly to be detected, even over many centuries. Instead, astrophysicists come to understand how stars evolve by observing numerous stars at various points in their lifetime, and by simulating stellar structure using computer models.\n== Star formation ==\n=== Protostar ===\nStellar evolution starts with the gravitational collapse of a giant molecular cloud. Typical giant molecular clouds are roughly 100 light-years (9.5×1014 km) across and contain up to 6,000,000 solar masses (1.2×1037 kg). As it collapses, a giant molecular cloud breaks into smaller and smaller pieces. In each of these fragments, the collapsing gas releases gravitational potential energy as heat. As its temperature and pressure increase, a fragment condenses into a rotating ball of superhot gas known as a protostar.  Filamentary structures are truly ubiquitous in the molecular cloud. Dense molecular filaments will fragment into gravitationally bound cores, which are the precursors of stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed fragmentation manner of the filaments. In supercritical filaments, observations have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded two protostars with gas outflows.\nA protostar continues to grow by accretion of gas and dust from the molecular cloud, becoming a pre-main-sequence star as it reaches its final mass. Further development is determined by its mass. Mass is typically compared to the mass of the Sun: 1.0 M☉ (2.0×1030 kg) means 1 solar mass.\nProtostars are encompassed in dust, and are thus more readily visible at infrared wavelengths.\nObservations from the Wide-field Infrared Survey Explorer (WISE) have been especially important for unveiling numerous galactic protostars and their parent star clusters.\n=== Brown dwarfs and sub-stellar objects ===\nProtostars with masses less than roughly 0.08 M☉ (1.6×1029 kg) never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (MJ), 2.5 × 1028 kg, or 0.0125 M☉). Objects smaller than 13 MJ are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets). Both types, deuterium-burning and not, shine dimly and fade away slowly, cooling gradually over hundreds of millions of years.\n=== Main sequence stellar mass objects ===\nFor a more-massive protostar, the core temperature will eventually reach 10 million kelvin, initiating the proton–proton chain reaction and allowing hydrogen to fuse, first to deuterium and then to helium. In stars of slightly over 1 M☉ (2.0×1030 kg), the carbon–nitrogen–oxygen fusion reaction (CNO cycle) contributes a large portion of the energy generation. The onset of nuclear fusion leads relatively quickly to a hydrostatic equilibrium in which energy released by the core maintains a high gas pressure, balancing the weight of the star\'s matter and preventing further gravitational collapse. The star thus evolves rapidly to a stable state, beginning the main-sequence phase of its evolution.\nA new star will sit at a specific point on the main sequence of the Hertzsprung–Russell diagram, with the main-sequence spectral type depending upon the mass of the star. Small, relatively cold, low-mass red dwarfs fuse hydrogen slowly and will remain on the main sequence for hundreds of billions of years or longer, whereas massive, hot O-type stars will leave the main sequence after just a few million years. A mid-sized yellow dwarf star, like the Sun, will remain on the main sequence for about 10 billion years. The Sun is thought to be in the middle of its main sequence lifespan.\n=== Planetary system ===\nA star may gain a protoplanetary disk, which furthermore can develop into a planetary system.\n== Mature stars ==\nEventually the star\'s core exhausts its supply of hydrogen and the star begins to evolve off the main sequence. Without the outward radiation pressure generated by the fusion of hydrogen to counteract the force of gravity, the core contracts until either electron degeneracy pressure becomes sufficient to oppose gravity or the core becomes hot enough (around 100 MK) for helium fusion to begin. Which of these happens first depends upon the star\'s mass.\n=== Low-mass stars ===\nWhat happens after a low-mass star ceases to produce energy through fusion has not been directly observed; the universe is around 13.8 billion years old, which is less time (by several orders of magnitude, in some cases) than it takes for fusion to cease in such stars.\nRecent astrophysical models suggest that red dwarfs of 0.1 M☉ may stay on the main sequence for some six to twelve trillion years, gradually increasing in both temperature and luminosity, and take several hundred billion years more to collapse, slowly, into a white dwarf.  Such stars will not become red giants as the whole star is a convection zone and it will not develop a degenerate helium core with a shell burning hydrogen.  Instead, hydrogen fusion will proceed until almost the whole star is helium.\nSlightly more massive stars do expand into red giants, but their helium cores are not massive enough to reach the temperatures required for helium fusion so they never reach the tip of the red-giant branch.  When hydrogen shell burning finishes, these stars move directly off the red-giant branch like a post-asymptotic-giant-branch (AGB) star, but at lower luminosity, to become a white dwarf.  A star with an initial mass about 0.6 M☉ will be able to reach temperatures high enough to fuse helium, and these "mid-sized" stars go on to further stages of evolution beyond the red-giant branch.\n=== Mid-sized stars ===\nStars of roughly 0.6–10 M☉ become red giants, which are large non-main-sequence stars of stellar classification K or M. Red giants lie along the right edge of the Hertzsprung–Russell diagram due to their red color and large luminosity. Examples include Aldebaran in the constellation Taurus and Arcturus in the constellation of Boötes.\nMid-sized stars are red giants during two different phases of their post-main-sequence evolution: red-giant-branch stars, with inert cores made of helium and hydrogen-burning shells, and asymptotic-giant-branch stars, with inert cores made of carbon and helium-burning shells inside the hydrogen-burning shells.  Between these two phases, stars spend a period on the horizontal branch with a helium-fusing core.  Many of these helium-fusing stars cluster towards the cool end of the horizontal branch as K-type giants and are referred to as red clump giants.\n==== Subgiant phase ====\nWhen a star exhausts the hydrogen in its core, it leaves the main sequence and begins to fuse hydrogen in a shell outside the core.  The core increases in mass as the shell produces more helium.  Depending on the mass of the helium core, this continues for several million to one or two billion years, with the star expanding and cooling at a similar or slightly lower luminosity to its main sequence state.  Eventually either the core becomes degenerate, in stars around the mass of the sun, or the outer layers cool sufficiently to become opaque, in more massive stars.  Either of these changes cause the hydrogen shell to increase in temperature and the luminosity of the star to increase, at which point the star expands onto the red-giant branch.\n==== Red-giant-branch phase ====', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', "A supernova explosion blows away the star's outer layers, leaving a remnant such as the Crab Nebula. The core is compressed into a neutron star, which sometimes manifests itself as a pulsar or X-ray burster. In the case of the largest stars, the remnant is a black hole greater than 4 M☉. In a neutron star the matter is in a state known as neutron-degenerate matter, with a more exotic form of degenerate matter, QCD matter, possibly present in the core.\nThe blown-off outer layers of dying stars include heavy elements, which may be recycled during the formation of new stars. These heavy elements allow the formation of rocky planets. The outflow from supernovae and the stellar wind of large stars play an important part in shaping the interstellar medium.\n==== Binary stars ====\nBinary stars' evolution may significantly differ from that of single stars of the same mass. For example, when any star expands to become a red giant, it may overflow its Roche lobe, the surrounding region where material is gravitationally bound to it; if stars in a binary system are close enough, some of that material may overflow to the other star, yielding phenomena including contact binaries, common-envelope binaries, cataclysmic variables, blue stragglers, and type Ia supernovae. Mass transfer leads to cases such as the Algol paradox, where the most-evolved star in a system is the least massive.\nThe evolution of binary star and higher-order star systems is intensely researched since so many stars have been found to be members of binary systems. Around half of Sun-like stars, and an even higher proportion of more massive stars, form in multiple systems, and this may greatly influence such phenomena as novae and supernovae, the formation of certain types of star, and the enrichment of space with nucleosynthesis products.\nThe influence of binary star evolution on the formation of evolved massive stars such as luminous blue variables, Wolf–Rayet stars, and the progenitors of certain classes of core collapse supernova is still disputed. Single massive stars may be unable to expel their outer layers fast enough to form the types and numbers of evolved stars that are observed, or to produce progenitors that would explode as the supernovae that are observed. Mass transfer through gravitational stripping in binary systems is seen by some astronomers as the solution to that problem.\n== Distribution ==\nStars are not spread uniformly across the universe but are normally grouped into galaxies along with interstellar gas and dust. A typical large galaxy like the Milky Way contains hundreds of billions of stars. There are more than 2 trillion (1012) galaxies, though most are less than 10% the mass of the Milky Way. Overall, there are likely to be between 1022 and 1024 stars, which are more stars than all the grains of sand on planet Earth. Most stars are within galaxies, but between 10 and 50% of the starlight in large galaxy clusters may come from stars outside of any galaxy.\nA multi-star system consists of two or more gravitationally bound stars that orbit each other. The simplest and most common multi-star system is a binary star, but systems of three or more stars exist. For reasons of orbital stability, such multi-star systems are often organized into hierarchical sets of binary stars. Larger groups are called star clusters. These range from loose stellar associations with only a few stars to open clusters with dozens to thousands of stars, up to enormous globular clusters with hundreds of thousands of stars. Such systems orbit their host galaxy. The stars in an open or globular cluster all formed from the same giant molecular cloud, so all members normally have similar ages and compositions.\nMany stars are observed, and most or all may have originally formed in gravitationally bound, multiple-star systems. This is particularly true for very massive O and B class stars, 80% of which are believed to be part of multiple-star systems. The proportion of single star systems increases with decreasing star mass, so that only 25% of red dwarfs are known to have stellar companions. As 85% of all stars are red dwarfs, more than two thirds of stars in the Milky Way are likely single red dwarfs. In a 2017 study of the Perseus molecular cloud, astronomers found that most of the newly formed stars are in binary systems. In the model that best explained the data, all stars initially formed as binaries, though some binaries later split up and leave single stars behind.\nThe nearest star to the Earth, apart from the Sun, is Proxima Centauri, 4.2465 light-years (40.175 trillion kilometres) away. Travelling at the orbital speed of the Space Shuttle, 8 kilometres per second (29,000 kilometres per hour), it would take about 150,000 years to arrive. This is typical of stellar separations in galactic discs. Stars can be much closer to each other in the centres of galaxies and in globular clusters, or much farther apart in galactic halos.\nDue to the relatively vast distances between stars outside the galactic nucleus, collisions between stars are thought to be rare. In denser regions such as the core of globular clusters or the galactic center, collisions can be more common. Such collisions can produce what are known as blue stragglers. These abnormal stars have a higher surface temperature and thus are bluer than stars at the main sequence turnoff in the cluster to which they belong; in standard stellar evolution, blue stragglers would already have evolved off the main sequence and thus would not be seen in the cluster.\n== Characteristics ==\nAlmost everything about a star is determined by its initial mass, including such characteristics as luminosity, size, evolution, lifespan, and its eventual fate.\n=== Age ===\nMost stars are between 1 billion and 10 billion years old. Some stars may even be close to 13.8 billion years old—the observed age of the universe. The oldest star yet discovered, HD 140283, nicknamed Methuselah star, is an estimated 14.46 ± 0.8 billion years old. (Due to the uncertainty in the value, this age for the star does not conflict with the age of the universe, determined by the Planck satellite as 13.799 ± 0.021).\nThe more massive the star, the shorter its lifespan, primarily because massive stars have greater pressure on their cores, causing them to burn hydrogen more rapidly. The most massive stars last an average of a few million years, while stars of minimum mass (red dwarfs) burn their fuel very slowly and can last tens to hundreds of billions of years.\n=== Chemical composition ===\nWhen stars form in the present Milky Way galaxy, they are composed of about 71% hydrogen and 27% helium, as measured by mass, with a small fraction of heavier elements. Typically the portion of heavy elements is measured in terms of the iron content of the stellar atmosphere, as iron is a common element and its absorption lines are relatively easy to measure. The portion of heavier elements may be an indicator of the likelihood that the star has a planetary system.\nAs of 2005 the star with the lowest iron content ever measured is the dwarf HE1327-2326, with only 1/200,000th the iron content of the Sun. By contrast, the super-metal-rich star μ Leonis has nearly double the abundance of iron as the Sun, while the planet-bearing star 14 Herculis has nearly triple the iron. Chemically peculiar stars show unusual abundances of certain elements in their spectrum; especially chromium and rare earth elements. Stars with cooler outer atmospheres, including the Sun, can form various diatomic and polyatomic molecules.\n=== Diameter ===\nDue to their great distance from the Earth, all stars except the Sun appear to the unaided eye as shining points in the night sky that twinkle because of the effect of the Earth's atmosphere. The Sun is close enough to the Earth to appear as a disk instead, and to provide daylight. Other than the Sun, the star with the largest apparent size is R Doradus, with an angular diameter of only 0.057 arcseconds.\nThe disks of most stars are much too small in angular size to be observed with current ground-based optical telescopes, so interferometer telescopes are required to produce images of these objects. Another technique for measuring the angular size of stars is through occultation. By precisely measuring the drop in brightness of a star as it is occulted by the Moon (or the rise in brightness when it reappears), the star's angular diameter can be computed.\nStars range in size from neutron stars, which vary anywhere from 20 to 40 km (25 mi) in diameter, to supergiants like Betelgeuse in the Orion constellation, which has a diameter about 640 times that of the Sun with a much lower density.\n=== Kinematics ===\nThe motion of a star relative to the Sun can provide useful information about the origin and age of a star, as well as the structure and evolution of the surrounding galaxy. The components of motion of a star consist of the radial velocity toward or away from the Sun, and the traverse angular movement, which is called its proper motion.\nRadial velocity is measured by the doppler shift of the star's spectral lines and is given in units of km/s. The proper motion of a star, its parallax, is determined by precise astrometric measurements in units of milli-arc seconds (mas) per year. With knowledge of the star's parallax and its distance, the proper motion velocity can be calculated. Together with the radial velocity, the total velocity can be calculated. Stars with high rates of proper motion are likely to be relatively close to the Sun, making them good candidates for parallax measurements.", "When both rates of movement are known, the space velocity of the star relative to the Sun or the galaxy can be computed. Among nearby stars, it has been found that younger population I stars have generally lower velocities than older, population II stars. The latter have elliptical orbits that are inclined to the plane of the galaxy. A comparison of the kinematics of nearby stars has allowed astronomers to trace their origin to common points in giant molecular clouds; such groups with common points of origin are referred to as stellar associations.\n=== Magnetic field ===\nThe magnetic field of a star is generated within regions of the interior where convective circulation occurs. This movement of conductive plasma functions like a dynamo, wherein the movement of electrical charges induce magnetic fields, as does a mechanical dynamo. Those magnetic fields have a great range that extend throughout and beyond the star. The strength of the magnetic field varies with the mass and composition of the star, and the amount of magnetic surface activity depends upon the star's rate of rotation. This surface activity produces starspots, which are regions of strong magnetic fields and lower than normal surface temperatures. Coronal loops are arching magnetic field flux lines that rise from a star's surface into the star's outer atmosphere, its corona. The coronal loops can be seen due to the plasma they conduct along their length. Stellar flares are bursts of high-energy particles that are emitted due to the same magnetic activity.\nYoung, rapidly rotating stars tend to have high levels of surface activity because of their magnetic field. The magnetic field can act upon a star's stellar wind, functioning as a brake to gradually slow the rate of rotation with time. Thus, older stars such as the Sun have a much slower rate of rotation and a lower level of surface activity. The activity levels of slowly rotating stars tend to vary in a cyclical manner and can shut down altogether for periods of time. During the Maunder Minimum, for example, the Sun underwent a 70-year period with almost no sunspot activity.\n=== Mass ===\nStars have masses ranging from less than half the solar mass to over 200 solar masses (see List of most massive stars). One of the most massive stars known is Eta Carinae, which, with 100–150 times as much mass as the Sun, will have a lifespan of only several million years. Studies of the most massive open clusters suggests 150 M☉ as a rough upper limit for stars in the current era of the universe. This represents an empirical value for the theoretical limit on the mass of forming stars due to increasing radiation pressure on the accreting gas cloud. Several stars in the R136 cluster in the Large Magellanic Cloud have been measured with larger masses, but it has been determined that they could have been created through the collision and merger of massive stars in close binary systems, sidestepping the 150 M☉ limit on massive star formation.\nThe first stars to form after the Big Bang may have been larger, up to 300 M☉, due to the complete absence of elements heavier than lithium in their composition. This generation of supermassive population III stars is likely to have existed in the very early universe (i.e., they are observed to have a high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life. In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at z = 6.60.\nWith a mass only 80 times that of Jupiter (MJ), 2MASS J0523-1403 is the smallest known star undergoing nuclear fusion in its core. For stars with metallicity similar to the Sun, the theoretical minimum mass the star can have and still undergo fusion at the core, is estimated to be about 75 MJ. When the metallicity is very low, the minimum star size seems to be about 8.3% of the solar mass, or about 87 MJ. Smaller bodies called brown dwarfs, occupy a poorly defined grey area between stars and gas giants.\nThe combination of the radius and the mass of a star determines its surface gravity. Giant stars have a much lower surface gravity than do main sequence stars, while the opposite is the case for degenerate, compact stars such as white dwarfs. The surface gravity can influence the appearance of a star's spectrum, with higher gravity causing a broadening of the absorption lines.\n=== Rotation ===\nThe rotation rate of stars can be determined through spectroscopic measurement, or more exactly determined by tracking their starspots. Young stars can have a rotation greater than 100 km/s at the equator. The B-class star Achernar, for example, has an equatorial velocity of about 225 km/s or greater, causing its equator to bulge outward and giving it an equatorial diameter that is more than 50% greater than between the poles. This rate of rotation is just below the critical velocity of 300 km/s at which speed the star would break apart. By contrast, the Sun rotates once every 25–35 days depending on latitude, with an equatorial velocity of 1.93 km/s. A main sequence star's magnetic field and the stellar wind serve to slow its rotation by a significant amount as it evolves on the main sequence.\nDegenerate stars have contracted into a compact mass, resulting in a rapid rate of rotation. However they have relatively low rates of rotation compared to what would be expected by conservation of angular momentum—the tendency of a rotating body to compensate for a contraction in size by increasing its rate of spin. A large portion of the star's angular momentum is dissipated as a result of mass loss through the stellar wind. In spite of this, the rate of rotation for a pulsar can be very rapid. The pulsar at the heart of the Crab nebula, for example, rotates 30 times per second. The rotation rate of the pulsar will gradually slow due to the emission of radiation.\n=== Temperature ===\nThe surface temperature of a main sequence star is determined by the rate of energy production of its core and by its radius, and is often estimated from the star's color index. The temperature is normally given in terms of an effective temperature, which is the temperature of an idealized black body that radiates its energy at the same luminosity per surface area as the star. The effective temperature is only representative of the surface, as the temperature increases toward the core. The temperature in the core region of a star is several million kelvins.\nThe stellar temperature will determine the rate of ionization of various elements, resulting in characteristic absorption lines in the spectrum. The surface temperature of a star, along with its visual absolute magnitude and absorption features, is used to classify a star (see classification below).\nMassive main sequence stars can have surface temperatures of 50,000 K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600 K; but they have a high luminosity due to their large exterior surface area.\n== Radiation ==\nThe energy produced by stars, a product of nuclear fusion, radiates to space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind, which streams from the outer layers as electrically charged protons and alpha and beta particles. A steady stream of almost massless neutrinos emanate directly from the star's core.\nThe production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers.\nThe color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the star's outer layers, including its photosphere. Besides visible light, stars emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics.\nUsing the stellar spectrum, astronomers can determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is found, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.) With these parameters, astronomers can estimate the age of the star.\n=== Luminosity ===\nThe luminosity of a star is the amount of light and other forms of radiant energy it radiates per unit of time. It has units of power. The luminosity of a star is determined by its radius and surface temperature. Many stars do not radiate uniformly across their entire surface. The rapidly rotating star Vega, for example, has a higher energy flux (power per unit area) at its poles than along its equator."]

Question: What is the main sequence in astronomy?

Choices:
Choice A) The main sequence is a type of galaxy that contains a large number of stars.
Choice B) The main sequence is a type of black hole that is formed from the collapse of a massive star.
Choice C) The main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. Stars on this band are known as main-sequence stars or dwarf stars.
Choice D) The main sequence is a group of planets that orbit around a star in a solar system.
Choice E) The main sequence is a type of nebula that is formed from the explosion of a supernova.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Ultraviolet', 'UVC LEDs are relatively new to the commercial market and are gaining in popularity. Due to their monochromatic nature (±5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.\nDisinfection using UV radiation is commonly used in wastewater treatment applications and is finding an increased usage in municipal drinking water treatment. Many bottlers of spring water use UV disinfection equipment to sterilize their water. Solar water disinfection has been researched for cheaply treating contaminated water using natural sunlight. The UVA irradiation and increased water temperature kill organisms in the water.\nUltraviolet radiation is used in several food processes to kill unwanted microorganisms. UV can be used to pasteurize fruit juices by flowing the juice over a high-intensity ultraviolet source. The effectiveness of such a process depends on the UV absorbance of the juice.\nPulsed light (PL) is a technique of killing microorganisms on surfaces using pulses of an intense broad spectrum, rich in UVC between 200 and 280 nm. Pulsed light works with xenon flash lamps that can produce flashes several times per second. Disinfection robots use pulsed UV.\nThe antimicrobial effectiveness of filtered far-UVC (222\u2009nm) light on a range of pathogens, including bacteria and fungi showed inhibition of pathogen growth, and since it has lesser harmful effects, it provides essential insights for reliable disinfection in healthcare settings, such as hospitals and long-term care homes. UVC has also been shown to be effective at degrading SARS-CoV-2 virus.\n==== Biological ====\nSome animals, including birds, reptiles, and insects such as bees, can see near-ultraviolet wavelengths. Many fruits, flowers, and seeds stand out more strongly from the background in ultraviolet wavelengths as compared to human color vision. Scorpions glow or take on a yellow to green color under UV illumination, thus assisting in the control of these arachnids. Many birds have patterns in their plumage that are invisible at usual wavelengths but observable in ultraviolet, and the urine and other secretions of some animals, including dogs, cats, and human beings, are much easier to spot with ultraviolet. Urine trails of rodents can be detected by pest control technicians for proper treatment of infested dwellings.\nButterflies use ultraviolet as a communication system for sex recognition and mating behavior. For example, in the Colias eurytheme butterfly, males rely on visual cues to locate and identify females. Instead of using chemical stimuli to find mates, males are attracted to the ultraviolet-reflecting color of female hind wings. In Pieris napi butterflies it was shown that females in northern Finland with less UV-radiation present in the environment possessed stronger UV signals to attract their males than those occurring further south. This suggested that it was evolutionarily more difficult to increase the UV-sensitivity of the eyes of the males than to increase the UV-signals emitted by the females.\nMany insects use the ultraviolet wavelength emissions from celestial objects as references for flight navigation. A local ultraviolet emitter will normally disrupt the navigation process and will eventually attract the flying insect.\nThe green fluorescent protein (GFP) is often used in genetics as a marker. Many substances, such as proteins, have significant light absorption bands in the ultraviolet that are of interest in biochemistry and related fields. UV-capable spectrophotometers are common in such laboratories.\nUltraviolet traps called bug zappers are used to eliminate various small flying insects. They are attracted to the UV and are killed using an electric shock, or trapped once they come into contact with the device. Different designs of ultraviolet radiation traps are also used by entomologists for collecting nocturnal insects during faunistic survey studies.\n==== Therapy ====\nUltraviolet radiation is helpful in the treatment of skin conditions such as psoriasis and vitiligo. Exposure to UVA, while the skin is hyper-photosensitive, by taking psoralens is an effective treatment for psoriasis. Due to the potential of psoralens to cause damage to the liver, PUVA therapy may be used only a limited number of times over a patient\'s lifetime.\nUVB phototherapy does not require additional medications or topical preparations for the therapeutic benefit; only the exposure is needed. However, phototherapy can be effective when used in conjunction with certain topical treatments such as anthralin, coal tar, and vitamin A and D derivatives, or systemic treatments such as methotrexate and Soriatane.\n==== Herpetology ====\nReptiles need UVB for biosynthesis of vitamin D, and other metabolic processes. Specifically cholecalciferol (vitamin D3), which is needed for basic cellular / neural functioning as well as the utilization of calcium for bone and egg production. The UVA wavelength is also visible to many reptiles and might play a significant role in their ability survive in the wild as well as in visual communication between individuals. Therefore, in a typical reptile enclosure, a fluorescent UV a/b source (at the proper strength / spectrum for the species), must be available for many captive species to survive. Simple supplementation with cholecalciferol (Vitamin D3) will not be enough as there is a complete biosynthetic pathway that is "leapfrogged" (risks of possible overdoses), the intermediate molecules and metabolites also play important functions in the animals health. Natural sunlight in the right levels is always going to be superior to artificial sources, but this might not be possible for keepers in different parts of the world.\nIt is a known problem that high levels of output of the UVa part of the spectrum can both cause cellular and DNA damage to sensitive parts of their bodies – especially the eyes where blindness is the result of an improper UVa/b source use and placement photokeratitis. For many keepers there must also be a provision for an adequate heat source this has resulted in the marketing of heat and light "combination" products. Keepers should be careful of these "combination" light/ heat and UVa/b generators, they typically emit high levels of UVa with lower levels of UVb that are set and difficult to control so that animals can have their needs met. A better strategy is to use individual sources of these elements and so they can be placed and controlled by the keepers for the max benefit of the animals.\n== Evolutionary significance ==\nThe evolution of early reproductive proteins and enzymes is attributed in modern models of evolutionary theory to ultraviolet radiation. UVB causes thymine base pairs next to each other in genetic sequences to bond together into thymine dimers, a disruption in the strand that reproductive enzymes cannot copy. This leads to frameshifting during genetic replication and protein synthesis, usually killing the cell. Before formation of the UV-blocking ozone layer, when early prokaryotes approached the surface of the ocean, they almost invariably died out. The few that survived had developed enzymes that monitored the genetic material and removed thymine dimers by nucleotide excision repair enzymes. Many enzymes and proteins involved in modern mitosis and meiosis are similar to repair enzymes, and are believed to be evolved modifications of the enzymes originally used to overcome DNA damages caused by UV.\nElevated levels of ultraviolet radiation, in particular UV-B, have also been speculated as a cause of mass extinctions in the fossil record.\n== Photobiology ==\nPhotobiology is the scientific study of the beneficial and harmful interactions of non-ionizing radiation in living organisms, conventionally demarcated around 10 eV, the first ionization energy of oxygen. UV ranges roughly from 3 to 30 eV in energy. Hence photobiology entertains some, but not all, of the UV spectrum.\n== See also ==\n== References ==\n== Further reading ==\nAllen, Jeannie (6 September 2001). Ultraviolet Radiation: How it Affects Life on Earth. Earth Observatory. NASA, USA.\nHockberger, Philip E. (2002). "A History of Ultraviolet Photobiology for Humans, Animals and Microorganisms". Photochemistry and Photobiology. 76 (6): 561–569. doi:10.1562/0031-8655(2002)0760561AHOUPF2.0.CO2. PMID 12511035. S2CID 222100404.\nHu, S; Ma, F; Collado-Mesa, F; Kirsner, R. S. (July 2004). "UV radiation, latitude, and melanoma in US Hispanics and blacks". Arch. Dermatol. 140 (7): 819–824. doi:10.1001/archderm.140.7.819. PMID 15262692.\nStrauss, CEM; Funk, DJ (1991). "Broadly tunable difference-frequency generation of VUV using two-photon resonances in H2 and Kr". Optics Letters. 16 (15): 1192–4. Bibcode:1991OptL...16.1192S. doi:10.1364/ol.16.001192. PMID 19776917.\n== External links ==\nMedia related to Ultraviolet light at Wikimedia Commons\nThe dictionary definition of ultraviolet at Wiktionary', 'Ultraviolet radiation, also known as simply UV, is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights.\nThe photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms.:\u200a25–26\u200a  Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature.:\u200a28\u200a  Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact.\nFor humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength "extreme" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life.\nThe lower wavelength limit of the visible spectrum is conventionally taken as 400 nm.  Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range.  Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.\n== Visibility ==\nUltraviolet rays are not usable for normal human vision.\nThe lens of the human eye and surgically implanted lens produced since 1986 blocks most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy.\nPeople lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet.  Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds "true" UV vision.\n== History and discovery ==\n"Ultraviolet" means "beyond violet" (from Latin ultra, "beyond"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light.\nUV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them "(de-)oxidizing rays" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from "heat rays", discovered the previous year at the other end of the visible spectrum. The simpler term "chemical rays" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them "tithonic rays"). The terms "chemical rays" and "heat rays" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established.\nThe discovery of the ultraviolet radiation with wavelengths below 200 nm, named "vacuum ultraviolet" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided "unanimously" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen.\n== Subtypes ==\nThe electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348:\nSeveral solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum.\nVacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers.\nTechnology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes.\nExtreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer  satellite.\nSome sources use the distinction of "hard UV" and "soft UV". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with "hard UV" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions.\n== Solar ultraviolet ==\nVery hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth\'s atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum.\nThe atmosphere blocks about 77% of the Sun\'s UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth\'s surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth\'s surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On "partly cloudy" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB.', 'Simple NUV sources can be used to highlight faded iron-based ink on vellum.\n==== Sanitary compliance ====\nUltraviolet helps detect organic material deposits that remain on surfaces where periodic cleaning and sanitizing may have failed. It is used in the hotel industry, manufacturing, and other industries where levels of cleanliness or contamination are inspected.\nPerennial news features for many television news organizations involve an investigative reporter using a similar device to reveal unsanitary conditions in hotels, public toilets, hand rails, and such.\n==== Chemistry ====\nUV/Vis spectroscopy is widely used as a technique in chemistry to analyze chemical structure, the most notable one being conjugated systems. UV radiation is often used to excite a given sample where the fluorescent emission is measured with a spectrofluorometer. In biological research, UV radiation is used for quantification of nucleic acids or proteins. In environmental chemistry, UV radiation could also be used to detect Contaminants of emerging concern in water samples.\nIn pollution control applications, ultraviolet analyzers are used to detect emissions of nitrogen oxides, sulfur compounds, mercury, and ammonia, for example in the flue gas of fossil-fired power plants. Ultraviolet radiation can detect thin sheens of spilled oil on water, either by the high reflectivity of oil films at UV wavelengths, fluorescence of compounds in oil, or by absorbing of UV created by Raman scattering in water. UV absorbance can also be used to quantify contaminants in wastewater. Most commonly used 254 nm UV absorbance is generally used as a surrogate parameters to quantify NOM. Another form of light-based detection method uses a wide spectrum of excitation emission matrix (EEM) to detect and identify contaminants based on their flourense properties. EEM could be used to discriminate different groups of NOM based on the difference in light emission and excitation of fluorophores. NOMs with certain molecular structures are reported to have fluorescent properties in a wide range of excitation/emission wavelengths.\nUltraviolet lamps are also used as part of the analysis of some minerals and gems.\n=== Material science uses ===\n==== Fire detection ====\nIn general, ultraviolet detectors use either a solid-state device, such as one based on silicon carbide or aluminium nitride, or a gas-filled tube as the sensing element. UV detectors that are sensitive to UV in any part of the spectrum respond to irradiation by sunlight and artificial light. A burning hydrogen flame, for instance, radiates strongly in the 185- to 260-nanometer range and only very weakly in the IR region, whereas a coal fire emits very weakly in the UV band yet very strongly at IR wavelengths; thus, a fire detector that operates using both UV and IR detectors is more reliable than one with a UV detector alone. Virtually all fires emit some radiation in the UVC band, whereas the Sun\'s radiation at this band is absorbed by the Earth\'s atmosphere. The result is that the UV detector is "solar blind", meaning it will not cause an alarm in response to radiation from the Sun, so it can easily be used both indoors and outdoors.\nUV detectors are sensitive to most fires, including hydrocarbons, metals, sulfur, hydrogen, hydrazine, and ammonia. Arc welding, electrical arcs, lightning, X-rays used in nondestructive metal testing equipment (though this is highly unlikely), and radioactive materials can produce levels that will activate a UV detection system. The presence of UV-absorbing gases and vapors will attenuate the UV radiation from a fire, adversely affecting the ability of the detector to detect flames. Likewise, the presence of an oil mist in the air or an oil film on the detector window will have the same effect.\n==== Photolithography ====\nUltraviolet radiation is used for very fine resolution photolithography, a procedure wherein a chemical called a photoresist is exposed to UV radiation that has passed through a mask. The exposure causes chemical reactions to occur in the photoresist. After removal of unwanted photoresist, a pattern determined by the mask remains on the sample. Steps may then be taken to "etch" away, deposit on or otherwise modify areas of the sample where no photoresist remains.\nPhotolithography is used in the manufacture of semiconductors, integrated circuit components, and printed circuit boards. Photolithography processes used to fabricate electronic integrated circuits presently use 193 nm UV and are experimentally using 13.5 nm UV for extreme ultraviolet lithography.\n==== Polymers ====\nElectronic components that require clear transparency for light to exit or enter (photovoltaic panels and sensors) can be potted using acrylic resins that are cured using UV energy. The advantages are low VOC emissions and rapid curing.\nCertain inks, coatings, and adhesives are formulated with photoinitiators and resins. When exposed to UV light, polymerization occurs, and so the adhesives harden or cure, usually within a few seconds. Applications include glass and plastic bonding, optical fiber coatings, the coating of flooring, UV coating and paper finishes in offset printing, dental fillings, and decorative fingernail "gels".\nUV sources for UV curing applications include UV lamps, UV LEDs, and excimer flash lamps. Fast processes such as flexo or offset printing require high-intensity light focused via reflectors onto a moving substrate and medium so high-pressure Hg (mercury) or Fe (iron, doped)-based bulbs are used, energized with electric arcs or microwaves. Lower-power fluorescent lamps and LEDs can be used for static applications. Small high-pressure lamps can have light focused and transmitted to the work area via liquid-filled or fiber-optic light guides.\nThe impact of UV on polymers is used for modification of the (roughness and hydrophobicity) of polymer surfaces. For example, a poly(methyl methacrylate) surface can be smoothed by vacuum ultraviolet.\nUV radiation is useful in preparing low-surface-energy polymers for adhesives. Polymers exposed to UV will oxidize, thus raising the surface energy of the polymer. Once the surface energy of the polymer has been raised, the bond between the adhesive and the polymer is stronger.\n=== Biology-related uses ===\n==== Air purification ====\nUV-C light is used in air conditioning systems as a method of improving indoor air quality by disinfecting the air and preventing microbial growth. UV-C light is effective at killing or inactivating harmful microorganisms, such as bacteria, viruses, mold, and mildew. When integrated into an air conditioning system, the ultraviolet light is typically placed in areas like the air handler or near the evaporator coil.\nIn air conditioning systems, UV-C light works by irradiating the airflow within the system, killing or neutralizing harmful microorganisms before they are recirculated into the indoor environment. The effectiveness of it in air conditioning systems depends on factors such as the intensity of the light, the duration of exposure, airflow speed, and the cleanliness of system components.\nUsing a catalytic chemical reaction from titanium dioxide and UVC exposure, oxidation of organic matter converts pathogens, pollens, and mold spores into harmless inert byproducts. However, the reaction of titanium dioxide and UVC is not a straight path. Several hundreds of reactions occur prior to the inert byproducts stage and can hinder the resulting reaction creating formaldehyde, aldehyde, and other VOC\'s en route to a final stage. Thus, the use of titanium dioxide and UVC requires very specific parameters for a successful outcome. The cleansing mechanism of UV is a photochemical process. Contaminants in the indoor environment are almost entirely organic carbon-based compounds, which break down when exposed to high-intensity UV at 240 to 280 nm. Short-wave ultraviolet radiation can destroy DNA in living microorganisms. UVC\'s effectiveness is directly related to intensity and exposure time.\nUV has also been shown to reduce gaseous contaminants such as carbon monoxide and VOCs. UV lamps radiating at 184 and 254 nm can remove low concentrations of hydrocarbons and carbon monoxide if the air is recycled between the room and the lamp chamber. This arrangement prevents the introduction of ozone into the treated air. Likewise, air may be treated by passing by a single UV source operating at 184 nm and passed over iron pentaoxide to remove the ozone produced by the UV lamp.\n==== Sterilization and disinfection ====\nUltraviolet lamps are used to sterilize workspaces and tools used in biology laboratories and medical facilities. Commercially available low-pressure mercury-vapor lamps emit about 86% of their radiation at 254 nanometers (nm), with 265 nm being the peak germicidal effectiveness curve. UV at these germicidal wavelengths damage a microorganism\'s DNA/RNA so that it cannot reproduce, making it harmless, (even though the organism may not be killed). Since microorganisms can be shielded from ultraviolet rays in small cracks and other shaded areas, these lamps are used only as a supplement to other sterilization techniques.\nUVC LEDs are relatively new to the commercial market and are gaining in popularity. Due to their monochromatic nature (±5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', "==== Red-giant-branch phase ====\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star.  For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower 12C/13C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\nThe helium core continues to grow on the red-giant branch.  It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase.  The star increases in luminosity towards the tip of the red-giant branch.  Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n==== Horizontal branch ====\nIn the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 108 times the luminosity of the Sun for a few days and 1011 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning.  These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n==== Asymptotic-giant-branch phase ====\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time).  Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star.  Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically.  This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface.  This is known as the second dredge up, and in some stars there may even be a third dredge up.  In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra.  A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\nAnother well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths.  These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity.  These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n==== Post-AGB ====\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n=== Massive stars ===\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants.  Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.\n==== Supergiant evolution ====\nExtremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\nThe core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core.  In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process.  At the end of helium fusion, the core of a star consists primarily of carbon and oxygen.  In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium.  Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.\nThe exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉.  After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse.  Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning.  For a range of stars of approximately 8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova.", 'The photochemical properties of melanin make it an excellent photoprotectant. However, sunscreen chemicals cannot dissipate the energy of the excited state as efficiently as melanin and therefore, if sunscreen ingredients penetrate into the lower layers of the skin, the amount of reactive oxygen species may be increased. The amount of sunscreen that penetrates through the stratum corneum may or may not be large enough to cause damage.\nIn an experiment by Hanson et al. that was published in 2006, the amount of harmful reactive oxygen species (ROS) was measured in untreated and in sunscreen treated skin. In the first 20 minutes, the film of sunscreen had a protective effect and the number of ROS species was smaller. After 60 minutes, however, the amount of absorbed sunscreen was so high that the amount of ROS was higher in the sunscreen-treated skin than in the untreated skin. The study indicates that sunscreen must be reapplied within 2 hours in order to prevent UV light from penetrating to sunscreen-infused live skin cells.\n==== Aggravation of certain skin conditions ====\nUltraviolet radiation can aggravate several skin conditions and diseases, including systemic lupus erythematosus, Sjögren\'s syndrome, Sinear Usher syndrome, rosacea, dermatomyositis, Darier\'s disease, Kindler–Weary syndrome and Porokeratosis.\n==== Eye damage ====\nThe eye is most sensitive to damage by UV in the lower UVC band at 265–275 nm. Radiation of this wavelength is almost absent from sunlight at the surface of the Earth but is emitted by artificial sources such as the electrical arcs employed in arc welding. Unprotected exposure to these sources can cause "welder\'s flash" or "arc eye" (photokeratitis) and can lead to cataracts, pterygium and pinguecula formation. To a lesser extent, UVB in sunlight from 310 to 280 nm also causes photokeratitis ("snow blindness"), and the cornea, the lens, and the retina can be damaged.\nProtective eyewear is beneficial to those exposed to ultraviolet radiation. Since light can reach the eyes from the sides, full-coverage eye protection is usually warranted if there is an increased risk of exposure, as in high-altitude mountaineering. Mountaineers are exposed to higher-than-ordinary levels of UV radiation, both because there is less atmospheric filtering and because of reflection from snow and ice.\nOrdinary, untreated eyeglasses give some protection. Most plastic lenses give more protection than glass lenses, because, as noted above, glass is transparent to UVA and the common acrylic plastic used for lenses is less so. Some plastic lens materials, such as polycarbonate, inherently block most UV.\n== Degradation of polymers, pigments and dyes ==\nUV degradation is one form of polymer degradation that affects plastics exposed to sunlight. The problem appears as discoloration or fading, cracking, loss of strength or disintegration. The effects of attack increase with exposure time and sunlight intensity. The addition of UV absorbers inhibits the effect.\nSensitive polymers include thermoplastics and speciality fibers like aramids. UV absorption leads to chain degradation and loss of strength at sensitive points in the chain structure. Aramid rope must be shielded with a sheath of thermoplastic if it is to retain its strength.\nMany pigments and dyes absorb UV and change colour, so paintings and textiles may need extra protection both from sunlight and fluorescent lamps, two common sources of UV radiation. Window glass absorbs some harmful UV, but valuable artifacts need extra shielding. Many museums place black curtains over watercolour paintings and ancient textiles, for example. Since watercolours can have very low pigment levels, they need extra protection from UV. Various forms of picture framing glass, including acrylics (plexiglass), laminates, and coatings, offer different degrees of UV (and visible light) protection.\n== Applications ==\nBecause of its ability to cause chemical reactions and excite fluorescence in materials, ultraviolet radiation has a number of applications. The following table gives some uses of specific wavelength bands in the UV spectrum.\n13.5 nm: Extreme ultraviolet lithography\n30–200 nm: Photoionization, ultraviolet photoelectron spectroscopy, standard integrated circuit manufacture by photolithography\n230–365 nm: UV-ID, label tracking, barcodes\n230–400 nm: Optical sensors, various instrumentation\n240–280 nm: Disinfection, decontamination of surfaces and water (DNA absorption has a peak at 260 nm), germicidal lamps\n200–400 nm: Forensic analysis, drug detection\n270–360 nm: Protein analysis, DNA sequencing, drug discovery\n280–400 nm: Medical imaging of cells\n300–320 nm: Light therapy in medicine\n300–365 nm: Curing of polymers and printer inks\n350–370 nm: Bug zappers (flies are most attracted to light at 365 nm)\n=== Photography ===\nPhotographic film responds to ultraviolet radiation but the glass lenses of cameras usually block radiation shorter than 350 nm. Slightly yellow UV-blocking filters are often used for outdoor photography to prevent unwanted bluing and overexposure by UV rays. For photography in the near UV, special filters may be used. Photography with wavelengths shorter than 350 nm requires special quartz lenses which do not absorb the radiation.\nDigital cameras sensors may have internal filters that block UV to improve color rendition accuracy. Sometimes these internal filters can be removed, or they may be absent, and an external visible-light filter prepares the camera for near-UV photography. A few cameras are designed for use in the UV.\nPhotography by reflected ultraviolet radiation is useful for medical, scientific, and forensic investigations, in applications as widespread as detecting bruising of skin, alterations of documents, or restoration work on paintings. Photography of the fluorescence produced by ultraviolet illumination uses visible wavelengths of light.\nIn ultraviolet astronomy, measurements are used to discern the chemical composition of the interstellar medium, and the temperature and composition of stars. Because the ozone layer blocks many UV frequencies from reaching telescopes on the surface of the Earth, most UV observations are made from space.\n=== Electrical and electronics industry ===\nCorona discharge on electrical apparatus can be detected by its ultraviolet emissions. Corona causes degradation of electrical insulation and emission of ozone and nitrogen oxide.\nEPROMs (Erasable Programmable Read-Only Memory) are erased by exposure to UV radiation. These modules have a transparent (quartz) window on the top of the chip that allows the UV radiation in.\n=== Fluorescent dye uses ===\nColorless fluorescent dyes that emit blue light under UV are added as optical brighteners to paper and fabrics. The blue light emitted by these agents counteracts yellow tints that may be present and causes the colors and whites to appear whiter or more brightly colored.\nUV fluorescent dyes that glow in the primary colors are used in paints, papers, and textiles either to enhance color under daylight illumination or to provide special effects when lit with UV lamps. Blacklight paints that contain dyes that glow under UV are used in a number of art and aesthetic applications.\nAmusement parks often use UV lighting to fluoresce ride artwork and backdrops. This often has the side effect of causing rider\'s white clothing to glow light-purple.\nTo help prevent counterfeiting of currency, or forgery of important documents such as driver\'s licenses and passports, the paper may include a UV watermark or fluorescent multicolor fibers that are visible under ultraviolet light. Postage stamps are tagged with a phosphor that glows under UV rays to permit automatic detection of the stamp and facing of the letter.\nUV fluorescent dyes are used in many applications (for example, biochemistry and forensics). Some brands of pepper spray will leave an invisible chemical (UV dye) that is not easily washed off on a pepper-sprayed attacker, which would help police identify the attacker later.\nIn some types of nondestructive testing UV stimulates fluorescent dyes to highlight defects in a broad range of materials. These dyes may be carried into surface-breaking defects by capillary action (liquid penetrant inspection) or they may be bound to ferrite particles caught in magnetic leakage fields in ferrous materials (magnetic particle inspection).\n=== Analytic uses ===\n==== Forensics ====\nUV is an investigative tool at the crime scene helpful in locating and identifying bodily fluids such as semen, blood, and saliva. For example, ejaculated fluids or saliva can be detected by high-power UV sources, irrespective of the structure or colour of the surface the fluid is deposited upon. UV–vis microspectroscopy is also used to analyze trace evidence, such as textile fibers and paint chips, as well as questioned documents.\nOther applications include the authentication of various collectibles and art, and detecting counterfeit currency. Even materials not specially marked with UV sensitive dyes may have distinctive fluorescence under UV exposure or may fluoresce differently under short-wave versus long-wave ultraviolet.\n==== Enhancing contrast of ink ====\nUsing multi-spectral imaging it is possible to read illegible papyrus, such as the burned papyri of the Villa of the Papyri or of Oxyrhynchus, or the Archimedes palimpsest. The technique involves taking pictures of the illegible document using different filters in the infrared or ultraviolet range, finely tuned to capture certain wavelengths of light. Thus, the optimum spectral portion can be found for distinguishing ink from paper on the papyrus surface.\nSimple NUV sources can be used to highlight faded iron-based ink on vellum.\n==== Sanitary compliance ====', 'Photosynthesis', 'The light curves for type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. Their optical energy output is driven by radioactive decay of ejected nickel-56 (half-life 6 days), which then decays to radioactive cobalt-56 (half-life 77 days). These radioisotopes excite the surrounding material to incandescence. Modern studies of cosmology rely on 56Ni radioactivity providing the energy for the optical brightness of supernovae of type Ia, which are the "standard candles" of cosmology but whose diagnostic 847 keV and 1,238 keV gamma rays were first detected only in 2014. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half-life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission from the remaining cobalt-56 becomes dominant, although this portion of the light curve has been little-studied.\nType Ib and Ic light curves are similar to type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of the created nickel-56. The peak luminosity varies considerably and there are even occasional type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increased peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.\nThe light curves for type II supernovae are characterised by a much slower decline than type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial destruction this hydrogen becomes heated and ionised. The majority of type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in type I supernovae, due to the efficiency of conversion into light by all the hydrogen.\nIn type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a type I supernova and the hydrogen even disappears from the spectrum after several weeks.\nType IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a superluminous supernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.\nLarge numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.\nNotes:\n=== Asymmetry ===\nA long-standing puzzle surrounding type II supernovae is why the remaining compact object receives a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high peculiar velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500 km/s or greater. This indicates an expansion asymmetry, but the mechanism by which momentum is transferred to the compact object remains a puzzle. Proposed explanations for this kick include convection in the collapsing star, asymmetric ejection of matter during neutron star formation, and asymmetrical neutrino emissions.\nOne possible explanation for this asymmetry is large-scale convection above the core. The convection can create radial variations in density giving rise to variations in the amount of energy absorbed from neutrino outflow. However analysis of this mechanism predicts only modest momentum transfer. Another possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova. (A similar model is used for explaining long gamma-ray bursts.) The dominant mechanism may depend upon the mass of the progenitor star.\nInitial asymmetries have also been confirmed in type Ia supernovae through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the expansion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarisation of the emitted light.\n=== Energy output ===\nAlthough supernovae are primarily known as luminous events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.\nThere is a fundamental difference between the balance of energy production in the different types of supernova. In type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.\nStandard type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is 56Ni generated from silicon burning. 56Ni is radioactive and decays into 56Co by beta plus decay (with a half life of six days) and gamma rays. 56Co itself decays by the beta plus (positron) path with a half life of 77 days into stable 56Fe. These two processes are responsible for the electromagnetic radiation from type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.\nCore collapse supernovae are on average visually fainter than type Ia supernovae, but the total energy released is far higher, as outlined in the following table.\nIn some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high-luminosity supernovae and is thought to be the cause of type Ic hypernovae and long-duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low-luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.\nWhen a supernova occurs inside a small dense cloud of circumstellar material, it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause type IIn hypernovae.\nAlthough pair-instability supernovae are core collapse supernovae with spectra and light curves similar to type II-P, the nature after core collapse is more like that of a giant type Ia with runaway fusion of carbon, oxygen and silicon. The total energy released by the highest-mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.\n=== Progenitor ===', 'Ultraviolet catastrophe\n\nThe ultraviolet catastrophe, also called the Rayleigh–Jeans catastrophe, was the prediction of late 19th century and early 20th century classical physics that an ideal black body at thermal equilibrium would emit an unbounded quantity of energy as wavelength decreased into the ultraviolet range.:\u200a6–7\u200a The term "ultraviolet catastrophe" was first used in 1911 by the Austrian physicist Paul Ehrenfest, but the concept originated with the 1900 statistical derivation of the Rayleigh–Jeans law.\nThe phrase refers to the fact that the empirically derived Rayleigh–Jeans law, which accurately predicted experimental results at large wavelengths, failed to do so for short wavelengths. (See the image for further elaboration.)  As the theory diverged from empirical observations when these frequencies reached the ultraviolet region of the electromagnetic spectrum, there was a problem. This problem was later found to be due to a property of quanta as proposed by Max Planck: There could be no fraction of a discrete energy package already carrying minimal energy.\nSince the first use of this term, it has also been used for other predictions of a similar nature, as in quantum electrodynamics and such cases as ultraviolet divergence.\n== Problem ==\nThe Rayleigh-Jeans law is an approximation to the spectral radiance of electromagnetic radiation as a function of wavelength from a black body at a given temperature through classical arguments. For wavelength\n{\\displaystyle \\lambda }\n, it is:\n{\\displaystyle B_{\\lambda }(T)={\\frac {2ck_{\\mathrm {B} }T}{\\lambda ^{4}}},}\nwhere\n{\\displaystyle B_{\\lambda }}\nis the spectral radiance, the power emitted per unit emitting area, per steradian, per unit wavelength;\n{\\displaystyle c}\nis the speed of light;\n{\\displaystyle k_{\\mathrm {B} }}\nis the Boltzmann constant; and\n{\\displaystyle T}\nis the temperature in kelvins.  For frequency\n{\\displaystyle \\nu }\n, the expression is instead\n{\\displaystyle B_{\\nu }(T)={\\frac {2\\nu ^{2}k_{\\mathrm {B} }T}{c^{2}}}.}\nThis formula is obtained from the equipartition theorem of classical statistical mechanics which states that all harmonic oscillator modes (degrees of freedom) of a system at equilibrium have an average energy of\n{\\displaystyle k_{\\rm {B}}T}\nThe "ultraviolet catastrophe" is the expression of the fact that the formula misbehaves at higher frequencies; it predicts infinite energy emission because\n{\\displaystyle B_{\\nu }(T)\\to \\infty }\nas\n{\\displaystyle \\nu \\to \\infty }\nAn example, from Mason\'s A History of the Sciences, illustrates multi-mode vibration via a piece of string. As a natural vibrator, the string will oscillate with specific modes (the standing waves of a string in harmonic resonance), dependent on the length of the string. In classical physics, a radiator of energy will act as a natural vibrator. Since each mode will have the same energy, most of the energy in a natural vibrator will be in the smaller wavelengths and higher frequencies, where most of the modes are.\nAccording to classical electromagnetism, the number of electromagnetic modes in a 3-dimensional cavity, per unit frequency, is proportional to the square of the frequency. This implies that the radiated power per unit frequency should be proportional to frequency squared. Thus, both the power at a given frequency and the total radiated power is unlimited as higher and higher frequencies are considered:  this is unphysical, as the total radiated power of a cavity is not observed to be infinite, a point that was made independently by Einstein, Lord Rayleigh, and Sir James Jeans in 1905.\n== Solution ==\nIn 1900, Max Planck derived the correct form for the intensity spectral distribution function by making some assumptions that were strange for the time. In particular, Planck assumed that electromagnetic radiation can be emitted or absorbed only in discrete packets, called quanta, of energy:\nquanta\n{\\displaystyle E_{\\text{quanta}}=h\\nu =h{\\frac {c}{\\lambda }},}\nwhere:\nh is the Planck constant,\nν is the frequency of light,\nc is the speed of light,\nλ is the wavelength of light.\nBy applying this new energy to the partition function in statistical mechanics, Planck\'s assumptions led to the correct form of the spectral distribution functions:\nexp\n{\\displaystyle B_{\\lambda }(\\lambda ,T)={\\frac {2hc^{2}}{\\lambda ^{5}}}{\\frac {1}{\\exp \\left({\\frac {hc}{\\lambda k_{\\mathrm {B} }T}}\\right)-1}}}\nwhere:\nT is the absolute temperature of the body,\nkB is the Boltzmann constant,\nexp denotes the exponential function.\nIn 1905, Albert Einstein solved the problem physically by postulating that Planck\'s quanta were real physical particles – what we now call photons, not just a mathematical fiction. They modified statistical mechanics in the style of Boltzmann to an ensemble of photons. Einstein\'s photon had an energy proportional to its frequency and also explained an unpublished law of Stokes and the photoelectric effect.  This published postulate was specifically cited by the Nobel Prize in Physics committee in their decision to award the prize for 1921 to Einstein.\n== See also ==\nWien approximation\nVacuum catastrophe\nPlanckian locus\n== References ==\n=== Bibliography ===\n== Further reading ==\nKroemer, Herbert; Kittel, Charles (1980). "Chapter 4". Thermal Physics (2 ed.). W. H. Freeman Company. ISBN 0-7167-1088-9.\nCohen-Tannoudji, Claude; Diu, Bernard; Laloë; Franck (1977). Quantum Mechanics: Volume One. Hermann, Paris. pp. 624–626. ISBN 0-471-16433-X.']

Question: What is the "ultraviolet catastrophe"?

Choices:
Choice A) It is a phenomenon that occurs only in multi-mode vibration.
Choice B) It is the misbehavior of a formula for higher frequencies.
Choice C) It is the standing wave of a string in harmonic resonance.
Choice D) It is a flaw in classical physics that results in the misallocation of energy.
Choice E) It is a disproven theory about the distribution of electromagnetic radiation.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Carnot heat engine', 'Carnot cycle', '=== Efficiency of real heat engines ===\nCarnot realized that, in reality, it is not possible to build a thermodynamically reversible engine.  So, real heat engines are even less efficient than indicated by Equation 3. In addition, real engines that operate along the Carnot cycle style (isothermal expansion / isentropic expansion / isothermal compression / isentropic compression) are rare. Nevertheless, Equation 3 is extremely useful for determining the maximum efficiency that could ever be expected for a given set of thermal reservoirs.\nAlthough Carnot\'s cycle is an idealization, Equation 3 as the expression of the Carnot efficiency is still useful. Consider the average temperatures,\nin\n{\\displaystyle \\langle T_{H}\\rangle ={\\frac {1}{\\Delta S}}\\int _{Q_{\\text{in}}}TdS}\nout\n{\\displaystyle \\langle T_{C}\\rangle ={\\frac {1}{\\Delta S}}\\int _{Q_{\\text{out}}}TdS}\nat which the first integral is over a part of a cycle where heat goes into the system and the second integral is over a cycle part where heat goes out from the system. Then, replace TH and TC in Equation 3 by ⟨TH⟩ and ⟨TC⟩, respectively, to estimate the efficiency a heat engine.\nFor the Carnot cycle, or its equivalent, the average value ⟨TH⟩ will equal the highest temperature available, namely TH, and ⟨TC⟩ the lowest, namely TC. For other less efficient thermodynamic cycles, ⟨TH⟩ will be lower than TH, and ⟨TC⟩ will be higher than TC. This can help illustrate, for example, why a reheater or a regenerator can improve the thermal efficiency of steam power plants and why the thermal efficiency of combined-cycle power plants (which incorporate gas turbines operating at even higher temperatures) exceeds that of conventional steam plants. The first prototype of the diesel engine was based on the principles of the Carnot cycle.\n== As a macroscopic construct ==\nThe Carnot heat engine is, ultimately, a theoretical construct based on an idealized thermodynamic system.  On a practical human-scale level the Carnot cycle has proven a valuable model, as in advancing the development of the diesel engine.  However, on a macroscopic scale limitations placed by the model\'s assumptions prove it impractical, and, ultimately, incapable of doing any work. As such, per Carnot\'s theorem, the Carnot engine may be thought as the theoretical limit of macroscopic scale heat engines rather than any practical device that could ever be built.\n== See also ==\nCarnot heat engine\nReversible process (thermodynamics)\n== References ==\nNotes\nSources\nCarnot, Sadi, Reflections on the Motive Power of Fire\nEwing, J. A. (1910) The Steam-Engine and Other Engines edition 3, page 62, via Internet Archive\nFeynman, Richard P.; Leighton, Robert B.; Sands, Matthew (1963). The Feynman Lectures on Physics. Addison-Wesley Publishing Company. pp. Chapter 44. ISBN 978-0-201-02116-5. {{cite book}}: ISBN / Date incompatibility (help)\nHalliday, David; Resnick, Robert (1978). Physics (3rd ed.). John Wiley & Sons. pp. 541–548. ISBN 978-0-471-02456-9.\nKittel, Charles; Kroemer, Herbert (1980). Thermal Physics (2nd ed.). W. H. Freeman Company. ISBN 978-0-7167-1088-2.\nKostic, M (2011). "Revisiting The Second Law of Energy Degradation and Entropy Generation: From Sadi Carnot\'s Ingenious Reasoning to Holistic Generalization". AIP Conf. Proc. AIP Conference Proceedings. 1411 (1): 327–350. Bibcode:2011AIPC.1411..327K. CiteSeerX 10.1.1.405.1945. doi:10.1063/1.3665247. American Institute of Physics, 2011. ISBN 978-0-7354-0985-9. Abstract at: [1]. Full article (24 pages [2]), also at [3].\n== External links ==\nHyperphysics article on the Carnot cycle.\nS. M. Blinder Carnot Cycle on Ideal Gas powered by Wolfram Mathematica', '{\\displaystyle T_{\\text{C}}}\nWe have only considered the magnitude of the entropy change here. Since the total change of entropy of the fluid system for the cyclic process is 0, we must have\nThe previous three equations, namely (3), (4), (5), substituted into (6) to give:\nFor  [ΔSh ≥ (Qh/Th)] +[ΔSc ≥ (Qc/Tc)] = 0\n[ΔSh ≥ (Qh/Th)] = - [ΔSc ≥ (Qc/Tc)]\n= [-ΔSc ≤ (-Qc/Tc)]\nit is at least (Qh/Th) ≤ (-Qc/Tc)\nEquations (2) and (7) combine to give\nTo derive this step needs two adiabatic processes involved to show an isentropic process property for the ratio of the changing volumes of two isothermal processes are equal.\nMost importantly, since the two adiabatic processes are volume works without heat lost, and since the ratio of volume changes for this two processes are the same, so the works for these two adiabatic processes are the same with opposite direction to each other, namely, one direction is work done by the system and the other is work done on the system; therefore, heat efficiency only concerns the amount of work done by the heat absorbed comparing to the amount of heat absorbed by the system.\nTherefore, (W/Qh) = (Qh - Qc) / Qh\n= 1 - (Qc/Qh)\n= 1 - (Tc/Th)\nAnd, from (7)\n(Qh/Th) ≤ (-Qc/Tc)                      here Qc it is less than 0 (release heat)\n(Tc/Th) ≤ (-Qc/Qh)\n-(Tc/Th) ≥ (Qc/Qh)\n1+[-(Tc/Th)] ≥ 1+(Qc/Qh)\n1 - (Tc/Th) ≥ (Qh + Qc)/Qh         here Qc<0,\n1 - (Tc/Th) ≥ (Qh - Qc)/Qh\n1 - (Tc/Th) ≥ W/Qh\nHence,\nwhere\n{\\displaystyle \\eta ={\\frac {W}{Q_{\\text{H}}}}}\nis the efficiency of the real engine, and\n{\\displaystyle \\eta _{\\text{I}}}\nis the efficiency of the Carnot engine working between the same two reservoirs at the temperatures\n{\\displaystyle T_{\\text{H}}}\nand\n{\\displaystyle T_{\\text{C}}}\n. For the Carnot engine, the entire process is \'reversible\', and Equation (7) is an equality. Hence, the efficiency of the real engine is always less than the ideal Carnot engine.\nEquation (7) signifies that the total entropy of system and surroundings (the fluid and the two reservoirs) increases for the real engine, because (in a surroundings-based analysis) the entropy gain of the cold reservoir as\n{\\displaystyle Q_{\\text{C}}}\nflows into it at the fixed temperature\n{\\displaystyle T_{\\text{C}}}\n, is greater than the entropy loss of the hot reservoir as\n{\\displaystyle Q_{\\text{H}}}\nleaves it at its fixed temperature\n{\\displaystyle T_{\\text{H}}}\n. The inequality in Equation (7) is essentially the statement of the Clausius theorem.\nAccording to the second theorem, "The efficiency of the Carnot engine is independent of the nature of the working substance".\n== The Carnot engine and Rudolf Diesel ==\nIn 1892 Rudolf Diesel patented an internal combustion engine inspired by the Carnot engine. Diesel knew a Carnot engine is an ideal that cannot be built, but he thought he had invented a working approximation. His principle was unsound, but in his struggle to implement it he developed a practical Diesel engine.\nThe conceptual problem was how to achieve isothermal expansion in an internal combustion engine, since burning fuel at the highest temperature of the cycle would only raise the temperature further. Diesel\'s patented solution was: having achieved the highest temperature just by compressing the air, to add a small amount of fuel at a controlled rate, such that heating caused by burning the fuel would be counteracted by cooling caused by air expansion as the piston moved. Hence all the heat from the fuel would be transformed into work during the isothermal expansion, as required by Carnot\'s theorem.\nFor the idea to work a small mass of fuel would have to be burnt in a huge mass of air. Diesel first proposed a working engine that would compress air to 250 atmospheres at 800 °C (1,450 °F), then cycle to one atmosphere at 20 °C (50 °F). However, this was well beyond the technological capabilities of the day, since it implied a compression ratio of 60:1. Such an engine, if it could have been built, would have had an efficiency of 73%. (In contrast, the best steam engines of his day achieved 7%.)\nAccordingly, Diesel sought to compromise. He calculated that, were he to reduce the peak pressure to a less ambitious 90 atmospheres, he would sacrifice only 5% of the thermal efficiency. Seeking financial support, he published the "Theory and Construction of a Rational Heat Engine to Take the Place of the Steam Engine and All Presently Known Combustion Engines" (1893). Endorsed by scientific opinion, including Lord Kelvin, he won the backing of Krupp and Maschinenfabrik Augsburg. He clung to the Carnot cycle as a symbol. But years of practical work failed to achieve an isothermal combustion engine, nor could have done, since it requires such an enormous quantity of air that it cannot develop enough power to compress it. Furthermore, controlled fuel injection turned out to be no easy matter.\nEven so, the Diesel engine slowly evolved over 25 years to become a practical high-compression air engine, its fuel injected near the end of the compression stroke and ignited by the heat of compression, capable by 1969 of 40% efficiency.\n== As a macroscopic construct ==\nThe Carnot heat engine is, ultimately, a theoretical construct based on an idealized thermodynamic system.  On a practical human-scale level the Carnot cycle has proven a valuable model, as in advancing the development of the diesel engine.  However, on a macroscopic scale limitations placed by the model\'s assumptions prove it impractical, and, ultimately, incapable of doing any work. As such, per Carnot\'s theorem, the Carnot engine may be thought as the theoretical limit of macroscopic scale heat engines rather than any practical device that could ever be built.\nFor example, for the isothermal expansion part of the Carnot cycle, the following\ninfinitesimal conditions must be satisfied simultaneously at every step in the expansion:\nThe hot reservoir temperature TH is infinitesimally higher than the system gas temperature T so heat flow (energy transfer) from the hot reservoir to the gas is made without increasing T (via infinitesimal work on the surroundings by the gas as another energy transfer); if TH is significantly higher than T, then T may be not uniform through the gas so the system would deviate from thermal equilibrium as well as not being a reversible process (i.e. not a Carnot cycle) or T might increase noticeably so it would not be an isothermal process.\nThe force externally applied on the piston (opposite to the internal force on the piston by the gas) needs to be infinitesimally reduced externally. Without this assistance, it would not be possible to follow a gas PV (Pressure-Volume) curve downward at a constant T since following this curve means that the gas-to-piston force decreases (P decreases) as the volume expands (the piston moves outward). If this assistance is so strong that the volume expansion is significant, the system may deviate from thermal equilibrium, and the process fail to be reversible (and thus not a Carnot cycle).\nSuch "infinitesimal" requirements as these (and others) cause the Carnot cycle to take an infinite amount of time, rendering the production of work impossible.\nOther practical requirements that make the Carnot cycle impractical to realize include fine control of the gas, and perfect thermal contact with the surroundings (including high and low temperature reservoirs).\n== Notes ==\n== External links ==\nEpisode 46. Engine of Nature: The Carnot engine, part one, beginning with simple steam engines. The Mechanical Universe. Caltech – via YouTube.\n== References ==\nBryant, Lynwood (August 1969). "Rudolf Diesel and His Rational Engine". Scientific American. 221 (2): 108–117. Bibcode:1969SciAm.221b.108B. doi:10.1038/scientificamerican0869-108. JSTOR 24926442.\nCarnot, Sadi (1824). Réflexions sur la puissance motrice du feu et sur les machines propres à développer cette puissance (in French). Paris: Bachelier. (First Edition 1824) and (Reissued Edition of 1878)\nCarnot, Sadi (1890). Thurston, Robert Henry (ed.). Reflections on the Motive Power of Heat and on Machines Fitted to Develop That Power. New York: J. Wiley & Sons. (full text of 1897 ed.) (Archived HTML version)', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'These pigments are embedded in plants and algae in complexes called antenna proteins. In such proteins, the pigments are arranged to work together. Such a combination of proteins is also called a light-harvesting complex.\nAlthough all cells in the green parts of a plant have chloroplasts, the majority of those are found in specially adapted structures called leaves. Certain species adapted to conditions of strong sunlight and aridity, such as many Euphorbia and cactus species, have their main photosynthetic organs in their stems. The cells in the interior tissues of a leaf, called the mesophyll, can contain between 450,000 and 800,000 chloroplasts for every square millimeter of leaf. The surface of the leaf is coated with a water-resistant waxy cuticle that protects the leaf from excessive evaporation of water and decreases the absorption of ultraviolet or blue light to minimize heating. The transparent epidermis layer allows light to pass through to the palisade mesophyll cells where most of the photosynthesis takes place.\n== Light-dependent reactions ==\nIn the light-dependent reactions, one molecule of the pigment chlorophyll absorbs one photon and loses one electron. This electron is taken up by a modified form of chlorophyll called pheophytin, which passes the electron to a quinone molecule, starting the flow of electrons down an electron transport chain that leads to the ultimate reduction of NADP to NADPH. In addition, this creates a proton gradient (energy gradient) across the chloroplast membrane, which is used by ATP synthase in the synthesis of ATP. The chlorophyll molecule ultimately regains the electron it lost when a water molecule is split in a process called photolysis, which releases oxygen.\nThe overall equation for the light-dependent reactions under the conditions of non-cyclic electron flow in green plants is:\nNot all wavelengths of light can support photosynthesis. The photosynthetic action spectrum depends on the type of accessory pigments present. For example, in green plants, the action spectrum resembles the absorption spectrum for chlorophylls and carotenoids with absorption peaks in violet-blue and red light. In red algae, the action spectrum is blue-green light, which allows these algae to use the blue end of the spectrum to grow in the deeper waters that filter out the longer wavelengths (red light) used by above-ground green plants. The non-absorbed part of the light spectrum is what gives photosynthetic organisms their color (e.g., green plants, red algae, purple bacteria) and is the least effective for photosynthesis in the respective organisms.\n=== Z scheme ===\nIn plants, light-dependent reactions occur in the thylakoid membranes of the chloroplasts where they drive the synthesis of ATP and NADPH. The light-dependent reactions are of two forms: cyclic and non-cyclic.\nIn the non-cyclic reaction, the photons are captured in the light-harvesting antenna complexes of photosystem II by chlorophyll and other accessory pigments (see diagram "Z-scheme"). The absorption of a photon by the antenna complex loosens an electron by a process called photoinduced charge separation. The antenna system is at the core of the chlorophyll molecule of the photosystem II reaction center. That loosened electron is taken up by the primary electron-acceptor molecule, pheophytin. As the electrons are shuttled through an electron transport chain (the so-called Z-scheme shown in the diagram), a chemiosmotic potential is generated by pumping proton cations (H+) across the membrane and into the thylakoid space. An ATP synthase enzyme uses that chemiosmotic potential to make ATP during photophosphorylation, whereas NADPH is a product of the terminal redox reaction in the Z-scheme. The electron enters a chlorophyll molecule in Photosystem I. There it is further excited by the light absorbed by that photosystem. The electron is then passed along a chain of electron acceptors to which it transfers some of its energy. The energy delivered to the electron acceptors is used to move hydrogen ions across the thylakoid membrane into the lumen. The electron is eventually used to reduce the coenzyme NADP with an H+ to NADPH (which has functions in the light-independent reaction); at that point, the path of that electron ends.\nThe cyclic reaction is similar to that of the non-cyclic but differs in that it generates only ATP, and no reduced NADP (NADPH) is created. The cyclic reaction takes place only at photosystem I. Once the electron is displaced from the photosystem, the electron is passed down the electron acceptor molecules and returns to photosystem I, from where it was emitted, hence the name cyclic reaction.\n=== Water photolysis ===\nLinear electron transport through a photosystem will leave the reaction center of that photosystem oxidized. Elevating another electron will first require re-reduction of the reaction center. The excited electrons lost from the reaction center (P700) of photosystem I are replaced by transfer from plastocyanin, whose electrons come from electron transport through photosystem II. Photosystem II, as the first step of the Z-scheme, requires an external source of electrons to reduce its oxidized chlorophyll a reaction center. The source of electrons for photosynthesis in green plants and cyanobacteria is water. Two water molecules are oxidized by the energy of four successive charge-separation reactions of photosystem II to yield a molecule of diatomic oxygen and four hydrogen ions. The electrons yielded are transferred to a redox-active tyrosine residue that is oxidized by the energy of P680+. This resets the ability of P680 to absorb another photon and release another photo-dissociated electron. The oxidation of water is catalyzed in photosystem II by a redox-active structure that contains four manganese ions and a calcium ion; this oxygen-evolving complex binds two water molecules and contains the four oxidizing equivalents that are used to drive the water-oxidizing reaction (Kok\'s S-state diagrams). The hydrogen ions are released in the thylakoid lumen and therefore contribute to the transmembrane chemiosmotic potential that leads to ATP synthesis. Oxygen is a waste product of light-dependent reactions, but the majority of organisms on Earth use oxygen and its energy for cellular respiration, including photosynthetic organisms.\n== Light-independent reactions ==\n=== Calvin cycle ===\nIn the light-independent (or "dark") reactions, the enzyme RuBisCO captures CO2 from the atmosphere and, in a process called the Calvin cycle, uses the newly formed NADPH and releases three-carbon sugars, which are later combined to form sucrose and starch. The overall equation for the light-independent reactions in green plants is:\u200a128\nCarbon fixation produces the three-carbon sugar intermediate, which is then converted into the final carbohydrate products. The simple carbon sugars photosynthesis produces are then used to form other organic compounds, such as the building material cellulose, the precursors for lipid and amino acid biosynthesis, or as a fuel in cellular respiration. The latter occurs not only in plants but also in animals when the carbon and energy from plants is passed through a food chain.\nThe fixation or reduction of carbon dioxide is a process in which carbon dioxide combines with a five-carbon sugar, ribulose 1,5-bisphosphate, to yield two molecules of a three-carbon compound, glycerate 3-phosphate, also known as 3-phosphoglycerate. Glycerate 3-phosphate, in the presence of ATP and NADPH produced during the light-dependent stages, is reduced to glyceraldehyde 3-phosphate. This product is also referred to as 3-phosphoglyceraldehyde (PGAL) or, more generically, as triose phosphate. Most (five out of six molecules) of the glyceraldehyde 3-phosphate produced are used to regenerate ribulose 1,5-bisphosphate so the process can continue. The triose phosphates not thus "recycled" often condense to form hexose phosphates, which ultimately yield sucrose, starch, and cellulose, as well as glucose and fructose. The sugars produced during carbon metabolism yield carbon skeletons that can be used for other metabolic reactions like the production of amino acids and lipids.\n=== Carbon concentrating mechanisms ===\n==== On land ====\nIn hot and dry conditions, plants close their stomata to prevent water loss. Under these conditions, CO2 will decrease and oxygen gas, produced by the light reactions of photosynthesis, will increase, causing an increase of photorespiration by the oxygenase activity of ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) and decrease in carbon fixation. Some plants have evolved mechanisms to increase the CO2 concentration in the leaves under these conditions.', '=== Carbon dioxide levels and photorespiration ===\nAs carbon dioxide concentrations rise, the rate at which sugars are made by the light-independent reactions increases until limited by other factors. RuBisCO, the enzyme that captures carbon dioxide in the light-independent reactions, has a binding affinity for both carbon dioxide and oxygen. When the concentration of carbon dioxide is high, RuBisCO will fix carbon dioxide. However, if the carbon dioxide concentration is low, RuBisCO will bind oxygen instead of carbon dioxide. This process, called photorespiration, uses energy, but does not produce sugars.\nRuBisCO oxygenase activity is disadvantageous to plants for several reasons:\nOne product of oxygenase activity is phosphoglycolate (2 carbon) instead of 3-phosphoglycerate (3 carbon). Phosphoglycolate cannot be metabolized by the Calvin-Benson cycle and represents carbon lost from the cycle. A high oxygenase activity, therefore, drains the sugars that are required to recycle ribulose 5-bisphosphate and for the continuation of the Calvin-Benson cycle.\nPhosphoglycolate is quickly metabolized to glycolate that is toxic to a plant at a high concentration; it inhibits photosynthesis.\nSalvaging glycolate is an energetically expensive process that uses the glycolate pathway, and only 75% of the carbon is returned to the Calvin-Benson cycle as 3-phosphoglycerate. The reactions also produce ammonia (NH3), which is able to diffuse out of the plant, leading to a loss of nitrogen.\nA highly simplified summary is:\n2 glycolate + ATP → 3-phosphoglycerate + carbon dioxide + ADP + NH3\nThe salvaging pathway for the products of RuBisCO oxygenase activity is more commonly known as photorespiration, since it is characterized by light-dependent oxygen consumption and the release of carbon dioxide.\n== See also ==\n== References ==\n== Further reading ==\n=== Books ===\n=== Papers ===\n== External links ==\nA collection of photosynthesis pages for all levels from a renowned expert (Govindjee)\nIn depth, advanced treatment of photosynthesis, also from Govindjee\nScience Aid: Photosynthesis Article appropriate for high school science\nMetabolism, Cellular Respiration and Photosynthesis – The Virtual Library of Biochemistry and Cell Biology\nOverall examination of Photosynthesis at an intermediate level\nOverall Energetics of Photosynthesis\nThe source of oxygen produced by photosynthesis Interactive animation, a textbook tutorial\nMarshall J (2011-03-29). "First practical artificial leaf makes debut". Discovery News. Archived from the original on 2012-03-22. Retrieved 2011-03-29.\nPhotosynthesis – Light Dependent & Light Independent Stages Archived 2011-09-10 at the Wayback Machine\nKhan Academy, video introduction', "In more massive stars, the fusion of neon proceeds without a runaway deflagration.  This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements.  Surrounding the core are shells of lighter elements still undergoing fusion.  The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged.  The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope.  The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars.  Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself.  The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.\n==== Supernova ====\nWhen the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole.  Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\n== Stellar remnants ==\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n=== White and black dwarfs ===\nFor a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years.\nThe chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n=== Neutron stars ===\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n=== Black holes ===\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉.", "==== Red-giant-branch phase ====\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star.  For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower 12C/13C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\nThe helium core continues to grow on the red-giant branch.  It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase.  The star increases in luminosity towards the tip of the red-giant branch.  Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n==== Horizontal branch ====\nIn the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 108 times the luminosity of the Sun for a few days and 1011 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning.  These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n==== Asymptotic-giant-branch phase ====\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time).  Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star.  Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically.  This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface.  This is known as the second dredge up, and in some stars there may even be a third dredge up.  In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra.  A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\nAnother well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths.  These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity.  These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n==== Post-AGB ====\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n=== Massive stars ===\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants.  Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.\n==== Supergiant evolution ====\nExtremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\nThe core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core.  In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process.  At the end of helium fusion, the core of a star consists primarily of carbon and oxygen.  In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium.  Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.\nThe exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉.  After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse.  Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning.  For a range of stars of approximately 8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova.", 'Photosynthesis']

Question: What is the Carnot engine?

Choices:
Choice A) The Carnot engine is a theoretical engine that operates in the limiting mode of extreme speed known as dynamic. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.
Choice B) The Carnot engine is an ideal heat engine that operates in the limiting mode of extreme slowness known as quasi-static. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.
Choice C) The Carnot engine is a real heat engine that operates in the limiting mode of extreme speed known as dynamic. It represents the theoretical minimum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.
Choice D) The Carnot engine is a theoretical engine that operates in the limiting mode of extreme slowness known as quasi-static. It represents the theoretical minimum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.
Choice E) The Carnot engine is a real engine that operates in the limiting mode of extreme slowness known as quasi-static. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Nuclear fusion', 'Brumfiel, Geoff (22 May 2006). "Chaos could keep fusion under control". Nature. doi:10.1038/news060522-2. ISSN 0028-0836. S2CID 62598131.\nBussard, Robert (8 October 2007) [9 November 2006]. Should Google Go Nuclear? Clean, cheap, nuclear power... (Video). Google TechTalks – via YouTube.\nWenisch, Antonia; Kromp, Richard; Reinberger, David (November 2007). "Science or Fiction: Is there a Future for Nuclear?" (PDF). Austrian Institute of Ecology. Archived from the original (PDF) on 26 January 2021. Retrieved 8 October 2008.\nKikuchi, Mitsuru; Lackner, Karl & Tran, M. Q. (2012). Fusion physics. Publication / Division of Scientific and Technical Information, International Atomic Energy Agency. Vienna: International Atomic Energy Agency. p. 22. ISBN 978-92-0-130410-0. Archived from the original on 8 December 2015. Retrieved 8 December 2015.\nJanev, R. K., ed. (1995). Atomic and Molecular Processes in Fusion Edge Plasmas. Boston, MA: Springer US. doi:10.1007/978-1-4757-9319-2. ISBN 978-1-4757-9321-5. Archived from the original on 16 January 2023. Retrieved 16 January 2023.\nIliadis, Christian (2015). Nuclear Physics of Stars, 2nd ed. Weinheim: Wiley-VCH. doi:10.1002/9783527692668. ISBN 9783527692668.\n== External links ==\nNuclearFiles.org – A repository of documents related to nuclear power.\nAnnotated bibliography for nuclear fusion from the Alsos Digital Library for Nuclear Issues\nNRL Fusion Formulary Archived 26 October 2020 at the Wayback Machine\n"FusionWiki".', 'Fusion powers stars and produces most elements lighter than cobalt in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 616 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.645% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and the result is fusion; this is an exothermic process.\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is 13.6 eV—less than one-millionth of the 17.6 MeV released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though individual fission reactions are generally much more energetic than individual fusion ones, which are themselves millions of times more energetic than chemical reactions. Via the mass–energy equivalence, fusion yields a 0.7% efficiency of reactant mass into energy. This can be only be exceeded by the extreme cases of the accretion process involving neutron stars or black holes, approaching 40% efficiency, and antimatter annihilation at 100% efficiency. (The complete conversion of one gram of matter would expel 9×1013 joules of energy.)\n== In astrophysics ==\nFusion is responsible for the astrophysical production of the majority of elements lighter than iron. This includes most types of Big Bang nucleosynthesis and stellar nucleosynthesis. Non-fusion processes that contribute include the s-process and r-process in neutron merger and supernova nucleosynthesis, responsible for elements heavier than iron.\n=== Stars ===\nAn important fusion process is the stellar nucleosynthesis that powers stars, including the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounts for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was unknown; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein\'s equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy had not yet been discovered, nor even that stars are largely composed of hydrogen (see metallicity). Eddington\'s paper reasoned that:\nThe leading theory of stellar energy, the contraction hypothesis, should cause the rotation of a star to visibly speed up due to conservation of angular momentum. But observations of Cepheid variable stars showed this was not happening.\nThe only other known plausible source of energy was conversion of matter to energy; Einstein had shown some years earlier that a small amount of matter was equivalent to a large amount of energy.\nFrancis Aston had also recently shown that the mass of a helium atom was about 0.8% less than the mass of the four hydrogen atoms which would, combined, form a helium atom (according to the then-prevailing theory of atomic structure which held atomic weight to be the distinguishing property between elements; work by Henry Moseley and Antonius van den Broek would later show that nucleic charge was the distinguishing property and that a helium nucleus, therefore, consisted of two hydrogen nuclei plus additional mass). This suggested that if such a combination could happen, it would release considerable energy as a byproduct.\nIf a star contained just 5% of fusible hydrogen, it would suffice to explain how stars got their energy. (It is now known that most \'ordinary\' stars are usually made of around 70% to 75% hydrogen)\nFurther elements might also be fused, and other scientists had speculated that stars were the "crucible" in which light elements combined to create heavy elements, but without more accurate measurements of their atomic masses nothing more could be said at the time.\nAll of these speculations were proven correct in the following decades.\nThe primary source of solar energy, and that of similar size stars, is the fusion of hydrogen to form helium (the proton–proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to fuse heavier elements. In massive cores, silicon-burning is the final fusion cycle, leading to a build-up of iron and nickel nuclei.\nNuclear binding energy makes the production of elements heavier than nickel via fusion energetically unfavorable. These elements are produced in non-fusion processes: the s-process, r-process, and the variety of processes that can produce p-nuclei. Such processes occur in giant star shells, or supernovae, or neutron star mergers.\n=== Brown dwarfs ===\nBrown dwarfs fuse deuterium and in very high mass cases also fuse lithium.\n=== White dwarfs ===\nCarbon-oxygen white dwarfs, which accrete matter either from an active stellar companion or white dwarf merger, approach the Chandrasekhar limit of 1.44 solar masses. Immediately prior, carbon burning fusion begins, destroying the Earth-sized dwarf within one second, in a Type Ia supernova.\nMuch more rarely, helium white dwarfs may merge, which does not cause an explosion but begins helium burning in an extreme type of helium star.\n=== Neutron stars ===\nSome neutron stars accrete hydrogen and helium from an active stellar companion. Periodically, the helium accretion reaches a critical level, and a thermonuclear burn wave propagates across the surface, on the timescale of one second.\n=== Black hole accretion disks ===\nSimilar to stellar fusion, extreme conditions within black hole accretion disks can allow fusion reactions. Calculations show the most energetic reactions occur around lower stellar mass black holes, below 10 solar masses, compared to those above 100. Beyond five Schwarzschild radii, carbon-burning and fusion of helium-3 dominates the reactions. Within this distance, around lower mass black holes, fusion of nitrogen, oxygen, neon, and magnesium can occur. In the extreme limit, the silicon-burning process can begin with the fusion of silicon and selenium nuclei.\n=== Big Bang ===\nFrom the period approximately 10 seconds to 20 minutes after the Big Bang, the universe cooled from over 100 keV to 1 keV. This allowed the combination of protons and neutrons in deuterium nuclei, and beginning a rapid fusion chain into tritium and helium-3 and ending in predominantly helium-4, with a minimal fraction of lithium, beryllium, and boron nuclei.\n== Requirements ==\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbors due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface-area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.', 'UVC LEDs are relatively new to the commercial market and are gaining in popularity. Due to their monochromatic nature (±5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.\nDisinfection using UV radiation is commonly used in wastewater treatment applications and is finding an increased usage in municipal drinking water treatment. Many bottlers of spring water use UV disinfection equipment to sterilize their water. Solar water disinfection has been researched for cheaply treating contaminated water using natural sunlight. The UVA irradiation and increased water temperature kill organisms in the water.\nUltraviolet radiation is used in several food processes to kill unwanted microorganisms. UV can be used to pasteurize fruit juices by flowing the juice over a high-intensity ultraviolet source. The effectiveness of such a process depends on the UV absorbance of the juice.\nPulsed light (PL) is a technique of killing microorganisms on surfaces using pulses of an intense broad spectrum, rich in UVC between 200 and 280 nm. Pulsed light works with xenon flash lamps that can produce flashes several times per second. Disinfection robots use pulsed UV.\nThe antimicrobial effectiveness of filtered far-UVC (222\u2009nm) light on a range of pathogens, including bacteria and fungi showed inhibition of pathogen growth, and since it has lesser harmful effects, it provides essential insights for reliable disinfection in healthcare settings, such as hospitals and long-term care homes. UVC has also been shown to be effective at degrading SARS-CoV-2 virus.\n==== Biological ====\nSome animals, including birds, reptiles, and insects such as bees, can see near-ultraviolet wavelengths. Many fruits, flowers, and seeds stand out more strongly from the background in ultraviolet wavelengths as compared to human color vision. Scorpions glow or take on a yellow to green color under UV illumination, thus assisting in the control of these arachnids. Many birds have patterns in their plumage that are invisible at usual wavelengths but observable in ultraviolet, and the urine and other secretions of some animals, including dogs, cats, and human beings, are much easier to spot with ultraviolet. Urine trails of rodents can be detected by pest control technicians for proper treatment of infested dwellings.\nButterflies use ultraviolet as a communication system for sex recognition and mating behavior. For example, in the Colias eurytheme butterfly, males rely on visual cues to locate and identify females. Instead of using chemical stimuli to find mates, males are attracted to the ultraviolet-reflecting color of female hind wings. In Pieris napi butterflies it was shown that females in northern Finland with less UV-radiation present in the environment possessed stronger UV signals to attract their males than those occurring further south. This suggested that it was evolutionarily more difficult to increase the UV-sensitivity of the eyes of the males than to increase the UV-signals emitted by the females.\nMany insects use the ultraviolet wavelength emissions from celestial objects as references for flight navigation. A local ultraviolet emitter will normally disrupt the navigation process and will eventually attract the flying insect.\nThe green fluorescent protein (GFP) is often used in genetics as a marker. Many substances, such as proteins, have significant light absorption bands in the ultraviolet that are of interest in biochemistry and related fields. UV-capable spectrophotometers are common in such laboratories.\nUltraviolet traps called bug zappers are used to eliminate various small flying insects. They are attracted to the UV and are killed using an electric shock, or trapped once they come into contact with the device. Different designs of ultraviolet radiation traps are also used by entomologists for collecting nocturnal insects during faunistic survey studies.\n==== Therapy ====\nUltraviolet radiation is helpful in the treatment of skin conditions such as psoriasis and vitiligo. Exposure to UVA, while the skin is hyper-photosensitive, by taking psoralens is an effective treatment for psoriasis. Due to the potential of psoralens to cause damage to the liver, PUVA therapy may be used only a limited number of times over a patient\'s lifetime.\nUVB phototherapy does not require additional medications or topical preparations for the therapeutic benefit; only the exposure is needed. However, phototherapy can be effective when used in conjunction with certain topical treatments such as anthralin, coal tar, and vitamin A and D derivatives, or systemic treatments such as methotrexate and Soriatane.\n==== Herpetology ====\nReptiles need UVB for biosynthesis of vitamin D, and other metabolic processes. Specifically cholecalciferol (vitamin D3), which is needed for basic cellular / neural functioning as well as the utilization of calcium for bone and egg production. The UVA wavelength is also visible to many reptiles and might play a significant role in their ability survive in the wild as well as in visual communication between individuals. Therefore, in a typical reptile enclosure, a fluorescent UV a/b source (at the proper strength / spectrum for the species), must be available for many captive species to survive. Simple supplementation with cholecalciferol (Vitamin D3) will not be enough as there is a complete biosynthetic pathway that is "leapfrogged" (risks of possible overdoses), the intermediate molecules and metabolites also play important functions in the animals health. Natural sunlight in the right levels is always going to be superior to artificial sources, but this might not be possible for keepers in different parts of the world.\nIt is a known problem that high levels of output of the UVa part of the spectrum can both cause cellular and DNA damage to sensitive parts of their bodies – especially the eyes where blindness is the result of an improper UVa/b source use and placement photokeratitis. For many keepers there must also be a provision for an adequate heat source this has resulted in the marketing of heat and light "combination" products. Keepers should be careful of these "combination" light/ heat and UVa/b generators, they typically emit high levels of UVa with lower levels of UVb that are set and difficult to control so that animals can have their needs met. A better strategy is to use individual sources of these elements and so they can be placed and controlled by the keepers for the max benefit of the animals.\n== Evolutionary significance ==\nThe evolution of early reproductive proteins and enzymes is attributed in modern models of evolutionary theory to ultraviolet radiation. UVB causes thymine base pairs next to each other in genetic sequences to bond together into thymine dimers, a disruption in the strand that reproductive enzymes cannot copy. This leads to frameshifting during genetic replication and protein synthesis, usually killing the cell. Before formation of the UV-blocking ozone layer, when early prokaryotes approached the surface of the ocean, they almost invariably died out. The few that survived had developed enzymes that monitored the genetic material and removed thymine dimers by nucleotide excision repair enzymes. Many enzymes and proteins involved in modern mitosis and meiosis are similar to repair enzymes, and are believed to be evolved modifications of the enzymes originally used to overcome DNA damages caused by UV.\nElevated levels of ultraviolet radiation, in particular UV-B, have also been speculated as a cause of mass extinctions in the fossil record.\n== Photobiology ==\nPhotobiology is the scientific study of the beneficial and harmful interactions of non-ionizing radiation in living organisms, conventionally demarcated around 10 eV, the first ionization energy of oxygen. UV ranges roughly from 3 to 30 eV in energy. Hence photobiology entertains some, but not all, of the UV spectrum.\n== See also ==\n== References ==\n== Further reading ==\nAllen, Jeannie (6 September 2001). Ultraviolet Radiation: How it Affects Life on Earth. Earth Observatory. NASA, USA.\nHockberger, Philip E. (2002). "A History of Ultraviolet Photobiology for Humans, Animals and Microorganisms". Photochemistry and Photobiology. 76 (6): 561–569. doi:10.1562/0031-8655(2002)0760561AHOUPF2.0.CO2. PMID 12511035. S2CID 222100404.\nHu, S; Ma, F; Collado-Mesa, F; Kirsner, R. S. (July 2004). "UV radiation, latitude, and melanoma in US Hispanics and blacks". Arch. Dermatol. 140 (7): 819–824. doi:10.1001/archderm.140.7.819. PMID 15262692.\nStrauss, CEM; Funk, DJ (1991). "Broadly tunable difference-frequency generation of VUV using two-photon resonances in H2 and Kr". Optics Letters. 16 (15): 1192–4. Bibcode:1991OptL...16.1192S. doi:10.1364/ol.16.001192. PMID 19776917.\n== External links ==\nMedia related to Ultraviolet light at Wikimedia Commons\nThe dictionary definition of ultraviolet at Wiktionary', 'Photosynthesis', 'In October 2019, Breakthrough Listen started a collaboration with scientists from the TESS team (Transiting Exoplanet Survey Satellite) to look for signs of advanced extraterrestrial life. Thousands of new planets found by TESS will be scanned for technosignatures by Breakthrough Listen partner facilities across the globe. Data from TESS monitoring of stars will also be searched for anomalies.\n=== FAST ===\nChina\'s 500 meter Aperture Spherical Telescope (FAST) lists detecting interstellar communication signals as part of its science mission. It is funded by the National Development and Reform Commission (NDRC) and managed by the National Astronomical observatories (NAOC) of the Chinese Academy of Sciences (CAS). FAST is the first radio observatory built with SETI as a core scientific goal. FAST consists of a fixed 500 m (1,600 ft) diameter spherical dish constructed in a natural depression sinkhole caused by karst processes in the region. It is the world\'s largest filled-aperture radio telescope.\nAccording to its website, FAST can search to 28 light-years, and is able to reach 1,400 stars. If the transmitter\'s radiated power were to be increased to 1,000,000 MW, FAST would be able to reach one million stars. This is compared to the former Arecibo 305 meter telescope detection distance of 18 light-years.\nOn 14 June 2022, astronomers, working with China\'s FAST telescope, reported the possibility of having detected artificial (presumably alien) signals, but cautioned that further studies were required to determine if a natural radio interference may be the source. More recently, on 18 June 2022, Dan Werthimer, chief scientist for several SETI-related projects, reportedly noted, "These signals are from radio interference; they are due to radio pollution from earthlings, not from E.T.".\n=== UCLA ===\nSince 2016, University of California Los Angeles (UCLA) undergraduate and graduate students have been participating in radio searches for technosignatures with the Green Bank Telescope. Targets include the Kepler field, TRAPPIST-1, and solar-type stars. The search is sensitive to Arecibo-class transmitters located within 420 light years of Earth and to transmitters that are 1,000 times more powerful than Arecibo located within 13,000 light years of Earth.\n== Community SETI projects ==\n=== SETI@home ===\nThe SETI@home project used volunteer computing to analyze signals acquired by the SERENDIP project.\nSETI@home was conceived by David Gedye along with Craig Kasnoff and is a popular volunteer computing project that was launched by the Berkeley SETI Research Center at the University of California, Berkeley, in May 1999. It was originally funded by The Planetary Society and Paramount Pictures, and later by the state of California. The project is run by director David P. Anderson and chief scientist Dan Werthimer. Any individual could become involved with SETI research by downloading the Berkeley Open Infrastructure for Network Computing (BOINC) software program, attaching to the SETI@home project, and allowing the program to run as a background process that uses idle computer power. The SETI@home program itself ran signal analysis on a "work unit" of data recorded from the central 2.5 MHz wide band of the SERENDIP IV instrument. After computation on the work unit was complete, the results were then automatically reported back to SETI@home servers at University of California, Berkeley. By June 28, 2009, the SETI@home project had over 180,000 active participants volunteering a total of over 290,000 computers. These computers gave SETI@home an average computational power of 617 teraFLOPS. In 2004 radio source SHGb02+14a set off speculation in the media that a signal had been detected but researchers noted the frequency drifted rapidly and the detection on three SETI@home computers fell within random chance.\nBy 2010, after 10 years of data collection, SETI@home had listened to that one frequency at every point of over 67 percent of the sky observable from Arecibo with at least three scans (out of the goal of nine scans), which covers about 20 percent of the full celestial sphere. On March 31, 2020, with 91,454 active users, the project stopped sending out new work to SETI@home users, bringing this particular SETI effort to an indefinite hiatus.\n=== SETI Net ===\nSETI Network was the only fully operational private search system. The SETI Net station consisted of off-the-shelf, consumer-grade electronics to minimize cost and to allow this design to be replicated as simply as possible. It had a 3-meter parabolic antenna that could be directed in azimuth and elevation, an LNA that covered 100 MHz of the 1420 MHz spectrum, a receiver to reproduce the wideband audio, and a standard personal computer as the control device and for deploying the detection algorithms. The antenna could be pointed and locked to one sky location in Ra and DEC which enabling the system to integrate on it for long periods. The Wow! signal area was monitored for many long periods. All search data was collected and is available on the Internet archive.\nSETI Net started operation in the early 1980s as a way to learn about the science of the search, and developed several software packages for the amateur SETI community. It provided an astronomical clock, a file manager to keep track of SETI data files, a spectrum analyzer optimized for amateur SETI, remote control of the station from the Internet, and other packages.\nSETI Net went dark and was decommissioned on 2021-12-04. The collected data is available on their website.\n=== The SETI League and Project Argus ===\nFounded in 1994 in response to the United States Congress cancellation of the NASA SETI program, The SETI League, Incorporated is a membership-supported nonprofit organization with 1,500 members in 62 countries. This grass-roots alliance of amateur and professional radio astronomers is headed by executive director emeritus H. Paul Shuch, the engineer credited with developing the world\'s first commercial home satellite TV receiver. Many SETI League members are licensed radio amateurs and microwave experimenters. Others are digital signal processing experts and computer enthusiasts.\nThe SETI League pioneered the conversion of backyard satellite TV dishes 3 to 5 m (10–16 ft) in diameter into research-grade radio telescopes of modest sensitivity. The organization concentrates on coordinating a global network of small, amateur-built radio telescopes under Project Argus, an all-sky survey seeking to achieve real-time coverage of the entire sky. Project Argus was conceived as a continuation of the all-sky survey component of the late NASA SETI program (the targeted search having been continued by the SETI Institute\'s Project Phoenix). There are currently 143 Project Argus radio telescopes operating in 27 countries. Project Argus instruments typically exhibit sensitivity on the order of 10−23 Watts/square metre, or roughly equivalent to that achieved by the Ohio State University Big Ear radio telescope in 1977, when it detected the landmark "Wow!" candidate signal.\nThe name "Argus" derives from the mythical Greek guard-beast who had 100 eyes, and could see in all directions at once. In the SETI context, the name has been used for radio telescopes in fiction (Arthur C. Clarke, "Imperial Earth"; Carl Sagan, "Contact"), was the name initially used for the NASA study ultimately known as "Cyclops," and is the name given to an omnidirectional radio telescope design being developed at the Ohio State University.\n== Optical experiments ==\nWhile most SETI sky searches have studied the radio spectrum, some SETI researchers have considered the possibility that alien civilizations might be using powerful lasers for interstellar communications at optical wavelengths. The idea was first suggested by R. N. Schwartz and Charles Hard Townes in a 1961 paper published in the journal Nature titled "Interstellar and Interplanetary Communication by Optical Masers". However, the 1971 Cyclops study discounted the possibility of optical SETI, reasoning that construction of a laser system that could outshine the bright central star of a remote star system would be too difficult. In 1983, Townes published a detailed study of the idea in the United States journal Proceedings of the National Academy of Sciences, which was met with interest by the SETI community.\nThere are two problems with optical SETI. The first problem is that lasers are highly "monochromatic", that is, they emit light only on one frequency, making it troublesome to figure out what frequency to look for. However, emitting light in narrow pulses results in a broad spectrum of emission; the spread in frequency becomes higher as the pulse width becomes narrower, making it easier to detect an emission.\nThe other problem is that while radio transmissions can be broadcast in all directions, lasers are highly directional. Interstellar gas and dust is almost transparent to near infrared, so these signals can be seen from greater distances, but the extraterrestrial laser signals would need to be transmitted in the direction of Earth in order to be detected.\nOptical SETI supporters have conducted paper studies of the effectiveness of using contemporary high-energy lasers and a ten-meter diameter mirror as an interstellar beacon. The analysis shows that an infrared pulse from a laser, focused into a narrow beam by such a mirror, would appear thousands of times brighter than the Sun to a distant civilization in the beam\'s line of fire. The Cyclops study proved incorrect in suggesting a laser beam would be inherently hard to see.', 'While supersymmetry has not been discovered at high energy, see Section Supersymmetry in particle physics, supersymmetry was found to be effectively realized at the intermediate energy of hadronic physics where baryons and mesons are superpartners. An exception is the pion that appears as a zero mode in the mass spectrum and thus protected by the supersymmetry: It has no baryonic partner. The realization of this effective supersymmetry is readily explained in quark–diquark models: Because two different color charges close together (e.g., blue and red) appear under coarse resolution as the corresponding anti-color (e.g. anti-green), a diquark cluster viewed with coarse resolution (i.e., at the energy-momentum scale used to study hadron structure) effectively appears as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves likes a meson.\n=== Supersymmetry in condensed matter physics ===\nSUSY concepts have provided useful extensions to the WKB approximation. Additionally, SUSY has been applied to disorder averaged systems both quantum and non-quantum (through statistical mechanics), the Fokker–Planck equation being an example of a non-quantum theory. The \'supersymmetry\' in all these systems arises from the fact that one is modelling one particle and as such the \'statistics\' do not matter. The use of the supersymmetry method provides a mathematical rigorous alternative to the replica trick, but only in non-interacting systems, which attempts to address the so-called \'problem of the denominator\' under disorder averaging. For more on the applications of supersymmetry in condensed matter physics see Efetov (1997).\nIn 2021, a group of researchers showed that, in theory,\n{\\displaystyle N=(0,1)}\nSUSY could be realised at the edge of a Moore–Read quantum Hall state. However, to date, no experiments have been done yet to realise it at an edge of a Moore–Read state. In 2022, a different group of researchers created a computer simulation of atoms in 1 dimensions that had supersymmetric topological quasiparticles.\n=== Supersymmetry in optics ===\nIn 2013, integrated optics was found to provide a fertile ground on which certain ramifications of SUSY can be explored in readily-accessible laboratory settings. Making use of the analogous mathematical structure of the quantum-mechanical Schrödinger equation and the wave equation governing the evolution of light in one-dimensional settings, one may interpret the refractive index distribution of a structure as a potential landscape in which optical wave packets propagate. In this manner, a new class of functional optical structures with possible applications in phase matching, mode conversion and space-division multiplexing becomes possible. SUSY transformations have been also proposed as a way to address inverse scattering problems in optics and as a one-dimensional transformation optics.\n=== Supersymmetry in dynamical systems ===\nAll stochastic (partial) differential equations, the models for all types of continuous time dynamical systems, possess topological supersymmetry. In the operator representation of stochastic evolution, the topological supersymmetry is the exterior derivative which is commutative with the stochastic evolution operator defined as the stochastically averaged pullback induced on differential forms by SDE-defined diffeomorphisms of the phase space. The topological sector of the so-emerging supersymmetric theory of stochastic dynamics can be recognized as the Witten-type topological field theory.\nThe meaning of the topological supersymmetry in dynamical systems is the preservation of the phase space continuity—infinitely close points will remain close during continuous time evolution even in the presence of noise. When the topological supersymmetry is broken spontaneously, this property is violated in the limit of the infinitely long temporal evolution and the model can be said to exhibit (the stochastic generalization of) the butterfly effect. From a more general perspective, spontaneous breakdown of the topological supersymmetry is the theoretical essence of the ubiquitous dynamical phenomenon variously known as chaos, turbulence, self-organized criticality etc. The Goldstone theorem explains the associated emergence of the long-range dynamical behavior that manifests itself as \u20601/f\u2060 noise, butterfly effect, and the scale-free statistics of sudden (instantonic) processes, such as earthquakes, neuroavalanches, and solar flares, known as the Zipf\'s law and the Richter scale.\n=== Supersymmetry in mathematics ===\nSUSY is also sometimes studied mathematically for its intrinsic properties. This is because it describes complex fields satisfying a property known as holomorphy, which allows holomorphic quantities to be exactly computed. This makes supersymmetric models useful "toy models" of more realistic theories. A prime example of this has been the demonstration of S-duality in four-dimensional gauge theories that interchanges particles and monopoles.\nThe proof of the Atiyah–Singer index theorem is much simplified by the use of supersymmetric quantum mechanics.\n=== Supersymmetry in string theory ===\nSupersymmetry is an integral part of string theory, a possible theory of everything. There are two types of string theory, supersymmetric string theory or superstring theory, and non-supersymmetric string theory. By definition of superstring theory, supersymmetry is required in superstring theory at some level. However, even in non-supersymmetric string theory, a type of supersymmetry called misaligned supersymmetry is still required in the theory in order to ensure no physical tachyons appear. Any string theories without some kind of supersymmetry, such as bosonic string theory and the\n{\\displaystyle E_{7}\\times E_{7}}\n16\n{\\displaystyle SU(16)}\n, and\n{\\displaystyle E_{8}}\nheterotic string theories, will have a tachyon and therefore the spacetime vacuum itself would be unstable and would decay into some tachyon-free string theory usually in a lower spacetime dimension. There is no experimental evidence that either supersymmetry or misaligned supersymmetry holds in our universe, and many physicists have moved on from supersymmetry and string theory entirely due to the non-detection of supersymmetry at the LHC.\nDespite the null results for supersymmetry at the LHC so far, some particle physicists have nevertheless moved to string theory in order to resolve the naturalness crisis for certain supersymmetric extensions of the Standard Model. According to the particle physicists, there exists a concept of "stringy naturalness" in string theory, where the string theory landscape could have a power law statistical pull on soft SUSY breaking terms to large values (depending on the number of hidden sector SUSY breaking fields contributing to the soft terms). If this is coupled with an anthropic requirement that contributions to the weak scale not exceed a factor between 2 and 5 from its measured value (as argued by Agrawal et al.), then the Higgs mass is pulled up to the vicinity of 125 GeV while most sparticles are pulled to values beyond the current reach of LHC. (The Higgs was determined to have a mass of 125 GeV ±0.15 GeV in 2022.) An exception occurs for higgsinos which gain mass not from SUSY breaking but rather from whatever mechanism solves the SUSY mu problem. Light higgsino pair production in association with hard initial state jet radiation leads to a soft opposite-sign dilepton plus jet plus missing transverse energy signal.\n== Supersymmetry in particle physics ==\nIn particle physics, a supersymmetric extension of the Standard Model is a possible candidate for undiscovered particle physics, and seen by some physicists as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to be incomplete and where limitations of current theories are well established. In particular, one supersymmetric extension of the Standard Model, the Minimal Supersymmetric Standard Model (MSSM), became popular in theoretical particle physics, as the Minimal Supersymmetric Standard Model is the simplest supersymmetric extension of the Standard Model that could resolve major hierarchy problems within the Standard Model, by guaranteeing that quadratic divergences of all orders will cancel out in perturbation theory. If a supersymmetric extension of the Standard Model is correct, superpartners of the existing elementary particles would be new and undiscovered particles and supersymmetry is expected to be spontaneously broken.\nThere is no experimental evidence that a supersymmetric extension to the Standard Model is correct, or whether or not other extensions to current models might be more accurate. It is only since around 2010 that particle accelerators specifically designed to study physics beyond the Standard Model have become operational (i.e. the Large Hadron Collider (LHC)), and it is not known where exactly to look, nor the energies required for a successful search. However, the negative results from the LHC since 2010 have already ruled out some supersymmetric extensions to the Standard Model, and many physicists believe that the Minimal Supersymmetric Standard Model, while not ruled out, is no longer able to fully resolve the hierarchy problem.\n=== Supersymmetric extensions of the Standard Model ===', 'The light curves for type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. Their optical energy output is driven by radioactive decay of ejected nickel-56 (half-life 6 days), which then decays to radioactive cobalt-56 (half-life 77 days). These radioisotopes excite the surrounding material to incandescence. Modern studies of cosmology rely on 56Ni radioactivity providing the energy for the optical brightness of supernovae of type Ia, which are the "standard candles" of cosmology but whose diagnostic 847 keV and 1,238 keV gamma rays were first detected only in 2014. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half-life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission from the remaining cobalt-56 becomes dominant, although this portion of the light curve has been little-studied.\nType Ib and Ic light curves are similar to type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of the created nickel-56. The peak luminosity varies considerably and there are even occasional type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increased peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.\nThe light curves for type II supernovae are characterised by a much slower decline than type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial destruction this hydrogen becomes heated and ionised. The majority of type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in type I supernovae, due to the efficiency of conversion into light by all the hydrogen.\nIn type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a type I supernova and the hydrogen even disappears from the spectrum after several weeks.\nType IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a superluminous supernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.\nLarge numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.\nNotes:\n=== Asymmetry ===\nA long-standing puzzle surrounding type II supernovae is why the remaining compact object receives a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high peculiar velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500 km/s or greater. This indicates an expansion asymmetry, but the mechanism by which momentum is transferred to the compact object remains a puzzle. Proposed explanations for this kick include convection in the collapsing star, asymmetric ejection of matter during neutron star formation, and asymmetrical neutrino emissions.\nOne possible explanation for this asymmetry is large-scale convection above the core. The convection can create radial variations in density giving rise to variations in the amount of energy absorbed from neutrino outflow. However analysis of this mechanism predicts only modest momentum transfer. Another possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova. (A similar model is used for explaining long gamma-ray bursts.) The dominant mechanism may depend upon the mass of the progenitor star.\nInitial asymmetries have also been confirmed in type Ia supernovae through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the expansion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarisation of the emitted light.\n=== Energy output ===\nAlthough supernovae are primarily known as luminous events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.\nThere is a fundamental difference between the balance of energy production in the different types of supernova. In type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.\nStandard type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is 56Ni generated from silicon burning. 56Ni is radioactive and decays into 56Co by beta plus decay (with a half life of six days) and gamma rays. 56Co itself decays by the beta plus (positron) path with a half life of 77 days into stable 56Fe. These two processes are responsible for the electromagnetic radiation from type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.\nCore collapse supernovae are on average visually fainter than type Ia supernovae, but the total energy released is far higher, as outlined in the following table.\nIn some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high-luminosity supernovae and is thought to be the cause of type Ic hypernovae and long-duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low-luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.\nWhen a supernova occurs inside a small dense cloud of circumstellar material, it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause type IIn hypernovae.\nAlthough pair-instability supernovae are core collapse supernovae with spectra and light curves similar to type II-P, the nature after core collapse is more like that of a giant type Ia with runaway fusion of carbon, oxygen and silicon. The total energy released by the highest-mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.\n=== Progenitor ===', 'In the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein\'s proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal. This has at times been called Planck\'s "second theory".\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his \'third theory\', that both emission and absorption of light were quantal.\nThe colourful term "ultraviolet catastrophe" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck\'s thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of "catastrophe". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called \'photons\', and by Rayleigh and by Jeans.\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν. In contrast to Planck\'s and Einstein\'s formulas, Bohr\'s formula referred explicitly and categorically to energy levels of atoms. Bohr\'s formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck\'s model, the frequency\n{\\displaystyle \\nu }\nhas no immediate relation to frequencies that might describe those quantum states themselves.\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck\'s law. The actual word \'photon\' was invented still later, by G.N. Lewis in 1926, who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word \'photon\' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein\'s 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\nUltimately, Planck\'s law of black-body radiation contributed to Einstein\'s concept of quanta of light carrying linear momentum, which became the fundamental basis for the development of quantum mechanics.\nThe above-mentioned linearity of Planck\'s mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg\'s original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg\'s theory accounted for Bohr\'s above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states. At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg\'s paper and recognized the matrix character of Heisenberg\'s theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg\'s original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics. Heisenberg\'s explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck\'s oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = \u2060h/2π\u2060, and ω = 2πν denotes angular frequency, and less often the equivalent formula E = hν. This statement about a really existing and propagating light quantum, based on Einstein\'s, has a physical meaning different from that of Planck\'s above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\nAn article by Helge Kragh published in Physics World gives an account of this history.\n== See also ==\nEmissivity\nRadiance\nSakuma–Hattori equation\n== References ==\n=== Bibliography ===\n== External links ==\nSummary of Radiation\nRadiation of a Blackbody – interactive simulation to play with Planck\'s law\nScienceworld entry on Planck\'s Law', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.']

Question: What is accelerator-based light-ion fusion?

Choices:
Choice A) Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. This method is relatively easy to implement and can be done in an efficient manner, requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer. Fusion can be observed with as little as 10 kV between the electrodes.
Choice B) Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce heavy-ion fusion reactions. This method is relatively difficult to implement and requires a complex system of vacuum tubes, electrodes, and transformers. Fusion can be observed with as little as 10 kV between the electrodes.
Choice C) Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. This method is relatively difficult to implement and requires a complex system of vacuum tubes, electrodes, and transformers. Fusion can be observed with as little as 100 kV between the electrodes.
Choice D) Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce heavy-ion fusion reactions. This method is relatively easy to implement and can be done in an efficient manner, requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer. Fusion can be observed with as little as 100 kV between the electrodes.
Choice E) Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fission reactions. This method is relatively easy to implement and can be done in an efficient manner, requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer. Fission can be observed with as little as 10 kV between the electrodes.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['In 1912, Vesto M. Slipher made spectrographic studies of the brightest spiral nebulae to determine their composition. Slipher discovered that the spiral nebulae have high Doppler shifts, indicating that they are moving at a rate exceeding the velocity of the stars he had measured. He found that the majority of these nebulae are moving away from us.\nIn 1917, Heber Doust Curtis observed nova S Andromedae within the "Great Andromeda Nebula", as the Andromeda Galaxy, Messier object M31, was then known. Searching the photographic record, he found 11 more novae. Curtis noticed that these novae were, on average, 10 magnitudes fainter than those that occurred within this galaxy. As a result, he was able to come up with a distance estimate of 150,000 parsecs. He became a proponent of the so-called "island universes" hypothesis, which holds that spiral nebulae are actually independent galaxies.\nIn 1920 a debate took place between Harlow Shapley and Heber Curtis, the Great Debate, concerning the nature of the Milky Way, spiral nebulae, and the dimensions of the universe. To support his claim that the Great Andromeda Nebula is an external galaxy, Curtis noted the appearance of dark lanes resembling the dust clouds in the Milky Way, as well as the significant Doppler shift.\nIn 1922, the Estonian astronomer Ernst Öpik gave a distance determination that supported the theory that the Andromeda Nebula is indeed a distant extra-galactic object. Using the new 100-inch Mt. Wilson telescope, Edwin Hubble was able to resolve the outer parts of some spiral nebulae as collections of individual stars and identified some Cepheid variables, thus allowing him to estimate the distance to the nebulae: they were far too distant to be part of the Milky Way. In 1926 Hubble produced a classification of galactic morphology that is used to this day.\n=== Multi-wavelength observation ===\nAdvances in astronomy have always been driven by technology. After centuries of success in optical astronomy, recent decades have seen major progress in other regions of the electromagnetic spectrum.\nThe dust present in the interstellar medium is opaque to visual light. It is more transparent to far-infrared, which can be used to observe the interior regions of giant molecular clouds and galactic cores in great detail. Infrared is also used to observe distant, red-shifted galaxies that were formed much earlier. Water vapor and carbon dioxide absorb a number of useful portions of the infrared spectrum, so high-altitude or space-based telescopes are used for infrared astronomy.\nThe first non-visual study of galaxies, particularly active galaxies, was made using radio frequencies. The Earth\'s atmosphere is nearly transparent to radio between 5 MHz and 30 GHz. The ionosphere blocks signals below this range. Large radio interferometers have been used to map the active jets emitted from active nuclei.\nUltraviolet and X-ray telescopes can observe highly energetic galactic phenomena. Ultraviolet flares are sometimes observed when a star in a distant galaxy is torn apart from the tidal forces of a nearby black hole. The distribution of hot gas in galactic clusters can be mapped by X-rays. The existence of supermassive black holes at the cores of galaxies was confirmed through X-ray astronomy.\n=== Modern research ===\nIn 1944, Hendrik van de Hulst predicted that microwave radiation with wavelength of 21 cm would be detectable from interstellar atomic hydrogen gas; and in 1951 it was observed. This radiation is not affected by dust absorption, and so its Doppler shift can be used to map the motion of the gas in this galaxy. These observations led to the hypothesis of a rotating bar structure in the center of this galaxy. With improved radio telescopes, hydrogen gas could also be traced in other galaxies.\nIn the 1970s, Vera Rubin uncovered a discrepancy between observed galactic rotation speed and that predicted by the visible mass of stars and gas. Today, the galaxy rotation problem is thought to be explained by the presence of large quantities of unseen dark matter.\nBeginning in the 1990s, the Hubble Space Telescope yielded improved observations. Among other things, its data helped establish that the missing dark matter in this galaxy could not consist solely of inherently faint and small stars. The Hubble Deep Field, an extremely long exposure of a relatively empty part of the sky, provided evidence that there are about 125 billion (1.25×1011) galaxies in the observable universe. Improved technology in detecting the spectra invisible to humans (radio telescopes, infrared cameras, and x-ray telescopes) allows detection of other galaxies that are not detected by Hubble. Particularly, surveys in the Zone of Avoidance (the region of sky blocked at visible-light wavelengths by the Milky Way) have revealed a number of new galaxies.\nA 2016 study published in The Astrophysical Journal, led by Christopher Conselice of the University of Nottingham, analyzed many sources of data to estimate that the observable universe (up to z=8) contained at least two trillion (2×1012) galaxies, a factor of 10 more than are directly observed in Hubble images.:\u200a12\u200a However, later observations with the New Horizons space probe from outside the zodiacal light observed less cosmic optical light than Conselice while still suggesting that direct observations are missing galaxies.\n== Types and morphology ==\nGalaxies come in three main types: ellipticals, spirals, and irregulars. A slightly more extensive description of galaxy types based on their appearance is given by the Hubble sequence. Since the Hubble sequence is entirely based upon visual morphological type (shape), it may miss certain important characteristics of galaxies such as star formation rate in starburst galaxies and activity in the cores of active galaxies.\nMany galaxies are thought to contain a supermassive black hole at their center. This includes the Milky Way, whose core region is called the Galactic Center.\n=== Ellipticals ===\nThe Hubble classification system rates elliptical galaxies on the basis of their ellipticity, ranging from E0, being nearly spherical, up to E7, which is highly elongated. These galaxies have an ellipsoidal profile, giving them an elliptical appearance regardless of the viewing angle. Their appearance shows little structure and they typically have relatively little interstellar matter. Consequently, these galaxies also have a low portion of open clusters and a reduced rate of new star formation. Instead, they are dominated by generally older, more evolved stars that are orbiting the common center of gravity in random directions. The stars contain low abundances of heavy elements because star formation ceases after the initial burst. In this sense they have some similarity to the much smaller globular clusters.\n==== Type-cD galaxies ====\nThe largest galaxies are the type-cD galaxies.\nFirst described in 1964 by a paper by Thomas A. Matthews and others, they are a subtype of the more general class of D galaxies, which are giant elliptical galaxies, except that they are much larger. They are popularly known as the supergiant elliptical galaxies and constitute the largest and most luminous galaxies known. These galaxies feature a central elliptical nucleus with an extensive, faint halo of stars extending to megaparsec scales. The profile of their surface brightnesses as a function of their radius (or distance from their cores) falls off more slowly than their smaller counterparts.\nThe formation of these cD galaxies remains an active area of research, but the leading model is that they are the result of the mergers of smaller galaxies in the environments of dense clusters, or even those outside of clusters with random overdensities. These processes are the mechanisms that drive the formation of fossil groups or fossil clusters, where a large, relatively isolated, supergiant elliptical resides in the middle of the cluster and are surrounded by an extensive cloud of X-rays as the residue of these galactic collisions. Another older model posits the phenomenon of cooling flow, where the heated gases in clusters collapses towards their centers as they cool, forming stars in the process, a phenomenon observed in clusters such as Perseus, and more recently in the Phoenix Cluster.\n==== Shell galaxy ====\nA shell galaxy is a type of elliptical galaxy where the stars in its halo are arranged in concentric shells. About one-tenth of elliptical galaxies have a shell-like structure, which has never been observed in spiral galaxies. These structures are thought to develop when a larger galaxy absorbs a smaller companion galaxy—that as the two galaxy centers approach, they start to oscillate around a center point, and the oscillation creates gravitational ripples forming the shells of stars, similar to ripples spreading on water. For example, galaxy NGC 3923 has over 20 shells.\n=== Spirals ===\nSpiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept.', "Planck's law", 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'for convex polyhedra to higher-dimensional polytopes:\n{\\displaystyle \\sum \\varphi =(-1)^{d-1}}\n== Generalisations of a polytope ==\n=== Infinite polytopes ===\nNot all manifolds are finite. Where a polytope is understood as a tiling or decomposition of a manifold,  this idea may be extended to infinite manifolds. plane tilings, space-filling (honeycombs) and hyperbolic tilings are in this sense polytopes, and are sometimes called apeirotopes because they have infinitely many cells.\nAmong these, there are regular forms including the regular skew polyhedra and the infinite series of tilings represented by the regular apeirogon, square tiling, cubic honeycomb, and so on.\n=== Abstract polytopes ===\nThe theory of abstract polytopes attempts to detach polytopes from the space containing them, considering their purely combinatorial properties. This allows the definition of the term to be extended to include objects for which it is difficult to define an intuitive underlying space, such as the 11-cell.\nAn abstract polytope is a partially ordered set of elements or members, which obeys certain rules. It is a purely algebraic structure, and the theory was developed in order to avoid some of the issues which make it difficult to reconcile the various geometric classes within a consistent mathematical framework. A geometric polytope is said to be a realization in some real space of the associated abstract polytope.\n=== Complex polytopes ===\nStructures analogous to polytopes exist in complex Hilbert spaces\n{\\displaystyle \\mathbb {C} ^{n}}\nwhere n real dimensions are accompanied by n imaginary ones. Regular complex polytopes are more appropriately treated as configurations.\n== Duality ==\nEvery n-polytope has a dual structure, obtained by interchanging its vertices for facets, edges for ridges, and so on generally interchanging its (j − 1)-dimensional elements for (n − j)-dimensional elements (for j = 1 to n − 1), while retaining the connectivity or incidence between elements.\nFor an abstract polytope, this simply reverses the ordering of the set. This reversal is seen in the Schläfli symbols for regular polytopes, where the symbol for the dual polytope is simply the reverse of the original. For example, {4, 3, 3} is dual to {3, 3, 4}.\nIn the case of a geometric polytope, some geometric rule for dualising is necessary, see for example the rules described for dual polyhedra. Depending on circumstance, the dual figure may or may not be another geometric polytope.\nIf the dual is reversed, then the original polytope is recovered. Thus, polytopes exist in dual pairs.\n=== Self-dual polytopes ===\nIf a polytope has the same number of vertices as facets, of edges as ridges, and so forth, and the same connectivities, then the dual figure will be similar to the original and the polytope is self-dual.\nSome common self-dual polytopes include:\nEvery regular n-simplex, in any number of dimensions, with Schläfli symbol {3n}. These include the equilateral triangle {3}, regular tetrahedron {3,3}, and 5-cell  {3,3,3}.\nEvery hypercubic honeycomb, in any number of dimensions. These include the apeirogon {∞}, square tiling {4,4} and cubic honeycomb {4,3,4}.\nNumerous compact, paracompact and noncompact hyperbolic tilings, such as the icosahedral honeycomb {3,5,3}, and order-5 pentagonal tiling {5,5}.\nIn 2 dimensions, all regular polygons (regular 2-polytopes)\nIn 3 dimensions, the canonical polygonal pyramids and elongated pyramids, and tetrahedrally diminished dodecahedron.\nIn 4 dimensions, the 24-cell, with Schläfli symbol {3,4,3}. Also the great 120-cell {5,5/2,5} and grand stellated 120-cell {5/2,5,5/2}.\n== History ==\nPolygons and polyhedra have been known since ancient times.\nAn early hint of higher dimensions came in 1827 when August Ferdinand Möbius discovered that two mirror-image solids can be superimposed by rotating one of them through a fourth mathematical dimension. By the 1850s, a handful of other mathematicians such as Arthur Cayley and Hermann Grassmann had also considered higher dimensions.\nLudwig Schläfli was the first to consider analogues of polygons and polyhedra in these higher spaces. He described the six convex regular 4-polytopes in 1852 but his work was not published until 1901, six years after his death. By 1854, Bernhard Riemann\'s Habilitationsschrift had firmly established the geometry of higher dimensions, and thus the concept of n-dimensional polytopes was made acceptable. Schläfli\'s polytopes were rediscovered many times in the following decades, even during his lifetime.\nIn 1882 Reinhold Hoppe, writing in German, coined the word polytop to refer to this more general concept of polygons and polyhedra. In due course Alicia Boole Stott, daughter of logician George Boole, introduced the anglicised polytope into the English language.:\u200avi\nIn 1895, Thorold Gosset not only rediscovered Schläfli\'s regular polytopes but also investigated the ideas of semiregular polytopes and space-filling tessellations in higher dimensions. Polytopes also began to be studied in non-Euclidean spaces such as hyperbolic space.\nAn important milestone was reached in 1948 with H. S. M. Coxeter\'s book Regular Polytopes, summarizing work to date and adding new findings of his own.\nMeanwhile, the French mathematician Henri Poincaré had developed the topological idea of a polytope as the piecewise decomposition (e.g. CW-complex) of a manifold. Branko Grünbaum published his influential work on Convex Polytopes in 1967.\nIn 1952 Geoffrey Colin Shephard generalised the idea as complex polytopes in complex space, where each real dimension has an imaginary one associated with it. Coxeter developed the theory further.\nThe conceptual issues raised by complex polytopes, non-convexity, duality and other phenomena led Grünbaum and others to the more general study of abstract combinatorial properties relating vertices, edges, faces and so on. A related idea was that of incidence complexes, which studied the incidence or connection of the various elements with one another. These developments led eventually to the theory of abstract polytopes as partially ordered sets, or posets, of such elements. Peter McMullen and Egon Schulte published their book Abstract Regular Polytopes in 2002.\nEnumerating the uniform polytopes, convex and nonconvex, in four or more dimensions remains an outstanding problem. The convex uniform 4-polytopes were fully enumerated by John Conway and Michael Guy using a computer in 1965; in higher dimensions this problem was still open as of 1997. The full enumeration for nonconvex uniform polytopes is not known in dimensions four and higher as of 2008.\nIn modern times, polytopes and related concepts have found many important applications in fields as diverse as computer graphics, optimization, search engines, cosmology, quantum mechanics and numerous other fields. In 2013 the amplituhedron was discovered as a simplifying construct in certain calculations of theoretical physics.\n== Applications ==\nIn the field of optimization, linear programming studies the maxima and minima of linear functions; these maxima and minima occur on the boundary of an n-dimensional polytope. In linear programming, polytopes occur in the use of generalized barycentric coordinates and slack variables.\nIn  twistor theory, a branch of theoretical physics, a polytope called the amplituhedron is used in to calculate the scattering amplitudes of subatomic particles when they collide. The construct is purely theoretical with no known physical manifestation, but is said to greatly simplify certain calculations.\n== See also ==\n== References ==\n=== Citations ===\n=== Bibliography ===\n== External links ==\nWeisstein, Eric W. "Polytope". MathWorld.\n"Math will rock your world" – application of polytopes to a database of articles used to support custom news feeds via the Internet – (Business Week Online)\nRegular and semi-regular convex polytopes a short historical overview:', 'In the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein\'s proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal. This has at times been called Planck\'s "second theory".\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his \'third theory\', that both emission and absorption of light were quantal.\nThe colourful term "ultraviolet catastrophe" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck\'s thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of "catastrophe". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called \'photons\', and by Rayleigh and by Jeans.\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν. In contrast to Planck\'s and Einstein\'s formulas, Bohr\'s formula referred explicitly and categorically to energy levels of atoms. Bohr\'s formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck\'s model, the frequency\n{\\displaystyle \\nu }\nhas no immediate relation to frequencies that might describe those quantum states themselves.\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck\'s law. The actual word \'photon\' was invented still later, by G.N. Lewis in 1926, who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word \'photon\' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein\'s 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\nUltimately, Planck\'s law of black-body radiation contributed to Einstein\'s concept of quanta of light carrying linear momentum, which became the fundamental basis for the development of quantum mechanics.\nThe above-mentioned linearity of Planck\'s mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg\'s original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg\'s theory accounted for Bohr\'s above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states. At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg\'s paper and recognized the matrix character of Heisenberg\'s theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg\'s original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics. Heisenberg\'s explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck\'s oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = \u2060h/2π\u2060, and ω = 2πν denotes angular frequency, and less often the equivalent formula E = hν. This statement about a really existing and propagating light quantum, based on Einstein\'s, has a physical meaning different from that of Planck\'s above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\nAn article by Helge Kragh published in Physics World gives an account of this history.\n== See also ==\nEmissivity\nRadiance\nSakuma–Hattori equation\n== References ==\n=== Bibliography ===\n== External links ==\nSummary of Radiation\nRadiation of a Blackbody – interactive simulation to play with Planck\'s law\nScienceworld entry on Planck\'s Law', 'Ultraviolet catastrophe\n\nThe ultraviolet catastrophe, also called the Rayleigh–Jeans catastrophe, was the prediction of late 19th century and early 20th century classical physics that an ideal black body at thermal equilibrium would emit an unbounded quantity of energy as wavelength decreased into the ultraviolet range.:\u200a6–7\u200a The term "ultraviolet catastrophe" was first used in 1911 by the Austrian physicist Paul Ehrenfest, but the concept originated with the 1900 statistical derivation of the Rayleigh–Jeans law.\nThe phrase refers to the fact that the empirically derived Rayleigh–Jeans law, which accurately predicted experimental results at large wavelengths, failed to do so for short wavelengths. (See the image for further elaboration.)  As the theory diverged from empirical observations when these frequencies reached the ultraviolet region of the electromagnetic spectrum, there was a problem. This problem was later found to be due to a property of quanta as proposed by Max Planck: There could be no fraction of a discrete energy package already carrying minimal energy.\nSince the first use of this term, it has also been used for other predictions of a similar nature, as in quantum electrodynamics and such cases as ultraviolet divergence.\n== Problem ==\nThe Rayleigh-Jeans law is an approximation to the spectral radiance of electromagnetic radiation as a function of wavelength from a black body at a given temperature through classical arguments. For wavelength\n{\\displaystyle \\lambda }\n, it is:\n{\\displaystyle B_{\\lambda }(T)={\\frac {2ck_{\\mathrm {B} }T}{\\lambda ^{4}}},}\nwhere\n{\\displaystyle B_{\\lambda }}\nis the spectral radiance, the power emitted per unit emitting area, per steradian, per unit wavelength;\n{\\displaystyle c}\nis the speed of light;\n{\\displaystyle k_{\\mathrm {B} }}\nis the Boltzmann constant; and\n{\\displaystyle T}\nis the temperature in kelvins.  For frequency\n{\\displaystyle \\nu }\n, the expression is instead\n{\\displaystyle B_{\\nu }(T)={\\frac {2\\nu ^{2}k_{\\mathrm {B} }T}{c^{2}}}.}\nThis formula is obtained from the equipartition theorem of classical statistical mechanics which states that all harmonic oscillator modes (degrees of freedom) of a system at equilibrium have an average energy of\n{\\displaystyle k_{\\rm {B}}T}\nThe "ultraviolet catastrophe" is the expression of the fact that the formula misbehaves at higher frequencies; it predicts infinite energy emission because\n{\\displaystyle B_{\\nu }(T)\\to \\infty }\nas\n{\\displaystyle \\nu \\to \\infty }\nAn example, from Mason\'s A History of the Sciences, illustrates multi-mode vibration via a piece of string. As a natural vibrator, the string will oscillate with specific modes (the standing waves of a string in harmonic resonance), dependent on the length of the string. In classical physics, a radiator of energy will act as a natural vibrator. Since each mode will have the same energy, most of the energy in a natural vibrator will be in the smaller wavelengths and higher frequencies, where most of the modes are.\nAccording to classical electromagnetism, the number of electromagnetic modes in a 3-dimensional cavity, per unit frequency, is proportional to the square of the frequency. This implies that the radiated power per unit frequency should be proportional to frequency squared. Thus, both the power at a given frequency and the total radiated power is unlimited as higher and higher frequencies are considered:  this is unphysical, as the total radiated power of a cavity is not observed to be infinite, a point that was made independently by Einstein, Lord Rayleigh, and Sir James Jeans in 1905.\n== Solution ==\nIn 1900, Max Planck derived the correct form for the intensity spectral distribution function by making some assumptions that were strange for the time. In particular, Planck assumed that electromagnetic radiation can be emitted or absorbed only in discrete packets, called quanta, of energy:\nquanta\n{\\displaystyle E_{\\text{quanta}}=h\\nu =h{\\frac {c}{\\lambda }},}\nwhere:\nh is the Planck constant,\nν is the frequency of light,\nc is the speed of light,\nλ is the wavelength of light.\nBy applying this new energy to the partition function in statistical mechanics, Planck\'s assumptions led to the correct form of the spectral distribution functions:\nexp\n{\\displaystyle B_{\\lambda }(\\lambda ,T)={\\frac {2hc^{2}}{\\lambda ^{5}}}{\\frac {1}{\\exp \\left({\\frac {hc}{\\lambda k_{\\mathrm {B} }T}}\\right)-1}}}\nwhere:\nT is the absolute temperature of the body,\nkB is the Boltzmann constant,\nexp denotes the exponential function.\nIn 1905, Albert Einstein solved the problem physically by postulating that Planck\'s quanta were real physical particles – what we now call photons, not just a mathematical fiction. They modified statistical mechanics in the style of Boltzmann to an ensemble of photons. Einstein\'s photon had an energy proportional to its frequency and also explained an unpublished law of Stokes and the photoelectric effect.  This published postulate was specifically cited by the Nobel Prize in Physics committee in their decision to award the prize for 1921 to Einstein.\n== See also ==\nWien approximation\nVacuum catastrophe\nPlanckian locus\n== References ==\n=== Bibliography ===\n== Further reading ==\nKroemer, Herbert; Kittel, Charles (1980). "Chapter 4". Thermal Physics (2 ed.). W. H. Freeman Company. ISBN 0-7167-1088-9.\nCohen-Tannoudji, Claude; Diu, Bernard; Laloë; Franck (1977). Quantum Mechanics: Volume One. Hermann, Paris. pp. 624–626. ISBN 0-471-16433-X.', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', '=== Efficiency of real heat engines ===\nCarnot realized that, in reality, it is not possible to build a thermodynamically reversible engine.  So, real heat engines are even less efficient than indicated by Equation 3. In addition, real engines that operate along the Carnot cycle style (isothermal expansion / isentropic expansion / isothermal compression / isentropic compression) are rare. Nevertheless, Equation 3 is extremely useful for determining the maximum efficiency that could ever be expected for a given set of thermal reservoirs.\nAlthough Carnot\'s cycle is an idealization, Equation 3 as the expression of the Carnot efficiency is still useful. Consider the average temperatures,\nin\n{\\displaystyle \\langle T_{H}\\rangle ={\\frac {1}{\\Delta S}}\\int _{Q_{\\text{in}}}TdS}\nout\n{\\displaystyle \\langle T_{C}\\rangle ={\\frac {1}{\\Delta S}}\\int _{Q_{\\text{out}}}TdS}\nat which the first integral is over a part of a cycle where heat goes into the system and the second integral is over a cycle part where heat goes out from the system. Then, replace TH and TC in Equation 3 by ⟨TH⟩ and ⟨TC⟩, respectively, to estimate the efficiency a heat engine.\nFor the Carnot cycle, or its equivalent, the average value ⟨TH⟩ will equal the highest temperature available, namely TH, and ⟨TC⟩ the lowest, namely TC. For other less efficient thermodynamic cycles, ⟨TH⟩ will be lower than TH, and ⟨TC⟩ will be higher than TC. This can help illustrate, for example, why a reheater or a regenerator can improve the thermal efficiency of steam power plants and why the thermal efficiency of combined-cycle power plants (which incorporate gas turbines operating at even higher temperatures) exceeds that of conventional steam plants. The first prototype of the diesel engine was based on the principles of the Carnot cycle.\n== As a macroscopic construct ==\nThe Carnot heat engine is, ultimately, a theoretical construct based on an idealized thermodynamic system.  On a practical human-scale level the Carnot cycle has proven a valuable model, as in advancing the development of the diesel engine.  However, on a macroscopic scale limitations placed by the model\'s assumptions prove it impractical, and, ultimately, incapable of doing any work. As such, per Carnot\'s theorem, the Carnot engine may be thought as the theoretical limit of macroscopic scale heat engines rather than any practical device that could ever be built.\n== See also ==\nCarnot heat engine\nReversible process (thermodynamics)\n== References ==\nNotes\nSources\nCarnot, Sadi, Reflections on the Motive Power of Fire\nEwing, J. A. (1910) The Steam-Engine and Other Engines edition 3, page 62, via Internet Archive\nFeynman, Richard P.; Leighton, Robert B.; Sands, Matthew (1963). The Feynman Lectures on Physics. Addison-Wesley Publishing Company. pp. Chapter 44. ISBN 978-0-201-02116-5. {{cite book}}: ISBN / Date incompatibility (help)\nHalliday, David; Resnick, Robert (1978). Physics (3rd ed.). John Wiley & Sons. pp. 541–548. ISBN 978-0-471-02456-9.\nKittel, Charles; Kroemer, Herbert (1980). Thermal Physics (2nd ed.). W. H. Freeman Company. ISBN 978-0-7167-1088-2.\nKostic, M (2011). "Revisiting The Second Law of Energy Degradation and Entropy Generation: From Sadi Carnot\'s Ingenious Reasoning to Holistic Generalization". AIP Conf. Proc. AIP Conference Proceedings. 1411 (1): 327–350. Bibcode:2011AIPC.1411..327K. CiteSeerX 10.1.1.405.1945. doi:10.1063/1.3665247. American Institute of Physics, 2011. ISBN 978-0-7354-0985-9. Abstract at: [1]. Full article (24 pages [2]), also at [3].\n== External links ==\nHyperphysics article on the Carnot cycle.\nS. M. Blinder Carnot Cycle on Ideal Gas powered by Wolfram Mathematica', 'However, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".\nIn October 2000 astronomers Iván Almár and Jill Tarter presented a paper to The SETI Permanent Study Group in Rio de Janeiro, Brazil which proposed a scale (modelled after the Torino scale) which is an ordinal scale between zero and ten that quantifies the impact of any public announcement regarding evidence of extraterrestrial intelligence; the Rio scale has since inspired the 2005 San Marino Scale (in regard to the risks of transmissions from Earth) and the 2010 London Scale (in regard to the detection of extraterrestrial life). The Rio scale itself was revised in 2018.\nThe SETI Institute does not officially recognize the Wow! signal as of extraterrestrial origin as it was unable to be verified, although in a 2020 Twitter post the organization stated that \'\'an astronomer might have pinpointed the host star\'\'. The SETI Institute has also publicly denied that the candidate signal Radio source SHGb02+14a is of extraterrestrial origin. Although other volunteering projects such as Zooniverse credit users for discoveries, there is currently no crediting or early notification by SETI@Home following the discovery of a signal.\nSome people, including Steven M. Greer, have expressed cynicism that the general public might not be informed in the event of a genuine discovery of extraterrestrial intelligence due to significant vested interests. Some, such as Bruce Jakosky have also argued that the official disclosure of extraterrestrial life may have far reaching and as yet undetermined implications for society, particularly for the world\'s religions.\n== Active SETI ==\nActive SETI, also known as messaging to extraterrestrial intelligence (METI), consists of sending signals into space in the hope that they will be detected by an alien intelligence.\n=== Realized interstellar radio message projects ===\nIn November 1974, a largely symbolic attempt was made at the Arecibo Observatory to send a message to other worlds. Known as the Arecibo Message, it was sent towards the globular cluster M13, which is 25,000 light-years from Earth. Further IRMs Cosmic Call, Teen Age Message, Cosmic Call 2, and A Message From Earth were transmitted in 1999, 2001, 2003 and 2008 from the Evpatoria Planetary Radar.\n=== Debate ===\nWhether or not to attempt to contact extraterrestrials has attracted significant academic debate in the fields of space ethics and space policy. Physicist Stephen Hawking, in his book A Brief History of Time, suggests that "alerting" extraterrestrial intelligences to our existence is foolhardy, citing humankind\'s history of treating its own kind harshly in meetings of civilizations with a significant technology gap, e.g., the extermination of Tasmanian aborigines. He suggests, in view of this history, that we "lay low". In one response to Hawking, in September 2016, astronomer Seth Shostak sought to allay such concerns. Astronomer Jill Tarter also disagrees with Hawking, arguing that aliens developed and long-lived enough to communicate and travel across interstellar distances would have evolved a cooperative and less violent intelligence. She however thinks it is too soon for humans to attempt active SETI and that humans should be more advanced technologically first but keep listening in the meantime.\n== Criticism ==\nAs various SETI projects have progressed, some have criticized early claims by researchers as being too "euphoric". For example, Peter Schenkel, while remaining a supporter of SETI projects, wrote in 2006 that:\n[i]n light of new findings and insights, it seems appropriate to put excessive euphoria to rest and to take a more down-to-earth view [...] We should quietly admit that the early estimates—that there may be a million, a hundred thousand, or ten thousand advanced extraterrestrial civilizations in our galaxy—may no longer be tenable.\nCritics claim that the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability, as explained in a 2009 editorial in Nature, which said:\nSeti... has always sat at the edge of mainstream astronomy. This is partly because, no matter how scientifically rigorous its practitioners try to be, SETI can\'t escape an association with UFO believers and other such crackpots. But it is also because SETI is arguably not a falsifiable experiment. Regardless of how exhaustively the Galaxy is searched, the null result of radio silence doesn\'t rule out the existence of alien civilizations. It means only that those civilizations might not be using radio to communicate.\nNature added that SETI was "marked by a hope, bordering on faith" that aliens were aiming signals at us, that a hypothetical alien SETI project looking at Earth with "similar faith" would be "sorely disappointed", despite our many untargeted radar and TV signals, and our few targeted Active SETI radio signals denounced by those fearing aliens, and that it had difficulties attracting even sympathetic working scientists and government funding because it was "an effort so likely to turn up nothing".\nHowever, Nature also added, "Nonetheless, a small SETI effort is well worth supporting, especially given the enormous implications if it did succeed" and that "happily, a handful of wealthy technologists and other private donors have proved willing to provide that support".\nSupporters of the Rare Earth Hypothesis argue that advanced lifeforms are likely to be very rare, and that, if that is so, then SETI efforts will be futile. However, the Rare Earth Hypothesis itself faces many criticisms.\nIn 1993, Roy Mash stated that "Arguments favoring the existence of extraterrestrial intelligence nearly always contain an overt appeal to big numbers, often combined with a covert reliance on generalization from a single instance" and concluded that "the dispute between believers and skeptics is seen to boil down to a conflict of intuitions which can barely be engaged, let alone resolved, given our present state of knowledge". In response, in 2012, Milan M. Ćirković, then research professor at the Astronomical Observatory of Belgrade and a research associate of the Future of Humanity Institute at the University of Oxford, said that Mash was unrealistically over-reliant on excessive abstraction that ignored the empirical information available to modern SETI researchers.\nGeorge Basalla, Emeritus Professor of History at the University of Delaware, is a critic of SETI who argued in 2006 that "extraterrestrials discussed by scientists are as imaginary as the spirits and gods of religion or myth", and was in turn criticized by Milan M. Ćirković for, among other things, being unable to distinguish between "SETI believers" and "scientists engaged in SETI", who are often sceptical (especially about quick detection), such as Freeman Dyson and, at least in their later years, Iosif Shklovsky and Sebastian von Hoerner, and for ignoring the difference between the knowledge underlying the arguments of modern scientists and those of ancient Greek thinkers.\nMassimo Pigliucci, Professor of Philosophy at CUNY – City College, asked in 2010 whether SETI is "uncomfortably close to the status of pseudoscience" due to the lack of any clear point at which negative results cause the hypothesis of Extraterrestrial Intelligence to be abandoned, before eventually concluding that SETI is "almost-science", which is described by Milan M. Ćirković as Pigliucci putting SETI in "the illustrious company of string theory, interpretations of quantum mechanics, evolutionary psychology and history (of the \'synthetic\' kind done recently by Jared Diamond)", while adding that his justification for doing so with SETI "is weak, outdated, and reflecting particular philosophical prejudices similar to the ones described above in Mash and Basalla".\nRichard Carrigan, a particle physicist at the Fermi National Accelerator Laboratory near Chicago, Illinois, suggested that passive SETI could also be dangerous and that a signal released onto the Internet could act as a computer virus. Computer security expert Bruce Schneier dismissed this possibility as a "bizarre movie-plot threat".\n=== Ufology ===\nUfologist Stanton Friedman has often criticized SETI researchers for, among other reasons, what he sees as their unscientific criticisms of Ufology, but, unlike SETI, Ufology has generally not been embraced by academia as a scientific field of study, and it is usually characterized as a partial or total pseudoscience. In a 2016 interview, Jill Tarter pointed out that it is still a misconception that SETI and UFOs are related. She states, "SETI uses the tools of the astronomer to attempt to find evidence of somebody else\'s technology coming from a great distance. If we ever claim detection of a signal, we will provide evidence and data that can be independently confirmed. UFOs—none of the above." The Galileo Project headed by Harvard astronomer Avi Loeb is one of the few scientific efforts to study UFOs or UAPs. Loeb criticized that the study of UAP is often dismissed and not sufficiently studied by scientists and should shift from "occupying the talking points of national security administrators and politicians" to the realm of science. The Galileo Project\'s position after the publication of the 2021 UFO Report by the U.S. Intelligence community is that the scientific community needs to "systematically, scientifically and transparently look for potential evidence of extraterrestrial technological equipment".\n== See also ==\n== References ==\n== Further reading ==', 'An early use of the piezoelectricity of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. The quartz clock is a familiar device using the mineral. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.\nAlmost all the industrial demand for quartz crystal (used primarily in electronics) is met with synthetic quartz produced by the hydrothermal process. However, synthetic crystals are less prized for use as gemstones. The popularity of crystal healing has increased the demand for natural quartz crystals, which are now often mined in developing countries using primitive mining methods, sometimes involving child labor.\n== See also ==\nFused quartz\nList of minerals\nQuartz fiber\nQuartz reef mining\nQuartzolite\nShocked quartz\n== References ==\n== External links ==\nQuartz varieties, properties, crystal morphology. Photos and illustrations\nGilbert Hart, "Nomenclature of Silica", American Mineralogist, Volume 12, pp. 383–395. 1927\n"The Quartz Watch – Inventors". The Lemelson Center, National Museum of American History, Smithsonian Institution. Archived from the original on 7 January 2009.\nTerminology used to describe the characteristics of quartz crystals when used as oscillators\nQuartz use as prehistoric stone tool raw material']

Question: What is a "coffee ring" in physics?

Choices:
Choice A) A type of coffee that is made by boiling coffee grounds in water.
Choice B) A pattern left by a particle-laden liquid after it is spilled, named for the characteristic ring-like deposit along the perimeter of a spill of coffee or red wine.
Choice C) A type of coffee that is made by mixing instant coffee with hot water.
Choice D) A type of coffee that is made by pouring hot water over coffee grounds in a filter.
Choice E) A pattern left by a particle-laden liquid after it evaporates, named for the characteristic ring-like deposit along the perimeter of a spill of coffee or red wine.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['=== Instant valve closure; compressible fluid ===\nThe pressure profile of the water hammer pulse can be calculated from the Joukowsky equation\n{\\displaystyle {\\frac {\\partial P}{\\partial t}}=\\rho a{\\frac {\\partial v}{\\partial t}}.}\nSo for a valve closing instantaneously, the maximal magnitude of the water hammer pulse is\n{\\displaystyle \\Delta P=\\rho a_{0}\\Delta v,}\nwhere ΔP is the magnitude of the pressure wave (Pa), ρ is the density of the fluid (kg/m3), a0 is the speed of sound in the fluid (m/s), and Δv is the change in the fluid\'s velocity (m/s). The pulse comes about due to Newton\'s laws of motion and the continuity equation applied to the deceleration of a fluid element.\n==== Equation for wave speed ====\nAs the speed of sound in a fluid is\n{\\displaystyle a={\\sqrt {\\frac {B}{\\rho }}}}\n, the peak pressure depends on the fluid compressibility if the valve is closed abruptly.\n{\\displaystyle B={\\frac {K}{(1+{\\frac {V}{a}})(1+c{\\frac {KD}{Et}})}},}\nwhere\na = wave speed,\nB = equivalent bulk modulus of elasticity of the system fluid–pipe,\nρ = density of the fluid,\nK = bulk modulus of elasticity of the fluid,\nE = elastic modulus of the pipe,\nD = internal pipe diameter,\nt = pipe wall thickness,\nc = dimensionless parameter due to system pipe-constraint condition on wave speed.\n=== Slow valve closure; incompressible fluid ===\nWhen the valve is closed slowly compared to the transit time for a pressure wave to travel the length of the pipe, the elasticity can be neglected, and the phenomenon can be described in terms of inertance or rigid column theory:\n{\\displaystyle F=ma=PA=\\rho LA{dv \\over dt}.}\nAssuming constant deceleration of the water column (dv/dt = v/t), this gives\n{\\displaystyle P=\\rho Lv/t.}\nwhere:\nF = force [N],\nm = mass of the fluid column [kg],\na = acceleration [m/s2],\nP = pressure [Pa],\nA = pipe cross-section [m2],\nρ = fluid density [kg/m3],\nL = pipe length [m],\nv = flow velocity [m/s],\nt = valve closure time [s].\nThe above formula becomes, for water and with imperial unit,\n0.0135\n{\\displaystyle P=0.0135\\,VL/t.}\nFor practical application, a safety factor of about 5 is recommended:\n0.07\n{\\displaystyle P=0.07\\,VL/t+P_{1},}\nwhere P1 is the inlet pressure in psi, V is the flow velocity in ft/s, t is the valve closing time in seconds, and L is the upstream pipe length in feet.\nHence, we can say that the magnitude of the water hammer largely depends upon the time of closure, elastic components of pipe & fluid properties.\n== Expression for the excess pressure due to water hammer ==\nWhen a valve with a volumetric flow rate Q is closed, an excess pressure ΔP is created upstream of the valve, whose value is given by the Joukowsky equation:\n{\\displaystyle \\Delta P=ZQ.}\nIn this expression:\nΔP is the overpressurization in Pa;\nQ is the volumetric flow in m3/s;\nZ is the hydraulic impedance, expressed in kg/m4/s.\nThe hydraulic impedance Z of the pipeline determines the magnitude of the water hammer pulse. It is itself defined by\n{\\displaystyle Z={\\frac {\\sqrt {\\rho B}}{A}},}\nwhere\nρ the density of the liquid, expressed in kg/m3;\nA cross sectional area of the pipe, m2;\nB equivalent modulus of compressibility of the liquid in the pipe, expressed in Pa.\nThe latter follows from a series of hydraulic concepts:\ncompressibility of the liquid, defined by its adiabatic compressibility modulus Bl, resulting from the equation of state of the liquid generally available from thermodynamic tables;\nthe elasticity of the walls of the pipe, which defines an equivalent bulk modulus of compressibility for the solid Bs. In the case of a pipe of circular cross-section whose thickness t is small compared to the diameter D, the equivalent modulus of compressibility is given by the formula\n{\\displaystyle B={\\frac {t}{D}}E}\n, in which E is the Young\'s modulus (in Pa) of the material of the pipe;\npossibly compressibility Bg of gas dissolved in the liquid, defined by\n{\\displaystyle B_{\\text{g}}={\\frac {\\gamma }{\\alpha }}P,}\nγ being the specific heat ratio of the gas,\nα the rate of ventilation (the volume fraction of undissolved gas),\nand P the pressure (in Pa).\nThus, the equivalent elasticity is the sum of the original elasticities:\n{\\displaystyle {\\frac {1}{B}}={\\frac {1}{B_{\\text{l}}}}+{\\frac {1}{B_{\\text{s}}}}+{\\frac {1}{B_{\\text{g}}}}.}\nAs a result, we see that we can reduce the water hammer by:\nincreasing the pipe diameter at constant flow, which reduces the flow velocity and hence the deceleration of the liquid column;\nemploying the solid material as tight as possible with respect to the internal fluid bulk (solid Young modulus low with respect to fluid bulk modulus);\nintroducing a device that increases the flexibility of the entire hydraulic system, such as a hydraulic accumulator;\nwhere possible, increasing the fraction of undissolved gases in the liquid.\n== Dynamic equations ==\nThe water hammer effect can be simulated by solving the following partial differential equations.\n{\\displaystyle {\\frac {\\partial V}{\\partial x}}+{\\frac {1}{B}}{\\frac {dP}{dt}}=0,}\n{\\displaystyle {\\frac {dV}{dt}}+{\\frac {1}{\\rho }}{\\frac {\\partial P}{\\partial x}}+{\\frac {f}{2D}}V|V|=0,}\nwhere V is the fluid velocity inside pipe,\n{\\displaystyle \\rho }\nis the fluid density, B is the equivalent bulk modulus, and f is the Darcy–Weisbach friction factor.\n== Column separation ==\nColumn separation is a phenomenon that can occur during a water-hammer event.  If the pressure in a pipeline drops below the vapor pressure of the liquid, cavitation will occur (some of the liquid vaporizes, forming a bubble in the pipeline, keeping the pressure close to the vapor pressure).  This is most likely to occur at specific locations such as closed ends, high points or knees (changes in pipe slope).  When subcooled liquid flows into the space previously occupied by vapor the area of contact between the vapor and the liquid increases.  This causes the vapor to condense into the liquid reducing the pressure in the vapor space.  The liquid on either side of the vapor space is then accelerated into this space by the pressure difference.  The collision of the two columns of liquid (or of one liquid column if at a closed end) causes a large and nearly instantaneous rise in pressure.  This pressure rise can damage hydraulic machinery, individual pipes and supporting structures.  Many repetitions of cavity formation and collapse may occur in a single water-hammer event.\n== Simulation software ==\nMost water hammer software packages use the method of characteristics to solve the differential equations involved. This method works well if the wave speed does not vary in time due to either air or gas entrainment in a pipeline. The wave method (WM) is also used in various software packages. WM lets operators analyze large networks efficiently. Many commercial and non-commercial packages are available.\nSoftware packages vary in complexity, dependent on the processes modeled. The more sophisticated packages may have any of the following features:\nMultiphase flow capabilities.\nAn algorithm for cavitation growth and collapse.\nUnsteady friction: the pressure waves dampen as turbulence is generated and due to variations in the flow velocity distribution.\nVarying bulk modulus for higher pressures (water becomes less compressible).\nFluid structure interaction: the pipeline reacts on the varying pressures and causes pressure waves itself.\n== Applications ==\nThe water hammer principle can be used to create a simple water pump called a hydraulic ram.\nLeaks can sometimes be detected using water hammer.\nEnclosed air pockets can be detected in pipelines.\nThe water hammer from a liquid jet created by a collapsing microcavity is studied for potential applications noninvasive transdermal drug delivery.\n== See also ==\nBlood hammer\nCavitation\nFluid dynamics\nHydraulophone – musical instruments employing water and other fluids\nImpact force\nRecoil (fluid behavior)\nTransient (civil engineering)\nWatson\'s water hammer pulse\n== References ==\n== External links ==\nWhat Is Water Hammer/Steam Hammer?\n"Water hammer"—YouTube (animation)\n"Water Hammer Theory Explained"—YouTube; with examples', 'Hydraulic shock', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.  The main forms of precipitation include drizzle, rain, Rain and snow mixed ("sleet" in Commonwealth usage), snow, ice pellets, graupel and hail. Precipitation occurs when a portion of the atmosphere becomes saturated with water vapor (reaching 100% relative humidity), so that the water condenses and "precipitates" or falls. Thus, fog and mist are not precipitation; their water vapor does not condense sufficiently to precipitate, so fog and mist do not fall. (Such a non-precipitating combination is a colloid.) Two processes, possibly acting together, can lead to air becoming saturated with water vapor: cooling the air or adding water vapor to the air. Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called showers.\nMoisture that is lifted or otherwise forced to rise over a layer of sub-freezing air at the surface may be condensed by the low temperature into clouds and rain. This process is typically active when freezing rain occurs. A stationary front is often present near the area of freezing rain and serves as the focus for forcing moist air to rise. Provided there is necessary and sufficient atmospheric moisture content, the moisture within the rising air will condense into clouds, namely nimbostratus and cumulonimbus if significant precipitation is involved. Eventually, the cloud droplets will grow large enough to form raindrops and descend toward the Earth where they will freeze on contact with exposed objects. Where relatively warm water bodies are present, for example due to water evaporation from lakes, lake-effect snowfall becomes a concern downwind of the warm lakes within the cold cyclonic flow around the backside of extratropical cyclones. Lake-effect snowfall can be locally heavy.  Thundersnow is possible within a cyclone\'s comma head and within lake effect precipitation bands. In mountainous areas, heavy precipitation is possible where upslope flow is maximized within windward sides of the terrain at elevation. On the leeward side of mountains, desert climates can exist due to the dry air caused by compressional heating. Most precipitation occurs within the tropics and is caused by convection.\nPrecipitation is a major component of the water cycle, and is responsible for depositing most of the fresh water on the planet. Approximately 505,000 cubic kilometres (121,000 cu mi) of water falls as precipitation each year: 398,000 cubic kilometres (95,000 cu mi) over oceans and 107,000 cubic kilometres (26,000 cu mi) over land. Given the Earth\'s surface area, that means the globally averaged annual precipitation is 990 millimetres (39 in), but over land it is only 715 millimetres (28.1 in). Climate classification systems such as the Köppen climate classification system use average annual rainfall to help differentiate between differing climate regimes. Global warming is already causing changes to weather, increasing precipitation in some geographies, and reducing it in others, resulting in additional extreme weather.\nPrecipitation may occur on other celestial bodies. Saturn\'s largest satellite, Titan, hosts methane precipitation as a slow-falling drizzle, which has been observed as rain puddles at its equator and polar regions.\n== Types ==\nMechanisms of producing precipitation include convective, stratiform, and orographic rainfall.  Convective processes involve strong vertical motions that can cause the overturning of the atmosphere in that location within an hour and cause heavy precipitation, while stratiform processes involve weaker upward motions and less intense precipitation.  Precipitation can be divided into three categories, based on whether it falls as liquid water, liquid water that freezes on contact with the surface, or ice. Mixtures of different types of precipitation, including types in different categories, can fall simultaneously. Liquid forms of precipitation include rain and drizzle. Rain or drizzle that freezes on contact within a subfreezing air mass is called "freezing rain" or "freezing drizzle". Frozen forms of precipitation include snow, ice needles, ice pellets, hail, and graupel.\n=== Measurement ===\nLiquid precipitation\nRainfall (including drizzle and rain) is usually measured using a rain gauge and expressed in units of millimeters (mm) of height or depth. Equivalently, it can be expressed as a physical quantity with dimension of volume of water per collection area, in units of liters per square meter (L/m2); as 1L = 1dm3 = 1mm·m2, the units of area (m2) cancel out, resulting in simply "mm". This also corresponds to an area density expressed in kg/m2, if assuming that 1 liter of water has a mass of 1 kg (water density), which is acceptable for most practical purposes. The corresponding English unit used is usually inches. In Australia before metrication, rainfall was also measured in "points", each of which was defined as one-hundredth of an inch.\nSolid precipitation\nA snow gauge is usually used to measure the amount of solid precipitation. Snowfall is usually measured in centimeters by letting snow fall into a container and then measure the height. The snow can then optionally be melted to obtain a water equivalent measurement in millimeters like for liquid precipitation. The relationship between snow height and water equivalent depends on the water content of the snow; the water equivalent can thus only provide a rough estimate of snow depth. Other forms of solid precipitation, such as snow pellets and hail or even rain and snow mixed ("sleet" in Commonwealth usage), can also be melted and measured as their respective water equivalents, usually expressed in millimeters as for liquid precipitation.\n== Air becomes saturated ==\n=== Cooling air to its dew point ===\nThe dew point is the temperature to which a parcel of air must be cooled in order to become saturated, and (unless super-saturation occurs) condenses to water.  Water vapor normally begins to condense on condensation nuclei such as dust, ice, and salt in order to form clouds. The cloud condensation nuclei concentration will determine the cloud microphysics. An elevated portion of a frontal zone forces broad areas of lift, which form cloud decks such as altostratus or cirrostratus.  Stratus is a stable cloud deck which tends to form when a cool, stable air mass is trapped underneath a warm air mass. It can also form due to the lifting of advection fog during breezy conditions.\nThere are four main mechanisms for cooling the air to its dew point: adiabatic cooling, conductive cooling, radiational cooling, and evaporative cooling. Adiabatic cooling occurs when air rises and expands. The air can rise due to convection, large-scale atmospheric motions, or a physical barrier such as a mountain (orographic lift). Conductive cooling occurs when the air comes into contact with a colder surface, usually by being blown from one surface to another, for example from a liquid water surface to colder land. Radiational cooling occurs due to the emission of infrared radiation, either by the air or by the surface underneath.  Evaporative cooling occurs when moisture is added to the air through evaporation, which forces the air temperature to cool to its wet-bulb temperature, or until it reaches saturation.\n=== Adding moisture to the air ===\nThe main ways water vapor is added to the air are: wind convergence into areas of upward motion, precipitation or virga falling from above, daytime heating evaporating water from the surface of oceans, water bodies or wet land, transpiration from plants, cool or dry air moving over warmer water, and lifting air over mountains.\n== Forms of precipitation ==\n=== Raindrops ===\nCoalescence occurs when water droplets fuse to create larger water droplets, or when water droplets freeze onto an ice crystal, which is known as the Bergeron process. The fall rate of very small droplets is negligible, hence clouds do not fall out of the sky; precipitation will only occur when these coalesce into larger drops. droplets with different size will have different terminal velocity that cause droplets collision and producing larger droplets, Turbulence will enhance the collision process. As these larger water droplets descend, coalescence continues, so that drops become heavy enough to overcome air resistance and fall as rain.\nRaindrops have sizes ranging from 5.1 to 20 millimetres (0.20 to 0.79 in) mean diameter, above which they tend to break up. Smaller drops are called cloud droplets, and their shape is spherical. As a raindrop increases in size, its shape becomes more oblate, with its largest cross-section facing the oncoming airflow. Contrary to the cartoon pictures of raindrops, their shape does not resemble a teardrop. Intensity and duration of rainfall are usually inversely related, i.e., high intensity storms are likely to be of short duration and low intensity storms can have a long duration.  Rain drops associated with melting hail tend to be larger than other rain drops.  The METAR code for rain is RA, while the coding for rain showers is SHRA.\n=== Ice pellets ===\nIce pellets ("sleet" in US usage) are a form of precipitation consisting of small, translucent balls of ice. Ice pellets are usually (but not always) smaller than hailstones.  They often bounce when they hit the ground, and generally do not freeze into a solid mass unless mixed with freezing rain. The METAR code for ice pellets is PL.', 'Time is the continuous progression of existence that occurs in an apparently irreversible succession from the past, through the present, and into the future. It is a component quantity of various measurements used to sequence events, to compare the duration of events (or the intervals between them), and to quantify rates of change of quantities in material reality or in the conscious experience. Time is often referred to as a fourth dimension, along with three spatial dimensions.\nTime is one of the seven fundamental physical quantities in both the International System of Units (SI) and International System of Quantities. The SI base unit of time is the second, which is defined by measuring the electronic transition frequency of caesium atoms. General relativity is the primary framework for understanding how spacetime works. Through advances in both theoretical and experimental investigations of spacetime, it has been shown that time can be distorted and dilated, particularly at the edges of black holes.\nThroughout history, time has been an important subject of study in religion, philosophy, and science. Temporal measurement has occupied scientists and technologists, and has been a prime motivation in navigation and astronomy. Time is also of significant social importance, having economic value ("time is money") as well as personal value, due to an awareness of the limited time in each day ("carpe diem") and in human life spans.\n== Definition ==\nThe concept of time can be complex. Multiple notions exist, and defining time in a manner applicable to all fields without circularity has consistently eluded scholars. Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems. Traditional definitions of time involved the observation of periodic motion such as the apparent motion of the sun across the sky, the phases of the moon, and the passage of a free-swinging pendulum. More modern systems include the Global Positioning System, other satellite systems, Coordinated Universal Time and mean solar time. Although these systems differ from one another, with careful measurements they can be synchronized.\nIn physics, time is a fundamental concept to define other quantities, such as velocity. To avoid a circular definition, time in physics is operationally defined as "what a clock reads", specifically a count of repeating events such as the SI second. Although this aids in practical measurements, it does not address the essence of time. Physicists developed the concept of the spacetime continuum, where events are assigned four coordinates: three for space and one for time. Events like particle collisions, supernovas, or rocket launches have coordinates that may vary for different observers, making concepts like "now" and "here" relative. In general relativity, these coordinates do not directly correspond to the causal structure of events. Instead, the spacetime interval is calculated and classified as either space-like or time-like, depending on whether an observer exists that would say the events are separated by space or by time. Since the time required for light to travel a specific distance is the same for all observers—a fact first publicly demonstrated by the Michelson–Morley experiment—all observers will consistently agree on this definition of time as a causal relation.\nGeneral relativity does not address the nature of time for extremely small intervals where quantum mechanics holds. In quantum mechanics, time is treated as a universal and absolute parameter, differing from general relativity\'s notion of independent clocks. The problem of time consists of reconciling these two theories. As of 2025, there is no generally accepted theory of quantum general relativity.\n== Measurement ==\nMethods of temporal measurement, or chronometry, generally take two forms. The first is a calendar, a mathematical tool for organising intervals of time on Earth, consulted for periods longer than a day. The second is a clock, a physical mechanism that indicates the passage of time, consulted for periods less than a day. The combined measurement marks a specific moment in time from a reference point, or epoch.\n=== History of the calendar ===\nArtifacts from the Paleolithic suggest that the moon was used to reckon time as early as 6,000 years ago. Lunar calendars were among the first to appear, with years of either 12 or 13 lunar months (either 354 or 384 days). Without intercalation to add days or months to some years, seasons quickly drift in a calendar based solely on twelve lunar months. Lunisolar calendars have a thirteenth month added to some years to make up for the difference between a full year (now known to be about 365.24 days) and a year of just twelve lunar months. The numbers twelve and thirteen came to feature prominently in many cultures, at least partly due to this relationship of months to years.\nOther early forms of calendars originated in Mesoamerica, particularly in ancient Mayan civilization, in which they developed the Maya calendar, consisting of multiple interrelated calendars. These calendars were religiously and astronomically based; the Haab\' calendar has 18 months in a year and 20 days in a month, plus five epagomenal days at the end of the year. In conjunction, the Maya also used a 260-day sacred calendar called the Tzolk\'in.\nThe reforms of Julius Caesar in 45 BC put the Roman world on a solar calendar. This Julian calendar was faulty in that its intercalation still allowed the astronomical solstices and equinoxes to advance against it by about 11 minutes per year. Pope Gregory XIII introduced a correction in 1582; the Gregorian calendar was only slowly adopted by different nations over a period of centuries, but it is now by far the most commonly used calendar around the world.\nDuring the French Revolution, a new clock and calendar were invented as part of the dechristianization of France and to create a more rational system in order to replace the Gregorian calendar. The French Republican Calendar\'s days consisted of ten hours of a hundred minutes of a hundred seconds, which marked a deviation from the base 12 (duodecimal) system used in many other devices by many cultures. The system was abolished in 1806.\n=== History of other devices ===\nA large variety of devices have been invented to measure time. The study of these devices is called horology. They can be driven by a variety of means, including gravity, springs, and various forms of electrical power, and regulated by a variety of means.\nA sundial is any device that uses the direction of sunlight to cast shadows from a gnomon onto a set of markings calibrated to indicate the local time, usually to the hour. The idea to separate the day into smaller parts is credited to Egyptians because of their sundials, which operated on a duodecimal system. The importance of the number 12 is due to the number of lunar cycles in a year and the number of stars used to count the passage of night. Obelisks made as a gnomon were built as early as c.\u20093500 BC. An Egyptian device that dates to c.\u20091500 BC, similar in shape to a bent T-square, also measured the passage of time from the shadow cast by its crossbar on a nonlinear rule. The T was oriented eastward in the mornings. At noon, the device was turned around so that it could cast its shadow in the evening direction.\nAlarm clocks reportedly first appeared in ancient Greece c.\u2009250 BC with a water clock made by Plato that would set off a whistle. The hydraulic alarm worked by gradually filling a series of vessels with water. After some time, the water emptied out of a siphon. Inventor Ctesibius revised Plato\'s design; the water clock uses a float as the power drive system and uses a sundial to correct the water flow rate.\nIn medieval philosophical writings, the atom was a unit of time referred to as the smallest possible division of time. The earliest known occurrence in English is in Byrhtferth\'s Enchiridion (a science text) of 1010–1012, where it was defined as 1/564 of a momentum (11⁄2 minutes), and thus equal to 15/94 of a second. It was used in the computus, the process of calculating the date of Easter. The most precise timekeeping device of the ancient world was the water clock, or clepsydra, one of which was found in the tomb of Egyptian pharaoh Amenhotep I. They could be used to measure the hours even at night but required manual upkeep to replenish the flow of water. The ancient Greeks and the people from Chaldea (southeastern Mesopotamia) regularly maintained timekeeping records as an essential part of their astronomical observations. Arab inventors and engineers, in particular, made improvements on the use of water clocks up to the Middle Ages. In the 11th century, Chinese inventors and engineers invented the first mechanical clocks driven by an escapement mechanism.\nIncense sticks and candles were, and are, commonly used to measure time in temples and churches across the globe. Water clocks, and, later, mechanical clocks, were used to mark the events of the abbeys and monasteries of the Middle Ages. The passage of the hours at sea can also be marked by bell. The hours were marked by bells in abbeys as well as at sea. Richard of Wallingford (1292–1336), abbot of St. Alban\'s abbey, famously built a mechanical clock as an astronomical orrery about 1330. The hourglass uses the flow of sand to measure the flow of time. They were also used in navigation. Ferdinand Magellan used 18 glasses on each ship for his circumnavigation of the globe (1522). The English word clock probably comes from the Middle Dutch word klocke which, in turn, derives from the medieval Latin word clocca, which ultimately derives from Celtic and is cognate with French, Latin, and German words that mean bell.', 'is known as the Stefan–Boltzmann constant.\n=== Radiative transfer ===\nThe equation of radiative transfer describes the way in which radiation is affected as it travels through a material medium. For the special case in which the material medium is in thermodynamic equilibrium in the neighborhood of a point in the medium, Planck\'s law is of special importance.\nFor simplicity, we can consider the linear steady state, without scattering. The equation of radiative transfer states that for a beam of light going through a small distance ds, energy is conserved: The change in the (spectral) radiance of that beam (Iν) is equal to the amount removed by the material medium plus the amount gained from the material medium. If the radiation field is in equilibrium with the material medium, these two contributions will be equal. The material medium will have a certain emission coefficient and absorption coefficient.\nThe absorption coefficient α is the fractional change in the intensity of the light beam as it travels the distance ds, and has units of length−1. It is composed of two parts, the decrease due to absorption and the increase due to stimulated emission. Stimulated emission is emission by the material body which is caused by and is proportional to the incoming radiation. It is included in the absorption term because, like absorption, it is proportional to the intensity of the incoming radiation. Since the amount of absorption will generally vary linearly as the density ρ of the material, we may define a "mass absorption coefficient" κν = \u2060α/ρ\u2060 which is a property of the material itself. The change in intensity of a light beam due to absorption as it traverses a small distance ds will then be\n{\\displaystyle dI_{\\nu }=-\\kappa _{\\nu }\\rho I_{\\nu }\\,ds}\nThe "mass emission coefficient" jν is equal to the radiance per unit volume of a small volume element divided by its mass (since, as for the mass absorption coefficient, the emission is proportional to the emitting mass) and has units of power⋅solid angle−1⋅frequency−1⋅density−1. Like the mass absorption coefficient, it too is a property of the material itself. The change in a light beam as it traverses a small distance ds will then be\n{\\displaystyle dI_{\\nu }=j_{\\nu }\\rho \\,ds}\nThe equation of radiative transfer will then be the sum of these two contributions:\n{\\displaystyle {\\frac {dI_{\\nu }}{ds}}=j_{\\nu }\\rho -\\kappa _{\\nu }\\rho I_{\\nu }.}\nIf the radiation field is in equilibrium with the material medium, then the radiation will be homogeneous (independent of position) so that dIν = 0 and:\n{\\displaystyle \\kappa _{\\nu }B_{\\nu }=j_{\\nu }}\nwhich is another statement of Kirchhoff\'s law, relating two material properties of the medium, and which yields the radiative transfer equation at a point around which the medium is in thermodynamic equilibrium:\n{\\displaystyle {\\frac {dI_{\\nu }}{ds}}=\\kappa _{\\nu }\\rho (B_{\\nu }-I_{\\nu }).}\n=== Einstein coefficients ===\nThe principle of detailed balance states that, at thermodynamic equilibrium, each elementary process is in equilibrium with its reverse process.\nIn 1916, Albert Einstein applied this principle on an atomic level to the case of an atom radiating and absorbing radiation due to transitions between two particular energy levels, giving a deeper insight into the equation of radiative transfer and Kirchhoff\'s law for this type of radiation. If level 1 is the lower energy level with energy E1, and level 2 is the upper energy level with energy E2, then the frequency ν of the radiation radiated or absorbed will be determined by Bohr\'s frequency condition:\n{\\displaystyle E_{2}-E_{1}=h\\nu .}\nIf n1 and n2 are the number densities of the atom in states 1 and 2 respectively, then the rate of change of these densities in time will be due to three processes:\nSpontaneous emission\n21\n{\\displaystyle \\left({\\frac {dn_{1}}{dt}}\\right)_{\\mathrm {spon} }=A_{21}n_{2}}\nStimulated emission\n21\n{\\displaystyle \\left({\\frac {dn_{1}}{dt}}\\right)_{\\mathrm {stim} }=B_{21}n_{2}u_{\\nu }}\nPhoto-absorption\n12\n{\\displaystyle \\left({\\frac {dn_{2}}{dt}}\\right)_{\\mathrm {abs} }=B_{12}n_{1}u_{\\nu }}\nwhere uν is the spectral energy density of the radiation field. The three parameters A21, B21 and B12, known as the Einstein coefficients, are associated with the photon frequency ν produced by the transition between two energy levels (states). As a result, each line in a spectrum has its own set of associated coefficients. When the atoms and the radiation field are in equilibrium, the radiance will be given by Planck\'s law and, by the principle of detailed balance, the sum of these rates must be zero:\n21\n21\n12\n{\\displaystyle 0=A_{21}n_{2}+B_{21}n_{2}{\\frac {4\\pi }{c}}B_{\\nu }(T)-B_{12}n_{1}{\\frac {4\\pi }{c}}B_{\\nu }(T)}\nSince the atoms are also in equilibrium, the populations of the two levels are related by the Boltzmann factor:\n{\\displaystyle {\\frac {n_{2}}{n_{1}}}={\\frac {g_{2}}{g_{1}}}e^{-h\\nu /k_{\\mathrm {B} }T}}\nwhere g1 and g2 are the multiplicities of the respective energy levels. Combining the above two equations with the requirement that they be valid at any temperature yields two relationships between the Einstein coefficients:\n21\n21\n{\\displaystyle {\\frac {A_{21}}{B_{21}}}={\\frac {8\\pi h\\nu ^{3}}{c^{3}}}}\n21\n12\n{\\displaystyle {\\frac {B_{21}}{B_{12}}}={\\frac {g_{1}}{g_{2}}}}\nso that knowledge of one coefficient will yield the other two.\nFor the case of isotropic absorption and emission, the emission coefficient (jν) and absorption coefficient (κν) defined in the radiative transfer section above, can be expressed in terms of the Einstein coefficients. The relationships between the Einstein coefficients will yield the expression of Kirchhoff\'s law expressed in the Radiative transfer section above, namely that\n{\\displaystyle j_{\\nu }=\\kappa _{\\nu }B_{\\nu }.}\nThese coefficients apply to both atoms and molecules.\n== Properties ==\n=== Peaks ===\nThe distributions Bν, Bω, Bν̃ and Bk peak at a photon energy of\n2.821\n{\\displaystyle E=\\left[3+W\\left(-3e^{-3}\\right)\\right]k_{\\mathrm {B} }T\\approx 2.821\\ k_{\\mathrm {B} }T,}\nwhere W is the Lambert W function and e is Euler\'s number.\nHowever, the distribution Bλ peaks at a different energy\n4.965\n{\\displaystyle E=\\left[5+W\\left(-5e^{-5}\\right)\\right]k_{\\mathrm {B} }T\\approx 4.965\\ k_{\\mathrm {B} }T,}\nThe reason for this is that, as mentioned above, one cannot go from (for example) Bν to Bλ simply by substituting ν by λ. In addition, one must also multiply by\n{\\textstyle \\left|{d\\nu }/{d\\lambda }\\right|=c/{\\lambda ^{2}}}\n, which shifts the peak of the distribution to higher energies. These peaks are the mode energy of a photon, when binned using equal-size bins of frequency or wavelength, respectively. Dividing hc (14387.770 μm·K) by these energy expression gives the wavelength of the peak.\nThe spectral radiance at these peaks is given by:\nmax\n1.896\n10\n19\n{\\displaystyle {\\begin{aligned}B_{\\nu ,{\\text{max}}}(T)&={\\frac {2k_{\\mathrm {B} }^{3}T^{3}x^{3}}{h^{2}c^{2}}}{\\frac {1}{e^{x}-1}}\\\\&\\approx 1.896\\times 10^{-19}{\\frac {\\mathrm {W} }{\\mathrm {m^{2}\\cdot Hz\\cdot sr} }}\\times (T/\\mathrm {K} )^{3}\\\\\\end{aligned}}}\nwith\n{\\displaystyle x=3+W(-3e^{-3}),}\nand\nmax\n4.096\n10\nsr\n{\\displaystyle {\\begin{aligned}B_{\\lambda ,{\\text{max}}}(T)&={\\frac {2k_{\\mathrm {B} }^{5}T^{5}x^{5}}{h^{4}c^{3}}}{\\frac {1}{e^{x}-1}}\\\\&\\approx 4.096\\times 10^{-6}{\\frac {\\text{W}}{{\\text{m}}^{2}\\cdot {\\text{sr}}}}\\times ~(T/{\\text{K}})^{5}\\end{aligned}}}\nwith\n{\\displaystyle x=5+W(-5e^{-5}).}\nMeanwhile, the average energy of a photon from a blackbody is\n30\n2.701\n{\\displaystyle E=\\left[{\\frac {\\pi ^{4}}{30\\ \\zeta (3)}}\\right]k_{\\mathrm {B} }T\\approx 2.701\\ k_{\\mathrm {B} }T,}\nwhere\n{\\displaystyle \\zeta }\nis the Riemann zeta function.\n=== Approximations ===\nIn the limit of low frequencies (i.e. long wavelengths), Planck\'s law becomes the Rayleigh–Jeans law\n{\\displaystyle B_{\\nu }(T)\\approx {\\frac {2\\nu ^{2}}{c^{2}}}k_{\\mathrm {B} }T}\nor\n{\\displaystyle B_{\\lambda }(T)\\approx {\\frac {2c}{\\lambda ^{4}}}k_{\\mathrm {B} }T}\nThe radiance increases as the square of the frequency, illustrating the ultraviolet catastrophe. In the limit of high frequencies (i.e. small wavelengths) Planck\'s law tends to the Wien approximation:\n{\\displaystyle B_{\\nu }(T)\\approx {\\frac {2h\\nu ^{3}}{c^{2}}}e^{-{\\frac {h\\nu }{k_{\\mathrm {B} }T}}}}\nor\n{\\displaystyle B_{\\lambda }(T)\\approx {\\frac {2hc^{2}}{\\lambda ^{5}}}e^{-{\\frac {hc}{\\lambda k_{\\mathrm {B} }T}}}.}\n=== Percentiles ===\nWien\'s displacement law in its stronger form states that the shape of Planck\'s law is independent of temperature. It is therefore possible to list the percentile points of the total radiation as well as the peaks for wavelength and frequency, in a form which gives the wavelength λ when divided by temperature T. The second column of the following table lists the corresponding values of λT, that is, those values of x for which the wavelength λ is \u2060x/T\u2060 micrometers at the radiance percentile point given by the corresponding entry in the first column.\nThat is, 0.01% of the radiation is at a wavelength below \u2060910/T\u2060 μm, 20% below \u20602676/T\u2060 μm, etc. The wavelength and frequency peaks are in bold and occur at 25.0% and 64.6% respectively. The 41.8% point is the wavelength-frequency-neutral peak (i.e. the peak in power per unit change in logarithm of wavelength or frequency). These are the points at which the respective Planck-law functions \u20601/λ5\u2060, ν3 and \u2060ν2/λ2\u2060, respectively, divided by exp(\u2060hν/kBT\u2060) − 1 attain their maxima. The much smaller gap in ratio of wavelengths between 0.1% and 0.01% (1110 is 22% more than 910) than between 99.9% and 99.99% (113374 is 120% more than 51613) reflects the exponential decay of energy at short wavelengths (left end) and polynomial decay at long.', 'Precipitation', 'Brown dwarfs form similarly to stars and are surrounded by protoplanetary disks, such as Cha 110913−773444. Disks around brown dwarfs have been found to have many of the same features as disks around stars; therefore, it is expected that there will be accretion-formed planets around brown dwarfs. Given the small mass of brown dwarf disks, most planets will be terrestrial planets rather than gas giants. If a giant planet orbits a brown dwarf across our line of sight, then, because they have approximately the same diameter, this would give a large signal for detection by transit. The accretion zone for planets around a brown dwarf is very close to the brown dwarf itself, so tidal forces would have a strong effect.\nIn 2020, the closest brown dwarf with an associated primordial disk (class II disk)—WISEA J120037.79-784508.3 (W1200-7845)—was discovered by the Disk Detective project when classification volunteers noted its infrared excess. It was vetted and analyzed by the science team who found that W1200-7845 had a 99.8% probability of being a member of the ε Chamaeleontis (ε Cha) young moving group association. Its parallax (using Gaia DR2 data) puts it at a distance of 102 parsecs (or 333 lightyears) from Earth—which is within the local Solar neighborhood.\nA paper from 2021 studied circumstellar discs around brown dwarfs in stellar associations that are a few million years old and 140 to 200 parsecs away. The researchers found that these disks are not massive enough to form planets in the future. There is evidence in these disks that might indicate that planet formation begins at earlier stages and that planets are already present in these disks. The evidence for disk evolution includes a decreasing disk mass over time, dust grain growth and dust settling. Two brown dwarf disks were also found in absorption and at least 4 disks are photoevaporating from external UV-ratiation in the Orion Nebula. Such objects are also called proplyds. Proplyd 181−247, which is a brown dwarf or low-mass star, is surrounded by a disk with a radius of 30 astronomical units and the disk has a mass of 6.2±1.0 MJ. Disks around brown dwarfs usually have a radius smaller than 40 astronomical units, but three disks in the more distant Taurus molecular cloud have a radius larger than 70 au and were resolved with ALMA. These larger disks are able to form rocky planets with a mass >1 ME. There are also brown dwarfs with disks in associations older than a few million years, which might be evidence that disks around brown dwarfs need more time to dissipate. Especially old disks (>20 Myrs) are sometimes called Peter Pan disks. Currently 2MASS J02265658-5327032 is the only known brown dwarf that has a Peter Pan disk.\nThe brown dwarf Cha 110913−773444, located 500 light-years away in the constellation Chamaeleon, may be in the process of forming a miniature planetary system. Astronomers from Pennsylvania State University have detected what they believe to be a disk of gas and dust similar to the one hypothesized to have formed the Solar System. Cha 110913−773444 is the smallest brown dwarf found to date (8 MJ), and if it formed a planetary system, it would be the smallest-known object to have one.\n== Planets around brown dwarfs ==\nAccording to the IAU working definition (from August 2018) an exoplanet can orbit a brown dwarf. It requires a mass below 13 MJ and a mass ratio of M/Mcentral<2/(25+√621). This means that an object with a mass up to 3.2 MJ around a brown dwarf with a mass of 80 MJ is considered a planet. It also means that an object with a mass up to 0.52 MJ around a brown dwarf with a mass of 13 MJ is considered a planet.\nThe super-Jupiter planetary-mass objects 2M1207b, 2MASS J044144 and Oph 98 B that are orbiting brown dwarfs at large orbital distances may have formed by cloud collapse rather than accretion and so may be sub-brown dwarfs rather than planets, which is inferred from relatively large masses and large orbits. The first discovery of a low-mass companion orbiting a brown dwarf (ChaHα8) at a small orbital distance using the radial velocity technique paved the way for the detection of planets around brown dwarfs on orbits of a few AU or smaller. However, with a mass ratio between the companion and primary in ChaHα8 of about 0.3, this system rather resembles a binary star. Then, in 2008, the first planetary-mass companion in a relatively small orbit (MOA-2007-BLG-192Lb) was discovered orbiting a brown dwarf.\nPlanets around brown dwarfs are likely to be carbon planets depleted of water.\nA 2017 study, based upon observations with Spitzer estimates that 175 brown dwarfs need to be monitored in order to guarantee (95%) at least one detection of a below earth-sized planet via the transiting method. JWST could potentially detect smaller planets. The orbits of planets and moons in the solar system often align with the orientation of the host star/planet they orbit. Assuming the orbit of a planet is aligned with the rotational axis of a brown dwarf or planetary-mass object, the geometric transit probability of an object similar to Io can be calculated with the formula cos(79.5°)/cos(inclination). The inclination was estimated for several brown dwarfs and planetary-mass objects. SIMP 0136 for example has an estimated inclination of 80°±12. Assuming the lower bound of i≥68° for SIMP 0136, this results in a transit probability of ≥48.6% for close-in planets. It is however not known how common close-in planets are around brown dwarfs and they might be more common for lower-mass objects, as disk sizes seem to decrease with mass.\nStrong evidence of a circumbinary planet in a polar orbit around 2M1510 was presented in 2025. The discovery was made with the Very Large Telescope.\n=== Habitability ===\nHabitability for hypothetical planets orbiting brown dwarfs has been studied. Computer models suggesting conditions for these bodies to have habitable planets are very stringent, the habitable zone being narrow, close (T dwarf 0.005 au) and decreasing with time, due to the cooling of the brown dwarf (they fuse for at most 10 million years).    The orbits there would have to be of extremely low eccentricity (on the order of 10−6) to avoid strong tidal forces that would trigger a runaway greenhouse effect on the planets, rendering them uninhabitable. There would also be no moons.\n== Superlative brown dwarfs ==\nIn 1984, it was postulated by some astronomers that the Sun may be orbited by an undetected brown dwarf (sometimes referred to as Nemesis) that could interact with the Oort cloud just as passing stars can. However, this hypothesis has fallen out of favor.\n=== Table of firsts ===\n=== Table of extremes ===\n== See also ==\nFusor (astronomy)\nBrown-dwarf desert – Theorized range of orbits around a star within which brown dwarfs cannot exist as companion objects\nBlue dwarf (red-dwarf stage) – Hypothetical class of star that develops from a red dwarf\nDark matter – Concept in cosmology\nExoplanet – Planet outside the Solar System\nStellification\nWD 0032-317 b\nList of brown dwarfs\nList of Y-dwarfs\n== Footnotes ==\n== References ==\n== External links ==\nHubbleSite newscenter – Weather patterns on a brown dwarf\nAllard, France; Homeier, Derek (2007). "Brown dwarfs". Scholarpedia. 2 (12): 4475. Bibcode:2007SchpJ...2.4475A. doi:10.4249/scholarpedia.4475.\n=== History ===\nKumar, Shiv S.; Low-Luminosity Stars. Gordon and Breach, London, 1969—an early overview paper on brown dwarfs\nThe Columbia Encyclopedia: "Brown Dwarfs"\n=== Details ===\nA current list of L and T dwarfs\nA geological definition of brown dwarfs, contrasted with stars and planets (via Berkeley)\nI. Neill Reid\'s pages at the Space Telescope Science Institute:\nOn spectral analysis of M dwarfs, L dwarfs, and T dwarfs\nTemperature and mass characteristics of low-temperature dwarfs\nFirst X-ray from brown dwarf observed, Spaceref.com, 2000\nMontes, David; "Brown Dwarfs and ultracool dwarfs (late-M, L, T)", UCM\nWild Weather: Iron Rain on Failed Stars—scientists are investigating astonishing weather patterns on brown dwarfs, Space.com, 2006\nNASA Brown dwarf detectives Archived 2014-10-17 at the Wayback Machine—Detailed information in a simplified sense\nBrown Dwarfs—Website with general information about brown dwarfs (has many detailed and colorful artist\'s impressions)\n=== Stars ===\nCha Halpha 1 stats and history\n"A census of observed brown dwarfs" (not all confirmed), 1998\nLuhman, Kevin L.; Adame, Lucía; d\'Alessio, Paola; Calvet, Nuria; Hartmann, Lee; Megeath, S. Thomas; Fazio, Giovanni G. (2005). "Discovery of a Planetary-Mass Brown Dwarf with a Circumstellar Disk". The Astrophysical Journal. 635 (1): L93 – L96. arXiv:astro-ph/0511807. Bibcode:2005ApJ...635L..93L. doi:10.1086/498868. S2CID 11685964.\nMichaud, Peter; Heyer, Inge; Leggett, Sandy K.; and Adamson, Andy; "Discovery Narrows the Gap Between Planets and Brown Dwarfs", Gemini and Joint Astronomy Centre, 2007\nDeacon, N. R.; Hambly, N. C. (2006). "The possiblity of detection of ultracool dwarfs with the UKIRT Infrared Deep Sky Survey". Monthly Notices of the Royal Astronomical Society. 371 (4): 1722–1730. arXiv:astro-ph/0607305. Bibcode:2006MNRAS.371.1722D. doi:10.1111/j.1365-2966.2006.10795.x.', 'An atmosphere (from Ancient Greek  ἀτμός (atmós) \'vapour, steam\' and  σφαῖρα (sphaîra) \'sphere\') is a layer of gases that envelop an astronomical object, held in place by the gravity of the object. A planet retains an atmosphere when the gravity is great and the temperature of the atmosphere is low. A stellar atmosphere is the outer region of a star, which includes the layers above the opaque photosphere; stars of low temperature might have outer atmospheres containing compound molecules.\nThe atmosphere of Earth is composed of nitrogen (78%), oxygen (21%), argon (0.9%), carbon dioxide (0.04%) and trace gases. Most organisms use oxygen for respiration; lightning and bacteria perform nitrogen fixation which produces ammonia that is used to make nucleotides and amino acids; plants, algae, and cyanobacteria use carbon dioxide for photosynthesis. The layered composition of the atmosphere minimises the harmful effects of sunlight, ultraviolet radiation, solar wind, and cosmic rays and thus protects the organisms from genetic damage. The current composition of the atmosphere of the Earth is the product of billions of years of biochemical modification of the paleoatmosphere by living organisms.\n== Occurrence and compositions ==\n=== Origins ===\nAtmospheres are clouds of gas bound to and engulfing an astronomical focal point of sufficiently dominating mass, adding to its mass, possibly escaping from it or collapsing into it.\nBecause of the latter, such planetary nucleus can develop from interstellar molecular clouds or protoplanetary disks into rocky astronomical objects with varyingly thick atmospheres, gas giants or fusors.\nComposition and thickness is originally determined by the stellar nebula\'s chemistry and temperature, but can also by a product processes within the astronomical body outgasing a different atmosphere.\n=== Compositions ===\nThe atmospheres of the planets Venus and Mars are principally composed of carbon dioxide and nitrogen, argon and oxygen.\nThe composition of Earth\'s atmosphere is determined by the by-products of the life that it sustains. Dry air (mixture of gases) from Earth\'s atmosphere contains 78.08% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and traces of hydrogen, helium, and other "noble" gases (by volume), but generally a variable amount of water vapor is also present, on average about 1% at sea level.\nThe low temperatures and higher gravity of the Solar System\'s giant planets—Jupiter, Saturn, Uranus and Neptune—allow them more readily to retain gases with low molecular masses. These planets have hydrogen–helium atmospheres, with trace amounts of more complex compounds.\nTwo satellites of the outer planets possess significant atmospheres. Titan, a moon of Saturn, and Triton, a moon of Neptune, have atmospheres mainly of nitrogen. When in the part of its orbit closest to the Sun, Pluto has an atmosphere of nitrogen and methane similar to Triton\'s, but these gases are frozen when it is farther from the Sun.\nOther bodies within the Solar System have extremely thin atmospheres not in equilibrium. These include the Moon (sodium gas), Mercury (sodium gas), Europa (oxygen), Io (sulfur), and Enceladus (water vapor).\nThe first exoplanet whose atmospheric composition was determined is HD 209458b, a gas giant with a close orbit around a star in the constellation Pegasus. Its atmosphere is heated to temperatures over 1,000 K, and is steadily escaping into space. Hydrogen, oxygen, carbon and sulfur have been detected in the planet\'s inflated atmosphere.\n=== Atmospheres in the Solar System ===\nAtmosphere of the Sun\nAtmosphere of Mercury\nAtmosphere of Venus\nAtmosphere of Earth\nAtmosphere of the Moon\nAtmosphere of Mars\nAtmosphere of Ceres\nAtmosphere of Jupiter\nAtmosphere of Io\nAtmosphere of Callisto\nAtmosphere of Europa\nAtmosphere of Ganymede\nAtmosphere of Saturn\nAtmosphere of Titan\nAtmosphere of Enceladus\nAtmosphere of Uranus\nAtmosphere of Titania\nAtmosphere of Neptune\nAtmosphere of Triton\nAtmosphere of Pluto\n== Structure of atmosphere ==\n=== Earth ===\nThe atmosphere of Earth is composed of layers with different properties, such as specific gaseous composition, temperature, and pressure.\nThe troposphere is the lowest layer of the atmosphere. This extends from the planetary surface to the bottom of the stratosphere. The troposphere contains 75–80% of the mass of the atmosphere, and is the atmospheric layer wherein the weather occurs; the height of the troposphere varies between 17 km at the equator and 7.0 km at the poles.\nThe stratosphere extends from the top of the troposphere to the bottom of the mesosphere, and contains the ozone layer, at an altitude between 15 km and 35 km. It is the atmospheric layer that absorbs most of the ultraviolet radiation that Earth receives from the Sun.\nThe mesosphere ranges from 50 km to 85 km and is the layer wherein most meteors are incinerated before reaching the surface.\nThe thermosphere extends from an altitude of 85 km to the base of the exosphere at 690 km and contains the ionosphere, where solar radiation ionizes the atmosphere. The density of the ionosphere is greater at short distances from the planetary surface in the daytime and decreases as the ionosphere rises at night-time, thereby allowing a greater range of radio frequencies to travel greater distances.\nThe exosphere begins at 690 to 1,000 km from the surface, and extends to roughly 10,000 km, where it interacts with the magnetosphere of Earth.\n== Pressure ==\nAtmospheric pressure is the force (per unit-area) perpendicular to a unit-area of planetary surface, as determined by the weight of the vertical column of atmospheric gases. In said atmospheric model, the atmospheric pressure, the weight of the mass of the gas, decreases at high altitude because of the diminishing mass of the gas above the point of barometric measurement. The units of air pressure are based upon the standard atmosphere (atm), which is 101,325 Pa (equivalent to 760 Torr or 14.696 psi). The height at which the atmospheric pressure declines by a factor of e (an irrational number equal to 2.71828) is called the scale height (H). For an atmosphere of uniform temperature, the scale height is proportional to the atmospheric temperature and is inversely proportional to the product of the mean molecular mass of dry air, and the local acceleration of gravity at the point of barometric measurement.\n== Escape ==\nSurface gravity differs significantly among the planets. For example, the large gravitational force of the giant planet Jupiter retains light gases such as hydrogen and helium that escape from objects with lower gravity. Secondly, the distance from the Sun determines the energy available to heat atmospheric gas to the point where some fraction of its molecules\' thermal motion exceed the planet\'s escape velocity, allowing those to escape a planet\'s gravitational grasp. Thus, distant and cold Titan, Triton, and Pluto are able to retain their atmospheres despite their relatively low gravities.\nSince a collection of gas molecules may be moving at a wide range of velocities, there will always be some fast enough to produce a slow leakage of gas into space. Lighter molecules move faster than heavier ones with the same thermal kinetic energy, and so gases of low molecular weight are lost more rapidly than those of high molecular weight. It is thought that Venus and Mars may have lost much of their water when, after being photodissociated into hydrogen and oxygen by solar ultraviolet radiation, the hydrogen escaped. Earth\'s magnetic field helps to prevent this, as, normally, the solar wind would greatly enhance the escape of hydrogen. However, over the past 3 billion years Earth may have lost gases through the magnetic polar regions due to auroral activity, including a net 2% of its atmospheric oxygen. The net effect, taking the most important escape processes into account, is that an intrinsic magnetic field does not protect a planet from atmospheric escape and that for some magnetizations the presence of a magnetic field works to increase the escape rate.\nOther mechanisms that can cause atmosphere depletion are solar wind-induced sputtering, impact erosion, weathering, and sequestration—sometimes referred to as "freezing out"—into the regolith and polar caps.\n== Terrain ==\nAtmospheres have dramatic effects on the surfaces of rocky bodies. Objects that have no atmosphere, or that have only an exosphere, have terrain that is covered in craters. Without an atmosphere, the planet has no protection from meteoroids, and all of them collide with the surface as meteorites and create craters.\nFor planets with a significant atmosphere, most meteoroids burn up as meteors before hitting a planet\'s surface. When meteoroids do impact, the effects are often erased by the action of wind.\nWind erosion is a significant factor in shaping the terrain of rocky planets with atmospheres, and over time can erase the effects of both craters and volcanoes. In addition, since liquids cannot exist without pressure, an atmosphere allows liquid to be present at the surface, resulting in lakes, rivers and oceans. Earth and Titan are known to have liquids at their surface and terrain on the planet suggests that Mars had liquid on its surface in the past.\n=== Outside the Solar System ===\nAtmosphere of HD 209458 b\n== Circulation ==\nThe circulation of the atmosphere occurs due to thermal differences when convection becomes a more efficient transporter of heat than thermal radiation. On planets where the primary heat source is solar radiation, excess heat in the tropics is transported to higher latitudes. When a planet generates a significant amount of heat internally, such as is the case for Jupiter, convection in the atmosphere can transport thermal energy from the higher temperature interior up to the surface.\n== Importance ==', 'An early use of the piezoelectricity of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. The quartz clock is a familiar device using the mineral. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.\nAlmost all the industrial demand for quartz crystal (used primarily in electronics) is met with synthetic quartz produced by the hydrothermal process. However, synthetic crystals are less prized for use as gemstones. The popularity of crystal healing has increased the demand for natural quartz crystals, which are now often mined in developing countries using primitive mining methods, sometimes involving child labor.\n== See also ==\nFused quartz\nList of minerals\nQuartz fiber\nQuartz reef mining\nQuartzolite\nShocked quartz\n== References ==\n== External links ==\nQuartz varieties, properties, crystal morphology. Photos and illustrations\nGilbert Hart, "Nomenclature of Silica", American Mineralogist, Volume 12, pp. 383–395. 1927\n"The Quartz Watch – Inventors". The Lemelson Center, National Museum of American History, Smithsonian Institution. Archived from the original on 7 January 2009.\nTerminology used to describe the characteristics of quartz crystals when used as oscillators\nQuartz use as prehistoric stone tool raw material']

Question: What is water hammer?

Choices:
Choice A) Water hammer is a type of water turbine used in hydroelectric generating stations to generate electricity.
Choice B) Water hammer is a type of air trap or standpipe used to dampen the sound of moving water in plumbing systems.
Choice C) Water hammer is a type of plumbing tool used to break pipelines and absorb the potentially damaging forces caused by moving water.
Choice D) Water hammer is a type of water pump used to increase the pressure of water in pipelines.
Choice E) Water hammer is a loud banging noise resembling a hammering sound that occurs when moving water is suddenly stopped, causing a rise in pressure and resulting shock wave.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Memristor', 'Caravelli, Francesco; Carbajal, Juan Pablo (January 2019). "Memristors for the curious outsiders". Technologies. 6 (4): 118. arXiv:1812.03389. doi:10.3390/technologies6040118. S2CID 54464654.\nMaan, Akshay Kumar; Jayadevi, Deepthi Anirudhan; James, Alex Pappachen (August 2017). "A Survey of Memristive Threshold Logic Circuits". IEEE Transactions on Neural Networks and Learning Systems. 28 (8): 1734–1746. arXiv:1604.07121. doi:10.1109/TNNLS.2016.2547842. PMID 27164608. S2CID 1798273.\nGhosh, M., Singh, A., Borah, S. S., Vista, J., Ranjan, A., Kumar, S. (2022). "MOSFET-based memristor for high-frequency signal processing". IEEE Transactions on Electron Devices. 69 (5): 2248–2255. Bibcode:2022ITED...69.2248G. doi:10.1109/ted.2022.3160940. ISSN 0018-9383. S2CID 247889089.\nSingh, A., Borah, S. S., Ghosh, M. (2021), Simple grounded meminductor emulator using transconductance amplifier, IEEE\n== External links ==\nFinding the missing memristor on YouTube\nInteractive database of memristor papers (2013)\nSimonite, Tom (2015-04-21). "Machine Dreams". Technology Review. Retrieved 2017-12-05.\n"Leon Chua: A bulb versus Google go player" - (in Polish) an interview with Leon Chua, the creator of memristor\n"Leon Chua: A bulb versus Google go player" - (in English) an interview with Leon Chua, the creator of memristor', 'An early use of the piezoelectricity of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. The quartz clock is a familiar device using the mineral. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.\nAlmost all the industrial demand for quartz crystal (used primarily in electronics) is met with synthetic quartz produced by the hydrothermal process. However, synthetic crystals are less prized for use as gemstones. The popularity of crystal healing has increased the demand for natural quartz crystals, which are now often mined in developing countries using primitive mining methods, sometimes involving child labor.\n== See also ==\nFused quartz\nList of minerals\nQuartz fiber\nQuartz reef mining\nQuartzolite\nShocked quartz\n== References ==\n== External links ==\nQuartz varieties, properties, crystal morphology. Photos and illustrations\nGilbert Hart, "Nomenclature of Silica", American Mineralogist, Volume 12, pp. 383–395. 1927\n"The Quartz Watch – Inventors". The Lemelson Center, National Museum of American History, Smithsonian Institution. Archived from the original on 7 January 2009.\nTerminology used to describe the characteristics of quartz crystals when used as oscillators\nQuartz use as prehistoric stone tool raw material', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==', 'for convex polyhedra to higher-dimensional polytopes:\n{\\displaystyle \\sum \\varphi =(-1)^{d-1}}\n== Generalisations of a polytope ==\n=== Infinite polytopes ===\nNot all manifolds are finite. Where a polytope is understood as a tiling or decomposition of a manifold,  this idea may be extended to infinite manifolds. plane tilings, space-filling (honeycombs) and hyperbolic tilings are in this sense polytopes, and are sometimes called apeirotopes because they have infinitely many cells.\nAmong these, there are regular forms including the regular skew polyhedra and the infinite series of tilings represented by the regular apeirogon, square tiling, cubic honeycomb, and so on.\n=== Abstract polytopes ===\nThe theory of abstract polytopes attempts to detach polytopes from the space containing them, considering their purely combinatorial properties. This allows the definition of the term to be extended to include objects for which it is difficult to define an intuitive underlying space, such as the 11-cell.\nAn abstract polytope is a partially ordered set of elements or members, which obeys certain rules. It is a purely algebraic structure, and the theory was developed in order to avoid some of the issues which make it difficult to reconcile the various geometric classes within a consistent mathematical framework. A geometric polytope is said to be a realization in some real space of the associated abstract polytope.\n=== Complex polytopes ===\nStructures analogous to polytopes exist in complex Hilbert spaces\n{\\displaystyle \\mathbb {C} ^{n}}\nwhere n real dimensions are accompanied by n imaginary ones. Regular complex polytopes are more appropriately treated as configurations.\n== Duality ==\nEvery n-polytope has a dual structure, obtained by interchanging its vertices for facets, edges for ridges, and so on generally interchanging its (j − 1)-dimensional elements for (n − j)-dimensional elements (for j = 1 to n − 1), while retaining the connectivity or incidence between elements.\nFor an abstract polytope, this simply reverses the ordering of the set. This reversal is seen in the Schläfli symbols for regular polytopes, where the symbol for the dual polytope is simply the reverse of the original. For example, {4, 3, 3} is dual to {3, 3, 4}.\nIn the case of a geometric polytope, some geometric rule for dualising is necessary, see for example the rules described for dual polyhedra. Depending on circumstance, the dual figure may or may not be another geometric polytope.\nIf the dual is reversed, then the original polytope is recovered. Thus, polytopes exist in dual pairs.\n=== Self-dual polytopes ===\nIf a polytope has the same number of vertices as facets, of edges as ridges, and so forth, and the same connectivities, then the dual figure will be similar to the original and the polytope is self-dual.\nSome common self-dual polytopes include:\nEvery regular n-simplex, in any number of dimensions, with Schläfli symbol {3n}. These include the equilateral triangle {3}, regular tetrahedron {3,3}, and 5-cell  {3,3,3}.\nEvery hypercubic honeycomb, in any number of dimensions. These include the apeirogon {∞}, square tiling {4,4} and cubic honeycomb {4,3,4}.\nNumerous compact, paracompact and noncompact hyperbolic tilings, such as the icosahedral honeycomb {3,5,3}, and order-5 pentagonal tiling {5,5}.\nIn 2 dimensions, all regular polygons (regular 2-polytopes)\nIn 3 dimensions, the canonical polygonal pyramids and elongated pyramids, and tetrahedrally diminished dodecahedron.\nIn 4 dimensions, the 24-cell, with Schläfli symbol {3,4,3}. Also the great 120-cell {5,5/2,5} and grand stellated 120-cell {5/2,5,5/2}.\n== History ==\nPolygons and polyhedra have been known since ancient times.\nAn early hint of higher dimensions came in 1827 when August Ferdinand Möbius discovered that two mirror-image solids can be superimposed by rotating one of them through a fourth mathematical dimension. By the 1850s, a handful of other mathematicians such as Arthur Cayley and Hermann Grassmann had also considered higher dimensions.\nLudwig Schläfli was the first to consider analogues of polygons and polyhedra in these higher spaces. He described the six convex regular 4-polytopes in 1852 but his work was not published until 1901, six years after his death. By 1854, Bernhard Riemann\'s Habilitationsschrift had firmly established the geometry of higher dimensions, and thus the concept of n-dimensional polytopes was made acceptable. Schläfli\'s polytopes were rediscovered many times in the following decades, even during his lifetime.\nIn 1882 Reinhold Hoppe, writing in German, coined the word polytop to refer to this more general concept of polygons and polyhedra. In due course Alicia Boole Stott, daughter of logician George Boole, introduced the anglicised polytope into the English language.:\u200avi\nIn 1895, Thorold Gosset not only rediscovered Schläfli\'s regular polytopes but also investigated the ideas of semiregular polytopes and space-filling tessellations in higher dimensions. Polytopes also began to be studied in non-Euclidean spaces such as hyperbolic space.\nAn important milestone was reached in 1948 with H. S. M. Coxeter\'s book Regular Polytopes, summarizing work to date and adding new findings of his own.\nMeanwhile, the French mathematician Henri Poincaré had developed the topological idea of a polytope as the piecewise decomposition (e.g. CW-complex) of a manifold. Branko Grünbaum published his influential work on Convex Polytopes in 1967.\nIn 1952 Geoffrey Colin Shephard generalised the idea as complex polytopes in complex space, where each real dimension has an imaginary one associated with it. Coxeter developed the theory further.\nThe conceptual issues raised by complex polytopes, non-convexity, duality and other phenomena led Grünbaum and others to the more general study of abstract combinatorial properties relating vertices, edges, faces and so on. A related idea was that of incidence complexes, which studied the incidence or connection of the various elements with one another. These developments led eventually to the theory of abstract polytopes as partially ordered sets, or posets, of such elements. Peter McMullen and Egon Schulte published their book Abstract Regular Polytopes in 2002.\nEnumerating the uniform polytopes, convex and nonconvex, in four or more dimensions remains an outstanding problem. The convex uniform 4-polytopes were fully enumerated by John Conway and Michael Guy using a computer in 1965; in higher dimensions this problem was still open as of 1997. The full enumeration for nonconvex uniform polytopes is not known in dimensions four and higher as of 2008.\nIn modern times, polytopes and related concepts have found many important applications in fields as diverse as computer graphics, optimization, search engines, cosmology, quantum mechanics and numerous other fields. In 2013 the amplituhedron was discovered as a simplifying construct in certain calculations of theoretical physics.\n== Applications ==\nIn the field of optimization, linear programming studies the maxima and minima of linear functions; these maxima and minima occur on the boundary of an n-dimensional polytope. In linear programming, polytopes occur in the use of generalized barycentric coordinates and slack variables.\nIn  twistor theory, a branch of theoretical physics, a polytope called the amplituhedron is used in to calculate the scattering amplitudes of subatomic particles when they collide. The construct is purely theoretical with no known physical manifestation, but is said to greatly simplify certain calculations.\n== See also ==\n== References ==\n=== Citations ===\n=== Bibliography ===\n== External links ==\nWeisstein, Eric W. "Polytope". MathWorld.\n"Math will rock your world" – application of polytopes to a database of articles used to support custom news feeds via the Internet – (Business Week Online)\nRegular and semi-regular convex polytopes a short historical overview:', 'Fast neutron detectors have the advantage of not requiring a moderator, and are therefore capable of measuring the neutron\'s energy, time of arrival, and in certain cases direction of incidence.\n== Sources and production ==\nFree neutrons are unstable, although they have the longest half-life of any unstable subatomic particle by several orders of magnitude. Their half-life is still only about 10 minutes, so they can be obtained only from sources that produce them continuously.\nNatural neutron background. A small natural background flux of free neutrons exists everywhere on Earth. In the atmosphere and deep into the ocean, the "neutron background" is caused by muons produced by cosmic ray interaction with the atmosphere. These high-energy muons are capable of penetration to considerable depths in water and soil. There, in striking atomic nuclei, among other reactions they induce spallation reactions in which a neutron is liberated from the nucleus. Within the Earth\'s crust a second source is neutrons produced primarily by spontaneous fission of uranium and thorium present in crustal minerals. The neutron background is not strong enough to be a biological hazard, but it is of importance to very high resolution particle detectors that are looking for very rare events, such as (hypothesized) interactions that might be caused by particles of dark matter. Recent research has shown that even thunderstorms can produce neutrons with energies of up to several tens of MeV. Recent research has shown that the fluence of these neutrons lies between 10−9 and 10−13 per ms and per m2 depending on the detection altitude. The energy of most of these neutrons, even with initial energies of 20 MeV, decreases down to the keV range within 1 ms.\nEven stronger neutron background radiation is produced at the surface of Mars, where the atmosphere is thick enough to generate neutrons from cosmic ray muon production and neutron-spallation, but not thick enough to provide significant protection from the neutrons produced. These neutrons not only produce a Martian surface neutron radiation hazard from direct downward-going neutron radiation but may also produce a significant hazard from reflection of neutrons from the Martian surface, which will produce reflected neutron radiation penetrating upward into a Martian craft or habitat from the floor.\nSources of neutrons for research. These include certain types of radioactive decay (spontaneous fission and neutron emission), and from certain nuclear reactions. Convenient nuclear reactions include tabletop reactions such as natural alpha and gamma bombardment of certain nuclides, often beryllium or deuterium, and induced nuclear fission, such as occurs in nuclear reactors. In addition, high-energy nuclear reactions (such as occur in cosmic radiation showers or accelerator collisions) also produce neutrons from disintegration of target nuclei. Small (tabletop) particle accelerators optimized to produce free neutrons in this way, are called neutron generators.\nIn practice, the most commonly used small laboratory sources of neutrons use radioactive decay to power neutron production. One noted neutron-producing radioisotope, californium-252 decays (half-life 2.65 years) by spontaneous fission 3% of the time with production of 3.7 neutrons per fission, and is used alone as a neutron source from this process. Nuclear reaction sources (that involve two materials) powered by radioisotopes use an alpha decay source plus a beryllium target, or else a source of high-energy gamma radiation from a source that undergoes beta decay followed by gamma decay, which produces photoneutrons on interaction of the high-energy gamma ray with ordinary stable beryllium, or else with the deuterium in heavy water. A popular source of the latter type is radioactive antimony-124 plus beryllium, a system with a half-life of 60.9 days, which can be constructed from natural antimony (which is 42.8% stable antimony-123) by activating it with neutrons in a nuclear reactor, then transported to where the neutron source is needed.\nNuclear fission reactors naturally produce free neutrons; their role is to sustain the energy-producing chain reaction. The intense neutron radiation can also be used to produce various radioisotopes through the process of neutron activation, which is a type of neutron capture.\nExperimental nuclear fusion reactors produce free neutrons as a waste product. But it is these neutrons that possess most of the energy and converting that energy to a useful form has proved a difficult engineering challenge. Fusion reactors that generate neutrons are likely to create radioactive waste, but the waste is composed of neutron-activated lighter isotopes, which have relatively short (50–100 years) decay periods as compared to typical half-lives of 10,000 years for fission waste, which is long due primarily to the long half-life of alpha-emitting transuranic actinides. Some nuclear fusion-fission hybrids are proposed to make use of those neutrons to either maintain a subcritical reactor or to aid in nuclear transmutation of harmful long lived nuclear waste to shorter lived or stable nuclides.\n=== Neutron beams and modification of beams after production ===\nFree neutron beams are obtained from neutron sources by neutron transport. For access to intense neutron sources, researchers must go to a specialized neutron facility that operates a research reactor or a spallation source.\nThe neutron\'s lack of total electric charge makes it difficult to steer or accelerate them. Charged particles can be accelerated, decelerated, or deflected by electric or magnetic fields. These methods have little effect on neutrons. But some effects may be attained by use of inhomogeneous magnetic fields because of the neutron\'s magnetic moment. Neutrons can be controlled by methods that include moderation, reflection, and velocity selection. Thermal neutrons can be polarized by transmission through magnetic materials in a method analogous to the Faraday effect for photons. Cold neutrons of wavelengths of 6–7 angstroms can be produced in beams of a high degree of polarization, by use of magnetic mirrors and magnetized interference filters.\n== Applications ==\n=== Nuclear energy ===\nBecause of the strength of the nuclear force at short distances, the nuclear energy binding nucleons is many orders of magnitude greater than the electromagnetic energy binding electrons in atoms.:\u200a4\u200a In nuclear fission, the absorption of a neutron by some heavy nuclides (such as uranium-235) can cause the nuclide to become unstable and break into lighter nuclides and additional neutrons. The positively charged light nuclides, or "fission fragments", then repel, releasing electromagnetic potential energy. If this reaction occurs within a mass of fissile material, the additional neutrons cause additional fission events, inducing a cascade known as a nuclear chain reaction.:\u200a12–13\u200a  For a given mass of fissile material, such nuclear reactions release energy that is approximately ten million times that from an equivalent mass of a conventional chemical explosive.:\u200a13\u200a Ultimately, the ability of the nuclear force to store energy arising from the electromagnetic repulsion of nuclear components is the basis for most of the energy that makes nuclear reactors or bombs possible; most of the energy released from fission is the kinetic energy of the fission fragments.:\u200a12\nThe neutron plays an important role in many nuclear reactions. For example, neutron capture often results in neutron activation, inducing radioactivity. In particular, knowledge of neutrons and their behavior has been important in the development of nuclear reactors and nuclear weapons. The fissioning of elements like uranium-235 and plutonium-239 is caused by their absorption of neutrons.\n=== Other uses ===\nCold, thermal, and hot neutron radiation is commonly employed in neutron scattering facilities for neutron diffraction, small-angle neutron scattering, and neutron reflectometry. Slow neutron matter waves exhibit properties similar to geometrical and wave optics of light, including reflection, refraction, diffraction, and interference. Neutrons are complementary to X-rays in terms of atomic contrasts by different scattering cross sections; sensitivity to magnetism; energy range for inelastic neutron spectroscopy; and deep penetration into matter.\nThe development of "neutron lenses" based on total internal reflection within hollow glass capillary tubes or by reflection from dimpled aluminum plates has driven ongoing research into neutron microscopy and neutron/gamma ray tomography.\nA major use of neutrons is to excite delayed and prompt gamma rays from elements in materials. This forms the basis of neutron activation analysis (NAA) and prompt gamma neutron activation analysis (PGNAA). NAA is most often used to analyze small samples of materials in a nuclear reactor whilst PGNAA is most often used to analyze subterranean rocks around bore holes and industrial bulk materials on conveyor belts.\nAnother use of neutron emitters is the detection of light nuclei, in particular the hydrogen found in water molecules. When a fast neutron collides with a light nucleus, it loses a large fraction of its energy. By measuring the rate at which slow neutrons return to the probe after reflecting off of hydrogen nuclei, a neutron probe may determine the water content in soil.\n== Medical therapies ==\nBecause neutron radiation is both penetrating and ionizing, it can be exploited for medical treatments. However, neutron radiation can have the unfortunate side-effect of leaving the affected area radioactive. Neutron tomography is therefore not a viable medical application.', 'Technosignatures can be divided into three broad categories: astroengineering projects, signals of planetary origin, and spacecraft within and outside the Solar System.\nAn astroengineering installation such as a Dyson sphere, designed to convert all of the incident radiation of its host star into energy, could be detected through the observation of an infrared excess from a solar analog star, or by the star\'s apparent disappearance in the visible spectrum over several years. After examining some 100,000 nearby large galaxies, a team of researchers has concluded that none of them display any obvious signs of highly advanced technological civilizations.\nAnother hypothetical form of astroengineering, the Shkadov thruster, moves its host star by reflecting some of the star\'s light back on itself, and would be detected by observing if its transits across the star abruptly end with the thruster in front. Asteroid mining within the Solar System is also a detectable technosignature of the first kind.\nIndividual extrasolar planets can be analyzed for signs of technology. Avi Loeb of the Center for Astrophysics | Harvard & Smithsonian has proposed that persistent light signals on the night side of an exoplanet can be an indication of the presence of cities and an advanced civilization. In addition, the excess infrared radiation and chemicals produced by various industrial processes or terraforming efforts may point to intelligence.\nLight and heat detected from planets need to be distinguished from natural sources to conclusively prove the existence of civilization on a planet. However, as argued by the Colossus team, a civilization heat signature should be within a "comfortable" temperature range, like terrestrial urban heat islands, i.e., only a few degrees warmer than the planet itself. In contrast, such natural sources as wild fires, volcanoes, etc. are significantly hotter, so they will be well distinguished by their maximum flux at a different wavelength.\nOther than astroengineering, technosignatures such as artificial satellites around exoplanets, particularly such in geostationary orbit, might be detectable even with today\'s technology and data, and would allow, similar to fossils on Earth, to find traces of extrasolar life from long ago.\nExtraterrestrial craft are another target in the search for technosignatures. Magnetic sail interstellar spacecraft should be detectable over thousands of light-years of distance through the synchrotron radiation they would produce through interaction with the interstellar medium; other interstellar spacecraft designs may be detectable at more modest distances. In addition, robotic probes within the Solar System are also being sought with optical and radio searches.\nFor a sufficiently advanced civilization, hyper energetic neutrinos from Planck scale accelerators should be detectable at a distance of many Mpc.\n=== Advances for Bio and Technosignature Detection ===\nA notable advancement in technosignature detection is the development of an algorithm for signal reconstruction in zero-knowledge one-way communication channels. This algorithm decodes signals from unknown sources without prior knowledge of the encoding scheme, using principles from Algorithmic Information Theory to identify the geometric and topological dimensions of the encoding space. It successfully reconstructed the Arecibo message despite significant noise. The work establishes a connection between syntax and semantics in SETI and technosignature detection, enhancing fields like cryptography and Information Theory.\nBased on fractal theory and the Weierstrass function, a known fractal, another method authored by the same group called fractal messaging offers a framework for space-time scale-free communication. This method leverages properties of self-similarity and scale invariance, enabling spatio-temporal scale-independent and parallel infinite-frequency communication. It also embodies the concept of sending a self-encoding/self-decoding signal as a mathematical formula, equivalent to self-executable computer code that unfolds to read a message at all possible time scales and in all possible channels simultaneously.\n== Fermi paradox ==\nItalian physicist Enrico Fermi suggested in the 1950s that if technologically advanced civilizations are common in the universe, then they should be detectable in one way or another. According to those who were there, Fermi either asked "Where are they?" or "Where is everybody?"\nThe Fermi paradox is commonly understood as asking why extraterrestrials have not visited Earth, but the same reasoning applies to the question of why signals from extraterrestrials have not been heard. The SETI version of the question is sometimes referred to as "the Great Silence".\nThe Fermi paradox can be stated more completely as follows:\nThe size and age of the universe incline us to believe that many technologically advanced civilizations must exist. However, this belief seems logically inconsistent with our lack of observational evidence to support it. Either (1) the initial assumption is incorrect and technologically advanced intelligent life is much rarer than we believe, or (2) our current observations are incomplete, and we simply have not detected them yet, or (3) our search methodologies are flawed and we are not searching for the correct indicators, or (4) it is the nature of intelligent life to destroy itself.\nThere are multiple explanations proposed for the Fermi paradox, ranging from analyses suggesting that intelligent life is rare (the "Rare Earth hypothesis"), to analyses suggesting that although extraterrestrial civilizations may be common, they would not communicate with us, would communicate in a way we have not discovered yet, could not travel across interstellar distances, or destroy themselves before they master the technology of either interstellar travel or communication.\nThe German astrophysicist and radio astronomer Sebastian von Hoerner suggested that the average duration of civilization was 6,500 years. After this time, according to him, it disappears for external reasons (the destruction of life on the planet, the destruction of only rational beings) or internal causes (mental or physical degeneration). According to his calculations, on a habitable planet (one in three million stars) there is a sequence of technological species over a time distance of hundreds of millions of years, and each of them "produces" an average of four technological species. With these assumptions, the average distance between civilizations in the Milky Way is 1,000 light years.\nScience writer Timothy Ferris has posited that since galactic societies are most likely only transitory, an obvious solution is an interstellar communications network, or a type of library consisting mostly of automated systems. They would store the cumulative knowledge of vanished civilizations and communicate that knowledge through the galaxy. Ferris calls this the "Interstellar Internet", with the various automated systems acting as network "servers". If such an Interstellar Internet exists, the hypothesis states, communications between servers are mostly through narrow-band, highly directional radio or laser links. Intercepting such signals is, as discussed earlier, very difficult. However, the network could maintain some broadcast nodes in hopes of making contact with new civilizations.\nAlthough somewhat dated in terms of "information culture" arguments, not to mention the obvious technological problems of a system that could work effectively for billions of years and requires multiple lifeforms agreeing on certain basics of communications technologies, this hypothesis is actually testable (see below).\n=== Difficulty of detection ===\nA significant problem is the vastness of space. Despite piggybacking on the world\'s most sensitive radio telescope, astronomer and initiator of SERENDIP Charles Stuart Bowyer noted the then world\'s largest instrument could not detect random radio noise emanating from a civilization like ours, which has been leaking radio and TV signals for less than 100 years. For SERENDIP and most other SETI projects to detect a signal from an extraterrestrial civilization, the civilization would have to be beaming a powerful signal directly at us. It also means that Earth civilization will only be detectable within a distance of 100 light-years.\n== Post-detection disclosure protocol ==\nThe International Academy of Astronautics (IAA) has a long-standing SETI Permanent Study Group (SPSG, formerly called the IAA SETI Committee), which addresses matters of SETI science, technology, and international policy. The SPSG meets in conjunction with the International Astronautical Congress (IAC), held annually at different locations around the world, and sponsors two SETI Symposia at each IAC. In 2005, the IAA established the SETI: Post-Detection Science and Technology Taskgroup (chairman, Professor Paul Davies) "to act as a Standing Committee to be available to be called on at any time to advise and consult on questions stemming from the discovery of a putative signal of extraterrestrial intelligent (ETI) origin."\nHowever, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".']

Question: What is the application of Memristor?

Choices:
Choice A) Memristor has applications in the production of electric cars, airplanes, and ships.
Choice B) Memristor has applications in the production of food, clothing, and shelter.
Choice C) Memristor has applications in the production of solar panels, wind turbines, and hydroelectric power plants.
Choice D) Memristor has applications in programmable logic signal processing, Super-resolution imaging, physical neural networks, control systems, reconfigurable computing, in-memory computing, brain–computer interfaces and RFID.
Choice E) Memristor has applications in optical fiber communication, satellite communication, and wireless communication.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Fermi level', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', "Planck's law", 'In the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein\'s proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal. This has at times been called Planck\'s "second theory".\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his \'third theory\', that both emission and absorption of light were quantal.\nThe colourful term "ultraviolet catastrophe" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck\'s thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of "catastrophe". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called \'photons\', and by Rayleigh and by Jeans.\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν. In contrast to Planck\'s and Einstein\'s formulas, Bohr\'s formula referred explicitly and categorically to energy levels of atoms. Bohr\'s formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck\'s model, the frequency\n{\\displaystyle \\nu }\nhas no immediate relation to frequencies that might describe those quantum states themselves.\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck\'s law. The actual word \'photon\' was invented still later, by G.N. Lewis in 1926, who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word \'photon\' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein\'s 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\nUltimately, Planck\'s law of black-body radiation contributed to Einstein\'s concept of quanta of light carrying linear momentum, which became the fundamental basis for the development of quantum mechanics.\nThe above-mentioned linearity of Planck\'s mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg\'s original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg\'s theory accounted for Bohr\'s above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states. At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg\'s paper and recognized the matrix character of Heisenberg\'s theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg\'s original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics. Heisenberg\'s explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck\'s oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = \u2060h/2π\u2060, and ω = 2πν denotes angular frequency, and less often the equivalent formula E = hν. This statement about a really existing and propagating light quantum, based on Einstein\'s, has a physical meaning different from that of Planck\'s above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\nAn article by Helge Kragh published in Physics World gives an account of this history.\n== See also ==\nEmissivity\nRadiance\nSakuma–Hattori equation\n== References ==\n=== Bibliography ===\n== External links ==\nSummary of Radiation\nRadiation of a Blackbody – interactive simulation to play with Planck\'s law\nScienceworld entry on Planck\'s Law', 'Fusion powers stars and produces most elements lighter than cobalt in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 616 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.645% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and the result is fusion; this is an exothermic process.\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is 13.6 eV—less than one-millionth of the 17.6 MeV released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though individual fission reactions are generally much more energetic than individual fusion ones, which are themselves millions of times more energetic than chemical reactions. Via the mass–energy equivalence, fusion yields a 0.7% efficiency of reactant mass into energy. This can be only be exceeded by the extreme cases of the accretion process involving neutron stars or black holes, approaching 40% efficiency, and antimatter annihilation at 100% efficiency. (The complete conversion of one gram of matter would expel 9×1013 joules of energy.)\n== In astrophysics ==\nFusion is responsible for the astrophysical production of the majority of elements lighter than iron. This includes most types of Big Bang nucleosynthesis and stellar nucleosynthesis. Non-fusion processes that contribute include the s-process and r-process in neutron merger and supernova nucleosynthesis, responsible for elements heavier than iron.\n=== Stars ===\nAn important fusion process is the stellar nucleosynthesis that powers stars, including the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounts for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was unknown; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein\'s equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy had not yet been discovered, nor even that stars are largely composed of hydrogen (see metallicity). Eddington\'s paper reasoned that:\nThe leading theory of stellar energy, the contraction hypothesis, should cause the rotation of a star to visibly speed up due to conservation of angular momentum. But observations of Cepheid variable stars showed this was not happening.\nThe only other known plausible source of energy was conversion of matter to energy; Einstein had shown some years earlier that a small amount of matter was equivalent to a large amount of energy.\nFrancis Aston had also recently shown that the mass of a helium atom was about 0.8% less than the mass of the four hydrogen atoms which would, combined, form a helium atom (according to the then-prevailing theory of atomic structure which held atomic weight to be the distinguishing property between elements; work by Henry Moseley and Antonius van den Broek would later show that nucleic charge was the distinguishing property and that a helium nucleus, therefore, consisted of two hydrogen nuclei plus additional mass). This suggested that if such a combination could happen, it would release considerable energy as a byproduct.\nIf a star contained just 5% of fusible hydrogen, it would suffice to explain how stars got their energy. (It is now known that most \'ordinary\' stars are usually made of around 70% to 75% hydrogen)\nFurther elements might also be fused, and other scientists had speculated that stars were the "crucible" in which light elements combined to create heavy elements, but without more accurate measurements of their atomic masses nothing more could be said at the time.\nAll of these speculations were proven correct in the following decades.\nThe primary source of solar energy, and that of similar size stars, is the fusion of hydrogen to form helium (the proton–proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to fuse heavier elements. In massive cores, silicon-burning is the final fusion cycle, leading to a build-up of iron and nickel nuclei.\nNuclear binding energy makes the production of elements heavier than nickel via fusion energetically unfavorable. These elements are produced in non-fusion processes: the s-process, r-process, and the variety of processes that can produce p-nuclei. Such processes occur in giant star shells, or supernovae, or neutron star mergers.\n=== Brown dwarfs ===\nBrown dwarfs fuse deuterium and in very high mass cases also fuse lithium.\n=== White dwarfs ===\nCarbon-oxygen white dwarfs, which accrete matter either from an active stellar companion or white dwarf merger, approach the Chandrasekhar limit of 1.44 solar masses. Immediately prior, carbon burning fusion begins, destroying the Earth-sized dwarf within one second, in a Type Ia supernova.\nMuch more rarely, helium white dwarfs may merge, which does not cause an explosion but begins helium burning in an extreme type of helium star.\n=== Neutron stars ===\nSome neutron stars accrete hydrogen and helium from an active stellar companion. Periodically, the helium accretion reaches a critical level, and a thermonuclear burn wave propagates across the surface, on the timescale of one second.\n=== Black hole accretion disks ===\nSimilar to stellar fusion, extreme conditions within black hole accretion disks can allow fusion reactions. Calculations show the most energetic reactions occur around lower stellar mass black holes, below 10 solar masses, compared to those above 100. Beyond five Schwarzschild radii, carbon-burning and fusion of helium-3 dominates the reactions. Within this distance, around lower mass black holes, fusion of nitrogen, oxygen, neon, and magnesium can occur. In the extreme limit, the silicon-burning process can begin with the fusion of silicon and selenium nuclei.\n=== Big Bang ===\nFrom the period approximately 10 seconds to 20 minutes after the Big Bang, the universe cooled from over 100 keV to 1 keV. This allowed the combination of protons and neutrons in deuterium nuclei, and beginning a rapid fusion chain into tritium and helium-3 and ending in predominantly helium-4, with a minimal fraction of lithium, beryllium, and boron nuclei.\n== Requirements ==\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbors due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface-area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.', 'Thus Kirchhoff\'s law of thermal radiation can be stated: For any material at all, radiating and absorbing in thermodynamic equilibrium at any given temperature T, for every wavelength λ, the ratio of emissive power to absorptive ratio has one universal value, which is characteristic of a perfect black body, and is an emissive power which we here represent by Bλ (λ, T). (For our notation Bλ (λ, T), Kirchhoff\'s original notation was simply e.)\nKirchhoff announced that the determination of the function Bλ (λ, T) was a problem of the highest importance, though he recognized that there would be experimental difficulties to be overcome. He supposed that like other functions that do not depend on the properties of individual bodies, it would be a simple function. That function Bλ (λ, T) has occasionally been called \'Kirchhoff\'s (emission, universal) function\', though its precise mathematical form would not be known for another forty years, till it was discovered by Planck in 1900. The theoretical proof for Kirchhoff\'s universality principle was worked on and debated by various physicists over the same time, and later. Kirchhoff stated later in 1860 that his theoretical proof was better than Balfour Stewart\'s, and in some respects it was so. Kirchhoff\'s 1860 paper did not mention the second law of thermodynamics, and of course did not mention the concept of entropy which had not at that time been established. In a more considered account in a book in 1862, Kirchhoff mentioned the connection of his law with "Carnot\'s principle", which is a form of the second law.\nAccording to Helge Kragh, "Quantum theory owes its origin to the study of thermal radiation, in particular to the "blackbody" radiation that Robert Kirchhoff had first defined in 1859–1860."\n=== Empirical and theoretical ingredients for the scientific induction of Planck\'s law ===\nIn 1860, Kirchhoff predicted experimental difficulties for the empirical determination of the function that described the dependence of the black-body spectrum as a function only of temperature and wavelength. And so it turned out. It took some forty years of development of improved methods of measurement of electromagnetic radiation to get a reliable result.\nIn 1865, John Tyndall described radiation from electrically heated filaments and from carbon arcs as visible and invisible. Tyndall spectrally decomposed the radiation by use of a rock salt prism, which passed heat as well as visible rays, and measured the radiation intensity by means of a thermopile.\nIn 1880, André-Prosper-Paul Crova published a diagram of the three-dimensional appearance of the graph of the strength of thermal radiation as a function of wavelength and temperature. He determined the spectral variable by use of prisms. He analyzed the surface through what he called "isothermal" curves, sections for a single temperature, with a spectral variable on the abscissa and a power variable on the ordinate. He put smooth curves through his experimental data points. They had one peak at a spectral value characteristic for the temperature, and fell either side of it towards the horizontal axis. Such spectral sections are widely shown even today.\nIn a series of papers from 1881 to 1886, Langley reported measurements of the spectrum of heat radiation, using diffraction gratings and prisms, and the most sensitive detectors that he could make. He reported that there was a peak intensity that increased with temperature, that the shape of the spectrum was not symmetrical about the peak, that there was a strong fall-off of intensity when the wavelength was shorter than an approximate cut-off value for each temperature, that the approximate cut-off wavelength decreased with increasing temperature, and that the wavelength of the peak intensity decreased with temperature, so that the intensity increased strongly with temperature for short wavelengths that were longer than the approximate cut-off for the temperature.\nHaving read Langley, in 1888, Russian physicist V.A. Michelson published a consideration of the idea that the unknown Kirchhoff radiation function could be explained physically and stated mathematically in terms of "complete irregularity of the vibrations of ... atoms". At this time, Planck was not studying radiation closely, and believed in neither atoms nor statistical physics. Michelson produced a formula for the spectrum for temperature:\nexp\n{\\displaystyle I_{\\lambda }=B_{1}\\theta ^{\\frac {3}{2}}\\exp \\left(-{\\frac {c}{\\lambda ^{2}\\theta }}\\right)\\lambda ^{-6},}\nwhere Iλ denotes specific radiative intensity at wavelength λ and temperature θ, and where B1 and c are empirical constants.\nIn 1898, Otto Lummer and Ferdinand Kurlbaum published an account of their cavity radiation source. Their design has been used largely unchanged for radiation measurements to the present day. It was a platinum box, divided by diaphragms, with its interior blackened with iron oxide. It was an important ingredient for the progressively improved measurements that led to the discovery of Planck\'s law. A version described in 1901 had its interior blackened with a mixture of chromium, nickel, and cobalt oxides.\nThe importance of the Lummer and Kurlbaum cavity radiation source was that it was an experimentally accessible source of black-body radiation, as distinct from radiation from a simply exposed incandescent solid body, which had been the nearest available experimental approximation to black-body radiation over a suitable range of temperatures. The simply exposed incandescent solid bodies, that had been used before, emitted radiation with departures from the black-body spectrum that made it impossible to find the true black-body spectrum from experiments.\n=== Planck\'s views before the empirical facts led him to find his eventual law ===\nPlanck first turned his attention to the problem of black-body radiation in 1897.\nTheoretical and empirical progress enabled Lummer and Pringsheim to write in 1899 that available experimental evidence was approximately consistent with the specific intensity law Cλ−5e−c⁄λT where C and c denote empirically measurable constants, and where λ and T denote wavelength and temperature respectively. For theoretical reasons, Planck at that time accepted this formulation, which has an effective cut-off of short wavelengths.\nGustav Kirchhoff was Max Planck\'s teacher and surmised that there was a universal law for blackbody radiation and this was called "Kirchhoff\'s challenge". Planck, a theorist, believed that Wilhelm Wien had discovered this law and Planck expanded on Wien\'s work presenting it in 1899 to the meeting of the German Physical Society. Experimentalists Otto Lummer, Ferdinand Kurlbaum, Ernst Pringsheim Sr., and Heinrich Rubens did experiments that appeared to support Wien\'s law especially at higher frequency short wavelengths which Planck so wholly endorsed at the German Physical Society that it began to be called the Wien-Planck Law. However, by September 1900, the experimentalists had proven beyond a doubt that the Wien-Planck law failed at the longer wavelengths. They would present their data on October 19.  Planck was informed by his friend Rubens and quickly created a formula within a few days. In June of that same year, Lord Rayleigh had created a formula that would work for short lower frequency wavelengths based on the widely accepted theory of equipartition. So Planck submitted a formula combining both Rayleigh\'s Law (or a similar equipartition theory) and Wien\'s law which would be weighted to one or the other law depending on wavelength to match the experimental data. However, although this equation worked, Planck himself said unless he could explain the formula derived from a "lucky intuition" into one of "true meaning" in physics, it did not have true significance. Planck explained that thereafter followed the hardest work of his life. Planck did not believe in atoms, nor did he think the second law of thermodynamics should be statistical because probability does not provide an absolute answer, and Boltzmann\'s entropy law rested on the hypothesis of atoms and was statistical. But Planck was unable to find a way to reconcile his Blackbody equation with continuous laws such as Maxwell\'s wave equations. So in what Planck called "an act of desperation", he turned to Boltzmann\'s atomic law of entropy as it was the only one that made his equation work. Therefore, he used the Boltzmann constant k and his new constant h to explain the blackbody radiation law which became widely known through his published paper.\n=== Finding the empirical law ===\nMax Planck produced his law on 19 October 1900 as an improvement upon the Wien approximation, published in 1896 by Wilhelm Wien, which fit the experimental data at short wavelengths (high frequencies) but deviated from it at long wavelengths (low frequencies). In June 1900, based on heuristic theoretical considerations, Rayleigh had suggested a formula that he proposed might be checked experimentally. The suggestion was that the Stewart–Kirchhoff universal function might be of the form c1Tλ−4exp(–\u2060c2/λT\u2060) . This was not the celebrated Rayleigh–Jeans formula 8πkBTλ−4, which did not emerge until 1905, though it did reduce to the latter for long wavelengths, which are the relevant ones here. According to Klein, one may speculate that it is likely that Planck had seen this suggestion though he did not mention it in his papers of 1900 and 1901. Planck would have been aware of various other proposed formulas which had been offered. On 7 October 1900, Rubens told Planck that in the complementary domain (long wavelength, low frequency), and only there, Rayleigh\'s 1900 formula fitted the observed data well.', 'Jordy, W. H. (1952). Henry Adams: Scientific Historian. New Haven. ISBN 978-0-685-26683-0. {{cite book}}: ISBN / Date incompatibility (help)\nKhan, Salman. "Maxwell\'s Demon". Archived from the original on 2010-03-17.\nMaroney, O. J. E. (2009) ""Information Processing and Thermodynamic Entropy" The Stanford Encyclopedia of Philosophy (Autumn 2009 Edition)\nMaxwell, J. C. (1871). Theory of Heat. London, New York [etc.] Longmans, Green., reprinted (2001) New York: Dover, ISBN 0-486-41735-2\nNorton, J. (2005). "Eaters of the lotus: Landauer\'s principle and the return of Maxwell\'s demon" (PDF). Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics. 36 (2): 375–411. Bibcode:2005SHPMP..36..375N. CiteSeerX 10.1.1.468.3017. doi:10.1016/j.shpsb.2004.12.002. S2CID 21104635. Archived (PDF) from the original on 2006-09-01.\nRaizen, Mark G. (2011) "Demons, Entropy, and the Quest for Absolute Zero", Scientific American, March, pp54-59\nReaney, Patricia. "Scientists build nanomachine", Reuters, February 1, 2007\nRubi, J Miguel, "Does Nature Break the Second Law of Thermodynamics?"; Scientific American, October 2008 :\nSplasho (2008) – Historical development of Maxwell\'s demon\nWeiss, Peter. "Breaking the Law – Can quantum mechanics + thermodynamics = perpetual motion?", Science News, October 7, 2000']

Question: What can be inferred about the electronic entropy of insulators and metals based on their densities of states at the Fermi level?

Choices:
Choice A) Insulators and metals have zero density of states at the Fermi level, and therefore, their density of states-based electronic entropy is essentially zero.
Choice B) Insulators have zero density of states at the Fermi level, and therefore, their density of states-based electronic entropy is essentially zero. Metals have non-zero density of states at the Fermi level, and thus, their electronic entropy should be proportional to the temperature and density of states at the Fermi level.
Choice C) Insulators have non-zero density of states at the Fermi level, and therefore, their density of states-based electronic entropy is proportional to the temperature and density of states at the Fermi level. Metals have zero density of states at the Fermi level, and thus, their electronic entropy is essentially zero.
Choice D) Insulators and metals have varying densities of states at the Fermi level, and thus, their electronic entropy may or may not be proportional to the temperature and density of states at the Fermi level.
Choice E) Insulators and metals have non-zero density of states at the Fermi level, and thus, their electronic entropy should be proportional to the temperature and density of states at the Fermi level.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Ultraviolet radiation, also known as simply UV, is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights.\nThe photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms.:\u200a25–26\u200a  Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature.:\u200a28\u200a  Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact.\nFor humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength "extreme" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life.\nThe lower wavelength limit of the visible spectrum is conventionally taken as 400 nm.  Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range.  Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.\n== Visibility ==\nUltraviolet rays are not usable for normal human vision.\nThe lens of the human eye and surgically implanted lens produced since 1986 blocks most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy.\nPeople lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet.  Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds "true" UV vision.\n== History and discovery ==\n"Ultraviolet" means "beyond violet" (from Latin ultra, "beyond"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light.\nUV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them "(de-)oxidizing rays" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from "heat rays", discovered the previous year at the other end of the visible spectrum. The simpler term "chemical rays" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them "tithonic rays"). The terms "chemical rays" and "heat rays" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established.\nThe discovery of the ultraviolet radiation with wavelengths below 200 nm, named "vacuum ultraviolet" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided "unanimously" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen.\n== Subtypes ==\nThe electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348:\nSeveral solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum.\nVacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers.\nTechnology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes.\nExtreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer  satellite.\nSome sources use the distinction of "hard UV" and "soft UV". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with "hard UV" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions.\n== Solar ultraviolet ==\nVery hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth\'s atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum.\nThe atmosphere blocks about 77% of the Sun\'s UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth\'s surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth\'s surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On "partly cloudy" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB.', "When both rates of movement are known, the space velocity of the star relative to the Sun or the galaxy can be computed. Among nearby stars, it has been found that younger population I stars have generally lower velocities than older, population II stars. The latter have elliptical orbits that are inclined to the plane of the galaxy. A comparison of the kinematics of nearby stars has allowed astronomers to trace their origin to common points in giant molecular clouds; such groups with common points of origin are referred to as stellar associations.\n=== Magnetic field ===\nThe magnetic field of a star is generated within regions of the interior where convective circulation occurs. This movement of conductive plasma functions like a dynamo, wherein the movement of electrical charges induce magnetic fields, as does a mechanical dynamo. Those magnetic fields have a great range that extend throughout and beyond the star. The strength of the magnetic field varies with the mass and composition of the star, and the amount of magnetic surface activity depends upon the star's rate of rotation. This surface activity produces starspots, which are regions of strong magnetic fields and lower than normal surface temperatures. Coronal loops are arching magnetic field flux lines that rise from a star's surface into the star's outer atmosphere, its corona. The coronal loops can be seen due to the plasma they conduct along their length. Stellar flares are bursts of high-energy particles that are emitted due to the same magnetic activity.\nYoung, rapidly rotating stars tend to have high levels of surface activity because of their magnetic field. The magnetic field can act upon a star's stellar wind, functioning as a brake to gradually slow the rate of rotation with time. Thus, older stars such as the Sun have a much slower rate of rotation and a lower level of surface activity. The activity levels of slowly rotating stars tend to vary in a cyclical manner and can shut down altogether for periods of time. During the Maunder Minimum, for example, the Sun underwent a 70-year period with almost no sunspot activity.\n=== Mass ===\nStars have masses ranging from less than half the solar mass to over 200 solar masses (see List of most massive stars). One of the most massive stars known is Eta Carinae, which, with 100–150 times as much mass as the Sun, will have a lifespan of only several million years. Studies of the most massive open clusters suggests 150 M☉ as a rough upper limit for stars in the current era of the universe. This represents an empirical value for the theoretical limit on the mass of forming stars due to increasing radiation pressure on the accreting gas cloud. Several stars in the R136 cluster in the Large Magellanic Cloud have been measured with larger masses, but it has been determined that they could have been created through the collision and merger of massive stars in close binary systems, sidestepping the 150 M☉ limit on massive star formation.\nThe first stars to form after the Big Bang may have been larger, up to 300 M☉, due to the complete absence of elements heavier than lithium in their composition. This generation of supermassive population III stars is likely to have existed in the very early universe (i.e., they are observed to have a high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life. In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at z = 6.60.\nWith a mass only 80 times that of Jupiter (MJ), 2MASS J0523-1403 is the smallest known star undergoing nuclear fusion in its core. For stars with metallicity similar to the Sun, the theoretical minimum mass the star can have and still undergo fusion at the core, is estimated to be about 75 MJ. When the metallicity is very low, the minimum star size seems to be about 8.3% of the solar mass, or about 87 MJ. Smaller bodies called brown dwarfs, occupy a poorly defined grey area between stars and gas giants.\nThe combination of the radius and the mass of a star determines its surface gravity. Giant stars have a much lower surface gravity than do main sequence stars, while the opposite is the case for degenerate, compact stars such as white dwarfs. The surface gravity can influence the appearance of a star's spectrum, with higher gravity causing a broadening of the absorption lines.\n=== Rotation ===\nThe rotation rate of stars can be determined through spectroscopic measurement, or more exactly determined by tracking their starspots. Young stars can have a rotation greater than 100 km/s at the equator. The B-class star Achernar, for example, has an equatorial velocity of about 225 km/s or greater, causing its equator to bulge outward and giving it an equatorial diameter that is more than 50% greater than between the poles. This rate of rotation is just below the critical velocity of 300 km/s at which speed the star would break apart. By contrast, the Sun rotates once every 25–35 days depending on latitude, with an equatorial velocity of 1.93 km/s. A main sequence star's magnetic field and the stellar wind serve to slow its rotation by a significant amount as it evolves on the main sequence.\nDegenerate stars have contracted into a compact mass, resulting in a rapid rate of rotation. However they have relatively low rates of rotation compared to what would be expected by conservation of angular momentum—the tendency of a rotating body to compensate for a contraction in size by increasing its rate of spin. A large portion of the star's angular momentum is dissipated as a result of mass loss through the stellar wind. In spite of this, the rate of rotation for a pulsar can be very rapid. The pulsar at the heart of the Crab nebula, for example, rotates 30 times per second. The rotation rate of the pulsar will gradually slow due to the emission of radiation.\n=== Temperature ===\nThe surface temperature of a main sequence star is determined by the rate of energy production of its core and by its radius, and is often estimated from the star's color index. The temperature is normally given in terms of an effective temperature, which is the temperature of an idealized black body that radiates its energy at the same luminosity per surface area as the star. The effective temperature is only representative of the surface, as the temperature increases toward the core. The temperature in the core region of a star is several million kelvins.\nThe stellar temperature will determine the rate of ionization of various elements, resulting in characteristic absorption lines in the spectrum. The surface temperature of a star, along with its visual absolute magnitude and absorption features, is used to classify a star (see classification below).\nMassive main sequence stars can have surface temperatures of 50,000 K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600 K; but they have a high luminosity due to their large exterior surface area.\n== Radiation ==\nThe energy produced by stars, a product of nuclear fusion, radiates to space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind, which streams from the outer layers as electrically charged protons and alpha and beta particles. A steady stream of almost massless neutrinos emanate directly from the star's core.\nThe production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers.\nThe color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the star's outer layers, including its photosphere. Besides visible light, stars emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics.\nUsing the stellar spectrum, astronomers can determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is found, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.) With these parameters, astronomers can estimate the age of the star.\n=== Luminosity ===\nThe luminosity of a star is the amount of light and other forms of radiant energy it radiates per unit of time. It has units of power. The luminosity of a star is determined by its radius and surface temperature. Many stars do not radiate uniformly across their entire surface. The rapidly rotating star Vega, for example, has a higher energy flux (power per unit area) at its poles than along its equator.", 'UVC LEDs are relatively new to the commercial market and are gaining in popularity. Due to their monochromatic nature (±5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.\nDisinfection using UV radiation is commonly used in wastewater treatment applications and is finding an increased usage in municipal drinking water treatment. Many bottlers of spring water use UV disinfection equipment to sterilize their water. Solar water disinfection has been researched for cheaply treating contaminated water using natural sunlight. The UVA irradiation and increased water temperature kill organisms in the water.\nUltraviolet radiation is used in several food processes to kill unwanted microorganisms. UV can be used to pasteurize fruit juices by flowing the juice over a high-intensity ultraviolet source. The effectiveness of such a process depends on the UV absorbance of the juice.\nPulsed light (PL) is a technique of killing microorganisms on surfaces using pulses of an intense broad spectrum, rich in UVC between 200 and 280 nm. Pulsed light works with xenon flash lamps that can produce flashes several times per second. Disinfection robots use pulsed UV.\nThe antimicrobial effectiveness of filtered far-UVC (222\u2009nm) light on a range of pathogens, including bacteria and fungi showed inhibition of pathogen growth, and since it has lesser harmful effects, it provides essential insights for reliable disinfection in healthcare settings, such as hospitals and long-term care homes. UVC has also been shown to be effective at degrading SARS-CoV-2 virus.\n==== Biological ====\nSome animals, including birds, reptiles, and insects such as bees, can see near-ultraviolet wavelengths. Many fruits, flowers, and seeds stand out more strongly from the background in ultraviolet wavelengths as compared to human color vision. Scorpions glow or take on a yellow to green color under UV illumination, thus assisting in the control of these arachnids. Many birds have patterns in their plumage that are invisible at usual wavelengths but observable in ultraviolet, and the urine and other secretions of some animals, including dogs, cats, and human beings, are much easier to spot with ultraviolet. Urine trails of rodents can be detected by pest control technicians for proper treatment of infested dwellings.\nButterflies use ultraviolet as a communication system for sex recognition and mating behavior. For example, in the Colias eurytheme butterfly, males rely on visual cues to locate and identify females. Instead of using chemical stimuli to find mates, males are attracted to the ultraviolet-reflecting color of female hind wings. In Pieris napi butterflies it was shown that females in northern Finland with less UV-radiation present in the environment possessed stronger UV signals to attract their males than those occurring further south. This suggested that it was evolutionarily more difficult to increase the UV-sensitivity of the eyes of the males than to increase the UV-signals emitted by the females.\nMany insects use the ultraviolet wavelength emissions from celestial objects as references for flight navigation. A local ultraviolet emitter will normally disrupt the navigation process and will eventually attract the flying insect.\nThe green fluorescent protein (GFP) is often used in genetics as a marker. Many substances, such as proteins, have significant light absorption bands in the ultraviolet that are of interest in biochemistry and related fields. UV-capable spectrophotometers are common in such laboratories.\nUltraviolet traps called bug zappers are used to eliminate various small flying insects. They are attracted to the UV and are killed using an electric shock, or trapped once they come into contact with the device. Different designs of ultraviolet radiation traps are also used by entomologists for collecting nocturnal insects during faunistic survey studies.\n==== Therapy ====\nUltraviolet radiation is helpful in the treatment of skin conditions such as psoriasis and vitiligo. Exposure to UVA, while the skin is hyper-photosensitive, by taking psoralens is an effective treatment for psoriasis. Due to the potential of psoralens to cause damage to the liver, PUVA therapy may be used only a limited number of times over a patient\'s lifetime.\nUVB phototherapy does not require additional medications or topical preparations for the therapeutic benefit; only the exposure is needed. However, phototherapy can be effective when used in conjunction with certain topical treatments such as anthralin, coal tar, and vitamin A and D derivatives, or systemic treatments such as methotrexate and Soriatane.\n==== Herpetology ====\nReptiles need UVB for biosynthesis of vitamin D, and other metabolic processes. Specifically cholecalciferol (vitamin D3), which is needed for basic cellular / neural functioning as well as the utilization of calcium for bone and egg production. The UVA wavelength is also visible to many reptiles and might play a significant role in their ability survive in the wild as well as in visual communication between individuals. Therefore, in a typical reptile enclosure, a fluorescent UV a/b source (at the proper strength / spectrum for the species), must be available for many captive species to survive. Simple supplementation with cholecalciferol (Vitamin D3) will not be enough as there is a complete biosynthetic pathway that is "leapfrogged" (risks of possible overdoses), the intermediate molecules and metabolites also play important functions in the animals health. Natural sunlight in the right levels is always going to be superior to artificial sources, but this might not be possible for keepers in different parts of the world.\nIt is a known problem that high levels of output of the UVa part of the spectrum can both cause cellular and DNA damage to sensitive parts of their bodies – especially the eyes where blindness is the result of an improper UVa/b source use and placement photokeratitis. For many keepers there must also be a provision for an adequate heat source this has resulted in the marketing of heat and light "combination" products. Keepers should be careful of these "combination" light/ heat and UVa/b generators, they typically emit high levels of UVa with lower levels of UVb that are set and difficult to control so that animals can have their needs met. A better strategy is to use individual sources of these elements and so they can be placed and controlled by the keepers for the max benefit of the animals.\n== Evolutionary significance ==\nThe evolution of early reproductive proteins and enzymes is attributed in modern models of evolutionary theory to ultraviolet radiation. UVB causes thymine base pairs next to each other in genetic sequences to bond together into thymine dimers, a disruption in the strand that reproductive enzymes cannot copy. This leads to frameshifting during genetic replication and protein synthesis, usually killing the cell. Before formation of the UV-blocking ozone layer, when early prokaryotes approached the surface of the ocean, they almost invariably died out. The few that survived had developed enzymes that monitored the genetic material and removed thymine dimers by nucleotide excision repair enzymes. Many enzymes and proteins involved in modern mitosis and meiosis are similar to repair enzymes, and are believed to be evolved modifications of the enzymes originally used to overcome DNA damages caused by UV.\nElevated levels of ultraviolet radiation, in particular UV-B, have also been speculated as a cause of mass extinctions in the fossil record.\n== Photobiology ==\nPhotobiology is the scientific study of the beneficial and harmful interactions of non-ionizing radiation in living organisms, conventionally demarcated around 10 eV, the first ionization energy of oxygen. UV ranges roughly from 3 to 30 eV in energy. Hence photobiology entertains some, but not all, of the UV spectrum.\n== See also ==\n== References ==\n== Further reading ==\nAllen, Jeannie (6 September 2001). Ultraviolet Radiation: How it Affects Life on Earth. Earth Observatory. NASA, USA.\nHockberger, Philip E. (2002). "A History of Ultraviolet Photobiology for Humans, Animals and Microorganisms". Photochemistry and Photobiology. 76 (6): 561–569. doi:10.1562/0031-8655(2002)0760561AHOUPF2.0.CO2. PMID 12511035. S2CID 222100404.\nHu, S; Ma, F; Collado-Mesa, F; Kirsner, R. S. (July 2004). "UV radiation, latitude, and melanoma in US Hispanics and blacks". Arch. Dermatol. 140 (7): 819–824. doi:10.1001/archderm.140.7.819. PMID 15262692.\nStrauss, CEM; Funk, DJ (1991). "Broadly tunable difference-frequency generation of VUV using two-photon resonances in H2 and Kr". Optics Letters. 16 (15): 1192–4. Bibcode:1991OptL...16.1192S. doi:10.1364/ol.16.001192. PMID 19776917.\n== External links ==\nMedia related to Ultraviolet light at Wikimedia Commons\nThe dictionary definition of ultraviolet at Wiktionary', 'Photometry\n\nPhotometry can refer to:\nPhotometry (optics), the science of measurement of visible light in terms of its perceived brightness to human vision\nPhotometry (astronomy), the measurement of the flux or intensity of an astronomical object\'s electromagnetic radiation\nA photometric study, sometimes also referred to as a lighting "layout" or "point by point"\n== See also ==\nPhotogrammetry\nRadiometry', '=== Carbon dioxide levels and photorespiration ===\nAs carbon dioxide concentrations rise, the rate at which sugars are made by the light-independent reactions increases until limited by other factors. RuBisCO, the enzyme that captures carbon dioxide in the light-independent reactions, has a binding affinity for both carbon dioxide and oxygen. When the concentration of carbon dioxide is high, RuBisCO will fix carbon dioxide. However, if the carbon dioxide concentration is low, RuBisCO will bind oxygen instead of carbon dioxide. This process, called photorespiration, uses energy, but does not produce sugars.\nRuBisCO oxygenase activity is disadvantageous to plants for several reasons:\nOne product of oxygenase activity is phosphoglycolate (2 carbon) instead of 3-phosphoglycerate (3 carbon). Phosphoglycolate cannot be metabolized by the Calvin-Benson cycle and represents carbon lost from the cycle. A high oxygenase activity, therefore, drains the sugars that are required to recycle ribulose 5-bisphosphate and for the continuation of the Calvin-Benson cycle.\nPhosphoglycolate is quickly metabolized to glycolate that is toxic to a plant at a high concentration; it inhibits photosynthesis.\nSalvaging glycolate is an energetically expensive process that uses the glycolate pathway, and only 75% of the carbon is returned to the Calvin-Benson cycle as 3-phosphoglycerate. The reactions also produce ammonia (NH3), which is able to diffuse out of the plant, leading to a loss of nitrogen.\nA highly simplified summary is:\n2 glycolate + ATP → 3-phosphoglycerate + carbon dioxide + ADP + NH3\nThe salvaging pathway for the products of RuBisCO oxygenase activity is more commonly known as photorespiration, since it is characterized by light-dependent oxygen consumption and the release of carbon dioxide.\n== See also ==\n== References ==\n== Further reading ==\n=== Books ===\n=== Papers ===\n== External links ==\nA collection of photosynthesis pages for all levels from a renowned expert (Govindjee)\nIn depth, advanced treatment of photosynthesis, also from Govindjee\nScience Aid: Photosynthesis Article appropriate for high school science\nMetabolism, Cellular Respiration and Photosynthesis – The Virtual Library of Biochemistry and Cell Biology\nOverall examination of Photosynthesis at an intermediate level\nOverall Energetics of Photosynthesis\nThe source of oxygen produced by photosynthesis Interactive animation, a textbook tutorial\nMarshall J (2011-03-29). "First practical artificial leaf makes debut". Discovery News. Archived from the original on 2012-03-22. Retrieved 2011-03-29.\nPhotosynthesis – Light Dependent & Light Independent Stages Archived 2011-09-10 at the Wayback Machine\nKhan Academy, video introduction', 'The photochemical properties of melanin make it an excellent photoprotectant. However, sunscreen chemicals cannot dissipate the energy of the excited state as efficiently as melanin and therefore, if sunscreen ingredients penetrate into the lower layers of the skin, the amount of reactive oxygen species may be increased. The amount of sunscreen that penetrates through the stratum corneum may or may not be large enough to cause damage.\nIn an experiment by Hanson et al. that was published in 2006, the amount of harmful reactive oxygen species (ROS) was measured in untreated and in sunscreen treated skin. In the first 20 minutes, the film of sunscreen had a protective effect and the number of ROS species was smaller. After 60 minutes, however, the amount of absorbed sunscreen was so high that the amount of ROS was higher in the sunscreen-treated skin than in the untreated skin. The study indicates that sunscreen must be reapplied within 2 hours in order to prevent UV light from penetrating to sunscreen-infused live skin cells.\n==== Aggravation of certain skin conditions ====\nUltraviolet radiation can aggravate several skin conditions and diseases, including systemic lupus erythematosus, Sjögren\'s syndrome, Sinear Usher syndrome, rosacea, dermatomyositis, Darier\'s disease, Kindler–Weary syndrome and Porokeratosis.\n==== Eye damage ====\nThe eye is most sensitive to damage by UV in the lower UVC band at 265–275 nm. Radiation of this wavelength is almost absent from sunlight at the surface of the Earth but is emitted by artificial sources such as the electrical arcs employed in arc welding. Unprotected exposure to these sources can cause "welder\'s flash" or "arc eye" (photokeratitis) and can lead to cataracts, pterygium and pinguecula formation. To a lesser extent, UVB in sunlight from 310 to 280 nm also causes photokeratitis ("snow blindness"), and the cornea, the lens, and the retina can be damaged.\nProtective eyewear is beneficial to those exposed to ultraviolet radiation. Since light can reach the eyes from the sides, full-coverage eye protection is usually warranted if there is an increased risk of exposure, as in high-altitude mountaineering. Mountaineers are exposed to higher-than-ordinary levels of UV radiation, both because there is less atmospheric filtering and because of reflection from snow and ice.\nOrdinary, untreated eyeglasses give some protection. Most plastic lenses give more protection than glass lenses, because, as noted above, glass is transparent to UVA and the common acrylic plastic used for lenses is less so. Some plastic lens materials, such as polycarbonate, inherently block most UV.\n== Degradation of polymers, pigments and dyes ==\nUV degradation is one form of polymer degradation that affects plastics exposed to sunlight. The problem appears as discoloration or fading, cracking, loss of strength or disintegration. The effects of attack increase with exposure time and sunlight intensity. The addition of UV absorbers inhibits the effect.\nSensitive polymers include thermoplastics and speciality fibers like aramids. UV absorption leads to chain degradation and loss of strength at sensitive points in the chain structure. Aramid rope must be shielded with a sheath of thermoplastic if it is to retain its strength.\nMany pigments and dyes absorb UV and change colour, so paintings and textiles may need extra protection both from sunlight and fluorescent lamps, two common sources of UV radiation. Window glass absorbs some harmful UV, but valuable artifacts need extra shielding. Many museums place black curtains over watercolour paintings and ancient textiles, for example. Since watercolours can have very low pigment levels, they need extra protection from UV. Various forms of picture framing glass, including acrylics (plexiglass), laminates, and coatings, offer different degrees of UV (and visible light) protection.\n== Applications ==\nBecause of its ability to cause chemical reactions and excite fluorescence in materials, ultraviolet radiation has a number of applications. The following table gives some uses of specific wavelength bands in the UV spectrum.\n13.5 nm: Extreme ultraviolet lithography\n30–200 nm: Photoionization, ultraviolet photoelectron spectroscopy, standard integrated circuit manufacture by photolithography\n230–365 nm: UV-ID, label tracking, barcodes\n230–400 nm: Optical sensors, various instrumentation\n240–280 nm: Disinfection, decontamination of surfaces and water (DNA absorption has a peak at 260 nm), germicidal lamps\n200–400 nm: Forensic analysis, drug detection\n270–360 nm: Protein analysis, DNA sequencing, drug discovery\n280–400 nm: Medical imaging of cells\n300–320 nm: Light therapy in medicine\n300–365 nm: Curing of polymers and printer inks\n350–370 nm: Bug zappers (flies are most attracted to light at 365 nm)\n=== Photography ===\nPhotographic film responds to ultraviolet radiation but the glass lenses of cameras usually block radiation shorter than 350 nm. Slightly yellow UV-blocking filters are often used for outdoor photography to prevent unwanted bluing and overexposure by UV rays. For photography in the near UV, special filters may be used. Photography with wavelengths shorter than 350 nm requires special quartz lenses which do not absorb the radiation.\nDigital cameras sensors may have internal filters that block UV to improve color rendition accuracy. Sometimes these internal filters can be removed, or they may be absent, and an external visible-light filter prepares the camera for near-UV photography. A few cameras are designed for use in the UV.\nPhotography by reflected ultraviolet radiation is useful for medical, scientific, and forensic investigations, in applications as widespread as detecting bruising of skin, alterations of documents, or restoration work on paintings. Photography of the fluorescence produced by ultraviolet illumination uses visible wavelengths of light.\nIn ultraviolet astronomy, measurements are used to discern the chemical composition of the interstellar medium, and the temperature and composition of stars. Because the ozone layer blocks many UV frequencies from reaching telescopes on the surface of the Earth, most UV observations are made from space.\n=== Electrical and electronics industry ===\nCorona discharge on electrical apparatus can be detected by its ultraviolet emissions. Corona causes degradation of electrical insulation and emission of ozone and nitrogen oxide.\nEPROMs (Erasable Programmable Read-Only Memory) are erased by exposure to UV radiation. These modules have a transparent (quartz) window on the top of the chip that allows the UV radiation in.\n=== Fluorescent dye uses ===\nColorless fluorescent dyes that emit blue light under UV are added as optical brighteners to paper and fabrics. The blue light emitted by these agents counteracts yellow tints that may be present and causes the colors and whites to appear whiter or more brightly colored.\nUV fluorescent dyes that glow in the primary colors are used in paints, papers, and textiles either to enhance color under daylight illumination or to provide special effects when lit with UV lamps. Blacklight paints that contain dyes that glow under UV are used in a number of art and aesthetic applications.\nAmusement parks often use UV lighting to fluoresce ride artwork and backdrops. This often has the side effect of causing rider\'s white clothing to glow light-purple.\nTo help prevent counterfeiting of currency, or forgery of important documents such as driver\'s licenses and passports, the paper may include a UV watermark or fluorescent multicolor fibers that are visible under ultraviolet light. Postage stamps are tagged with a phosphor that glows under UV rays to permit automatic detection of the stamp and facing of the letter.\nUV fluorescent dyes are used in many applications (for example, biochemistry and forensics). Some brands of pepper spray will leave an invisible chemical (UV dye) that is not easily washed off on a pepper-sprayed attacker, which would help police identify the attacker later.\nIn some types of nondestructive testing UV stimulates fluorescent dyes to highlight defects in a broad range of materials. These dyes may be carried into surface-breaking defects by capillary action (liquid penetrant inspection) or they may be bound to ferrite particles caught in magnetic leakage fields in ferrous materials (magnetic particle inspection).\n=== Analytic uses ===\n==== Forensics ====\nUV is an investigative tool at the crime scene helpful in locating and identifying bodily fluids such as semen, blood, and saliva. For example, ejaculated fluids or saliva can be detected by high-power UV sources, irrespective of the structure or colour of the surface the fluid is deposited upon. UV–vis microspectroscopy is also used to analyze trace evidence, such as textile fibers and paint chips, as well as questioned documents.\nOther applications include the authentication of various collectibles and art, and detecting counterfeit currency. Even materials not specially marked with UV sensitive dyes may have distinctive fluorescence under UV exposure or may fluoresce differently under short-wave versus long-wave ultraviolet.\n==== Enhancing contrast of ink ====\nUsing multi-spectral imaging it is possible to read illegible papyrus, such as the burned papyri of the Villa of the Papyri or of Oxyrhynchus, or the Archimedes palimpsest. The technique involves taking pictures of the illegible document using different filters in the infrared or ultraviolet range, finely tuned to capture certain wavelengths of light. Thus, the optimum spectral portion can be found for distinguishing ink from paper on the papyrus surface.\nSimple NUV sources can be used to highlight faded iron-based ink on vellum.\n==== Sanitary compliance ====', '==== Type II ====\nStars with initial masses less than about 8 M☉ never develop a core large enough to collapse and they eventually lose their atmospheres to become white dwarfs. Stars with at least 9 M☉ (possibly as much as 12 M☉) evolve in a complex fashion, progressively burning heavier elements at hotter temperatures in their cores. The star becomes layered like an onion, with the burning of more easily fused elements occurring in larger shells. Although popularly described as an onion with an iron core, the least massive supernova progenitors only have oxygen-neon(-magnesium) cores. These super-AGB stars may form the majority of core collapse supernovae, although less luminous and so less commonly observed than those from more massive progenitors.\nIf core collapse occurs during a supergiant phase when the star still has a hydrogen envelope, the result is a type II supernova. The rate of mass loss for luminous stars depends on the metallicity and luminosity. Extremely luminous stars at near solar metallicity will lose all their hydrogen before they reach core collapse and so will not form a supernova of type II. At low metallicity, all stars will reach core collapse with a hydrogen envelope but sufficiently massive stars collapse directly to a black hole without producing a visible supernova.\nStars with an initial mass up to about 90 times the Sun, or a little less at high metallicity, result in a type II-P supernova, which is the most commonly observed type. At moderate to high metallicity, stars near the upper end of that mass range will have lost most of their hydrogen when core collapse occurs and the result will be a type II-L supernova. At very low metallicity, stars of around 140–250 M☉ will reach core collapse by pair instability while they still have a hydrogen atmosphere and an oxygen core and the result will be a supernova with type II characteristics but a very large mass of ejected 56Ni and high luminosity.\n==== Type Ib and Ic ====\nThese supernovae, like those of type II, are massive stars that undergo core collapse. Unlike the progenitors of type II supernovae, the stars which become types Ib and Ic supernovae have lost most of their outer (hydrogen) envelopes due to strong stellar winds or else from interaction with a companion. These stars are known as Wolf–Rayet stars, and they occur at moderate to high metallicity where continuum driven winds cause sufficiently high mass-loss rates. Observations of type Ib/c supernova do not match the observed or expected occurrence of Wolf–Rayet stars. Alternate explanations for this type of core collapse supernova involve stars stripped of their hydrogen by binary interactions. Binary models provide a better match for the observed supernovae, with the proviso that no suitable binary helium stars have ever been observed.\nType Ib supernovae are the more common and result from Wolf–Rayet stars of type WC which still have helium in their atmospheres. For a narrow range of masses, stars evolve further before reaching core collapse to become WO stars with very little helium remaining, and these are the progenitors of type Ic supernovae.\nA few percent of the type Ic supernovae are associated with gamma-ray bursts (GRB), though it is also believed that any hydrogen-stripped type Ib or Ic supernova could produce a GRB, depending on the circumstances of the geometry. The mechanism for producing this type of GRB is the jets produced by the magnetic field of the rapidly spinning magnetar formed at the collapsing core of the star. The jets would also transfer energy into the expanding outer shell, producing a super-luminous supernova.\nUltra-stripped supernovae occur when the exploding star has been stripped (almost) all the way to the metal core, via mass transfer in a close binary. As a result, very little material is ejected from the exploding star (c. 0.1 M☉). In the most extreme cases, ultra-stripped supernovae can occur in naked metal cores, barely above the Chandrasekhar mass limit. SN 2005ek might be the first observational example of an ultra-stripped supernova, giving rise to a relatively dim and fast decaying light curve. The nature of ultra-stripped supernovae can be both iron core-collapse and electron capture supernovae, depending on the mass of the collapsing core. Ultra-stripped supernovae are believed to be associated with the second supernova explosion in a binary system, producing for example a tight double neutron star system.\nIn 2022 a team of astronomers led by researchers from the Weizmann Institute of Science reported the first supernova explosion showing direct evidence for a Wolf-Rayet progenitor star. SN 2019hgp was a type Icn supernova and is also the first in which the element neon has been detected.\n==== Electron-capture supernovae ====\nIn 1980, a "third type" of supernova was predicted by Ken\'ichi Nomoto of the University of Tokyo, called an electron-capture supernova. It would arise when a star "in the transitional range (~8 to 10 solar masses) between white dwarf formation and iron core-collapse supernovae", and with a degenerate O+Ne+Mg core, imploded after its core ran out of nuclear fuel, causing gravity to compress the electrons in the star\'s core into their atomic nuclei, leading to a supernova explosion and leaving behind a neutron star. In June 2021, a paper in the journal Nature Astronomy reported that the 2018 supernova SN 2018zd (in the galaxy NGC 2146, about 31 million light-years from Earth) appeared to be the first observation of an electron-capture supernova. The 1054 supernova explosion that created the Crab Nebula in our galaxy had been thought to be the best candidate for an electron-capture supernova, and the 2021 paper makes it more likely that this was correct.\n=== Failed supernovae ===\nThe core collapse of some massive stars may not result in a visible supernova. This happens if the initial core collapse cannot be reversed by the mechanism that produces an explosion, usually because the core is too massive. These events are difficult to detect, but large surveys have detected possible candidates. The red supergiant N6946-BH1 in NGC 6946 underwent a modest outburst in March 2009, before fading from view. Only a faint infrared source remains at the star\'s location.\n=== Light curves ===\nThe ejecta gases would dim quickly without some energy input to keep them hot. The source of this energy—which can maintain the optical supernova glow for months—was, at first, a puzzle. Some considered rotational energy from the central pulsar as a source. Although the energy that initially powers each type of supernovae is delivered promptly, the light curves are dominated by subsequent radioactive heating of the rapidly expanding ejecta. The intensely radioactive nature of the ejecta gases was first calculated on sound nucleosynthesis grounds in the late 1960s, and this has since been demonstrated as correct for most supernovae. It was not until SN 1987A that direct observation of gamma-ray lines unambiguously identified the major radioactive nuclei.\nIt is now known by direct observation that much of the light curve (the graph of luminosity as a function of time) after the occurrence of a type II Supernova, such as SN 1987A, is explained by those predicted radioactive decays. Although the luminous emission consists of optical photons, it is the radioactive power absorbed by the ejected gases that keeps the remnant hot enough to radiate light. The radioactive decay of 56Ni through its daughters 56Co to 56Fe produces gamma-ray photons, primarily with energies of 847 keV and 1,238 keV, that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of 56Ni to 56Co (half-life 6 days) while energy for the later light curve in particular fit very closely with the 77.3-day half-life of 56Co decaying to 56Fe. Later measurements by space gamma-ray telescopes of the small fraction of the 56Co and 57Co gamma rays that escaped the SN 1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power sources.\nThe late-time decay phase of visual light curves for different supernova types all depend on radioactive heating, but they vary in shape and amplitude because of the underlying mechanisms, the way that visible radiation is produced, the epoch of its observation, and the transparency of the ejected material. The light curves can be significantly different at other wavelengths. For example, at ultraviolet wavelengths there is an early extremely luminous peak lasting only a few hours corresponding to the breakout of the shock launched by the initial event, but that breakout is hardly detectable optically.', 'In the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein\'s proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal. This has at times been called Planck\'s "second theory".\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his \'third theory\', that both emission and absorption of light were quantal.\nThe colourful term "ultraviolet catastrophe" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck\'s thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of "catastrophe". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called \'photons\', and by Rayleigh and by Jeans.\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν. In contrast to Planck\'s and Einstein\'s formulas, Bohr\'s formula referred explicitly and categorically to energy levels of atoms. Bohr\'s formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck\'s model, the frequency\n{\\displaystyle \\nu }\nhas no immediate relation to frequencies that might describe those quantum states themselves.\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck\'s law. The actual word \'photon\' was invented still later, by G.N. Lewis in 1926, who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word \'photon\' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein\'s 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\nUltimately, Planck\'s law of black-body radiation contributed to Einstein\'s concept of quanta of light carrying linear momentum, which became the fundamental basis for the development of quantum mechanics.\nThe above-mentioned linearity of Planck\'s mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg\'s original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg\'s theory accounted for Bohr\'s above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states. At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg\'s paper and recognized the matrix character of Heisenberg\'s theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg\'s original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics. Heisenberg\'s explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck\'s oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = \u2060h/2π\u2060, and ω = 2πν denotes angular frequency, and less often the equivalent formula E = hν. This statement about a really existing and propagating light quantum, based on Einstein\'s, has a physical meaning different from that of Planck\'s above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\nAn article by Helge Kragh published in Physics World gives an account of this history.\n== See also ==\nEmissivity\nRadiance\nSakuma–Hattori equation\n== References ==\n=== Bibliography ===\n== External links ==\nSummary of Radiation\nRadiation of a Blackbody – interactive simulation to play with Planck\'s law\nScienceworld entry on Planck\'s Law', 'The light curves for type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. Their optical energy output is driven by radioactive decay of ejected nickel-56 (half-life 6 days), which then decays to radioactive cobalt-56 (half-life 77 days). These radioisotopes excite the surrounding material to incandescence. Modern studies of cosmology rely on 56Ni radioactivity providing the energy for the optical brightness of supernovae of type Ia, which are the "standard candles" of cosmology but whose diagnostic 847 keV and 1,238 keV gamma rays were first detected only in 2014. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half-life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission from the remaining cobalt-56 becomes dominant, although this portion of the light curve has been little-studied.\nType Ib and Ic light curves are similar to type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of the created nickel-56. The peak luminosity varies considerably and there are even occasional type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increased peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.\nThe light curves for type II supernovae are characterised by a much slower decline than type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial destruction this hydrogen becomes heated and ionised. The majority of type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in type I supernovae, due to the efficiency of conversion into light by all the hydrogen.\nIn type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a type I supernova and the hydrogen even disappears from the spectrum after several weeks.\nType IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a superluminous supernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.\nLarge numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.\nNotes:\n=== Asymmetry ===\nA long-standing puzzle surrounding type II supernovae is why the remaining compact object receives a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high peculiar velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500 km/s or greater. This indicates an expansion asymmetry, but the mechanism by which momentum is transferred to the compact object remains a puzzle. Proposed explanations for this kick include convection in the collapsing star, asymmetric ejection of matter during neutron star formation, and asymmetrical neutrino emissions.\nOne possible explanation for this asymmetry is large-scale convection above the core. The convection can create radial variations in density giving rise to variations in the amount of energy absorbed from neutrino outflow. However analysis of this mechanism predicts only modest momentum transfer. Another possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova. (A similar model is used for explaining long gamma-ray bursts.) The dominant mechanism may depend upon the mass of the progenitor star.\nInitial asymmetries have also been confirmed in type Ia supernovae through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the expansion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarisation of the emitted light.\n=== Energy output ===\nAlthough supernovae are primarily known as luminous events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.\nThere is a fundamental difference between the balance of energy production in the different types of supernova. In type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.\nStandard type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is 56Ni generated from silicon burning. 56Ni is radioactive and decays into 56Co by beta plus decay (with a half life of six days) and gamma rays. 56Co itself decays by the beta plus (positron) path with a half life of 77 days into stable 56Fe. These two processes are responsible for the electromagnetic radiation from type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.\nCore collapse supernovae are on average visually fainter than type Ia supernovae, but the total energy released is far higher, as outlined in the following table.\nIn some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high-luminosity supernovae and is thought to be the cause of type Ic hypernovae and long-duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low-luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.\nWhen a supernova occurs inside a small dense cloud of circumstellar material, it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause type IIn hypernovae.\nAlthough pair-instability supernovae are core collapse supernovae with spectra and light curves similar to type II-P, the nature after core collapse is more like that of a giant type Ia with runaway fusion of carbon, oxygen and silicon. The total energy released by the highest-mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.\n=== Progenitor ===', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===']

Question: What is the difference between illuminance and luminance?

Choices:
Choice A) Illuminance is the amount of light absorbed by a surface per unit area, while luminance is the amount of light reflected by a surface per unit area.
Choice B) Illuminance is the amount of light falling on a surface per unit area, while luminance is the amount of light emitted by a source per unit area.
Choice C) Illuminance is the amount of light concentrated into a smaller area, while luminance is the amount of light filling a larger solid angle.
Choice D) Illuminance is the amount of light emitted by a source per unit area, while luminance is the amount of light falling on a surface per unit area.
Choice E) Illuminance is the amount of light reflected by a surface per unit area, while luminance is the amount of light absorbed by a surface per unit area.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Classical mechanics', 'After Newton, classical mechanics became a principal field of study in mathematics as well as physics. Mathematical formulations progressively allowed finding solutions to a far greater number of problems. The first notable mathematical treatment was in 1788 by Joseph Louis Lagrange. Lagrangian mechanics was in turn re-formulated in 1833 by William Rowan Hamilton.\nSome difficulties were discovered in the late 19th century that could only be resolved by more modern physics. Some of these difficulties related to compatibility with electromagnetic theory, and the famous Michelson–Morley experiment. The resolution of these problems led to the special theory of relativity, often still considered a part of classical mechanics.\nA second set of difficulties were related to thermodynamics. When combined with thermodynamics, classical mechanics leads to the Gibbs paradox of classical statistical mechanics, in which entropy is not a well-defined quantity. Black-body radiation was not explained without the introduction of quanta. As experiments reached the atomic level, classical mechanics failed to explain, even approximately, such basic things as the energy levels and sizes of atoms and the photo-electric effect. The effort at resolving these problems led to the development of quantum mechanics.\nSince the end of the 20th century, classical mechanics in physics has no longer been an independent theory. Instead, classical mechanics is now considered an approximate theory to the more general quantum mechanics. Emphasis has shifted to understanding the fundamental forces of nature as in the Standard Model and its more modern extensions into a unified theory of everything. Classical mechanics is a theory useful for the study of the motion of non-quantum mechanical, low-energy particles in weak gravitational fields.\n== See also ==\n== Notes ==\n== References ==\n== Further reading ==\nAlonso, M.; Finn, J. (1992). Fundamental University Physics. Addison-Wesley.\nFeynman, Richard (1999). The Feynman Lectures on Physics. Perseus Publishing. ISBN 978-0-7382-0092-7.\nFeynman, Richard; Phillips, Richard (1998). Six Easy Pieces. Perseus Publishing. ISBN 978-0-201-32841-7.\nGoldstein, Herbert; Charles P. Poole; John L. Safko (2002). Classical Mechanics (3rd ed.). Addison Wesley. ISBN 978-0-201-65702-9.\nKibble, Tom W.B.; Berkshire, Frank H. (2004). Classical Mechanics (5th ed.). Imperial College Press. ISBN 978-1-86094-424-6.\nKleppner, D.; Kolenkow, R.J. (1973). An Introduction to Mechanics. McGraw-Hill. ISBN 978-0-07-035048-9.\nLandau, L.D.; Lifshitz, E.M. (1972). Course of Theoretical Physics, Vol. 1 – Mechanics. Franklin Book Company. ISBN 978-0-08-016739-8.\nMorin, David (2008). Introduction to Classical Mechanics: With Problems and Solutions (1st ed.). Cambridge: Cambridge University Press. ISBN 978-0-521-87622-3.\nGerald Jay Sussman; Jack Wisdom (2001). Structure and Interpretation of Classical Mechanics. MIT Press. ISBN 978-0-262-19455-6.\nO\'Donnell, Peter J. (2015). Essential Dynamics and Relativity. CRC Press. ISBN 978-1-4665-8839-4.\nThornton, Stephen T.; Marion, Jerry B. (2003). Classical Dynamics of Particles and Systems (5th ed.). Brooks Cole. ISBN 978-0-534-40896-1.\n== External links ==\nCrowell, Benjamin. Light and Matter (an introductory text, uses algebra with optional sections involving calculus)\nFitzpatrick, Richard. Classical Mechanics (uses calculus)\nHoiland, Paul (2004). Preferred Frames of Reference & Relativity\nHorbatsch, Marko, "Classical Mechanics Course Notes".\nRosu, Haret C., "Classical Mechanics". Physics Education. 1999. [arxiv.org : physics/9909035]\nShapiro, Joel A. (2003). Classical Mechanics\nSussman, Gerald Jay & Wisdom, Jack &  Mayer, Meinhard E. (2001). Structure and Interpretation of Classical Mechanics\nTong, David. Classical Dynamics (Cambridge lecture notes on Lagrangian and Hamiltonian formalism)\nKinematic Models for Design Digital Library (KMODDL) Movies and photos of hundreds of working mechanical-systems models at Cornell University. Also includes an e-book library of classic texts on mechanical design and engineering.\nMIT OpenCourseWare 8.01: Classical Mechanics Free videos of actual course lectures with links to lecture notes, assignments and exams.\nAlejandro A. Torassa, On Classical Mechanics', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  ħωk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, θk†, θk, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the θk, and the Fock space is built by applying all products of creation operators θk† to |0⟩.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at φ = 0, so that the vacuum minimizing the Hamiltonian satisfies ⟨φ⟩ = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  φ = v .  This occurs for example, if V(φ) = gφ4 − 2m2φ2 with g > 0 and m2 > 0, for which the minimum energy is found at v = ±m/√g. The value of v in one of these vacua may be considered as condensate of the field φ. Canonical quantization then can be carried out for the shifted field  φ(x,t) − v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator  [A, B]  expressed in the phase space formulation is iħ{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  ħ-deformations are highly nonunique—quantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  ħ → 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an ħ-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional Schrödinger Hilbert space. If the chosen variables are complex, we get something like the Segal–Bargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', '{\\displaystyle L_{z}=m_{\\ell }\\hbar }\nThe values of mℓ range from −ℓ to ℓ, with integer intervals.\nThe s subshell (ℓ = 0) contains only one orbital, and therefore the mℓ of an electron in an s orbital will always be 0. The p subshell (ℓ = 1) contains three orbitals, so the mℓ of an electron in a p orbital will be −1, 0, or 1. The d subshell (ℓ = 2) contains five orbitals, with mℓ values of −2, −1, 0, 1, and 2.\n=== Spin magnetic quantum number ===\nThe spin magnetic quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis:\n{\\displaystyle S_{z}=m_{s}\\hbar }\nIn general, the values of ms range from −s to s, where s is the spin quantum number, associated with the magnitude of particle\'s intrinsic spin angular momentum:\n{\\displaystyle m_{s}=-s,-s+1,-s+2,\\cdots ,s-2,s-1,s}\nAn electron state has spin number s = \u20601/2\u2060, consequently ms will be +\u20601/2\u2060 ("spin up") or −\u20601/2\u2060 "spin down" states. Since electron are fermions they obey the Pauli exclusion principle: each electron state must have different quantum numbers.  Therefore, every orbital will be occupied with at most two electrons, one for each spin state.\n=== The Aufbau principle and Hund\'s Rules ===\nA multi-electron atom can be modeled qualitatively as a hydrogen like atom with higher nuclear charge and correspondingly more electrons. The occupation of the electron states in such an atom can be predicted by the Aufbau principle and Hund\'s empirical rules for the quantum numbers.  The Aufbau principle fills orbitals based on their principal and azimuthal quantum numbers (lowest n + l first, with lowest n breaking ties; Hund\'s rule favors unpaired electrons in the outermost orbital). These rules are empirical but they can be related to electron physics.:\u200a10\u200a:\u200a260\n== Spin-orbit coupled systems ==\nWhen one takes the spin–orbit interaction into consideration, the L and S operators no longer commute with the Hamiltonian, and the eigenstates of the system no longer have well-defined orbital angular momentum and spin. Thus another set of quantum numbers should be used. This set includes\nThe total angular momentum quantum number:\n{\\displaystyle j=|\\ell \\pm s|,}\nwhich gives the total angular momentum through the relation\n{\\displaystyle J^{2}=\\hbar ^{2}j(j+1).}\nThe projection of the total angular momentum along a specified axis:\n{\\displaystyle m_{j}=-j,-j+1,-j+2,\\cdots ,j-2,j-1,j}\nanalogous to the above and satisfies both\n{\\displaystyle m_{j}=m_{\\ell }+m_{s},}\nand\n{\\displaystyle |m_{\\ell }+m_{s}|\\leq j.}\nParityThis is the eigenvalue under reflection: positive (+1) for states which came from even ℓ and negative (−1) for states which came from odd ℓ. The former is also known as even parity and the latter as odd parity, and is given by\n{\\displaystyle P=(-1)^{\\ell }.}\nFor example, consider the following 8 states, defined by their quantum numbers:\nThe quantum states in the system can be described as linear combination of these 8 states. However, in the presence of spin–orbit interaction, if one wants to describe the same system by 8 states that are eigenvectors of the Hamiltonian (i.e. each represents a state that does not mix with others over time), we should consider the following 8 states:\n== Atomic nuclei ==\nIn nuclei, the entire assembly of protons and neutrons (nucleons) has a resultant angular momentum due to the angular momenta of each nucleon, usually denoted I. If the total angular momentum of a neutron is jn = ℓ + s and for a proton is jp = ℓ + s (where s for protons and neutrons happens to be \u20601/2\u2060 again (see note)), then the nuclear angular momentum quantum numbers I are given by:\n{\\displaystyle I=|j_{n}-j_{p}|,|j_{n}-j_{p}|+1,|j_{n}-j_{p}|+2,\\cdots ,(j_{n}+j_{p})-2,(j_{n}+j_{p})-1,(j_{n}+j_{p})}\nNote: The orbital angular momenta of the nuclear (and atomic) states are all integer multiples of ħ while the intrinsic angular momentum of the neutron and  proton are half-integer multiples.  It should be immediately apparent that the combination of the intrinsic spins of the nucleons with their orbital motion will always give half-integer values for the total spin, I, of any odd-A nucleus and integer values for any even-A nucleus.\nParity with the number I is used to label nuclear angular momentum states, examples for some isotopes of hydrogen (H), carbon (C), and sodium (Na) are;\nThe reason for the unusual fluctuations in I, even by differences of just one nucleon, are due to the odd and even numbers of protons and neutrons – pairs of nucleons have a total angular momentum of zero (just like electrons in orbitals), leaving an odd or even number of unpaired nucleons. The property of nuclear spin is an important factor for the operation of NMR spectroscopy in organic chemistry, and MRI in nuclear medicine, due to the nuclear magnetic moment interacting with an external magnetic field.\n== Elementary particles ==\nElementary particles contain many quantum numbers which are usually said to be intrinsic to them. However, it should be understood that the elementary particles are quantum states of the standard model of particle physics, and hence the quantum numbers of these particles bear the same relation to the Hamiltonian of this model as the quantum numbers of the Bohr atom does to its Hamiltonian. In other words, each quantum number denotes a symmetry of the problem. It is more useful in quantum field theory to distinguish between spacetime and internal symmetries.\nTypical quantum numbers related to spacetime symmetries are spin (related to rotational symmetry), the parity, C-parity and T-parity (related to the Poincaré symmetry of spacetime). Typical internal symmetries are lepton number and baryon number or the electric charge. (For a full list of quantum numbers of this kind see the article on flavour.)\n== Multiplicative quantum numbers ==\nMost conserved quantum numbers are additive, so in an elementary particle reaction, the sum of the quantum numbers should be the same before and after the reaction. However, some, usually called a parity, are multiplicative; i.e., their product is conserved. All multiplicative quantum numbers belong to a symmetry (like parity) in which applying the symmetry transformation twice is equivalent to doing nothing (involution).\n== See also ==\nElectron configuration\n== References ==\n== Further reading ==\nDirac, Paul A. M. (1982). Principles of Quantum Mechanics. Oxford University Press. ISBN 0-19-852011-5.\nGriffiths, David J. (2004). Introduction to Quantum Mechanics (2nd ed.). Prentice Hall. ISBN 0-13-805326-X.\nHalzen, Francis & Martin, Alan D. (1984). Quarks and Leptons: An Introductory Course in Modern Particle Physics. John Wiley & Sons. ISBN 0-471-88741-2.\nEisberg, Robert Martin; Resnick, Robert (1985). Quantum Physics of Atoms, Molecules, Solids, Nuclei and Particles (2nd ed.). John Wiley & Sons. ISBN 978-0-471-87373-0 – via Internet Archive.', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Almost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought that, in principle, every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.\nNon-logical axioms are often simply referred to as axioms in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For instance, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom, we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.\n==== Examples ====\nThis section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.\nBasic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo–Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann–Bernays–Gödel set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse–Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe is used, but in fact, most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.\nThe study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of abstract algebra brought with itself group theory, rings, fields, and Galois theory.\nThis list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.\n===== Arithmetic =====\nThe Peano axioms are the most widely used axiomatization of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed Gödel to establish his famous second incompleteness theorem.\nWe have a language\n{\\displaystyle {\\mathfrak {L}}_{NT}=\\{0,S\\}}\nwhere\n{\\displaystyle 0}\nis a constant symbol and\n{\\displaystyle S}\nis a unary function and the following axioms:\n{\\displaystyle \\forall x.\\lnot (Sx=0)}\n{\\displaystyle \\forall x.\\forall y.(Sx=Sy\\to x=y)}\n{\\displaystyle (\\phi (0)\\land \\forall x.\\,(\\phi (x)\\to \\phi (Sx)))\\to \\forall x.\\phi (x)}\nfor any\n{\\displaystyle {\\mathfrak {L}}_{NT}}\nformula\n{\\displaystyle \\phi }\nwith one free variable.\nThe standard structure is\n{\\displaystyle {\\mathfrak {N}}=\\langle \\mathbb {N} ,0,S\\rangle }\nwhere\n{\\displaystyle \\mathbb {N} }\nis the set of natural numbers,\n{\\displaystyle S}\nis the successor function and\n{\\displaystyle 0}\nis naturally interpreted as the number 0.\n===== Euclidean geometry =====\nProbably the oldest, and most famous, list of axioms are the 4 + 1 Euclid\'s postulates of plane geometry. The axioms are referred to as "4 + 1" because for nearly two millennia the fifth (parallel) postulate ("through a point outside a line there is exactly one parallel") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. One can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate ("a line can be extended indefinitely") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.\n===== Real analysis =====\nThe objectives of the study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a Dedekind complete ordered field, meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires the use of second-order logic. The Löwenheim–Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.\n=== Role in mathematical logic ===\n==== Deductive systems and completeness ====\nA deductive system consists of a set\n{\\displaystyle \\Lambda }\nof logical axioms, a set\n{\\displaystyle \\Sigma }\nof non-logical axioms, and a set\n{\\displaystyle \\{(\\Gamma ,\\phi )\\}}\nof rules of inference.  A desirable property of a deductive system is that it be complete.  A system is said to be complete if, for all formulas\n{\\displaystyle \\phi }\nthat is, for any statement that is a logical consequence of\n{\\displaystyle \\Sigma }\nthere actually exists a deduction of the statement from\n{\\displaystyle \\Sigma }\n.  This is sometimes expressed as "everything that is true is provable", but it must be understood that "true" here means "made true by the set of axioms", and not, for example, "true in the intended interpretation". Gödel\'s completeness theorem establishes the completeness of a certain commonly used type of deductive system.\nNote that "completeness" has a different meaning here than it does in the context of Gödel\'s first incompleteness theorem, which states that no recursive, consistent set of non-logical axioms\n{\\displaystyle \\Sigma }\nof the Theory of Arithmetic is complete, in the sense that there will always exist an arithmetic statement\n{\\displaystyle \\phi }\nsuch that neither\n{\\displaystyle \\phi }\nnor\n{\\displaystyle \\lnot \\phi }\ncan be proved from the given set of axioms.\nThere is thus, on the one hand, the notion of completeness of a deductive system and on the other hand that of completeness of a set of non-logical axioms.  The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.\n=== Further discussion ===\nEarly mathematicians regarded axiomatic geometry as a model of physical space, implying, there could ultimately only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details, and modern algebra was born.  In the modern view, axioms may be any set of formulas, as long as they are not known to be inconsistent.\n== See also ==\nAxiomatic system\nDogma\nFirst principle, axiom in science and philosophy\nList of axioms\nModel theory\nRegulæ Juris\nTheorem\nPresupposition\nPrinciple\n== Notes ==\n== References ==\n== Further reading ==\nMendelson, Elliot (1987). Introduction to mathematical logic. Belmont, California: Wadsworth & Brooks. ISBN 0-534-06624-0\nJohn Cook Wilson (1889), On an Evolutionist Theory of Axioms: inaugural lecture delivered October 15, 1889 (1st ed.), Oxford, Wikidata Q26720682{{citation}}:  CS1 maint: location missing publisher (link)\n== External links ==\nAxiom at PhilPapers\nAxiom at PlanetMath.\nMetamath axioms page', 'The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions, which groups by the nature of the outcome being considered (discrete, absolutely continuous, multivariate, etc.)\nAll of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution.\n=== Linear growth (e.g. errors, offsets) ===\nNormal distribution (Gaussian distribution), for a single such quantity; the most commonly used absolutely continuous distribution\n=== Exponential growth (e.g. prices, incomes, populations) ===\nLog-normal distribution, for a single such quantity whose log is normally distributed\nPareto distribution, for a single such quantity whose log is exponentially distributed; the prototypical power law distribution\n=== Uniformly distributed quantities ===\nDiscrete uniform distribution, for a finite set of values (e.g. the outcome of a fair dice)\nContinuous uniform distribution, for absolutely continuously distributed values\n=== Bernoulli trials (yes/no events, with a given probability) ===\nBasic distributions:\nBernoulli distribution, for the outcome of a single Bernoulli trial (e.g. success/failure, yes/no)\nBinomial distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed total number of independent occurrences\nNegative binomial distribution, for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs\nGeometric distribution, for binomial-type observations but where the quantity of interest is the number of failures before the first success; a special case of the negative binomial distribution\nRelated to sampling schemes over a finite population:\nHypergeometric distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, using sampling without replacement\nBeta-binomial distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, sampling using a Pólya urn model (in some sense, the "opposite" of sampling without replacement)\n=== Categorical outcomes (events with K possible outcomes) ===\nCategorical distribution, for a single categorical outcome (e.g. yes/no/maybe in a survey); a generalization of the Bernoulli distribution\nMultinomial distribution, for the number of each type of categorical outcome, given a fixed number of total outcomes; a generalization of the binomial distribution\nMultivariate hypergeometric distribution, similar to the multinomial distribution, but using sampling without replacement; a generalization of the hypergeometric distribution\n=== Poisson process (events that occur independently with a given rate) ===\nPoisson distribution, for the number of occurrences of a Poisson-type event in a given period of time\nExponential distribution, for the time before the next Poisson-type event occurs\nGamma distribution, for the time before the next k Poisson-type events occur\n=== Absolute values of vectors with normally distributed components ===\nRayleigh distribution, for the distribution of vector magnitudes with Gaussian distributed orthogonal components. Rayleigh distributions are found in RF signals with Gaussian real and imaginary components.\nRice distribution, a generalization of the Rayleigh distributions for where there is a stationary background signal component. Found in Rician fading of radio signals due to multipath propagation and in MR images with noise corruption on non-zero NMR signals.\n=== Normally distributed quantities operated with sum of squares ===\nChi-squared distribution, the distribution of a sum of squared standard normal variables; useful e.g. for inference regarding the sample variance of normally distributed samples (see chi-squared test)\nStudent\'s t distribution, the distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable; useful for inference regarding the mean of normally distributed samples with unknown variance (see Student\'s t-test)\nF-distribution, the distribution of the ratio of two scaled chi squared variables; useful e.g. for inferences that involve comparing variances or involving R-squared (the squared correlation coefficient)\n=== As conjugate prior distributions in Bayesian inference ===\nBeta distribution, for a single probability (real number between 0 and 1); conjugate to the Bernoulli distribution and binomial distribution\nGamma distribution, for a non-negative scaling parameter; conjugate to the rate parameter of a Poisson distribution or exponential distribution, the precision (inverse variance) of a normal distribution, etc.\nDirichlet distribution, for a vector of probabilities that must sum to 1; conjugate to the categorical distribution and multinomial distribution; generalization of the beta distribution\nWishart distribution, for a symmetric non-negative definite matrix; conjugate to the inverse of the covariance matrix of a multivariate normal distribution; generalization of the gamma distribution\n=== Some specialized applications of probability distributions ===\nThe cache language models and other statistical language models used in natural language processing to assign probabilities to the occurrence of particular words and word sequences do so by means of probability distributions.\nIn quantum mechanics, the probability density of finding the particle at a given point is proportional to the square of the magnitude of the particle\'s wavefunction at that point (see Born rule). Therefore, the probability distribution function of the position of a particle is described by\n{\\textstyle P_{a\\leq x\\leq b}(t)=\\int _{a}^{b}dx\\,|\\Psi (x,t)|^{2}}\n, probability that the particle\'s position x will be in the interval a ≤ x ≤ b in dimension one, and a similar triple integral in dimension three. This is a key principle of quantum mechanics.\nProbabilistic load flow in power-flow study explains the uncertainties of input variables as probability distribution and provides the power flow calculation also in term of probability distribution.\nPrediction of natural phenomena occurrences based on previous frequency distributions such as tropical cyclones, hail, time in between events, etc.\n== Fitting ==\n== See also ==\nConditional probability distribution\nEmpirical probability distribution\nHistogram\nJoint probability distribution\nProbability measure\nQuasiprobability distribution\nRiemann–Stieltjes integral application to probability theory\n=== Lists ===\nList of probability distributions\nList of statistical topics\n== References ==\n=== Citations ===\n=== Sources ===\n== External links ==\n"Probability distribution", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nField Guide to Continuous Probability Distributions, Gavin E. Crooks.\nDistinguishing probability measure, function and distribution, Math Stack Exchange', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', '=== Efficiency of real heat engines ===\nCarnot realized that, in reality, it is not possible to build a thermodynamically reversible engine.  So, real heat engines are even less efficient than indicated by Equation 3. In addition, real engines that operate along the Carnot cycle style (isothermal expansion / isentropic expansion / isothermal compression / isentropic compression) are rare. Nevertheless, Equation 3 is extremely useful for determining the maximum efficiency that could ever be expected for a given set of thermal reservoirs.\nAlthough Carnot\'s cycle is an idealization, Equation 3 as the expression of the Carnot efficiency is still useful. Consider the average temperatures,\nin\n{\\displaystyle \\langle T_{H}\\rangle ={\\frac {1}{\\Delta S}}\\int _{Q_{\\text{in}}}TdS}\nout\n{\\displaystyle \\langle T_{C}\\rangle ={\\frac {1}{\\Delta S}}\\int _{Q_{\\text{out}}}TdS}\nat which the first integral is over a part of a cycle where heat goes into the system and the second integral is over a cycle part where heat goes out from the system. Then, replace TH and TC in Equation 3 by ⟨TH⟩ and ⟨TC⟩, respectively, to estimate the efficiency a heat engine.\nFor the Carnot cycle, or its equivalent, the average value ⟨TH⟩ will equal the highest temperature available, namely TH, and ⟨TC⟩ the lowest, namely TC. For other less efficient thermodynamic cycles, ⟨TH⟩ will be lower than TH, and ⟨TC⟩ will be higher than TC. This can help illustrate, for example, why a reheater or a regenerator can improve the thermal efficiency of steam power plants and why the thermal efficiency of combined-cycle power plants (which incorporate gas turbines operating at even higher temperatures) exceeds that of conventional steam plants. The first prototype of the diesel engine was based on the principles of the Carnot cycle.\n== As a macroscopic construct ==\nThe Carnot heat engine is, ultimately, a theoretical construct based on an idealized thermodynamic system.  On a practical human-scale level the Carnot cycle has proven a valuable model, as in advancing the development of the diesel engine.  However, on a macroscopic scale limitations placed by the model\'s assumptions prove it impractical, and, ultimately, incapable of doing any work. As such, per Carnot\'s theorem, the Carnot engine may be thought as the theoretical limit of macroscopic scale heat engines rather than any practical device that could ever be built.\n== See also ==\nCarnot heat engine\nReversible process (thermodynamics)\n== References ==\nNotes\nSources\nCarnot, Sadi, Reflections on the Motive Power of Fire\nEwing, J. A. (1910) The Steam-Engine and Other Engines edition 3, page 62, via Internet Archive\nFeynman, Richard P.; Leighton, Robert B.; Sands, Matthew (1963). The Feynman Lectures on Physics. Addison-Wesley Publishing Company. pp. Chapter 44. ISBN 978-0-201-02116-5. {{cite book}}: ISBN / Date incompatibility (help)\nHalliday, David; Resnick, Robert (1978). Physics (3rd ed.). John Wiley & Sons. pp. 541–548. ISBN 978-0-471-02456-9.\nKittel, Charles; Kroemer, Herbert (1980). Thermal Physics (2nd ed.). W. H. Freeman Company. ISBN 978-0-7167-1088-2.\nKostic, M (2011). "Revisiting The Second Law of Energy Degradation and Entropy Generation: From Sadi Carnot\'s Ingenious Reasoning to Holistic Generalization". AIP Conf. Proc. AIP Conference Proceedings. 1411 (1): 327–350. Bibcode:2011AIPC.1411..327K. CiteSeerX 10.1.1.405.1945. doi:10.1063/1.3665247. American Institute of Physics, 2011. ISBN 978-0-7354-0985-9. Abstract at: [1]. Full article (24 pages [2]), also at [3].\n== External links ==\nHyperphysics article on the Carnot cycle.\nS. M. Blinder Carnot Cycle on Ideal Gas powered by Wolfram Mathematica']

Question: What is classical mechanics?

Choices:
Choice A) Classical mechanics is the branch of physics that describes the motion of macroscopic objects using concepts such as mass, acceleration, and force. It is based on a three-dimensional Euclidean space with fixed axes, and utilises many equations and mathematical concepts to relate physical quantities to one another.
Choice B) Classical mechanics is the branch of physics that describes the motion of microscopic objects using concepts such as energy, momentum, and wave-particle duality. It is based on a four-dimensional space-time continuum and utilises many equations and mathematical concepts to relate physical quantities to one another.
Choice C) Classical mechanics is the branch of physics that studies the behaviour of subatomic particles such as electrons and protons. It is based on the principles of quantum mechanics and utilises many equations and mathematical concepts to describe the properties of these particles.
Choice D) Classical mechanics is the branch of physics that studies the behaviour of light and electromagnetic radiation. It is based on the principles of wave-particle duality and utilises many equations and mathematical concepts to describe the properties of light.
Choice E) Classical mechanics is the branch of physics that studies the behaviour of fluids and gases. It is based on the principles of thermodynamics and utilises many equations and mathematical concepts to describe the properties of these substances.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Canonical quantization', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  ħωk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, θk†, θk, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the θk, and the Fock space is built by applying all products of creation operators θk† to |0⟩.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at φ = 0, so that the vacuum minimizing the Hamiltonian satisfies ⟨φ⟩ = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  φ = v .  This occurs for example, if V(φ) = gφ4 − 2m2φ2 with g > 0 and m2 > 0, for which the minimum energy is found at v = ±m/√g. The value of v in one of these vacua may be considered as condensate of the field φ. Canonical quantization then can be carried out for the shifted field  φ(x,t) − v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator  [A, B]  expressed in the phase space formulation is iħ{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  ħ-deformations are highly nonunique—quantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  ħ → 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an ħ-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional Schrödinger Hilbert space. If the chosen variables are complex, we get something like the Segal–Bargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', "Planck's law", 'Quantum number', '{\\displaystyle L_{z}=m_{\\ell }\\hbar }\nThe values of mℓ range from −ℓ to ℓ, with integer intervals.\nThe s subshell (ℓ = 0) contains only one orbital, and therefore the mℓ of an electron in an s orbital will always be 0. The p subshell (ℓ = 1) contains three orbitals, so the mℓ of an electron in a p orbital will be −1, 0, or 1. The d subshell (ℓ = 2) contains five orbitals, with mℓ values of −2, −1, 0, 1, and 2.\n=== Spin magnetic quantum number ===\nThe spin magnetic quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis:\n{\\displaystyle S_{z}=m_{s}\\hbar }\nIn general, the values of ms range from −s to s, where s is the spin quantum number, associated with the magnitude of particle\'s intrinsic spin angular momentum:\n{\\displaystyle m_{s}=-s,-s+1,-s+2,\\cdots ,s-2,s-1,s}\nAn electron state has spin number s = \u20601/2\u2060, consequently ms will be +\u20601/2\u2060 ("spin up") or −\u20601/2\u2060 "spin down" states. Since electron are fermions they obey the Pauli exclusion principle: each electron state must have different quantum numbers.  Therefore, every orbital will be occupied with at most two electrons, one for each spin state.\n=== The Aufbau principle and Hund\'s Rules ===\nA multi-electron atom can be modeled qualitatively as a hydrogen like atom with higher nuclear charge and correspondingly more electrons. The occupation of the electron states in such an atom can be predicted by the Aufbau principle and Hund\'s empirical rules for the quantum numbers.  The Aufbau principle fills orbitals based on their principal and azimuthal quantum numbers (lowest n + l first, with lowest n breaking ties; Hund\'s rule favors unpaired electrons in the outermost orbital). These rules are empirical but they can be related to electron physics.:\u200a10\u200a:\u200a260\n== Spin-orbit coupled systems ==\nWhen one takes the spin–orbit interaction into consideration, the L and S operators no longer commute with the Hamiltonian, and the eigenstates of the system no longer have well-defined orbital angular momentum and spin. Thus another set of quantum numbers should be used. This set includes\nThe total angular momentum quantum number:\n{\\displaystyle j=|\\ell \\pm s|,}\nwhich gives the total angular momentum through the relation\n{\\displaystyle J^{2}=\\hbar ^{2}j(j+1).}\nThe projection of the total angular momentum along a specified axis:\n{\\displaystyle m_{j}=-j,-j+1,-j+2,\\cdots ,j-2,j-1,j}\nanalogous to the above and satisfies both\n{\\displaystyle m_{j}=m_{\\ell }+m_{s},}\nand\n{\\displaystyle |m_{\\ell }+m_{s}|\\leq j.}\nParityThis is the eigenvalue under reflection: positive (+1) for states which came from even ℓ and negative (−1) for states which came from odd ℓ. The former is also known as even parity and the latter as odd parity, and is given by\n{\\displaystyle P=(-1)^{\\ell }.}\nFor example, consider the following 8 states, defined by their quantum numbers:\nThe quantum states in the system can be described as linear combination of these 8 states. However, in the presence of spin–orbit interaction, if one wants to describe the same system by 8 states that are eigenvectors of the Hamiltonian (i.e. each represents a state that does not mix with others over time), we should consider the following 8 states:\n== Atomic nuclei ==\nIn nuclei, the entire assembly of protons and neutrons (nucleons) has a resultant angular momentum due to the angular momenta of each nucleon, usually denoted I. If the total angular momentum of a neutron is jn = ℓ + s and for a proton is jp = ℓ + s (where s for protons and neutrons happens to be \u20601/2\u2060 again (see note)), then the nuclear angular momentum quantum numbers I are given by:\n{\\displaystyle I=|j_{n}-j_{p}|,|j_{n}-j_{p}|+1,|j_{n}-j_{p}|+2,\\cdots ,(j_{n}+j_{p})-2,(j_{n}+j_{p})-1,(j_{n}+j_{p})}\nNote: The orbital angular momenta of the nuclear (and atomic) states are all integer multiples of ħ while the intrinsic angular momentum of the neutron and  proton are half-integer multiples.  It should be immediately apparent that the combination of the intrinsic spins of the nucleons with their orbital motion will always give half-integer values for the total spin, I, of any odd-A nucleus and integer values for any even-A nucleus.\nParity with the number I is used to label nuclear angular momentum states, examples for some isotopes of hydrogen (H), carbon (C), and sodium (Na) are;\nThe reason for the unusual fluctuations in I, even by differences of just one nucleon, are due to the odd and even numbers of protons and neutrons – pairs of nucleons have a total angular momentum of zero (just like electrons in orbitals), leaving an odd or even number of unpaired nucleons. The property of nuclear spin is an important factor for the operation of NMR spectroscopy in organic chemistry, and MRI in nuclear medicine, due to the nuclear magnetic moment interacting with an external magnetic field.\n== Elementary particles ==\nElementary particles contain many quantum numbers which are usually said to be intrinsic to them. However, it should be understood that the elementary particles are quantum states of the standard model of particle physics, and hence the quantum numbers of these particles bear the same relation to the Hamiltonian of this model as the quantum numbers of the Bohr atom does to its Hamiltonian. In other words, each quantum number denotes a symmetry of the problem. It is more useful in quantum field theory to distinguish between spacetime and internal symmetries.\nTypical quantum numbers related to spacetime symmetries are spin (related to rotational symmetry), the parity, C-parity and T-parity (related to the Poincaré symmetry of spacetime). Typical internal symmetries are lepton number and baryon number or the electric charge. (For a full list of quantum numbers of this kind see the article on flavour.)\n== Multiplicative quantum numbers ==\nMost conserved quantum numbers are additive, so in an elementary particle reaction, the sum of the quantum numbers should be the same before and after the reaction. However, some, usually called a parity, are multiplicative; i.e., their product is conserved. All multiplicative quantum numbers belong to a symmetry (like parity) in which applying the symmetry transformation twice is equivalent to doing nothing (involution).\n== See also ==\nElectron configuration\n== References ==\n== Further reading ==\nDirac, Paul A. M. (1982). Principles of Quantum Mechanics. Oxford University Press. ISBN 0-19-852011-5.\nGriffiths, David J. (2004). Introduction to Quantum Mechanics (2nd ed.). Prentice Hall. ISBN 0-13-805326-X.\nHalzen, Francis & Martin, Alan D. (1984). Quarks and Leptons: An Introductory Course in Modern Particle Physics. John Wiley & Sons. ISBN 0-471-88741-2.\nEisberg, Robert Martin; Resnick, Robert (1985). Quantum Physics of Atoms, Molecules, Solids, Nuclei and Particles (2nd ed.). John Wiley & Sons. ISBN 978-0-471-87373-0 – via Internet Archive.', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', '=== Arrow of time ===\nUnlike space, where an object can travel in the opposite directions (and in 3 dimensions), time appears to have only one dimension and only one direction—the past lies behind, fixed and immutable, while the future lies ahead and is not necessarily fixed. Yet most laws of physics allow any process to proceed both forward and in reverse. There are only a few physical phenomena that violate the reversibility of time. This time directionality is known as the arrow of time. Acknowledged examples of the arrow of time are:\nRadiative arrow of time, manifested in waves (e.g., light and sound) travelling only expanding (rather than focusing) in time (see light cone);\nEntropic arrow of time: according to the second law of thermodynamics an isolated system evolves toward a larger disorder rather than orders spontaneously;\nQuantum arrow time, which is related to irreversibility of measurement in quantum mechanics according to the Copenhagen interpretation of quantum mechanics;\nWeak arrow of time: preference for a certain time direction of weak force in particle physics (see violation of CP symmetry);\nCosmological arrow of time, which follows the accelerated expansion of the Universe after the Big Bang.\nThe relationships between these different arrows of time is a hotly debated topic in theoretical physics.\nThe second law of thermodynamics states that entropy must increase over time. Brian Greene theorizes that, according to the equations, the change in entropy occurs symmetrically whether going forward or backward in time. So entropy tends to increase in either direction, and our current low-entropy universe is a statistical aberration, in a similar manner as tossing a coin often enough that eventually heads will result ten times in a row. However, this theory is not supported empirically in local experiment.\n=== Classical mechanics ===\nIn non-relativistic classical mechanics, Newton\'s concept of "relative, apparent, and common time" can be used in the formulation of a prescription for the synchronization of clocks. Events seen by two different observers in motion relative to each other produce a mathematical concept of time that works sufficiently well for describing the everyday phenomena of most people\'s experience. In the late nineteenth century, physicists encountered problems with the classical understanding of time, in connection with the behavior of electricity and magnetism. The 1860s Maxwell\'s equations described that light always travels at a constant speed (in a vacuum). However, classical mechanics assumed that motion was measured relative to a fixed reference frame. The Michelson–Morley experiment contradicted the assumption. Einstein later proposed a method of synchronizing clocks using the constant, finite speed of light as the maximum signal velocity. This led directly to the conclusion that observers in motion relative to one another measure different elapsed times for the same event.\n=== Spacetime ===\nTime has historically been closely related with space, the two together merging into spacetime in Einstein\'s special relativity and general relativity. According to these theories, the concept of time depends on the spatial reference frame of the observer, and the human perception, as well as the measurement by instruments such as clocks, are different for observers in relative motion. For example, if a spaceship carrying a clock flies through space at (very nearly) the speed of light, its crew does not notice a change in the speed of time on board their vessel because everything traveling at the same speed slows down at the same rate (including the clock, the crew\'s thought processes, and the functions of their bodies). However, to a stationary observer watching the spaceship fly by, the spaceship appears flattened in the direction it is traveling and the clock on board the spaceship appears to move very slowly.\nOn the other hand, the crew on board the spaceship also perceives the observer as slowed down and flattened along the spaceship\'s direction of travel, because both are moving at very nearly the speed of light relative to each other. Because the outside universe appears flattened to the spaceship, the crew perceives themselves as quickly traveling between regions of space that (to the stationary observer) are many light years apart. This is reconciled by the fact that the crew\'s perception of time is different from the stationary observer\'s; what seems like seconds to the crew might be hundreds of years to the stationary observer. In either case, however, causality remains unchanged: the past is the set of events that can send light signals to an entity and the future is the set of events to which an entity can send light signals.\n=== Dilation ===\nEinstein showed in his thought experiments that people travelling at different speeds, while agreeing on cause and effect, measure different time separations between events, and can even observe different chronological orderings between non-causally related events. Though these effects are typically minute in the human experience, the effect becomes much more pronounced for objects moving at speeds approaching the speed of light. Subatomic particles exist for a well-known average fraction of a second in a lab relatively at rest, but when travelling close to the speed of light they are measured to travel farther and exist for much longer than when at rest.\nAccording to the special theory of relativity, in the high-speed particle\'s frame of reference, it exists, on the average, for a standard amount of time known as its mean lifetime, and the distance it travels in that time is zero, because its velocity is zero. Relative to a frame of reference at rest, time seems to "slow down" for the particle. Relative to the high-speed particle, distances seem to shorten. Einstein showed how both temporal and spatial dimensions can be altered (or "warped") by high-speed motion.\nEinstein (The Meaning of Relativity): "Two events taking place at the points A and B of a system K are simultaneous if they appear at the same instant when observed from the middle point, M, of the interval AB. Time is then defined as the ensemble of the indications of similar clocks, at rest relative to K, which register the same simultaneously." Einstein wrote in his book, Relativity, that simultaneity is also relative, i.e., two events that appear simultaneous to an observer in a particular inertial reference frame need not be judged as simultaneous by a second observer in a different inertial frame of reference.\nAccording to general relativity, time also runs slower in stronger gravitational fields; this is gravitational time dilation. The effect of the dilation becomes more noticeable in a mass-dense object. A famous example of time dilation is a thought experiment of a subject approaching the event horizon of a black hole. As a consequence of how gravitational fields warp spacetime, the subject will experience gravitational time dilation. From the perspective of the subject itself, they will experience time normally. Meanwhile, an observer from the outside will see the subject move closer to the black hole until the extreme, in which the subject appears \'frozen\' in time and eventually fade to nothingness due to the diminishing amount of light returning.\n=== Relativistic versus Newtonian ===\nThe animations visualise the different treatments of time in the Newtonian and the relativistic descriptions. At the heart of these differences are the Galilean and Lorentz transformations applicable in the Newtonian and relativistic theories, respectively. In the figures, the vertical direction indicates time. The horizontal direction indicates distance (only one spatial dimension is taken into account), and the thick dashed curve is the spacetime trajectory ("world line") of the observer. The small dots indicate specific (past and future) events in spacetime. The slope of the world line (deviation from being vertical) gives the relative velocity to the observer.\nIn the Newtonian description these changes are such that time is absolute: the movements of the observer do not influence whether an event occurs in the \'now\' (i.e., whether an event passes the horizontal line through the observer). However, in the relativistic description the observability of events is absolute: the movements of the observer do not influence whether an event passes the "light cone" of the observer. Notice that with the change from a Newtonian to a relativistic description, the concept of absolute time is no longer applicable: events move up and down in the figure depending on the acceleration of the observer.\n=== Quantization ===\nTime quantization refers to the theory that time has the smallest possible unit. Time quantization is a hypothetical concept. In the modern established physical theories like the Standard Model of particle physics and general relativity time is not quantized. Planck time (~ 5.4 × 10−44 seconds) is the unit of time in the system of natural units known as Planck units. Current established physical theories are believed to fail at this time scale, and many physicists expect that the Planck time might be the smallest unit of time that could ever be measured, even in principle. Though tentative physical theories that attempt to describe phenomena at this scale exist; an example is loop quantum gravity. Loop quantum gravity suggests that time is quantized; if gravity is quantized, spacetime is also quantized.\n== Travel ==', 'Classical mechanics']

Question: What is the Peierls bracket in canonical quantization?

Choices:
Choice A) The Peierls bracket is a mathematical symbol used to represent the Poisson algebra in the canonical quantization method.
Choice B) The Peierls bracket is a mathematical tool used to generate the Hamiltonian in the canonical quantization method.
Choice C) The Peierls bracket is a Poisson bracket derived from the action in the canonical quantization method that converts the quotient algebra into a Poisson algebra.
Choice D) The Peierls bracket is a mathematical symbol used to represent the quotient algebra in the canonical quantization method.
Choice E) The Peierls bracket is a mathematical tool used to generate the Euler-Lagrange equations in the canonical quantization method.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Cosmology', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', 'Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.', "== Importance ==\nFrom the perspective of a planetary geologist, the atmosphere acts to shape a planetary surface. Wind picks up dust and other particles which, when they collide with the terrain, erode the relief and leave deposits (eolian processes). Frost and precipitations, which depend on the atmospheric composition, also influence the relief. Climate changes can influence a planet's geological history. Conversely, studying the surface of the Earth leads to an understanding of the atmosphere and climate of other planets.\nFor a meteorologist, the composition of the Earth's atmosphere is a factor affecting the climate and its variations.\nFor a biologist or paleontologist, the Earth's atmospheric composition is closely dependent on the appearance of life and its evolution.\n== See also ==\nAtmometer (evaporimeter)\nAtmospheric pressure\nInternational Standard Atmosphere\nKármán line\nSky\n== References ==\n== Further reading ==\nSanchez-Lavega, Agustin (2010). An Introduction to Planetary Atmospheres. Taylor & Francis. ISBN 978-1420067323.\n== External links ==\nProperties of atmospheric strata – The flight environment of the atmosphere\nAtmosphere – Everything you need to know", '=== Progenitor ===\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic type Ic-BL events that are associated with long-duration gamma-ray bursts.\n== External impact ==\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\n=== Source of heavy elements ===\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\n=== Role in stellar evolution ===\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n=== Cosmic rays ===\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\n=== Gravitational waves ===', 'An atmosphere (from Ancient Greek  ἀτμός (atmós) \'vapour, steam\' and  σφαῖρα (sphaîra) \'sphere\') is a layer of gases that envelop an astronomical object, held in place by the gravity of the object. A planet retains an atmosphere when the gravity is great and the temperature of the atmosphere is low. A stellar atmosphere is the outer region of a star, which includes the layers above the opaque photosphere; stars of low temperature might have outer atmospheres containing compound molecules.\nThe atmosphere of Earth is composed of nitrogen (78%), oxygen (21%), argon (0.9%), carbon dioxide (0.04%) and trace gases. Most organisms use oxygen for respiration; lightning and bacteria perform nitrogen fixation which produces ammonia that is used to make nucleotides and amino acids; plants, algae, and cyanobacteria use carbon dioxide for photosynthesis. The layered composition of the atmosphere minimises the harmful effects of sunlight, ultraviolet radiation, solar wind, and cosmic rays and thus protects the organisms from genetic damage. The current composition of the atmosphere of the Earth is the product of billions of years of biochemical modification of the paleoatmosphere by living organisms.\n== Occurrence and compositions ==\n=== Origins ===\nAtmospheres are clouds of gas bound to and engulfing an astronomical focal point of sufficiently dominating mass, adding to its mass, possibly escaping from it or collapsing into it.\nBecause of the latter, such planetary nucleus can develop from interstellar molecular clouds or protoplanetary disks into rocky astronomical objects with varyingly thick atmospheres, gas giants or fusors.\nComposition and thickness is originally determined by the stellar nebula\'s chemistry and temperature, but can also by a product processes within the astronomical body outgasing a different atmosphere.\n=== Compositions ===\nThe atmospheres of the planets Venus and Mars are principally composed of carbon dioxide and nitrogen, argon and oxygen.\nThe composition of Earth\'s atmosphere is determined by the by-products of the life that it sustains. Dry air (mixture of gases) from Earth\'s atmosphere contains 78.08% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and traces of hydrogen, helium, and other "noble" gases (by volume), but generally a variable amount of water vapor is also present, on average about 1% at sea level.\nThe low temperatures and higher gravity of the Solar System\'s giant planets—Jupiter, Saturn, Uranus and Neptune—allow them more readily to retain gases with low molecular masses. These planets have hydrogen–helium atmospheres, with trace amounts of more complex compounds.\nTwo satellites of the outer planets possess significant atmospheres. Titan, a moon of Saturn, and Triton, a moon of Neptune, have atmospheres mainly of nitrogen. When in the part of its orbit closest to the Sun, Pluto has an atmosphere of nitrogen and methane similar to Triton\'s, but these gases are frozen when it is farther from the Sun.\nOther bodies within the Solar System have extremely thin atmospheres not in equilibrium. These include the Moon (sodium gas), Mercury (sodium gas), Europa (oxygen), Io (sulfur), and Enceladus (water vapor).\nThe first exoplanet whose atmospheric composition was determined is HD 209458b, a gas giant with a close orbit around a star in the constellation Pegasus. Its atmosphere is heated to temperatures over 1,000 K, and is steadily escaping into space. Hydrogen, oxygen, carbon and sulfur have been detected in the planet\'s inflated atmosphere.\n=== Atmospheres in the Solar System ===\nAtmosphere of the Sun\nAtmosphere of Mercury\nAtmosphere of Venus\nAtmosphere of Earth\nAtmosphere of the Moon\nAtmosphere of Mars\nAtmosphere of Ceres\nAtmosphere of Jupiter\nAtmosphere of Io\nAtmosphere of Callisto\nAtmosphere of Europa\nAtmosphere of Ganymede\nAtmosphere of Saturn\nAtmosphere of Titan\nAtmosphere of Enceladus\nAtmosphere of Uranus\nAtmosphere of Titania\nAtmosphere of Neptune\nAtmosphere of Triton\nAtmosphere of Pluto\n== Structure of atmosphere ==\n=== Earth ===\nThe atmosphere of Earth is composed of layers with different properties, such as specific gaseous composition, temperature, and pressure.\nThe troposphere is the lowest layer of the atmosphere. This extends from the planetary surface to the bottom of the stratosphere. The troposphere contains 75–80% of the mass of the atmosphere, and is the atmospheric layer wherein the weather occurs; the height of the troposphere varies between 17 km at the equator and 7.0 km at the poles.\nThe stratosphere extends from the top of the troposphere to the bottom of the mesosphere, and contains the ozone layer, at an altitude between 15 km and 35 km. It is the atmospheric layer that absorbs most of the ultraviolet radiation that Earth receives from the Sun.\nThe mesosphere ranges from 50 km to 85 km and is the layer wherein most meteors are incinerated before reaching the surface.\nThe thermosphere extends from an altitude of 85 km to the base of the exosphere at 690 km and contains the ionosphere, where solar radiation ionizes the atmosphere. The density of the ionosphere is greater at short distances from the planetary surface in the daytime and decreases as the ionosphere rises at night-time, thereby allowing a greater range of radio frequencies to travel greater distances.\nThe exosphere begins at 690 to 1,000 km from the surface, and extends to roughly 10,000 km, where it interacts with the magnetosphere of Earth.\n== Pressure ==\nAtmospheric pressure is the force (per unit-area) perpendicular to a unit-area of planetary surface, as determined by the weight of the vertical column of atmospheric gases. In said atmospheric model, the atmospheric pressure, the weight of the mass of the gas, decreases at high altitude because of the diminishing mass of the gas above the point of barometric measurement. The units of air pressure are based upon the standard atmosphere (atm), which is 101,325 Pa (equivalent to 760 Torr or 14.696 psi). The height at which the atmospheric pressure declines by a factor of e (an irrational number equal to 2.71828) is called the scale height (H). For an atmosphere of uniform temperature, the scale height is proportional to the atmospheric temperature and is inversely proportional to the product of the mean molecular mass of dry air, and the local acceleration of gravity at the point of barometric measurement.\n== Escape ==\nSurface gravity differs significantly among the planets. For example, the large gravitational force of the giant planet Jupiter retains light gases such as hydrogen and helium that escape from objects with lower gravity. Secondly, the distance from the Sun determines the energy available to heat atmospheric gas to the point where some fraction of its molecules\' thermal motion exceed the planet\'s escape velocity, allowing those to escape a planet\'s gravitational grasp. Thus, distant and cold Titan, Triton, and Pluto are able to retain their atmospheres despite their relatively low gravities.\nSince a collection of gas molecules may be moving at a wide range of velocities, there will always be some fast enough to produce a slow leakage of gas into space. Lighter molecules move faster than heavier ones with the same thermal kinetic energy, and so gases of low molecular weight are lost more rapidly than those of high molecular weight. It is thought that Venus and Mars may have lost much of their water when, after being photodissociated into hydrogen and oxygen by solar ultraviolet radiation, the hydrogen escaped. Earth\'s magnetic field helps to prevent this, as, normally, the solar wind would greatly enhance the escape of hydrogen. However, over the past 3 billion years Earth may have lost gases through the magnetic polar regions due to auroral activity, including a net 2% of its atmospheric oxygen. The net effect, taking the most important escape processes into account, is that an intrinsic magnetic field does not protect a planet from atmospheric escape and that for some magnetizations the presence of a magnetic field works to increase the escape rate.\nOther mechanisms that can cause atmosphere depletion are solar wind-induced sputtering, impact erosion, weathering, and sequestration—sometimes referred to as "freezing out"—into the regolith and polar caps.\n== Terrain ==\nAtmospheres have dramatic effects on the surfaces of rocky bodies. Objects that have no atmosphere, or that have only an exosphere, have terrain that is covered in craters. Without an atmosphere, the planet has no protection from meteoroids, and all of them collide with the surface as meteorites and create craters.\nFor planets with a significant atmosphere, most meteoroids burn up as meteors before hitting a planet\'s surface. When meteoroids do impact, the effects are often erased by the action of wind.\nWind erosion is a significant factor in shaping the terrain of rocky planets with atmospheres, and over time can erase the effects of both craters and volcanoes. In addition, since liquids cannot exist without pressure, an atmosphere allows liquid to be present at the surface, resulting in lakes, rivers and oceans. Earth and Titan are known to have liquids at their surface and terrain on the planet suggests that Mars had liquid on its surface in the past.\n=== Outside the Solar System ===\nAtmosphere of HD 209458 b\n== Circulation ==\nThe circulation of the atmosphere occurs due to thermal differences when convection becomes a more efficient transporter of heat than thermal radiation. On planets where the primary heat source is solar radiation, excess heat in the tropics is transported to higher latitudes. When a planet generates a significant amount of heat internally, such as is the case for Jupiter, convection in the atmosphere can transport thermal energy from the higher temperature interior up to the surface.\n== Importance ==', "In more massive stars, the fusion of neon proceeds without a runaway deflagration.  This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements.  Surrounding the core are shells of lighter elements still undergoing fusion.  The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged.  The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope.  The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars.  Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself.  The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.\n==== Supernova ====\nWhen the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole.  Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\n== Stellar remnants ==\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n=== White and black dwarfs ===\nFor a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years.\nThe chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n=== Neutron stars ===\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n=== Black holes ===\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉.", '=== Spirals ===\nSpiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept.\nSpiral galaxies consist of a rotating disk of stars and interstellar medium, along with a central bulge of generally older stars. Extending outward from the bulge are relatively bright arms. In the Hubble classification scheme, spiral galaxies are listed as type S, followed by a letter (a, b, or c) which indicates the degree of tightness of the spiral arms and the size of the central bulge. An Sa galaxy has tightly wound, poorly defined arms and possesses a relatively large core region. At the other extreme, an Sc galaxy has open, well-defined arms and a small core region. A galaxy with poorly defined arms is sometimes referred to as a flocculent spiral galaxy; in contrast to the grand design spiral galaxy that has prominent and well-defined spiral arms. The speed in which a galaxy rotates is thought to correlate with the flatness of the disc as some spiral galaxies have thick bulges, while others are thin and dense.\nIn spiral galaxies, the spiral arms do have the shape of approximate logarithmic spirals, a pattern that can be theoretically shown to result from a disturbance in a uniformly rotating mass of stars. Like the stars, the spiral arms rotate around the center, but they do so with constant angular velocity. The spiral arms are thought to be areas of high-density matter, or "density waves". As stars move through an arm, the space velocity of each stellar system is modified by the gravitational force of the higher density. (The velocity returns to normal after the stars depart on the other side of the arm.) This effect is akin to a "wave" of slowdowns moving along a highway full of moving cars. The arms are visible because the high density facilitates star formation, and therefore they harbor many bright and young stars.\n==== Barred spiral galaxy ====\nA majority of spiral galaxies, including the Milky Way galaxy, have a linear, bar-shaped band of stars that extends outward to either side of the core, then merges into the spiral arm structure. In the Hubble classification scheme, these are designated by an SB, followed by a lower-case letter (a, b or c) which indicates the form of the spiral arms (in the same manner as the categorization of normal spiral galaxies). Bars are thought to be temporary structures that can occur as a result of a density wave radiating outward from the core, or else due to a tidal interaction with another galaxy. Many barred spiral galaxies are active, possibly as a result of gas being channeled into the core along the arms.\nOur own galaxy, the Milky Way, is a large disk-shaped barred-spiral galaxy about 30 kiloparsecs in diameter and a kiloparsec thick. It contains about two hundred billion (2×1011) stars and has a total mass of about six hundred billion (6×1011) times the mass of the Sun.\n==== Super-luminous spiral ====\nRecently, researchers described galaxies called super-luminous spirals. They are very large with an upward diameter of 437,000 light-years (compared to the Milky Way\'s 87,400 light-year diameter). With a mass of 340 billion solar masses, they generate a significant amount of ultraviolet and mid-infrared light. They are thought to have an increased star formation rate around 30 times faster than the Milky Way.\n=== Other morphologies ===\nPeculiar galaxies are galactic formations that develop unusual properties due to tidal interactions with other galaxies.\nA ring galaxy has a ring-like structure of stars and interstellar medium surrounding a bare core. A ring galaxy is thought to occur when a smaller galaxy passes through the core of a spiral galaxy. Such an event may have affected the Andromeda Galaxy, as it displays a multi-ring-like structure when viewed in infrared radiation.\nA lenticular galaxy is an intermediate form that has properties of both elliptical and spiral galaxies. These are categorized as Hubble type S0, and they possess ill-defined spiral arms with an elliptical halo of stars (barred lenticular galaxies receive Hubble classification SB0).\nIrregular galaxies are galaxies that can not be readily classified into an elliptical or spiral morphology.\nAn Irr-I galaxy has some structure but does not align cleanly with the Hubble classification scheme.\nIrr-II galaxies do not possess any structure that resembles a Hubble classification, and may have been disrupted. Nearby examples of (dwarf) irregular galaxies include the Magellanic Clouds.\nA dark or "ultra diffuse" galaxy is an extremely-low-luminosity galaxy. It may be the same size as the Milky Way, but have a visible star count only one percent of the Milky Way\'s. Multiple mechanisms for producing this type of galaxy have been proposed, and it is possible that different dark galaxies formed by different means. One candidate explanation for the low luminosity is that the galaxy lost its star-forming gas at an early stage, resulting in old stellar populations.\n=== Dwarfs ===\nDespite the prominence of large elliptical and spiral galaxies, most galaxies are dwarf galaxies. They are relatively small when compared with other galactic formations, being about one hundredth the size of the Milky Way, with only a few billion stars. Blue compact dwarf galaxies contains large clusters of young, hot, massive stars. Ultra-compact dwarf galaxies have been discovered that are only 100 parsecs across.\nMany dwarf galaxies may orbit a single larger galaxy; the Milky Way has at least a dozen such satellites, with an estimated 300–500 yet to be discovered.\nMost of the information we have about dwarf galaxies come from observations of the local group, containing two spiral galaxies, the Milky Way and Andromeda, and many dwarf galaxies. These dwarf galaxies are classified as either irregular or dwarf elliptical/dwarf spheroidal galaxies.\nA study of 27 Milky Way neighbors found that in all dwarf galaxies, the central mass is approximately 10 million solar masses, regardless of whether it has thousands or millions of stars. This suggests that galaxies are largely formed by dark matter, and that the minimum size may indicate a form of warm dark matter incapable of gravitational coalescence on a smaller scale.\n== Variants ==\n=== Interacting ===\nInteractions between galaxies are relatively frequent, and they can play an important role in galactic evolution. Near misses between galaxies result in warping distortions due to tidal interactions, and may cause some exchange of gas and dust.\nCollisions occur when two galaxies pass directly through each other and have sufficient relative momentum not to merge. The stars of interacting galaxies usually do not collide, but the gas and dust within the two forms interacts, sometimes triggering star formation. A collision can severely distort the galaxies\' shapes, forming bars, rings or tail-like structures.\nAt the extreme of interactions are galactic mergers, where the galaxies\' relative momentums are insufficient to allow them to pass through each other. Instead, they gradually merge to form a single, larger galaxy. Mergers can result in significant changes to the galaxies\' original morphology. If one of the galaxies is much more massive than the other, the result is known as cannibalism, where the more massive larger galaxy remains relatively undisturbed, and the smaller one is torn apart. The Milky Way galaxy is currently in the process of cannibalizing the Sagittarius Dwarf Elliptical Galaxy and the Canis Major Dwarf Galaxy.\n=== Starburst ===\nStars are created within galaxies from a reserve of cold gas that forms giant molecular clouds. Some galaxies have been observed to form stars at an exceptional rate, which is known as a starburst. If they continue to do so, they would consume their reserve of gas in a time span less than the galaxy\'s lifespan. Hence starburst activity usually lasts only about ten million years, a relatively brief period in a galaxy\'s history. Starburst galaxies were more common during the universe\'s early history, but still contribute an estimated 15% to total star production.\nStarburst galaxies are characterized by dusty concentrations of gas and the appearance of newly formed stars, including massive stars that ionize the surrounding clouds to create H II regions. These stars produce supernova explosions, creating expanding remnants that interact powerfully with the surrounding gas. These outbursts trigger a chain reaction of star-building that spreads throughout the gaseous region. Only when the available gas is nearly consumed or dispersed does the activity end.\nStarbursts are often associated with merging or interacting galaxies. The prototype example of such a starburst-forming interaction is M82, which experienced a close encounter with the larger M81. Irregular galaxies often exhibit spaced knots of starburst activity.\n=== Radio galaxy ===\nA radio galaxy is a galaxy with giant regions of radio emission extending well beyond its visible structure. These energetic radio lobes are powered by jets from its active galactic nucleus. Radio galaxies are classified according to their Fanaroff–Riley classification. The FR I class have lower radio luminosity and exhibit structures which are more elongated; the FR II class are higher radio luminosity. The correlation of radio luminosity and structure suggests that the sources in these two types of galaxies may differ.', 'Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==', 'What is the ultimate reason (if any) for the existence of the universe? Does the cosmos have a purpose? (see teleology)\nDoes the existence of consciousness have a role in the existence of reality? How do we know what we know about the totality of the cosmos? Does cosmological reasoning reveal metaphysical truths? (see epistemology)\nCharles Kahn, a historian of philosophy, attributed the origins of ancient Greek cosmology to Anaximander.\n== Historical cosmologies ==\nTable notes: the term "static" simply means not expanding and not contracting. Symbol G represents Newton\'s gravitational constant; Λ (Lambda) is the cosmological constant.\n== See also ==\n== References ==\n== Sources ==\nBragg, Melvyn (2023). "The Universe\'s Shape". bbc.co.uk. BBC. Retrieved 23 May 2023. Melvyn Bragg discusses shape, size and topology of the universe and examines theories about its expansion. If it is already infinite, how can it be getting any bigger? And is there really only one?\n"Cosmic Journey: A History of Scientific Cosmology". history.aip.org. American Institute of Physics. 2023. Retrieved 23 May 2023. The history of cosmology is a grand story of discovery, from ancient Greek astronomy to -space telescopes.\nDodelson, Scott; Schmidt, Fabian (2020). Modern Cosmology 2nd Edition. Academic Press. ISBN 978-0128159484. Download full text: Dodelson, Scott; Schmidt, Fabian (2020). "Scott Dodelson - Fabian Schmidt - Modern Cosmology (2021) PDF" (PDF). scribd.com. Academic Press. Retrieved 23 May 2023.\nCharles Kahn. 1994. Anaximander and the Origins of Greek Cosmology. Indianapolis: Hackett.\n"Genesis, Search for Origins. End of mission wrap up". genesismission.jpl.nasa.gov. NASA, Jet Propulsion Laboratory, California Institute of Technology. Retrieved 23 May 2023. About 4.6 billion years ago, the solar nebula transformed into the present solar system. In order to chemically model the processes which drove that transformation, we would, ideally, like to have a sample of that original nebula to use as a baseline from which we can track changes.\nLeonard, Scott A; McClure, Michael (2004). Myth and Knowing. McGraw-Hill. ISBN 978-0-7674-1957-4.\nLyth, David (12 December 1993). "Introduction to Cosmology". arXiv:astro-ph/9312022. These notes form an introduction to cosmology with special emphasis on large scale structure, the cmb anisotropy and inflation. Lectures given at the Summer School in High Energy Physics and Cosmology, ICTP (Trieste) 1993.) 60 pages, plus 5 Figures.\n"NASA/IPAC Extragalactic Database (NED)". ned.ipac.caltech.edu. NASA. 2023. Retrieved 23 May 2023. April 2023 Release Highlights Database Updates\n"NASA/IPAC Extragalactic Database (NED)". ned.ipac.caltech.edu. NASA. 2020. Retrieved 23 May 2023. NED-D: A Master List of Redshift-Independent Extragalactic Distances\nSophia Centre. The Sophia Centre for the Study of Cosmology in Culture, University of Wales Trinity Saint David.']

Question: What is the proposed name for the field that is responsible for cosmic inflation and the metric expansion of space?

Choices:
Choice A) Inflaton
Choice B) Quanta
Choice C) Scalar
Choice D) Metric
Choice E) Conformal cyclic cosmology

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Frame-dragging', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==', 'Leidenfrost effect', 'Shower-curtain effect\n\nThe shower-curtain effect in physics describes the phenomenon of a shower curtain being blown inward when a shower is running. The problem of identifying the cause of this effect has been featured in Scientific American magazine, with several theories given to explain the phenomenon but no definite conclusion.\nThe shower-curtain effect may also be used to describe the observation of how nearby phase front distortions of an optical wave are more severe than remote distortions of the same amplitude.\n== Hypotheses ==\n=== Buoyancy hypothesis ===\nAlso called chimney effect or stack effect, observes that warm air (from the hot shower) rises out over the shower curtain as cooler air (near the floor) pushes in under the curtain to replace the rising air.  By pushing the curtain in towards the shower, the (short range) vortex and Coandă effects become more significant. However, the shower-curtain effect persists when cold water is used, implying that this is not the sole mechanism.\n=== Bernoulli effect hypothesis ===\nThe most popular explanation given for the shower-curtain effect is Bernoulli\'s principle.  Bernoulli\'s principle states that an increase in velocity results in a decrease in pressure.  This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water.  This movement would be parallel to the plane of the shower curtain.  If air is moving across the inside surface of the shower curtain, Bernoulli\'s principle says the air pressure there will drop.  This would result in a pressure differential between the inside and outside, causing the curtain to move inward.  It would be strongest when the gap between the bather and the curtain is smallest, resulting in the curtain attaching to the bather.\n=== Horizontal vortex hypothesis ===\nA computer simulation of a typical bathroom found that none of the above theories pan out in their analysis, but instead found that the spray from the shower-head drives a horizontal vortex. This vortex has a low-pressure zone in the centre, which sucks the curtain.\nDavid Schmidt of the University of Massachusetts was awarded the 2001 Ig Nobel Prize in Physics for his partial solution to the question of why shower curtains billow inwards. He used a computational fluid dynamics code to achieve the results.  Professor Schmidt is adamant that this was done "for fun" in his own free time without the use of grants.\n=== Coandă effect ===\nThe Coandă effect, also known as "boundary layer attachment", is the tendency of a moving fluid to adhere to an adjacent wall.\n=== Condensation ===\nA hot shower will produce steam that condenses on the shower side of the curtain, lowering the pressure there.  In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.\n=== Air pressure ===\nColder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.\n== Solutions ==\nMany shower curtains come with features to reduce the shower-curtain effect. They may have adhesive suction cups on the bottom edges of the curtain, which are then pushed onto the sides of the shower when in use. Others may have magnets at the bottom, though these are not effective on acrylic or fiberglass tubs.\nIt is possible to use a telescopic shower curtain rod to block the curtain on its lower part and to prevent it from sucking inside.\nHanging the curtain rod higher or lower, or especially further away from the shower head, can reduce the effect. A convex shower rod can also be used to hold the curtain against the inside wall of a tub.\nA weight can be attached to a long string and the string attached to the curtain rod in the middle of the curtain (on the inside). Hanging the weight low against the curtain just above the rim of the shower pan or tub makes it an effective billowing deterrent without allowing the weight to hit the pan or tub and damage it.\nThere are a few alternative solutions that either attach to the shower curtain directly, attach to the shower rod or attach to the wall.\n== References ==\n== External links ==\nScientific American: Why does the shower curtain move toward the water?\nWhy does the shower curtain blow up and in instead of down and out?\nVideo demonstration of how this phenomenon could be solved.\nThe Straight Dope: Why does the shower curtain blow in despite the water pushing it out (revisited)?\n2001 Ig Nobel Prize Winners\nFluent NEWS: Shower Curtain Grabs Scientist – But He Lives to Tell Why\nArggh, Why Does the Shower Curtain Attack Me? by Joe Palca. All Things Considered, National Public Radio.  November 4, 2006. (audio)\nExperimental Investigation of the Influence of the Relative Position of the Scattering Layer on Image Quality: the Shower Curtain Effect\nThe shower curtain effect; ESA', 'Brown dwarfs form similarly to stars and are surrounded by protoplanetary disks, such as Cha 110913−773444. Disks around brown dwarfs have been found to have many of the same features as disks around stars; therefore, it is expected that there will be accretion-formed planets around brown dwarfs. Given the small mass of brown dwarf disks, most planets will be terrestrial planets rather than gas giants. If a giant planet orbits a brown dwarf across our line of sight, then, because they have approximately the same diameter, this would give a large signal for detection by transit. The accretion zone for planets around a brown dwarf is very close to the brown dwarf itself, so tidal forces would have a strong effect.\nIn 2020, the closest brown dwarf with an associated primordial disk (class II disk)—WISEA J120037.79-784508.3 (W1200-7845)—was discovered by the Disk Detective project when classification volunteers noted its infrared excess. It was vetted and analyzed by the science team who found that W1200-7845 had a 99.8% probability of being a member of the ε Chamaeleontis (ε Cha) young moving group association. Its parallax (using Gaia DR2 data) puts it at a distance of 102 parsecs (or 333 lightyears) from Earth—which is within the local Solar neighborhood.\nA paper from 2021 studied circumstellar discs around brown dwarfs in stellar associations that are a few million years old and 140 to 200 parsecs away. The researchers found that these disks are not massive enough to form planets in the future. There is evidence in these disks that might indicate that planet formation begins at earlier stages and that planets are already present in these disks. The evidence for disk evolution includes a decreasing disk mass over time, dust grain growth and dust settling. Two brown dwarf disks were also found in absorption and at least 4 disks are photoevaporating from external UV-ratiation in the Orion Nebula. Such objects are also called proplyds. Proplyd 181−247, which is a brown dwarf or low-mass star, is surrounded by a disk with a radius of 30 astronomical units and the disk has a mass of 6.2±1.0 MJ. Disks around brown dwarfs usually have a radius smaller than 40 astronomical units, but three disks in the more distant Taurus molecular cloud have a radius larger than 70 au and were resolved with ALMA. These larger disks are able to form rocky planets with a mass >1 ME. There are also brown dwarfs with disks in associations older than a few million years, which might be evidence that disks around brown dwarfs need more time to dissipate. Especially old disks (>20 Myrs) are sometimes called Peter Pan disks. Currently 2MASS J02265658-5327032 is the only known brown dwarf that has a Peter Pan disk.\nThe brown dwarf Cha 110913−773444, located 500 light-years away in the constellation Chamaeleon, may be in the process of forming a miniature planetary system. Astronomers from Pennsylvania State University have detected what they believe to be a disk of gas and dust similar to the one hypothesized to have formed the Solar System. Cha 110913−773444 is the smallest brown dwarf found to date (8 MJ), and if it formed a planetary system, it would be the smallest-known object to have one.\n== Planets around brown dwarfs ==\nAccording to the IAU working definition (from August 2018) an exoplanet can orbit a brown dwarf. It requires a mass below 13 MJ and a mass ratio of M/Mcentral<2/(25+√621). This means that an object with a mass up to 3.2 MJ around a brown dwarf with a mass of 80 MJ is considered a planet. It also means that an object with a mass up to 0.52 MJ around a brown dwarf with a mass of 13 MJ is considered a planet.\nThe super-Jupiter planetary-mass objects 2M1207b, 2MASS J044144 and Oph 98 B that are orbiting brown dwarfs at large orbital distances may have formed by cloud collapse rather than accretion and so may be sub-brown dwarfs rather than planets, which is inferred from relatively large masses and large orbits. The first discovery of a low-mass companion orbiting a brown dwarf (ChaHα8) at a small orbital distance using the radial velocity technique paved the way for the detection of planets around brown dwarfs on orbits of a few AU or smaller. However, with a mass ratio between the companion and primary in ChaHα8 of about 0.3, this system rather resembles a binary star. Then, in 2008, the first planetary-mass companion in a relatively small orbit (MOA-2007-BLG-192Lb) was discovered orbiting a brown dwarf.\nPlanets around brown dwarfs are likely to be carbon planets depleted of water.\nA 2017 study, based upon observations with Spitzer estimates that 175 brown dwarfs need to be monitored in order to guarantee (95%) at least one detection of a below earth-sized planet via the transiting method. JWST could potentially detect smaller planets. The orbits of planets and moons in the solar system often align with the orientation of the host star/planet they orbit. Assuming the orbit of a planet is aligned with the rotational axis of a brown dwarf or planetary-mass object, the geometric transit probability of an object similar to Io can be calculated with the formula cos(79.5°)/cos(inclination). The inclination was estimated for several brown dwarfs and planetary-mass objects. SIMP 0136 for example has an estimated inclination of 80°±12. Assuming the lower bound of i≥68° for SIMP 0136, this results in a transit probability of ≥48.6% for close-in planets. It is however not known how common close-in planets are around brown dwarfs and they might be more common for lower-mass objects, as disk sizes seem to decrease with mass.\nStrong evidence of a circumbinary planet in a polar orbit around 2M1510 was presented in 2025. The discovery was made with the Very Large Telescope.\n=== Habitability ===\nHabitability for hypothetical planets orbiting brown dwarfs has been studied. Computer models suggesting conditions for these bodies to have habitable planets are very stringent, the habitable zone being narrow, close (T dwarf 0.005 au) and decreasing with time, due to the cooling of the brown dwarf (they fuse for at most 10 million years).    The orbits there would have to be of extremely low eccentricity (on the order of 10−6) to avoid strong tidal forces that would trigger a runaway greenhouse effect on the planets, rendering them uninhabitable. There would also be no moons.\n== Superlative brown dwarfs ==\nIn 1984, it was postulated by some astronomers that the Sun may be orbited by an undetected brown dwarf (sometimes referred to as Nemesis) that could interact with the Oort cloud just as passing stars can. However, this hypothesis has fallen out of favor.\n=== Table of firsts ===\n=== Table of extremes ===\n== See also ==\nFusor (astronomy)\nBrown-dwarf desert – Theorized range of orbits around a star within which brown dwarfs cannot exist as companion objects\nBlue dwarf (red-dwarf stage) – Hypothetical class of star that develops from a red dwarf\nDark matter – Concept in cosmology\nExoplanet – Planet outside the Solar System\nStellification\nWD 0032-317 b\nList of brown dwarfs\nList of Y-dwarfs\n== Footnotes ==\n== References ==\n== External links ==\nHubbleSite newscenter – Weather patterns on a brown dwarf\nAllard, France; Homeier, Derek (2007). "Brown dwarfs". Scholarpedia. 2 (12): 4475. Bibcode:2007SchpJ...2.4475A. doi:10.4249/scholarpedia.4475.\n=== History ===\nKumar, Shiv S.; Low-Luminosity Stars. Gordon and Breach, London, 1969—an early overview paper on brown dwarfs\nThe Columbia Encyclopedia: "Brown Dwarfs"\n=== Details ===\nA current list of L and T dwarfs\nA geological definition of brown dwarfs, contrasted with stars and planets (via Berkeley)\nI. Neill Reid\'s pages at the Space Telescope Science Institute:\nOn spectral analysis of M dwarfs, L dwarfs, and T dwarfs\nTemperature and mass characteristics of low-temperature dwarfs\nFirst X-ray from brown dwarf observed, Spaceref.com, 2000\nMontes, David; "Brown Dwarfs and ultracool dwarfs (late-M, L, T)", UCM\nWild Weather: Iron Rain on Failed Stars—scientists are investigating astonishing weather patterns on brown dwarfs, Space.com, 2006\nNASA Brown dwarf detectives Archived 2014-10-17 at the Wayback Machine—Detailed information in a simplified sense\nBrown Dwarfs—Website with general information about brown dwarfs (has many detailed and colorful artist\'s impressions)\n=== Stars ===\nCha Halpha 1 stats and history\n"A census of observed brown dwarfs" (not all confirmed), 1998\nLuhman, Kevin L.; Adame, Lucía; d\'Alessio, Paola; Calvet, Nuria; Hartmann, Lee; Megeath, S. Thomas; Fazio, Giovanni G. (2005). "Discovery of a Planetary-Mass Brown Dwarf with a Circumstellar Disk". The Astrophysical Journal. 635 (1): L93 – L96. arXiv:astro-ph/0511807. Bibcode:2005ApJ...635L..93L. doi:10.1086/498868. S2CID 11685964.\nMichaud, Peter; Heyer, Inge; Leggett, Sandy K.; and Adamson, Andy; "Discovery Narrows the Gap Between Planets and Brown Dwarfs", Gemini and Joint Astronomy Centre, 2007\nDeacon, N. R.; Hambly, N. C. (2006). "The possiblity of detection of ultracool dwarfs with the UKIRT Infrared Deep Sky Survey". Monthly Notices of the Royal Astronomical Society. 371 (4): 1722–1730. arXiv:astro-ph/0607305. Bibcode:2006MNRAS.371.1722D. doi:10.1111/j.1365-2966.2006.10795.x.', 'Classical mechanics', 'is known as the Stefan–Boltzmann constant.\n=== Radiative transfer ===\nThe equation of radiative transfer describes the way in which radiation is affected as it travels through a material medium. For the special case in which the material medium is in thermodynamic equilibrium in the neighborhood of a point in the medium, Planck\'s law is of special importance.\nFor simplicity, we can consider the linear steady state, without scattering. The equation of radiative transfer states that for a beam of light going through a small distance ds, energy is conserved: The change in the (spectral) radiance of that beam (Iν) is equal to the amount removed by the material medium plus the amount gained from the material medium. If the radiation field is in equilibrium with the material medium, these two contributions will be equal. The material medium will have a certain emission coefficient and absorption coefficient.\nThe absorption coefficient α is the fractional change in the intensity of the light beam as it travels the distance ds, and has units of length−1. It is composed of two parts, the decrease due to absorption and the increase due to stimulated emission. Stimulated emission is emission by the material body which is caused by and is proportional to the incoming radiation. It is included in the absorption term because, like absorption, it is proportional to the intensity of the incoming radiation. Since the amount of absorption will generally vary linearly as the density ρ of the material, we may define a "mass absorption coefficient" κν = \u2060α/ρ\u2060 which is a property of the material itself. The change in intensity of a light beam due to absorption as it traverses a small distance ds will then be\n{\\displaystyle dI_{\\nu }=-\\kappa _{\\nu }\\rho I_{\\nu }\\,ds}\nThe "mass emission coefficient" jν is equal to the radiance per unit volume of a small volume element divided by its mass (since, as for the mass absorption coefficient, the emission is proportional to the emitting mass) and has units of power⋅solid angle−1⋅frequency−1⋅density−1. Like the mass absorption coefficient, it too is a property of the material itself. The change in a light beam as it traverses a small distance ds will then be\n{\\displaystyle dI_{\\nu }=j_{\\nu }\\rho \\,ds}\nThe equation of radiative transfer will then be the sum of these two contributions:\n{\\displaystyle {\\frac {dI_{\\nu }}{ds}}=j_{\\nu }\\rho -\\kappa _{\\nu }\\rho I_{\\nu }.}\nIf the radiation field is in equilibrium with the material medium, then the radiation will be homogeneous (independent of position) so that dIν = 0 and:\n{\\displaystyle \\kappa _{\\nu }B_{\\nu }=j_{\\nu }}\nwhich is another statement of Kirchhoff\'s law, relating two material properties of the medium, and which yields the radiative transfer equation at a point around which the medium is in thermodynamic equilibrium:\n{\\displaystyle {\\frac {dI_{\\nu }}{ds}}=\\kappa _{\\nu }\\rho (B_{\\nu }-I_{\\nu }).}\n=== Einstein coefficients ===\nThe principle of detailed balance states that, at thermodynamic equilibrium, each elementary process is in equilibrium with its reverse process.\nIn 1916, Albert Einstein applied this principle on an atomic level to the case of an atom radiating and absorbing radiation due to transitions between two particular energy levels, giving a deeper insight into the equation of radiative transfer and Kirchhoff\'s law for this type of radiation. If level 1 is the lower energy level with energy E1, and level 2 is the upper energy level with energy E2, then the frequency ν of the radiation radiated or absorbed will be determined by Bohr\'s frequency condition:\n{\\displaystyle E_{2}-E_{1}=h\\nu .}\nIf n1 and n2 are the number densities of the atom in states 1 and 2 respectively, then the rate of change of these densities in time will be due to three processes:\nSpontaneous emission\n21\n{\\displaystyle \\left({\\frac {dn_{1}}{dt}}\\right)_{\\mathrm {spon} }=A_{21}n_{2}}\nStimulated emission\n21\n{\\displaystyle \\left({\\frac {dn_{1}}{dt}}\\right)_{\\mathrm {stim} }=B_{21}n_{2}u_{\\nu }}\nPhoto-absorption\n12\n{\\displaystyle \\left({\\frac {dn_{2}}{dt}}\\right)_{\\mathrm {abs} }=B_{12}n_{1}u_{\\nu }}\nwhere uν is the spectral energy density of the radiation field. The three parameters A21, B21 and B12, known as the Einstein coefficients, are associated with the photon frequency ν produced by the transition between two energy levels (states). As a result, each line in a spectrum has its own set of associated coefficients. When the atoms and the radiation field are in equilibrium, the radiance will be given by Planck\'s law and, by the principle of detailed balance, the sum of these rates must be zero:\n21\n21\n12\n{\\displaystyle 0=A_{21}n_{2}+B_{21}n_{2}{\\frac {4\\pi }{c}}B_{\\nu }(T)-B_{12}n_{1}{\\frac {4\\pi }{c}}B_{\\nu }(T)}\nSince the atoms are also in equilibrium, the populations of the two levels are related by the Boltzmann factor:\n{\\displaystyle {\\frac {n_{2}}{n_{1}}}={\\frac {g_{2}}{g_{1}}}e^{-h\\nu /k_{\\mathrm {B} }T}}\nwhere g1 and g2 are the multiplicities of the respective energy levels. Combining the above two equations with the requirement that they be valid at any temperature yields two relationships between the Einstein coefficients:\n21\n21\n{\\displaystyle {\\frac {A_{21}}{B_{21}}}={\\frac {8\\pi h\\nu ^{3}}{c^{3}}}}\n21\n12\n{\\displaystyle {\\frac {B_{21}}{B_{12}}}={\\frac {g_{1}}{g_{2}}}}\nso that knowledge of one coefficient will yield the other two.\nFor the case of isotropic absorption and emission, the emission coefficient (jν) and absorption coefficient (κν) defined in the radiative transfer section above, can be expressed in terms of the Einstein coefficients. The relationships between the Einstein coefficients will yield the expression of Kirchhoff\'s law expressed in the Radiative transfer section above, namely that\n{\\displaystyle j_{\\nu }=\\kappa _{\\nu }B_{\\nu }.}\nThese coefficients apply to both atoms and molecules.\n== Properties ==\n=== Peaks ===\nThe distributions Bν, Bω, Bν̃ and Bk peak at a photon energy of\n2.821\n{\\displaystyle E=\\left[3+W\\left(-3e^{-3}\\right)\\right]k_{\\mathrm {B} }T\\approx 2.821\\ k_{\\mathrm {B} }T,}\nwhere W is the Lambert W function and e is Euler\'s number.\nHowever, the distribution Bλ peaks at a different energy\n4.965\n{\\displaystyle E=\\left[5+W\\left(-5e^{-5}\\right)\\right]k_{\\mathrm {B} }T\\approx 4.965\\ k_{\\mathrm {B} }T,}\nThe reason for this is that, as mentioned above, one cannot go from (for example) Bν to Bλ simply by substituting ν by λ. In addition, one must also multiply by\n{\\textstyle \\left|{d\\nu }/{d\\lambda }\\right|=c/{\\lambda ^{2}}}\n, which shifts the peak of the distribution to higher energies. These peaks are the mode energy of a photon, when binned using equal-size bins of frequency or wavelength, respectively. Dividing hc (14387.770 μm·K) by these energy expression gives the wavelength of the peak.\nThe spectral radiance at these peaks is given by:\nmax\n1.896\n10\n19\n{\\displaystyle {\\begin{aligned}B_{\\nu ,{\\text{max}}}(T)&={\\frac {2k_{\\mathrm {B} }^{3}T^{3}x^{3}}{h^{2}c^{2}}}{\\frac {1}{e^{x}-1}}\\\\&\\approx 1.896\\times 10^{-19}{\\frac {\\mathrm {W} }{\\mathrm {m^{2}\\cdot Hz\\cdot sr} }}\\times (T/\\mathrm {K} )^{3}\\\\\\end{aligned}}}\nwith\n{\\displaystyle x=3+W(-3e^{-3}),}\nand\nmax\n4.096\n10\nsr\n{\\displaystyle {\\begin{aligned}B_{\\lambda ,{\\text{max}}}(T)&={\\frac {2k_{\\mathrm {B} }^{5}T^{5}x^{5}}{h^{4}c^{3}}}{\\frac {1}{e^{x}-1}}\\\\&\\approx 4.096\\times 10^{-6}{\\frac {\\text{W}}{{\\text{m}}^{2}\\cdot {\\text{sr}}}}\\times ~(T/{\\text{K}})^{5}\\end{aligned}}}\nwith\n{\\displaystyle x=5+W(-5e^{-5}).}\nMeanwhile, the average energy of a photon from a blackbody is\n30\n2.701\n{\\displaystyle E=\\left[{\\frac {\\pi ^{4}}{30\\ \\zeta (3)}}\\right]k_{\\mathrm {B} }T\\approx 2.701\\ k_{\\mathrm {B} }T,}\nwhere\n{\\displaystyle \\zeta }\nis the Riemann zeta function.\n=== Approximations ===\nIn the limit of low frequencies (i.e. long wavelengths), Planck\'s law becomes the Rayleigh–Jeans law\n{\\displaystyle B_{\\nu }(T)\\approx {\\frac {2\\nu ^{2}}{c^{2}}}k_{\\mathrm {B} }T}\nor\n{\\displaystyle B_{\\lambda }(T)\\approx {\\frac {2c}{\\lambda ^{4}}}k_{\\mathrm {B} }T}\nThe radiance increases as the square of the frequency, illustrating the ultraviolet catastrophe. In the limit of high frequencies (i.e. small wavelengths) Planck\'s law tends to the Wien approximation:\n{\\displaystyle B_{\\nu }(T)\\approx {\\frac {2h\\nu ^{3}}{c^{2}}}e^{-{\\frac {h\\nu }{k_{\\mathrm {B} }T}}}}\nor\n{\\displaystyle B_{\\lambda }(T)\\approx {\\frac {2hc^{2}}{\\lambda ^{5}}}e^{-{\\frac {hc}{\\lambda k_{\\mathrm {B} }T}}}.}\n=== Percentiles ===\nWien\'s displacement law in its stronger form states that the shape of Planck\'s law is independent of temperature. It is therefore possible to list the percentile points of the total radiation as well as the peaks for wavelength and frequency, in a form which gives the wavelength λ when divided by temperature T. The second column of the following table lists the corresponding values of λT, that is, those values of x for which the wavelength λ is \u2060x/T\u2060 micrometers at the radiance percentile point given by the corresponding entry in the first column.\nThat is, 0.01% of the radiation is at a wavelength below \u2060910/T\u2060 μm, 20% below \u20602676/T\u2060 μm, etc. The wavelength and frequency peaks are in bold and occur at 25.0% and 64.6% respectively. The 41.8% point is the wavelength-frequency-neutral peak (i.e. the peak in power per unit change in logarithm of wavelength or frequency). These are the points at which the respective Planck-law functions \u20601/λ5\u2060, ν3 and \u2060ν2/λ2\u2060, respectively, divided by exp(\u2060hν/kBT\u2060) − 1 attain their maxima. The much smaller gap in ratio of wavelengths between 0.1% and 0.01% (1110 is 22% more than 910) than between 99.9% and 99.99% (113374 is 120% more than 51613) reflects the exponential decay of energy at short wavelengths (left end) and polynomial decay at long.', "== Importance ==\nFrom the perspective of a planetary geologist, the atmosphere acts to shape a planetary surface. Wind picks up dust and other particles which, when they collide with the terrain, erode the relief and leave deposits (eolian processes). Frost and precipitations, which depend on the atmospheric composition, also influence the relief. Climate changes can influence a planet's geological history. Conversely, studying the surface of the Earth leads to an understanding of the atmosphere and climate of other planets.\nFor a meteorologist, the composition of the Earth's atmosphere is a factor affecting the climate and its variations.\nFor a biologist or paleontologist, the Earth's atmospheric composition is closely dependent on the appearance of life and its evolution.\n== See also ==\nAtmometer (evaporimeter)\nAtmospheric pressure\nInternational Standard Atmosphere\nKármán line\nSky\n== References ==\n== Further reading ==\nSanchez-Lavega, Agustin (2010). An Introduction to Planetary Atmospheres. Taylor & Francis. ISBN 978-1420067323.\n== External links ==\nProperties of atmospheric strata – The flight environment of the atmosphere\nAtmosphere – Everything you need to know", 'Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.']

Question: What is linear frame dragging?

Choices:
Choice A) Linear frame dragging is the effect of the general principle of relativity applied to the mass of a body when other masses are placed nearby. It is a tiny effect that is difficult to confirm experimentally and often omitted from articles on frame-dragging.
Choice B) Linear frame dragging is the effect of the general principle of relativity applied to rotational momentum, which is a large effect that is easily confirmed experimentally and often discussed in articles on frame-dragging.
Choice C) Linear frame dragging is the effect of the general principle of relativity applied to rotational momentum, which is similarly inevitable to the linear effect. It is a tiny effect that is difficult to confirm experimentally and often omitted from articles on frame-dragging.
Choice D) Linear frame dragging is the effect of the general principle of relativity applied to linear momentum, which is similarly inevitable to the rotational effect. It is a tiny effect that is difficult to confirm experimentally and often omitted from articles on frame-dragging.
Choice E) Linear frame dragging is the effect of the general principle of relativity applied to linear momentum, which is a large effect that is easily confirmed experimentally and often discussed in articles on frame-dragging.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Ultraviolet radiation, also known as simply UV, is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights.\nThe photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms.:\u200a25–26\u200a  Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature.:\u200a28\u200a  Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact.\nFor humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength "extreme" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life.\nThe lower wavelength limit of the visible spectrum is conventionally taken as 400 nm.  Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range.  Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.\n== Visibility ==\nUltraviolet rays are not usable for normal human vision.\nThe lens of the human eye and surgically implanted lens produced since 1986 blocks most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy.\nPeople lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet.  Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds "true" UV vision.\n== History and discovery ==\n"Ultraviolet" means "beyond violet" (from Latin ultra, "beyond"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light.\nUV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them "(de-)oxidizing rays" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from "heat rays", discovered the previous year at the other end of the visible spectrum. The simpler term "chemical rays" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them "tithonic rays"). The terms "chemical rays" and "heat rays" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established.\nThe discovery of the ultraviolet radiation with wavelengths below 200 nm, named "vacuum ultraviolet" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided "unanimously" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen.\n== Subtypes ==\nThe electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348:\nSeveral solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum.\nVacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers.\nTechnology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes.\nExtreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer  satellite.\nSome sources use the distinction of "hard UV" and "soft UV". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with "hard UV" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions.\n== Solar ultraviolet ==\nVery hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth\'s atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum.\nThe atmosphere blocks about 77% of the Sun\'s UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth\'s surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth\'s surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On "partly cloudy" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB.', 'An atmosphere (from Ancient Greek  ἀτμός (atmós) \'vapour, steam\' and  σφαῖρα (sphaîra) \'sphere\') is a layer of gases that envelop an astronomical object, held in place by the gravity of the object. A planet retains an atmosphere when the gravity is great and the temperature of the atmosphere is low. A stellar atmosphere is the outer region of a star, which includes the layers above the opaque photosphere; stars of low temperature might have outer atmospheres containing compound molecules.\nThe atmosphere of Earth is composed of nitrogen (78%), oxygen (21%), argon (0.9%), carbon dioxide (0.04%) and trace gases. Most organisms use oxygen for respiration; lightning and bacteria perform nitrogen fixation which produces ammonia that is used to make nucleotides and amino acids; plants, algae, and cyanobacteria use carbon dioxide for photosynthesis. The layered composition of the atmosphere minimises the harmful effects of sunlight, ultraviolet radiation, solar wind, and cosmic rays and thus protects the organisms from genetic damage. The current composition of the atmosphere of the Earth is the product of billions of years of biochemical modification of the paleoatmosphere by living organisms.\n== Occurrence and compositions ==\n=== Origins ===\nAtmospheres are clouds of gas bound to and engulfing an astronomical focal point of sufficiently dominating mass, adding to its mass, possibly escaping from it or collapsing into it.\nBecause of the latter, such planetary nucleus can develop from interstellar molecular clouds or protoplanetary disks into rocky astronomical objects with varyingly thick atmospheres, gas giants or fusors.\nComposition and thickness is originally determined by the stellar nebula\'s chemistry and temperature, but can also by a product processes within the astronomical body outgasing a different atmosphere.\n=== Compositions ===\nThe atmospheres of the planets Venus and Mars are principally composed of carbon dioxide and nitrogen, argon and oxygen.\nThe composition of Earth\'s atmosphere is determined by the by-products of the life that it sustains. Dry air (mixture of gases) from Earth\'s atmosphere contains 78.08% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and traces of hydrogen, helium, and other "noble" gases (by volume), but generally a variable amount of water vapor is also present, on average about 1% at sea level.\nThe low temperatures and higher gravity of the Solar System\'s giant planets—Jupiter, Saturn, Uranus and Neptune—allow them more readily to retain gases with low molecular masses. These planets have hydrogen–helium atmospheres, with trace amounts of more complex compounds.\nTwo satellites of the outer planets possess significant atmospheres. Titan, a moon of Saturn, and Triton, a moon of Neptune, have atmospheres mainly of nitrogen. When in the part of its orbit closest to the Sun, Pluto has an atmosphere of nitrogen and methane similar to Triton\'s, but these gases are frozen when it is farther from the Sun.\nOther bodies within the Solar System have extremely thin atmospheres not in equilibrium. These include the Moon (sodium gas), Mercury (sodium gas), Europa (oxygen), Io (sulfur), and Enceladus (water vapor).\nThe first exoplanet whose atmospheric composition was determined is HD 209458b, a gas giant with a close orbit around a star in the constellation Pegasus. Its atmosphere is heated to temperatures over 1,000 K, and is steadily escaping into space. Hydrogen, oxygen, carbon and sulfur have been detected in the planet\'s inflated atmosphere.\n=== Atmospheres in the Solar System ===\nAtmosphere of the Sun\nAtmosphere of Mercury\nAtmosphere of Venus\nAtmosphere of Earth\nAtmosphere of the Moon\nAtmosphere of Mars\nAtmosphere of Ceres\nAtmosphere of Jupiter\nAtmosphere of Io\nAtmosphere of Callisto\nAtmosphere of Europa\nAtmosphere of Ganymede\nAtmosphere of Saturn\nAtmosphere of Titan\nAtmosphere of Enceladus\nAtmosphere of Uranus\nAtmosphere of Titania\nAtmosphere of Neptune\nAtmosphere of Triton\nAtmosphere of Pluto\n== Structure of atmosphere ==\n=== Earth ===\nThe atmosphere of Earth is composed of layers with different properties, such as specific gaseous composition, temperature, and pressure.\nThe troposphere is the lowest layer of the atmosphere. This extends from the planetary surface to the bottom of the stratosphere. The troposphere contains 75–80% of the mass of the atmosphere, and is the atmospheric layer wherein the weather occurs; the height of the troposphere varies between 17 km at the equator and 7.0 km at the poles.\nThe stratosphere extends from the top of the troposphere to the bottom of the mesosphere, and contains the ozone layer, at an altitude between 15 km and 35 km. It is the atmospheric layer that absorbs most of the ultraviolet radiation that Earth receives from the Sun.\nThe mesosphere ranges from 50 km to 85 km and is the layer wherein most meteors are incinerated before reaching the surface.\nThe thermosphere extends from an altitude of 85 km to the base of the exosphere at 690 km and contains the ionosphere, where solar radiation ionizes the atmosphere. The density of the ionosphere is greater at short distances from the planetary surface in the daytime and decreases as the ionosphere rises at night-time, thereby allowing a greater range of radio frequencies to travel greater distances.\nThe exosphere begins at 690 to 1,000 km from the surface, and extends to roughly 10,000 km, where it interacts with the magnetosphere of Earth.\n== Pressure ==\nAtmospheric pressure is the force (per unit-area) perpendicular to a unit-area of planetary surface, as determined by the weight of the vertical column of atmospheric gases. In said atmospheric model, the atmospheric pressure, the weight of the mass of the gas, decreases at high altitude because of the diminishing mass of the gas above the point of barometric measurement. The units of air pressure are based upon the standard atmosphere (atm), which is 101,325 Pa (equivalent to 760 Torr or 14.696 psi). The height at which the atmospheric pressure declines by a factor of e (an irrational number equal to 2.71828) is called the scale height (H). For an atmosphere of uniform temperature, the scale height is proportional to the atmospheric temperature and is inversely proportional to the product of the mean molecular mass of dry air, and the local acceleration of gravity at the point of barometric measurement.\n== Escape ==\nSurface gravity differs significantly among the planets. For example, the large gravitational force of the giant planet Jupiter retains light gases such as hydrogen and helium that escape from objects with lower gravity. Secondly, the distance from the Sun determines the energy available to heat atmospheric gas to the point where some fraction of its molecules\' thermal motion exceed the planet\'s escape velocity, allowing those to escape a planet\'s gravitational grasp. Thus, distant and cold Titan, Triton, and Pluto are able to retain their atmospheres despite their relatively low gravities.\nSince a collection of gas molecules may be moving at a wide range of velocities, there will always be some fast enough to produce a slow leakage of gas into space. Lighter molecules move faster than heavier ones with the same thermal kinetic energy, and so gases of low molecular weight are lost more rapidly than those of high molecular weight. It is thought that Venus and Mars may have lost much of their water when, after being photodissociated into hydrogen and oxygen by solar ultraviolet radiation, the hydrogen escaped. Earth\'s magnetic field helps to prevent this, as, normally, the solar wind would greatly enhance the escape of hydrogen. However, over the past 3 billion years Earth may have lost gases through the magnetic polar regions due to auroral activity, including a net 2% of its atmospheric oxygen. The net effect, taking the most important escape processes into account, is that an intrinsic magnetic field does not protect a planet from atmospheric escape and that for some magnetizations the presence of a magnetic field works to increase the escape rate.\nOther mechanisms that can cause atmosphere depletion are solar wind-induced sputtering, impact erosion, weathering, and sequestration—sometimes referred to as "freezing out"—into the regolith and polar caps.\n== Terrain ==\nAtmospheres have dramatic effects on the surfaces of rocky bodies. Objects that have no atmosphere, or that have only an exosphere, have terrain that is covered in craters. Without an atmosphere, the planet has no protection from meteoroids, and all of them collide with the surface as meteorites and create craters.\nFor planets with a significant atmosphere, most meteoroids burn up as meteors before hitting a planet\'s surface. When meteoroids do impact, the effects are often erased by the action of wind.\nWind erosion is a significant factor in shaping the terrain of rocky planets with atmospheres, and over time can erase the effects of both craters and volcanoes. In addition, since liquids cannot exist without pressure, an atmosphere allows liquid to be present at the surface, resulting in lakes, rivers and oceans. Earth and Titan are known to have liquids at their surface and terrain on the planet suggests that Mars had liquid on its surface in the past.\n=== Outside the Solar System ===\nAtmosphere of HD 209458 b\n== Circulation ==\nThe circulation of the atmosphere occurs due to thermal differences when convection becomes a more efficient transporter of heat than thermal radiation. On planets where the primary heat source is solar radiation, excess heat in the tropics is transported to higher latitudes. When a planet generates a significant amount of heat internally, such as is the case for Jupiter, convection in the atmosphere can transport thermal energy from the higher temperature interior up to the surface.\n== Importance ==', 'In the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein\'s proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal. This has at times been called Planck\'s "second theory".\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his \'third theory\', that both emission and absorption of light were quantal.\nThe colourful term "ultraviolet catastrophe" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck\'s thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of "catastrophe". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called \'photons\', and by Rayleigh and by Jeans.\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν. In contrast to Planck\'s and Einstein\'s formulas, Bohr\'s formula referred explicitly and categorically to energy levels of atoms. Bohr\'s formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck\'s model, the frequency\n{\\displaystyle \\nu }\nhas no immediate relation to frequencies that might describe those quantum states themselves.\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck\'s law. The actual word \'photon\' was invented still later, by G.N. Lewis in 1926, who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word \'photon\' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein\'s 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\nUltimately, Planck\'s law of black-body radiation contributed to Einstein\'s concept of quanta of light carrying linear momentum, which became the fundamental basis for the development of quantum mechanics.\nThe above-mentioned linearity of Planck\'s mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg\'s original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg\'s theory accounted for Bohr\'s above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states. At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg\'s paper and recognized the matrix character of Heisenberg\'s theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg\'s original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics. Heisenberg\'s explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck\'s oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = \u2060h/2π\u2060, and ω = 2πν denotes angular frequency, and less often the equivalent formula E = hν. This statement about a really existing and propagating light quantum, based on Einstein\'s, has a physical meaning different from that of Planck\'s above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\nAn article by Helge Kragh published in Physics World gives an account of this history.\n== See also ==\nEmissivity\nRadiance\nSakuma–Hattori equation\n== References ==\n=== Bibliography ===\n== External links ==\nSummary of Radiation\nRadiation of a Blackbody – interactive simulation to play with Planck\'s law\nScienceworld entry on Planck\'s Law', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'UVC LEDs are relatively new to the commercial market and are gaining in popularity. Due to their monochromatic nature (±5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.\nDisinfection using UV radiation is commonly used in wastewater treatment applications and is finding an increased usage in municipal drinking water treatment. Many bottlers of spring water use UV disinfection equipment to sterilize their water. Solar water disinfection has been researched for cheaply treating contaminated water using natural sunlight. The UVA irradiation and increased water temperature kill organisms in the water.\nUltraviolet radiation is used in several food processes to kill unwanted microorganisms. UV can be used to pasteurize fruit juices by flowing the juice over a high-intensity ultraviolet source. The effectiveness of such a process depends on the UV absorbance of the juice.\nPulsed light (PL) is a technique of killing microorganisms on surfaces using pulses of an intense broad spectrum, rich in UVC between 200 and 280 nm. Pulsed light works with xenon flash lamps that can produce flashes several times per second. Disinfection robots use pulsed UV.\nThe antimicrobial effectiveness of filtered far-UVC (222\u2009nm) light on a range of pathogens, including bacteria and fungi showed inhibition of pathogen growth, and since it has lesser harmful effects, it provides essential insights for reliable disinfection in healthcare settings, such as hospitals and long-term care homes. UVC has also been shown to be effective at degrading SARS-CoV-2 virus.\n==== Biological ====\nSome animals, including birds, reptiles, and insects such as bees, can see near-ultraviolet wavelengths. Many fruits, flowers, and seeds stand out more strongly from the background in ultraviolet wavelengths as compared to human color vision. Scorpions glow or take on a yellow to green color under UV illumination, thus assisting in the control of these arachnids. Many birds have patterns in their plumage that are invisible at usual wavelengths but observable in ultraviolet, and the urine and other secretions of some animals, including dogs, cats, and human beings, are much easier to spot with ultraviolet. Urine trails of rodents can be detected by pest control technicians for proper treatment of infested dwellings.\nButterflies use ultraviolet as a communication system for sex recognition and mating behavior. For example, in the Colias eurytheme butterfly, males rely on visual cues to locate and identify females. Instead of using chemical stimuli to find mates, males are attracted to the ultraviolet-reflecting color of female hind wings. In Pieris napi butterflies it was shown that females in northern Finland with less UV-radiation present in the environment possessed stronger UV signals to attract their males than those occurring further south. This suggested that it was evolutionarily more difficult to increase the UV-sensitivity of the eyes of the males than to increase the UV-signals emitted by the females.\nMany insects use the ultraviolet wavelength emissions from celestial objects as references for flight navigation. A local ultraviolet emitter will normally disrupt the navigation process and will eventually attract the flying insect.\nThe green fluorescent protein (GFP) is often used in genetics as a marker. Many substances, such as proteins, have significant light absorption bands in the ultraviolet that are of interest in biochemistry and related fields. UV-capable spectrophotometers are common in such laboratories.\nUltraviolet traps called bug zappers are used to eliminate various small flying insects. They are attracted to the UV and are killed using an electric shock, or trapped once they come into contact with the device. Different designs of ultraviolet radiation traps are also used by entomologists for collecting nocturnal insects during faunistic survey studies.\n==== Therapy ====\nUltraviolet radiation is helpful in the treatment of skin conditions such as psoriasis and vitiligo. Exposure to UVA, while the skin is hyper-photosensitive, by taking psoralens is an effective treatment for psoriasis. Due to the potential of psoralens to cause damage to the liver, PUVA therapy may be used only a limited number of times over a patient\'s lifetime.\nUVB phototherapy does not require additional medications or topical preparations for the therapeutic benefit; only the exposure is needed. However, phototherapy can be effective when used in conjunction with certain topical treatments such as anthralin, coal tar, and vitamin A and D derivatives, or systemic treatments such as methotrexate and Soriatane.\n==== Herpetology ====\nReptiles need UVB for biosynthesis of vitamin D, and other metabolic processes. Specifically cholecalciferol (vitamin D3), which is needed for basic cellular / neural functioning as well as the utilization of calcium for bone and egg production. The UVA wavelength is also visible to many reptiles and might play a significant role in their ability survive in the wild as well as in visual communication between individuals. Therefore, in a typical reptile enclosure, a fluorescent UV a/b source (at the proper strength / spectrum for the species), must be available for many captive species to survive. Simple supplementation with cholecalciferol (Vitamin D3) will not be enough as there is a complete biosynthetic pathway that is "leapfrogged" (risks of possible overdoses), the intermediate molecules and metabolites also play important functions in the animals health. Natural sunlight in the right levels is always going to be superior to artificial sources, but this might not be possible for keepers in different parts of the world.\nIt is a known problem that high levels of output of the UVa part of the spectrum can both cause cellular and DNA damage to sensitive parts of their bodies – especially the eyes where blindness is the result of an improper UVa/b source use and placement photokeratitis. For many keepers there must also be a provision for an adequate heat source this has resulted in the marketing of heat and light "combination" products. Keepers should be careful of these "combination" light/ heat and UVa/b generators, they typically emit high levels of UVa with lower levels of UVb that are set and difficult to control so that animals can have their needs met. A better strategy is to use individual sources of these elements and so they can be placed and controlled by the keepers for the max benefit of the animals.\n== Evolutionary significance ==\nThe evolution of early reproductive proteins and enzymes is attributed in modern models of evolutionary theory to ultraviolet radiation. UVB causes thymine base pairs next to each other in genetic sequences to bond together into thymine dimers, a disruption in the strand that reproductive enzymes cannot copy. This leads to frameshifting during genetic replication and protein synthesis, usually killing the cell. Before formation of the UV-blocking ozone layer, when early prokaryotes approached the surface of the ocean, they almost invariably died out. The few that survived had developed enzymes that monitored the genetic material and removed thymine dimers by nucleotide excision repair enzymes. Many enzymes and proteins involved in modern mitosis and meiosis are similar to repair enzymes, and are believed to be evolved modifications of the enzymes originally used to overcome DNA damages caused by UV.\nElevated levels of ultraviolet radiation, in particular UV-B, have also been speculated as a cause of mass extinctions in the fossil record.\n== Photobiology ==\nPhotobiology is the scientific study of the beneficial and harmful interactions of non-ionizing radiation in living organisms, conventionally demarcated around 10 eV, the first ionization energy of oxygen. UV ranges roughly from 3 to 30 eV in energy. Hence photobiology entertains some, but not all, of the UV spectrum.\n== See also ==\n== References ==\n== Further reading ==\nAllen, Jeannie (6 September 2001). Ultraviolet Radiation: How it Affects Life on Earth. Earth Observatory. NASA, USA.\nHockberger, Philip E. (2002). "A History of Ultraviolet Photobiology for Humans, Animals and Microorganisms". Photochemistry and Photobiology. 76 (6): 561–569. doi:10.1562/0031-8655(2002)0760561AHOUPF2.0.CO2. PMID 12511035. S2CID 222100404.\nHu, S; Ma, F; Collado-Mesa, F; Kirsner, R. S. (July 2004). "UV radiation, latitude, and melanoma in US Hispanics and blacks". Arch. Dermatol. 140 (7): 819–824. doi:10.1001/archderm.140.7.819. PMID 15262692.\nStrauss, CEM; Funk, DJ (1991). "Broadly tunable difference-frequency generation of VUV using two-photon resonances in H2 and Kr". Optics Letters. 16 (15): 1192–4. Bibcode:1991OptL...16.1192S. doi:10.1364/ol.16.001192. PMID 19776917.\n== External links ==\nMedia related to Ultraviolet light at Wikimedia Commons\nThe dictionary definition of ultraviolet at Wiktionary', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', 'Fusion powers stars and produces most elements lighter than cobalt in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 616 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.645% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and the result is fusion; this is an exothermic process.\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is 13.6 eV—less than one-millionth of the 17.6 MeV released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though individual fission reactions are generally much more energetic than individual fusion ones, which are themselves millions of times more energetic than chemical reactions. Via the mass–energy equivalence, fusion yields a 0.7% efficiency of reactant mass into energy. This can be only be exceeded by the extreme cases of the accretion process involving neutron stars or black holes, approaching 40% efficiency, and antimatter annihilation at 100% efficiency. (The complete conversion of one gram of matter would expel 9×1013 joules of energy.)\n== In astrophysics ==\nFusion is responsible for the astrophysical production of the majority of elements lighter than iron. This includes most types of Big Bang nucleosynthesis and stellar nucleosynthesis. Non-fusion processes that contribute include the s-process and r-process in neutron merger and supernova nucleosynthesis, responsible for elements heavier than iron.\n=== Stars ===\nAn important fusion process is the stellar nucleosynthesis that powers stars, including the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounts for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was unknown; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein\'s equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy had not yet been discovered, nor even that stars are largely composed of hydrogen (see metallicity). Eddington\'s paper reasoned that:\nThe leading theory of stellar energy, the contraction hypothesis, should cause the rotation of a star to visibly speed up due to conservation of angular momentum. But observations of Cepheid variable stars showed this was not happening.\nThe only other known plausible source of energy was conversion of matter to energy; Einstein had shown some years earlier that a small amount of matter was equivalent to a large amount of energy.\nFrancis Aston had also recently shown that the mass of a helium atom was about 0.8% less than the mass of the four hydrogen atoms which would, combined, form a helium atom (according to the then-prevailing theory of atomic structure which held atomic weight to be the distinguishing property between elements; work by Henry Moseley and Antonius van den Broek would later show that nucleic charge was the distinguishing property and that a helium nucleus, therefore, consisted of two hydrogen nuclei plus additional mass). This suggested that if such a combination could happen, it would release considerable energy as a byproduct.\nIf a star contained just 5% of fusible hydrogen, it would suffice to explain how stars got their energy. (It is now known that most \'ordinary\' stars are usually made of around 70% to 75% hydrogen)\nFurther elements might also be fused, and other scientists had speculated that stars were the "crucible" in which light elements combined to create heavy elements, but without more accurate measurements of their atomic masses nothing more could be said at the time.\nAll of these speculations were proven correct in the following decades.\nThe primary source of solar energy, and that of similar size stars, is the fusion of hydrogen to form helium (the proton–proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to fuse heavier elements. In massive cores, silicon-burning is the final fusion cycle, leading to a build-up of iron and nickel nuclei.\nNuclear binding energy makes the production of elements heavier than nickel via fusion energetically unfavorable. These elements are produced in non-fusion processes: the s-process, r-process, and the variety of processes that can produce p-nuclei. Such processes occur in giant star shells, or supernovae, or neutron star mergers.\n=== Brown dwarfs ===\nBrown dwarfs fuse deuterium and in very high mass cases also fuse lithium.\n=== White dwarfs ===\nCarbon-oxygen white dwarfs, which accrete matter either from an active stellar companion or white dwarf merger, approach the Chandrasekhar limit of 1.44 solar masses. Immediately prior, carbon burning fusion begins, destroying the Earth-sized dwarf within one second, in a Type Ia supernova.\nMuch more rarely, helium white dwarfs may merge, which does not cause an explosion but begins helium burning in an extreme type of helium star.\n=== Neutron stars ===\nSome neutron stars accrete hydrogen and helium from an active stellar companion. Periodically, the helium accretion reaches a critical level, and a thermonuclear burn wave propagates across the surface, on the timescale of one second.\n=== Black hole accretion disks ===\nSimilar to stellar fusion, extreme conditions within black hole accretion disks can allow fusion reactions. Calculations show the most energetic reactions occur around lower stellar mass black holes, below 10 solar masses, compared to those above 100. Beyond five Schwarzschild radii, carbon-burning and fusion of helium-3 dominates the reactions. Within this distance, around lower mass black holes, fusion of nitrogen, oxygen, neon, and magnesium can occur. In the extreme limit, the silicon-burning process can begin with the fusion of silicon and selenium nuclei.\n=== Big Bang ===\nFrom the period approximately 10 seconds to 20 minutes after the Big Bang, the universe cooled from over 100 keV to 1 keV. This allowed the combination of protons and neutrons in deuterium nuclei, and beginning a rapid fusion chain into tritium and helium-3 and ending in predominantly helium-4, with a minimal fraction of lithium, beryllium, and boron nuclei.\n== Requirements ==\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbors due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface-area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.', "==== Red-giant-branch phase ====\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star.  For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower 12C/13C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\nThe helium core continues to grow on the red-giant branch.  It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase.  The star increases in luminosity towards the tip of the red-giant branch.  Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n==== Horizontal branch ====\nIn the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 108 times the luminosity of the Sun for a few days and 1011 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning.  These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n==== Asymptotic-giant-branch phase ====\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time).  Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star.  Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically.  This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface.  This is known as the second dredge up, and in some stars there may even be a third dredge up.  In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra.  A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\nAnother well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths.  These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity.  These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n==== Post-AGB ====\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n=== Massive stars ===\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants.  Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.\n==== Supergiant evolution ====\nExtremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\nThe core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core.  In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process.  At the end of helium fusion, the core of a star consists primarily of carbon and oxygen.  In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium.  Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.\nThe exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉.  After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse.  Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning.  For a range of stars of approximately 8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova."]

Question: What is the Ozma Problem?

Choices:
Choice A) The Ozma Problem is a chapter in a book that discusses the versatility of carbon and chirality in biochemistry.
Choice B) The Ozma Problem is a discussion about time invariance and reversal in particle physics, theoretical physics, and cosmology.
Choice C) The Ozma Problem is a conundrum that examines whether there is any fundamental asymmetry to the universe. It concerns various aspects of atomic and subatomic physics and how they relate to mirror asymmetry and the related concepts of chirality, antimatter, magnetic and electrical polarity, parity, charge and spin.
Choice D) The Ozma Problem is a measure of how symmetry and asymmetry have evolved from the beginning of life on Earth.
Choice E) The Ozma Problem is a comparison between the level of a desired signal and the level of background noise used in science and engineering.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Photosynthesis', 'These pigments are embedded in plants and algae in complexes called antenna proteins. In such proteins, the pigments are arranged to work together. Such a combination of proteins is also called a light-harvesting complex.\nAlthough all cells in the green parts of a plant have chloroplasts, the majority of those are found in specially adapted structures called leaves. Certain species adapted to conditions of strong sunlight and aridity, such as many Euphorbia and cactus species, have their main photosynthetic organs in their stems. The cells in the interior tissues of a leaf, called the mesophyll, can contain between 450,000 and 800,000 chloroplasts for every square millimeter of leaf. The surface of the leaf is coated with a water-resistant waxy cuticle that protects the leaf from excessive evaporation of water and decreases the absorption of ultraviolet or blue light to minimize heating. The transparent epidermis layer allows light to pass through to the palisade mesophyll cells where most of the photosynthesis takes place.\n== Light-dependent reactions ==\nIn the light-dependent reactions, one molecule of the pigment chlorophyll absorbs one photon and loses one electron. This electron is taken up by a modified form of chlorophyll called pheophytin, which passes the electron to a quinone molecule, starting the flow of electrons down an electron transport chain that leads to the ultimate reduction of NADP to NADPH. In addition, this creates a proton gradient (energy gradient) across the chloroplast membrane, which is used by ATP synthase in the synthesis of ATP. The chlorophyll molecule ultimately regains the electron it lost when a water molecule is split in a process called photolysis, which releases oxygen.\nThe overall equation for the light-dependent reactions under the conditions of non-cyclic electron flow in green plants is:\nNot all wavelengths of light can support photosynthesis. The photosynthetic action spectrum depends on the type of accessory pigments present. For example, in green plants, the action spectrum resembles the absorption spectrum for chlorophylls and carotenoids with absorption peaks in violet-blue and red light. In red algae, the action spectrum is blue-green light, which allows these algae to use the blue end of the spectrum to grow in the deeper waters that filter out the longer wavelengths (red light) used by above-ground green plants. The non-absorbed part of the light spectrum is what gives photosynthetic organisms their color (e.g., green plants, red algae, purple bacteria) and is the least effective for photosynthesis in the respective organisms.\n=== Z scheme ===\nIn plants, light-dependent reactions occur in the thylakoid membranes of the chloroplasts where they drive the synthesis of ATP and NADPH. The light-dependent reactions are of two forms: cyclic and non-cyclic.\nIn the non-cyclic reaction, the photons are captured in the light-harvesting antenna complexes of photosystem II by chlorophyll and other accessory pigments (see diagram "Z-scheme"). The absorption of a photon by the antenna complex loosens an electron by a process called photoinduced charge separation. The antenna system is at the core of the chlorophyll molecule of the photosystem II reaction center. That loosened electron is taken up by the primary electron-acceptor molecule, pheophytin. As the electrons are shuttled through an electron transport chain (the so-called Z-scheme shown in the diagram), a chemiosmotic potential is generated by pumping proton cations (H+) across the membrane and into the thylakoid space. An ATP synthase enzyme uses that chemiosmotic potential to make ATP during photophosphorylation, whereas NADPH is a product of the terminal redox reaction in the Z-scheme. The electron enters a chlorophyll molecule in Photosystem I. There it is further excited by the light absorbed by that photosystem. The electron is then passed along a chain of electron acceptors to which it transfers some of its energy. The energy delivered to the electron acceptors is used to move hydrogen ions across the thylakoid membrane into the lumen. The electron is eventually used to reduce the coenzyme NADP with an H+ to NADPH (which has functions in the light-independent reaction); at that point, the path of that electron ends.\nThe cyclic reaction is similar to that of the non-cyclic but differs in that it generates only ATP, and no reduced NADP (NADPH) is created. The cyclic reaction takes place only at photosystem I. Once the electron is displaced from the photosystem, the electron is passed down the electron acceptor molecules and returns to photosystem I, from where it was emitted, hence the name cyclic reaction.\n=== Water photolysis ===\nLinear electron transport through a photosystem will leave the reaction center of that photosystem oxidized. Elevating another electron will first require re-reduction of the reaction center. The excited electrons lost from the reaction center (P700) of photosystem I are replaced by transfer from plastocyanin, whose electrons come from electron transport through photosystem II. Photosystem II, as the first step of the Z-scheme, requires an external source of electrons to reduce its oxidized chlorophyll a reaction center. The source of electrons for photosynthesis in green plants and cyanobacteria is water. Two water molecules are oxidized by the energy of four successive charge-separation reactions of photosystem II to yield a molecule of diatomic oxygen and four hydrogen ions. The electrons yielded are transferred to a redox-active tyrosine residue that is oxidized by the energy of P680+. This resets the ability of P680 to absorb another photon and release another photo-dissociated electron. The oxidation of water is catalyzed in photosystem II by a redox-active structure that contains four manganese ions and a calcium ion; this oxygen-evolving complex binds two water molecules and contains the four oxidizing equivalents that are used to drive the water-oxidizing reaction (Kok\'s S-state diagrams). The hydrogen ions are released in the thylakoid lumen and therefore contribute to the transmembrane chemiosmotic potential that leads to ATP synthesis. Oxygen is a waste product of light-dependent reactions, but the majority of organisms on Earth use oxygen and its energy for cellular respiration, including photosynthetic organisms.\n== Light-independent reactions ==\n=== Calvin cycle ===\nIn the light-independent (or "dark") reactions, the enzyme RuBisCO captures CO2 from the atmosphere and, in a process called the Calvin cycle, uses the newly formed NADPH and releases three-carbon sugars, which are later combined to form sucrose and starch. The overall equation for the light-independent reactions in green plants is:\u200a128\nCarbon fixation produces the three-carbon sugar intermediate, which is then converted into the final carbohydrate products. The simple carbon sugars photosynthesis produces are then used to form other organic compounds, such as the building material cellulose, the precursors for lipid and amino acid biosynthesis, or as a fuel in cellular respiration. The latter occurs not only in plants but also in animals when the carbon and energy from plants is passed through a food chain.\nThe fixation or reduction of carbon dioxide is a process in which carbon dioxide combines with a five-carbon sugar, ribulose 1,5-bisphosphate, to yield two molecules of a three-carbon compound, glycerate 3-phosphate, also known as 3-phosphoglycerate. Glycerate 3-phosphate, in the presence of ATP and NADPH produced during the light-dependent stages, is reduced to glyceraldehyde 3-phosphate. This product is also referred to as 3-phosphoglyceraldehyde (PGAL) or, more generically, as triose phosphate. Most (five out of six molecules) of the glyceraldehyde 3-phosphate produced are used to regenerate ribulose 1,5-bisphosphate so the process can continue. The triose phosphates not thus "recycled" often condense to form hexose phosphates, which ultimately yield sucrose, starch, and cellulose, as well as glucose and fructose. The sugars produced during carbon metabolism yield carbon skeletons that can be used for other metabolic reactions like the production of amino acids and lipids.\n=== Carbon concentrating mechanisms ===\n==== On land ====\nIn hot and dry conditions, plants close their stomata to prevent water loss. Under these conditions, CO2 will decrease and oxygen gas, produced by the light reactions of photosynthesis, will increase, causing an increase of photorespiration by the oxygenase activity of ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) and decrease in carbon fixation. Some plants have evolved mechanisms to increase the CO2 concentration in the leaves under these conditions.', '=== Carbon dioxide levels and photorespiration ===\nAs carbon dioxide concentrations rise, the rate at which sugars are made by the light-independent reactions increases until limited by other factors. RuBisCO, the enzyme that captures carbon dioxide in the light-independent reactions, has a binding affinity for both carbon dioxide and oxygen. When the concentration of carbon dioxide is high, RuBisCO will fix carbon dioxide. However, if the carbon dioxide concentration is low, RuBisCO will bind oxygen instead of carbon dioxide. This process, called photorespiration, uses energy, but does not produce sugars.\nRuBisCO oxygenase activity is disadvantageous to plants for several reasons:\nOne product of oxygenase activity is phosphoglycolate (2 carbon) instead of 3-phosphoglycerate (3 carbon). Phosphoglycolate cannot be metabolized by the Calvin-Benson cycle and represents carbon lost from the cycle. A high oxygenase activity, therefore, drains the sugars that are required to recycle ribulose 5-bisphosphate and for the continuation of the Calvin-Benson cycle.\nPhosphoglycolate is quickly metabolized to glycolate that is toxic to a plant at a high concentration; it inhibits photosynthesis.\nSalvaging glycolate is an energetically expensive process that uses the glycolate pathway, and only 75% of the carbon is returned to the Calvin-Benson cycle as 3-phosphoglycerate. The reactions also produce ammonia (NH3), which is able to diffuse out of the plant, leading to a loss of nitrogen.\nA highly simplified summary is:\n2 glycolate + ATP → 3-phosphoglycerate + carbon dioxide + ADP + NH3\nThe salvaging pathway for the products of RuBisCO oxygenase activity is more commonly known as photorespiration, since it is characterized by light-dependent oxygen consumption and the release of carbon dioxide.\n== See also ==\n== References ==\n== Further reading ==\n=== Books ===\n=== Papers ===\n== External links ==\nA collection of photosynthesis pages for all levels from a renowned expert (Govindjee)\nIn depth, advanced treatment of photosynthesis, also from Govindjee\nScience Aid: Photosynthesis Article appropriate for high school science\nMetabolism, Cellular Respiration and Photosynthesis – The Virtual Library of Biochemistry and Cell Biology\nOverall examination of Photosynthesis at an intermediate level\nOverall Energetics of Photosynthesis\nThe source of oxygen produced by photosynthesis Interactive animation, a textbook tutorial\nMarshall J (2011-03-29). "First practical artificial leaf makes debut". Discovery News. Archived from the original on 2012-03-22. Retrieved 2011-03-29.\nPhotosynthesis – Light Dependent & Light Independent Stages Archived 2011-09-10 at the Wayback Machine\nKhan Academy, video introduction', 'UVC LEDs are relatively new to the commercial market and are gaining in popularity. Due to their monochromatic nature (±5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.\nDisinfection using UV radiation is commonly used in wastewater treatment applications and is finding an increased usage in municipal drinking water treatment. Many bottlers of spring water use UV disinfection equipment to sterilize their water. Solar water disinfection has been researched for cheaply treating contaminated water using natural sunlight. The UVA irradiation and increased water temperature kill organisms in the water.\nUltraviolet radiation is used in several food processes to kill unwanted microorganisms. UV can be used to pasteurize fruit juices by flowing the juice over a high-intensity ultraviolet source. The effectiveness of such a process depends on the UV absorbance of the juice.\nPulsed light (PL) is a technique of killing microorganisms on surfaces using pulses of an intense broad spectrum, rich in UVC between 200 and 280 nm. Pulsed light works with xenon flash lamps that can produce flashes several times per second. Disinfection robots use pulsed UV.\nThe antimicrobial effectiveness of filtered far-UVC (222\u2009nm) light on a range of pathogens, including bacteria and fungi showed inhibition of pathogen growth, and since it has lesser harmful effects, it provides essential insights for reliable disinfection in healthcare settings, such as hospitals and long-term care homes. UVC has also been shown to be effective at degrading SARS-CoV-2 virus.\n==== Biological ====\nSome animals, including birds, reptiles, and insects such as bees, can see near-ultraviolet wavelengths. Many fruits, flowers, and seeds stand out more strongly from the background in ultraviolet wavelengths as compared to human color vision. Scorpions glow or take on a yellow to green color under UV illumination, thus assisting in the control of these arachnids. Many birds have patterns in their plumage that are invisible at usual wavelengths but observable in ultraviolet, and the urine and other secretions of some animals, including dogs, cats, and human beings, are much easier to spot with ultraviolet. Urine trails of rodents can be detected by pest control technicians for proper treatment of infested dwellings.\nButterflies use ultraviolet as a communication system for sex recognition and mating behavior. For example, in the Colias eurytheme butterfly, males rely on visual cues to locate and identify females. Instead of using chemical stimuli to find mates, males are attracted to the ultraviolet-reflecting color of female hind wings. In Pieris napi butterflies it was shown that females in northern Finland with less UV-radiation present in the environment possessed stronger UV signals to attract their males than those occurring further south. This suggested that it was evolutionarily more difficult to increase the UV-sensitivity of the eyes of the males than to increase the UV-signals emitted by the females.\nMany insects use the ultraviolet wavelength emissions from celestial objects as references for flight navigation. A local ultraviolet emitter will normally disrupt the navigation process and will eventually attract the flying insect.\nThe green fluorescent protein (GFP) is often used in genetics as a marker. Many substances, such as proteins, have significant light absorption bands in the ultraviolet that are of interest in biochemistry and related fields. UV-capable spectrophotometers are common in such laboratories.\nUltraviolet traps called bug zappers are used to eliminate various small flying insects. They are attracted to the UV and are killed using an electric shock, or trapped once they come into contact with the device. Different designs of ultraviolet radiation traps are also used by entomologists for collecting nocturnal insects during faunistic survey studies.\n==== Therapy ====\nUltraviolet radiation is helpful in the treatment of skin conditions such as psoriasis and vitiligo. Exposure to UVA, while the skin is hyper-photosensitive, by taking psoralens is an effective treatment for psoriasis. Due to the potential of psoralens to cause damage to the liver, PUVA therapy may be used only a limited number of times over a patient\'s lifetime.\nUVB phototherapy does not require additional medications or topical preparations for the therapeutic benefit; only the exposure is needed. However, phototherapy can be effective when used in conjunction with certain topical treatments such as anthralin, coal tar, and vitamin A and D derivatives, or systemic treatments such as methotrexate and Soriatane.\n==== Herpetology ====\nReptiles need UVB for biosynthesis of vitamin D, and other metabolic processes. Specifically cholecalciferol (vitamin D3), which is needed for basic cellular / neural functioning as well as the utilization of calcium for bone and egg production. The UVA wavelength is also visible to many reptiles and might play a significant role in their ability survive in the wild as well as in visual communication between individuals. Therefore, in a typical reptile enclosure, a fluorescent UV a/b source (at the proper strength / spectrum for the species), must be available for many captive species to survive. Simple supplementation with cholecalciferol (Vitamin D3) will not be enough as there is a complete biosynthetic pathway that is "leapfrogged" (risks of possible overdoses), the intermediate molecules and metabolites also play important functions in the animals health. Natural sunlight in the right levels is always going to be superior to artificial sources, but this might not be possible for keepers in different parts of the world.\nIt is a known problem that high levels of output of the UVa part of the spectrum can both cause cellular and DNA damage to sensitive parts of their bodies – especially the eyes where blindness is the result of an improper UVa/b source use and placement photokeratitis. For many keepers there must also be a provision for an adequate heat source this has resulted in the marketing of heat and light "combination" products. Keepers should be careful of these "combination" light/ heat and UVa/b generators, they typically emit high levels of UVa with lower levels of UVb that are set and difficult to control so that animals can have their needs met. A better strategy is to use individual sources of these elements and so they can be placed and controlled by the keepers for the max benefit of the animals.\n== Evolutionary significance ==\nThe evolution of early reproductive proteins and enzymes is attributed in modern models of evolutionary theory to ultraviolet radiation. UVB causes thymine base pairs next to each other in genetic sequences to bond together into thymine dimers, a disruption in the strand that reproductive enzymes cannot copy. This leads to frameshifting during genetic replication and protein synthesis, usually killing the cell. Before formation of the UV-blocking ozone layer, when early prokaryotes approached the surface of the ocean, they almost invariably died out. The few that survived had developed enzymes that monitored the genetic material and removed thymine dimers by nucleotide excision repair enzymes. Many enzymes and proteins involved in modern mitosis and meiosis are similar to repair enzymes, and are believed to be evolved modifications of the enzymes originally used to overcome DNA damages caused by UV.\nElevated levels of ultraviolet radiation, in particular UV-B, have also been speculated as a cause of mass extinctions in the fossil record.\n== Photobiology ==\nPhotobiology is the scientific study of the beneficial and harmful interactions of non-ionizing radiation in living organisms, conventionally demarcated around 10 eV, the first ionization energy of oxygen. UV ranges roughly from 3 to 30 eV in energy. Hence photobiology entertains some, but not all, of the UV spectrum.\n== See also ==\n== References ==\n== Further reading ==\nAllen, Jeannie (6 September 2001). Ultraviolet Radiation: How it Affects Life on Earth. Earth Observatory. NASA, USA.\nHockberger, Philip E. (2002). "A History of Ultraviolet Photobiology for Humans, Animals and Microorganisms". Photochemistry and Photobiology. 76 (6): 561–569. doi:10.1562/0031-8655(2002)0760561AHOUPF2.0.CO2. PMID 12511035. S2CID 222100404.\nHu, S; Ma, F; Collado-Mesa, F; Kirsner, R. S. (July 2004). "UV radiation, latitude, and melanoma in US Hispanics and blacks". Arch. Dermatol. 140 (7): 819–824. doi:10.1001/archderm.140.7.819. PMID 15262692.\nStrauss, CEM; Funk, DJ (1991). "Broadly tunable difference-frequency generation of VUV using two-photon resonances in H2 and Kr". Optics Letters. 16 (15): 1192–4. Bibcode:1991OptL...16.1192S. doi:10.1364/ol.16.001192. PMID 19776917.\n== External links ==\nMedia related to Ultraviolet light at Wikimedia Commons\nThe dictionary definition of ultraviolet at Wiktionary', 'Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==', 'Ultraviolet radiation, also known as simply UV, is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights.\nThe photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms.:\u200a25–26\u200a  Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature.:\u200a28\u200a  Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact.\nFor humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength "extreme" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life.\nThe lower wavelength limit of the visible spectrum is conventionally taken as 400 nm.  Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range.  Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.\n== Visibility ==\nUltraviolet rays are not usable for normal human vision.\nThe lens of the human eye and surgically implanted lens produced since 1986 blocks most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy.\nPeople lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet.  Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds "true" UV vision.\n== History and discovery ==\n"Ultraviolet" means "beyond violet" (from Latin ultra, "beyond"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light.\nUV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them "(de-)oxidizing rays" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from "heat rays", discovered the previous year at the other end of the visible spectrum. The simpler term "chemical rays" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them "tithonic rays"). The terms "chemical rays" and "heat rays" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established.\nThe discovery of the ultraviolet radiation with wavelengths below 200 nm, named "vacuum ultraviolet" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided "unanimously" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen.\n== Subtypes ==\nThe electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348:\nSeveral solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum.\nVacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers.\nTechnology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes.\nExtreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer  satellite.\nSome sources use the distinction of "hard UV" and "soft UV". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with "hard UV" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions.\n== Solar ultraviolet ==\nVery hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth\'s atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum.\nThe atmosphere blocks about 77% of the Sun\'s UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth\'s surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth\'s surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On "partly cloudy" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB.', "In more massive stars, the fusion of neon proceeds without a runaway deflagration.  This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements.  Surrounding the core are shells of lighter elements still undergoing fusion.  The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged.  The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope.  The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars.  Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself.  The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.\n==== Supernova ====\nWhen the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole.  Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\n== Stellar remnants ==\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n=== White and black dwarfs ===\nFor a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years.\nThe chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n=== Neutron stars ===\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n=== Black holes ===\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉.", 'Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.', 'Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word κρύσταλλος (krústallos; "clear ice, rock-crystal"), and γράφειν (gráphein; "to write"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well new developments in these fields.\n== History and timeline ==\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the Davisson–Germer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n== Methodology ==\nCrystallographic methods depend mainly on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. Crystallographers often explicitly state the type of beam used, as in the terms X-ray diffraction, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.\nX-rays interact with the spatial distribution of electrons in the sample.\nNeutrons are scattered by the atomic nuclei through the strong nuclear forces, but in addition the magnetic moment of neutrons is non-zero, so they are also scattered by magnetic fields. When neutrons are scattered from hydrogen-containing materials, they produce diffraction patterns with high noise levels, which can sometimes be resolved by substituting deuterium for hydrogen.\nElectrons are charged particles and therefore interact with the total charge distribution of both the atomic nuclei and the electrons of the sample.:\u200aChpt 4\nIt is hard to focus x-rays or neutrons, but since electrons are charged they can be focused and are used in electron microscope to produce magnified images. There are many ways that transmission electron microscopy and related techniques such as scanning transmission electron microscopy, high-resolution electron microscopy can be used to obtain images with in many cases atomic resolution from which crystallographic information can be obtained. There are also other methods such as low-energy electron diffraction, low-energy electron microscopy and reflection high-energy electron diffraction which can be used to obtain crystallographic information about surfaces.\n== Applications in various areas ==\n=== Materials science ===\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Most materials do not occur as a single crystal, but are poly-crystalline in nature (they exist as an aggregate of small crystals with different orientations). As such, powder diffraction techniques, which take diffraction patterns of samples with a large number of crystals, play an important role in structural determination.\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements. Crystallographic studies help elucidate the relationship between a material\'s structure and its properties, aiding in developing new materials with tailored characteristics. This understanding is crucial in various fields, including metallurgy, geology, and materials science. Advancements in crystallographic techniques, such as electron diffraction and X-ray crystallography, continue to expand our understanding of material behavior at the atomic level.\nIn another example, iron transforms from a body-centered cubic (bcc) structure called ferrite to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material\'s properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which structures are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory.\n=== Biology ===\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. The double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol, Pymol or VMD can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron diffraction has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n== Notation ==\nCoordinates in square brackets such as [100] denote a direction vector (in real space).\nCoordinates in angle brackets or chevrons such as <100> denote a family of directions which are related by symmetry operations. In the cubic crystal system for example, <100> would mean [100], [010], [001] or the negative of any of those directions.\nMiller indices in parentheses such as (100) denote a plane of the crystal structure, and regular repetitions of that plane with a particular spacing. In the cubic system, the normal to the (hkl) plane is the direction [hkl], but in lower-symmetry cases, the normal to (hkl) is not parallel to [hkl].\nIndices in curly brackets or braces such as {100} denote a family of planes and their normals. In cubic materials the symmetry makes them equivalent, just as the way angle brackets denote a family of directions. In non-cubic materials, <hkl> is not necessarily perpendicular to {hkl}.\n== Reference literature ==\nThe International Tables for Crystallography is an eight-book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure through x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and do not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:', 'An atmosphere (from Ancient Greek  ἀτμός (atmós) \'vapour, steam\' and  σφαῖρα (sphaîra) \'sphere\') is a layer of gases that envelop an astronomical object, held in place by the gravity of the object. A planet retains an atmosphere when the gravity is great and the temperature of the atmosphere is low. A stellar atmosphere is the outer region of a star, which includes the layers above the opaque photosphere; stars of low temperature might have outer atmospheres containing compound molecules.\nThe atmosphere of Earth is composed of nitrogen (78%), oxygen (21%), argon (0.9%), carbon dioxide (0.04%) and trace gases. Most organisms use oxygen for respiration; lightning and bacteria perform nitrogen fixation which produces ammonia that is used to make nucleotides and amino acids; plants, algae, and cyanobacteria use carbon dioxide for photosynthesis. The layered composition of the atmosphere minimises the harmful effects of sunlight, ultraviolet radiation, solar wind, and cosmic rays and thus protects the organisms from genetic damage. The current composition of the atmosphere of the Earth is the product of billions of years of biochemical modification of the paleoatmosphere by living organisms.\n== Occurrence and compositions ==\n=== Origins ===\nAtmospheres are clouds of gas bound to and engulfing an astronomical focal point of sufficiently dominating mass, adding to its mass, possibly escaping from it or collapsing into it.\nBecause of the latter, such planetary nucleus can develop from interstellar molecular clouds or protoplanetary disks into rocky astronomical objects with varyingly thick atmospheres, gas giants or fusors.\nComposition and thickness is originally determined by the stellar nebula\'s chemistry and temperature, but can also by a product processes within the astronomical body outgasing a different atmosphere.\n=== Compositions ===\nThe atmospheres of the planets Venus and Mars are principally composed of carbon dioxide and nitrogen, argon and oxygen.\nThe composition of Earth\'s atmosphere is determined by the by-products of the life that it sustains. Dry air (mixture of gases) from Earth\'s atmosphere contains 78.08% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and traces of hydrogen, helium, and other "noble" gases (by volume), but generally a variable amount of water vapor is also present, on average about 1% at sea level.\nThe low temperatures and higher gravity of the Solar System\'s giant planets—Jupiter, Saturn, Uranus and Neptune—allow them more readily to retain gases with low molecular masses. These planets have hydrogen–helium atmospheres, with trace amounts of more complex compounds.\nTwo satellites of the outer planets possess significant atmospheres. Titan, a moon of Saturn, and Triton, a moon of Neptune, have atmospheres mainly of nitrogen. When in the part of its orbit closest to the Sun, Pluto has an atmosphere of nitrogen and methane similar to Triton\'s, but these gases are frozen when it is farther from the Sun.\nOther bodies within the Solar System have extremely thin atmospheres not in equilibrium. These include the Moon (sodium gas), Mercury (sodium gas), Europa (oxygen), Io (sulfur), and Enceladus (water vapor).\nThe first exoplanet whose atmospheric composition was determined is HD 209458b, a gas giant with a close orbit around a star in the constellation Pegasus. Its atmosphere is heated to temperatures over 1,000 K, and is steadily escaping into space. Hydrogen, oxygen, carbon and sulfur have been detected in the planet\'s inflated atmosphere.\n=== Atmospheres in the Solar System ===\nAtmosphere of the Sun\nAtmosphere of Mercury\nAtmosphere of Venus\nAtmosphere of Earth\nAtmosphere of the Moon\nAtmosphere of Mars\nAtmosphere of Ceres\nAtmosphere of Jupiter\nAtmosphere of Io\nAtmosphere of Callisto\nAtmosphere of Europa\nAtmosphere of Ganymede\nAtmosphere of Saturn\nAtmosphere of Titan\nAtmosphere of Enceladus\nAtmosphere of Uranus\nAtmosphere of Titania\nAtmosphere of Neptune\nAtmosphere of Triton\nAtmosphere of Pluto\n== Structure of atmosphere ==\n=== Earth ===\nThe atmosphere of Earth is composed of layers with different properties, such as specific gaseous composition, temperature, and pressure.\nThe troposphere is the lowest layer of the atmosphere. This extends from the planetary surface to the bottom of the stratosphere. The troposphere contains 75–80% of the mass of the atmosphere, and is the atmospheric layer wherein the weather occurs; the height of the troposphere varies between 17 km at the equator and 7.0 km at the poles.\nThe stratosphere extends from the top of the troposphere to the bottom of the mesosphere, and contains the ozone layer, at an altitude between 15 km and 35 km. It is the atmospheric layer that absorbs most of the ultraviolet radiation that Earth receives from the Sun.\nThe mesosphere ranges from 50 km to 85 km and is the layer wherein most meteors are incinerated before reaching the surface.\nThe thermosphere extends from an altitude of 85 km to the base of the exosphere at 690 km and contains the ionosphere, where solar radiation ionizes the atmosphere. The density of the ionosphere is greater at short distances from the planetary surface in the daytime and decreases as the ionosphere rises at night-time, thereby allowing a greater range of radio frequencies to travel greater distances.\nThe exosphere begins at 690 to 1,000 km from the surface, and extends to roughly 10,000 km, where it interacts with the magnetosphere of Earth.\n== Pressure ==\nAtmospheric pressure is the force (per unit-area) perpendicular to a unit-area of planetary surface, as determined by the weight of the vertical column of atmospheric gases. In said atmospheric model, the atmospheric pressure, the weight of the mass of the gas, decreases at high altitude because of the diminishing mass of the gas above the point of barometric measurement. The units of air pressure are based upon the standard atmosphere (atm), which is 101,325 Pa (equivalent to 760 Torr or 14.696 psi). The height at which the atmospheric pressure declines by a factor of e (an irrational number equal to 2.71828) is called the scale height (H). For an atmosphere of uniform temperature, the scale height is proportional to the atmospheric temperature and is inversely proportional to the product of the mean molecular mass of dry air, and the local acceleration of gravity at the point of barometric measurement.\n== Escape ==\nSurface gravity differs significantly among the planets. For example, the large gravitational force of the giant planet Jupiter retains light gases such as hydrogen and helium that escape from objects with lower gravity. Secondly, the distance from the Sun determines the energy available to heat atmospheric gas to the point where some fraction of its molecules\' thermal motion exceed the planet\'s escape velocity, allowing those to escape a planet\'s gravitational grasp. Thus, distant and cold Titan, Triton, and Pluto are able to retain their atmospheres despite their relatively low gravities.\nSince a collection of gas molecules may be moving at a wide range of velocities, there will always be some fast enough to produce a slow leakage of gas into space. Lighter molecules move faster than heavier ones with the same thermal kinetic energy, and so gases of low molecular weight are lost more rapidly than those of high molecular weight. It is thought that Venus and Mars may have lost much of their water when, after being photodissociated into hydrogen and oxygen by solar ultraviolet radiation, the hydrogen escaped. Earth\'s magnetic field helps to prevent this, as, normally, the solar wind would greatly enhance the escape of hydrogen. However, over the past 3 billion years Earth may have lost gases through the magnetic polar regions due to auroral activity, including a net 2% of its atmospheric oxygen. The net effect, taking the most important escape processes into account, is that an intrinsic magnetic field does not protect a planet from atmospheric escape and that for some magnetizations the presence of a magnetic field works to increase the escape rate.\nOther mechanisms that can cause atmosphere depletion are solar wind-induced sputtering, impact erosion, weathering, and sequestration—sometimes referred to as "freezing out"—into the regolith and polar caps.\n== Terrain ==\nAtmospheres have dramatic effects on the surfaces of rocky bodies. Objects that have no atmosphere, or that have only an exosphere, have terrain that is covered in craters. Without an atmosphere, the planet has no protection from meteoroids, and all of them collide with the surface as meteorites and create craters.\nFor planets with a significant atmosphere, most meteoroids burn up as meteors before hitting a planet\'s surface. When meteoroids do impact, the effects are often erased by the action of wind.\nWind erosion is a significant factor in shaping the terrain of rocky planets with atmospheres, and over time can erase the effects of both craters and volcanoes. In addition, since liquids cannot exist without pressure, an atmosphere allows liquid to be present at the surface, resulting in lakes, rivers and oceans. Earth and Titan are known to have liquids at their surface and terrain on the planet suggests that Mars had liquid on its surface in the past.\n=== Outside the Solar System ===\nAtmosphere of HD 209458 b\n== Circulation ==\nThe circulation of the atmosphere occurs due to thermal differences when convection becomes a more efficient transporter of heat than thermal radiation. On planets where the primary heat source is solar radiation, excess heat in the tropics is transported to higher latitudes. When a planet generates a significant amount of heat internally, such as is the case for Jupiter, convection in the atmosphere can transport thermal energy from the higher temperature interior up to the surface.\n== Importance ==']

Question: What is the significance of the high degree of fatty-acyl disorder in the thylakoid membranes of plants?

Choices:
Choice A) The high degree of fatty-acyl disorder in the thylakoid membranes of plants is responsible for the low fluidity of membrane lipid fatty-acyl chains in the gel phase.
Choice B) The high degree of fatty-acyl disorder in the thylakoid membranes of plants is responsible for the exposure of chloroplast thylakoid membranes to cold environmental temperatures.
Choice C) The high degree of fatty-acyl disorder in the thylakoid membranes of plants allows for innate fluidity even at relatively low temperatures.
Choice D) The high degree of fatty-acyl disorder in the thylakoid membranes of plants allows for a gel-to-liquid crystalline phase transition temperature to be determined by many techniques.
Choice E) The high degree of fatty-acyl disorder in the thylakoid membranes of plants restricts the movement of membrane proteins, thus hindering their physiological role.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Time zone', 'Time', '== Travel ==\nTime travel is the concept of moving backwards or forwards to different points in time, in a manner analogous to moving through space, and different from the normal "flow" of time to an earthbound observer. In this view, all points in time (including future times) "persist" in some way. Time travel has been a plot device in fiction since the 19th century. Travelling backwards or forwards in time has never been verified as a process, and doing so presents many theoretical problems and contradictory logic which to date have not been overcome. Any technological device, whether fictional or hypothetical, that is used to achieve time travel is known as a time machine.\nA central problem with time travel to the past is the violation of causality; should an effect precede its cause, it would give rise to the possibility of a temporal paradox. Some interpretations of time travel resolve this by accepting the possibility of travel between branch points, parallel realities, or universes. The many-worlds interpretation has been used as a way to solve causality paradoxes arising from time travel. Any quantum event creates another branching timeline, and all possible outcomes coexist without any wave function collapse. This interpretation was an alternative but is opposite from the Copenhagen interpretation, which suggests that wave functions do collapse. In science, hypothetical faster-than-light particles are known as tachyons; the mathematics of Einstein\'s relativity suggests that they would have an imaginary rest mass. Some interpretations suggest that it might move backward in time. General relativity permits the existence of closed timelike curves, which could allow an observer to travel back in time to the same space. Though for the Gödel metric, such an occurrence requires a globally rotating universe, which has been contradicted by observations of the redshifts of distant galaxies and the cosmic background radiation. However, it has been suggested that a slowly rotating universe model may solve the Hubble tension, so it can not yet be ruled out.\nAnother solution to the problem of causality-based temporal paradoxes is that such paradoxes cannot arise simply because they have not arisen. As illustrated in numerous works of fiction, free will either ceases to exist in the past or the outcomes of such decisions are predetermined. A famous example is the grandfather paradox, in which a person is supposed to travel back in time to kill their own grandfather. This would not be possible to enact because it is a historical fact that one\'s grandfather was not killed before his child (one\'s parent) was conceived. This view does not simply hold that history is an unchangeable constant, but that any change made by a hypothetical future time traveller would already have happened in their past, resulting in the reality that the traveller moves from. The Novikov self-consistency principle asserts that due to causality constraints, time travel to the past is impossible.\n== Perception ==\nThe specious present refers to the time duration wherein one\'s perceptions are considered to be in the present. The experienced present is said to be \'specious\' in that, unlike the objective present, it is an interval and not a durationless instant. The term specious present was first introduced by the psychologist E. R. Clay, and later developed by William James.\n=== Biopsychology ===\nThe brain\'s judgment of time is known to be a highly distributed system, including at least the cerebral cortex, cerebellum, and basal ganglia as its components. One particular component, the suprachiasmatic nuclei, is responsible for the circadian (or daily) rhythm, while other cell clusters appear capable of shorter-range (ultradian) timekeeping. Mental chronometry is the use of response time in perceptual-motor tasks to infer the content, duration, and temporal sequencing of cognitive operations. Judgments of time can be altered by temporal illusions (like the kappa effect), age, psychoactive drugs, and hypnosis. The sense of time is impaired in some people with neurological diseases such as Parkinson\'s disease and attention deficit disorder.\nPsychoactive drugs can impair the judgment of time. Stimulants can lead both humans and rats to overestimate time intervals, while depressants can have the opposite effect. The level of activity in the brain of neurotransmitters such as dopamine and norepinephrine may be the reason for this. Such chemicals will either excite or inhibit the firing of neurons in the brain, with a greater firing rate allowing the brain to register the occurrence of more events within a given interval (speed up time) and a decreased firing rate reducing the brain\'s capacity to distinguish events occurring within a given interval (slow down time).\nPsychologists assert that time seems to go faster with age, but the literature on this age-related perception of time remains controversial. Those who support this notion argue that young people, having more excitatory neurotransmitters, are able to cope with faster external events. Some also argued that the perception of time is also influenced by memory and how much one have experienced; for example, as one get older, they will have spend less part of their total life waiting a month. Meanwhile children\'s expanding cognitive abilities allow them to understand time in a different way. Two- and three-year-olds\' understanding of time is mainly limited to "now and not now". Five- and six-year-olds can grasp the ideas of past, present, and future. Seven- to ten-year-olds can use clocks and calendars. Socioemotional selectivity theory proposed that when people perceive their time as open-ended and nebulous, they focus more on future-oriented goals.\n=== Spatial conceptualization ===\nAlthough time is regarded as an abstract concept, there is increasing evidence that time is conceptualized in the mind in terms of space. That is, instead of thinking about time in a general, abstract way, humans think about time in a spatial way and mentally organize it as such. Using space to think about time allows humans to mentally organize temporal events in a specific way. This spatial representation of time is often represented in the mind as a mental timeline (MTL). These origins are shaped by many environmental factors. Literacy appears to play a large role in the different types of MTLs, as reading/writing direction provides an everyday temporal orientation that differs from culture to culture. In Western cultures, the MTL may unfold rightward (with the past on the left and the future on the right) since people mostly read and write from left to right. Western calendars also continue this trend by placing the past on the left with the future progressing toward the right. Conversely, speakers of Arabic, Farsi, Urdu, and Hebrew read from right to left, and their MTLs unfold leftward (past on the right with future on the left); evidence suggests these speakers organize time events in their minds like this as well.\nThere is also evidence that some cultures use an allocentric spatialization, often based on environmental features. A study of the indigenous Yupno people of Papua New Guinea found that they may use an allocentric MTL, in which time flows uphill; when speaking of the past, individuals gestured downhill, where the river of the valley flowed into the ocean. When speaking of the future, they gestured uphill, toward the source of the river. This was common regardless of which direction the person faced. A similar study of the Pormpuraawans, an aboriginal group in Australia, reported that when they were asked to organize photos of a man aging "in order," individuals consistently placed the youngest photos to the east and the oldest photos to the west, regardless of which direction they faced. This directly clashed with an American group that consistently organized the photos from left to right. Therefore, this group also appears to have an allocentric MTL, but based on the cardinal directions instead of geographical features. The wide array of distinctions in the way different groups think about time leads to the broader question that different groups may also think about other abstract concepts in different ways as well, such as causality and number.\n== Use ==\nIn sociology and anthropology, time discipline is the general name given to social and economic rules, conventions, customs, and expectations governing the measurement of time, the social currency and awareness of time measurements, and people\'s expectations concerning the observance of these customs by others. Arlie Russell Hochschild and Norbert Elias have written on the use of time from a sociological perspective.\nThe use of time is an important issue in understanding human behavior, education, and travel behavior. Time-use research is a developing field of study. The question concerns how time is allocated across a number of activities (such as time spent at home, at work, shopping, etc.). Time use changes with technology, as the television or the Internet created new opportunities to use time in different ways. However, some aspects of time use are relatively stable over long periods of time, such as the amount of time spent traveling to work, which despite major changes in transport, has been observed to be about 20–30 minutes one-way for a large number of cities over a long period.\nTime management is the organization of tasks or events by first estimating how much time a task requires and when it must be completed, and adjusting events that would interfere with its completion so it is done in the appropriate amount of time. Calendars and day planners are common examples of time management tools.\n== Sequence of events ==', 'Time is the continuous progression of existence that occurs in an apparently irreversible succession from the past, through the present, and into the future. It is a component quantity of various measurements used to sequence events, to compare the duration of events (or the intervals between them), and to quantify rates of change of quantities in material reality or in the conscious experience. Time is often referred to as a fourth dimension, along with three spatial dimensions.\nTime is one of the seven fundamental physical quantities in both the International System of Units (SI) and International System of Quantities. The SI base unit of time is the second, which is defined by measuring the electronic transition frequency of caesium atoms. General relativity is the primary framework for understanding how spacetime works. Through advances in both theoretical and experimental investigations of spacetime, it has been shown that time can be distorted and dilated, particularly at the edges of black holes.\nThroughout history, time has been an important subject of study in religion, philosophy, and science. Temporal measurement has occupied scientists and technologists, and has been a prime motivation in navigation and astronomy. Time is also of significant social importance, having economic value ("time is money") as well as personal value, due to an awareness of the limited time in each day ("carpe diem") and in human life spans.\n== Definition ==\nThe concept of time can be complex. Multiple notions exist, and defining time in a manner applicable to all fields without circularity has consistently eluded scholars. Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems. Traditional definitions of time involved the observation of periodic motion such as the apparent motion of the sun across the sky, the phases of the moon, and the passage of a free-swinging pendulum. More modern systems include the Global Positioning System, other satellite systems, Coordinated Universal Time and mean solar time. Although these systems differ from one another, with careful measurements they can be synchronized.\nIn physics, time is a fundamental concept to define other quantities, such as velocity. To avoid a circular definition, time in physics is operationally defined as "what a clock reads", specifically a count of repeating events such as the SI second. Although this aids in practical measurements, it does not address the essence of time. Physicists developed the concept of the spacetime continuum, where events are assigned four coordinates: three for space and one for time. Events like particle collisions, supernovas, or rocket launches have coordinates that may vary for different observers, making concepts like "now" and "here" relative. In general relativity, these coordinates do not directly correspond to the causal structure of events. Instead, the spacetime interval is calculated and classified as either space-like or time-like, depending on whether an observer exists that would say the events are separated by space or by time. Since the time required for light to travel a specific distance is the same for all observers—a fact first publicly demonstrated by the Michelson–Morley experiment—all observers will consistently agree on this definition of time as a causal relation.\nGeneral relativity does not address the nature of time for extremely small intervals where quantum mechanics holds. In quantum mechanics, time is treated as a universal and absolute parameter, differing from general relativity\'s notion of independent clocks. The problem of time consists of reconciling these two theories. As of 2025, there is no generally accepted theory of quantum general relativity.\n== Measurement ==\nMethods of temporal measurement, or chronometry, generally take two forms. The first is a calendar, a mathematical tool for organising intervals of time on Earth, consulted for periods longer than a day. The second is a clock, a physical mechanism that indicates the passage of time, consulted for periods less than a day. The combined measurement marks a specific moment in time from a reference point, or epoch.\n=== History of the calendar ===\nArtifacts from the Paleolithic suggest that the moon was used to reckon time as early as 6,000 years ago. Lunar calendars were among the first to appear, with years of either 12 or 13 lunar months (either 354 or 384 days). Without intercalation to add days or months to some years, seasons quickly drift in a calendar based solely on twelve lunar months. Lunisolar calendars have a thirteenth month added to some years to make up for the difference between a full year (now known to be about 365.24 days) and a year of just twelve lunar months. The numbers twelve and thirteen came to feature prominently in many cultures, at least partly due to this relationship of months to years.\nOther early forms of calendars originated in Mesoamerica, particularly in ancient Mayan civilization, in which they developed the Maya calendar, consisting of multiple interrelated calendars. These calendars were religiously and astronomically based; the Haab\' calendar has 18 months in a year and 20 days in a month, plus five epagomenal days at the end of the year. In conjunction, the Maya also used a 260-day sacred calendar called the Tzolk\'in.\nThe reforms of Julius Caesar in 45 BC put the Roman world on a solar calendar. This Julian calendar was faulty in that its intercalation still allowed the astronomical solstices and equinoxes to advance against it by about 11 minutes per year. Pope Gregory XIII introduced a correction in 1582; the Gregorian calendar was only slowly adopted by different nations over a period of centuries, but it is now by far the most commonly used calendar around the world.\nDuring the French Revolution, a new clock and calendar were invented as part of the dechristianization of France and to create a more rational system in order to replace the Gregorian calendar. The French Republican Calendar\'s days consisted of ten hours of a hundred minutes of a hundred seconds, which marked a deviation from the base 12 (duodecimal) system used in many other devices by many cultures. The system was abolished in 1806.\n=== History of other devices ===\nA large variety of devices have been invented to measure time. The study of these devices is called horology. They can be driven by a variety of means, including gravity, springs, and various forms of electrical power, and regulated by a variety of means.\nA sundial is any device that uses the direction of sunlight to cast shadows from a gnomon onto a set of markings calibrated to indicate the local time, usually to the hour. The idea to separate the day into smaller parts is credited to Egyptians because of their sundials, which operated on a duodecimal system. The importance of the number 12 is due to the number of lunar cycles in a year and the number of stars used to count the passage of night. Obelisks made as a gnomon were built as early as c.\u20093500 BC. An Egyptian device that dates to c.\u20091500 BC, similar in shape to a bent T-square, also measured the passage of time from the shadow cast by its crossbar on a nonlinear rule. The T was oriented eastward in the mornings. At noon, the device was turned around so that it could cast its shadow in the evening direction.\nAlarm clocks reportedly first appeared in ancient Greece c.\u2009250 BC with a water clock made by Plato that would set off a whistle. The hydraulic alarm worked by gradually filling a series of vessels with water. After some time, the water emptied out of a siphon. Inventor Ctesibius revised Plato\'s design; the water clock uses a float as the power drive system and uses a sundial to correct the water flow rate.\nIn medieval philosophical writings, the atom was a unit of time referred to as the smallest possible division of time. The earliest known occurrence in English is in Byrhtferth\'s Enchiridion (a science text) of 1010–1012, where it was defined as 1/564 of a momentum (11⁄2 minutes), and thus equal to 15/94 of a second. It was used in the computus, the process of calculating the date of Easter. The most precise timekeeping device of the ancient world was the water clock, or clepsydra, one of which was found in the tomb of Egyptian pharaoh Amenhotep I. They could be used to measure the hours even at night but required manual upkeep to replenish the flow of water. The ancient Greeks and the people from Chaldea (southeastern Mesopotamia) regularly maintained timekeeping records as an essential part of their astronomical observations. Arab inventors and engineers, in particular, made improvements on the use of water clocks up to the Middle Ages. In the 11th century, Chinese inventors and engineers invented the first mechanical clocks driven by an escapement mechanism.\nIncense sticks and candles were, and are, commonly used to measure time in temples and churches across the globe. Water clocks, and, later, mechanical clocks, were used to mark the events of the abbeys and monasteries of the Middle Ages. The passage of the hours at sea can also be marked by bell. The hours were marked by bells in abbeys as well as at sea. Richard of Wallingford (1292–1336), abbot of St. Alban\'s abbey, famously built a mechanical clock as an astronomical orrery about 1330. The hourglass uses the flow of sand to measure the flow of time. They were also used in navigation. Ferdinand Magellan used 18 glasses on each ship for his circumnavigation of the globe (1522). The English word clock probably comes from the Middle Dutch word klocke which, in turn, derives from the medieval Latin word clocca, which ultimately derives from Celtic and is cognate with French, Latin, and German words that mean bell.', '==== Microsoft Windows ====\nWindows-based computer systems prior to Windows 95 and Windows NT used local time, but Windows 95 and later, and Windows NT, base system time on UTC. They allow a program to fetch the system time as UTC, represented as a year, month, day, hour, minute, second, and millisecond; Windows 95 and later, and Windows NT 3.5 and later, also allow the system time to be fetched as a count of 100 ns units since 1601-01-01 00:00:00 UTC. The system registry contains time zone information that includes the offset from UTC and rules that indicate the start and end dates for daylight saving in each zone. Interaction with the user normally uses local time, and application software is able to calculate the time in various zones. Terminal Servers allow remote computers to redirect their time zone settings to the Terminal Server so that users see the correct time for their time zone in their desktop/application sessions. Terminal Services uses the server base time on the Terminal Server and the client time zone information to calculate the time in the session.\n=== Programming languages ===\n==== Java ====\nWhile most application software will use the underlying operating system for time zone and daylight saving time rule information, the Java Platform, from version 1.3.1, has maintained its own database of time zone and daylight saving time rule information. This database is updated whenever time zone or daylight saving time rules change. Oracle provides an updater tool for this purpose.\nAs an alternative to the information bundled with the Java Platform, programmers may choose to use the Joda-Time library. This library includes its own data based on the IANA time zone database.\nAs of Java 8 there is a new date and time API that can help with converting times.\n==== JavaScript ====\nTraditionally, there was very little in the way of time zone support for JavaScript. Essentially the programmer had to extract the UTC offset by instantiating a time object, getting a GMT time from it, and differencing the two. This does not provide a solution for more complex daylight saving variations, such as divergent DST directions between northern and southern hemispheres.\nECMA-402, the standard on Internationalization API for JavaScript, provides ways of formatting Time Zones. However, due to size constraint, some implementations or distributions do not include it.\n==== Perl ====\nThe DateTime object in Perl supports all entries in the IANA time zone database and includes the ability to get, set and convert between time zones.\n==== PHP ====\nThe DateTime objects and related functions have been compiled into the PHP core since 5.2. This includes the ability to get and set the default script time zone, and DateTime is aware of its own time zone internally. PHP.net provides extensive documentation on this. As noted there, the most current time zone database can be implemented via the PECL timezonedb.\n==== Python ====\nThe standard module datetime included with Python stores and operates on the time zone information class tzinfo. The third party pytz module provides access to the full IANA time zone database. Negated time zone offset in seconds is stored time.timezone and time.altzone attributes. From Python 3.9, the zoneinfo module introduces timezone management without need for third party module.\n==== Smalltalk ====\nEach Smalltalk dialect comes with its own built-in classes for dates, times and timestamps, only a few of which implement the DateAndTime and Duration classes as specified by the ANSI Smalltalk Standard. VisualWorks provides a TimeZone class that supports up to two annually recurring offset transitions, which are assumed to apply to all years (same behavior as Windows time zones). Squeak provides a Timezone class that does not support any offset transitions. Dolphin Smalltalk does not support time zones at all.\nFor full support of the tz database (zoneinfo) in a Smalltalk application (including support for any number of annually recurring offset transitions, and support for different intra-year offset transition rules in different years) the third-party, open-source, ANSI-Smalltalk-compliant Chronos Date/Time Library is available for use with any of the following Smalltalk dialects: VisualWorks, Squeak, Gemstone, or Dolphin.\n== Time in outer space ==\nOrbiting spacecraft may experience many sunrises and sunsets, or none, in a 24-hour period. Therefore, it is not possible to calibrate the time with respect to the Sun and still respect a 24-hour sleep/wake cycle. A common practice for space exploration is to use the Earth-based time of the launch site or mission control, synchronizing the sleeping cycles of the crew and controllers. The International Space Station normally uses Greenwich Mean Time (GMT).\nTimekeeping on Mars can be more complex, since the planet has a solar day of approximately 24 hours and 40 minutes, known as a sol. Earth controllers for some Mars missions have synchronized their sleep/wake cycles with the Martian day, when specifically solar-powered rover activity occurs.\n== See also ==\nJet lag\nLists of time zones\nMetric time\nTime by country\nTime in Europe\nAbolition of time zones – Replacing time zones with UTC\nWorld clock – Clock that displays the times in various locations around the globe\nInternational Date Line – Imaginary line that demarcates the change of one calendar day to the next\n== Notes ==\n== References ==\n== Sources ==\nAsimov, Isaac (1964). "Abbe, Cleveland". Asimov\'s Biographical Encyclopedia of Science and Technology: The Living Stories of More than 1000 Great Scientists from the Age of Greece to the Space Age. Garden City, NY: Doubleday & Company, Inc. pp. 343–344. LCCN 64016199.\nDebus, Allen G., ed. (1968). "Abbe, Cleveland". World Who\'s Who in Science: A Biographical Dictionary of Notable Scientists from Antiquity to the Present (1st ed.). Chicago, IL: A. N. Marquis Company. ISBN 0-8379-1001-3. LCCN 68056149.\n== Further reading ==\nBiswas, Soutik (February 12, 2019). "How India\'s single time zone is hurting its people". BBC News. Retrieved February 12, 2019.\nMaulik Jagnani, economist at Cornell University (January 15, 2019). "Poor Sleep: Sunset Time and Human Capital Production" (Job Market Paper). Retrieved April 28, 2025.\n"Time Bandits: The countries rebelling against GMT" (Video). BBC News. August 14, 2015. Retrieved February 12, 2019.\n"How time zones confused the world". BBC News. August 7, 2015. Retrieved February 12, 2019.\nLane, Megan (May 10, 2011). "How does a country change its time zone?". BBC News. Retrieved February 12, 2019.\n"A brief history of time zones" (Video). BBC News. March 24, 2011. Retrieved February 12, 2019.\nThe Time Zone Information Format (TZif). doi:10.17487/RFC8536. RFC 8536.\n== External links ==\nMedia related to Time zones at Wikimedia Commons', 'Time management is the organization of tasks or events by first estimating how much time a task requires and when it must be completed, and adjusting events that would interfere with its completion so it is done in the appropriate amount of time. Calendars and day planners are common examples of time management tools.\n== Sequence of events ==\nA sequence of events, or series of events, is a sequence of items, facts, events, actions, changes, or procedural steps, arranged in time order (chronological order), often with causality relationships among the items. Because of causality, cause precedes effect, or cause and effect may appear together in a single item, but effect never precedes cause. A sequence of events can be presented in text, tables, charts, or timelines. The description of the items or events may include a timestamp. A sequence of events that includes the time along with place or location information to describe a sequential path may be referred to as a world line.\nUses of a sequence of events include stories, historical events (chronology), directions and steps in procedures, and timetables for scheduling activities. A sequence of events may also be used to help describe processes in science, technology, and medicine. A sequence of events may be focused on past events (e.g., stories, history, chronology), on future events that must be in a predetermined order (e.g., plans, schedules, procedures, timetables), or focused on the observation of past events with the expectation that the events will occur in the future (e.g., processes, projections). The use of a sequence of events occurs in fields as diverse as machines (cam timer), documentaries (Seconds From Disaster), law (choice of law), finance (directional-change intrinsic time), computer simulation (discrete event simulation), and electric power transmission (sequence of events recorder). A specific example of a sequence of events is the timeline of the Fukushima Daiichi nuclear disaster.\n== See also ==\nList of UTC timing centers\nLoschmidt\'s paradox\nTime metrology\n=== Organizations ===\nAntiquarian Horological Society – AHS (United Kingdom)\nChronometrophilia (Switzerland)\nDeutsche Gesellschaft für Chronometrie – DGC (Germany)\nNational Association of Watch and Clock Collectors – NAWCC (United States)\n== References ==\n== Further reading ==\n== External links ==\nDifferent systems of measuring time (archived 16 October 2015).\nTime on In Our Time at the BBC.\n"Time". Merriam-Webster.com Dictionary. Merriam-Webster.\nTime in the Internet Encyclopedia of Philosophy, by Bradley Dowden.\nLe Poidevin, Robin (Winter 2004). "The Experience and Perception of Time". In Edward N. Zalta (ed.). The Stanford Encyclopedia of Philosophy. Retrieved 9 April 2011.\nTime Expansion Experiences: Time may just be a creation of our minds by Steve Taylor Ph.D. January 31, 2025, Psychology Today.', 'Great advances in accurate time-keeping were made by Galileo Galilei and especially Christiaan Huygens with the invention of pendulum-driven clocks along with the invention of the minute hand by Jost Burgi. There is also a clock that was designed to keep time for 10,000 years called the Clock of the Long Now. Alarm clock devices were later mechanized. Levi Hutchins\'s alarm clock has been credited as the first American alarm clock, though it can only ring at 4 a.m. Antoine Redier was also credited as the first person to patent an adjustable mechanical alarm clock in 1847. Digital forms of alarm clocks became more accessible through digitization and integration with other technologies, such as smartphones.\nThe most accurate timekeeping devices are atomic clocks, which are accurate to seconds in many millions of years, and are used to calibrate other clocks and timekeeping instruments. Atomic clocks use the frequency of electronic transitions in certain atoms to measure the second. One of the atoms used is caesium; most modern atomic clocks probe caesium with microwaves to determine the frequency of these electron vibrations. Since 1967, the International System of Measurements bases its unit of time, the second, on the properties of caesium atoms. SI defines the second as 9,192,631,770 cycles of the radiation that corresponds to the transition between two electron spin energy levels of the ground state of the 133Cs atom. A portable timekeeper that meets certain precision standards is called a chronometer. Initially, the term was used to refer to the marine chronometer, a timepiece used to determine longitude by means of celestial navigation, a precision first achieved by John Harrison. More recently, the term has also been applied to the chronometer watch, a watch that meets precision standards set by the Swiss agency COSC.\nIn modern times, the Global Positioning System in coordination with the Network Time Protocol can be used to synchronize timekeeping systems across the globe. As of May 2010, the smallest time interval uncertainty in direct measurements is on the order of 12 attoseconds (1.2 × 10−17 seconds), about 3.7 × 1026 Planck times. The time measured was the delay caused by out-of-sync electron waves\' interference patterns.\n=== Units ===\nThe second (s) is the SI base unit. A minute (min) is 60 seconds in length (or, rarely, 59 or 61 seconds when leap seconds are employed), and an hour is 60 minutes or 3600 seconds in length. A day is usually 24 hours or 86,400 seconds in length; however, the duration of a calendar day can vary due to daylight saving time and leap seconds.\n== Time standards ==\nA time standard is a specification for measuring time: assigning a number or calendar date to an instant (point in time), quantifying the duration of a time interval, and establishing a chronology (ordering of events). In modern times, several time specifications have been officially recognized as standards, where formerly they were matters of custom and practice. The invention in 1955 of the caesium atomic clock has led to the replacement of older and purely astronomical time standards such as sidereal time and ephemeris time, for most practical purposes, by newer time standards based wholly or partly on atomic time using the SI second.\nInternational Atomic Time (TAI) is the primary international time standard from which other time standards are calculated. Universal Time (UT1) is mean solar time at 0° longitude, computed from astronomical observations. It varies from TAI because of the irregularities in Earth\'s rotation. Coordinated Universal Time (UTC) is an atomic time scale designed to approximate Universal Time. UTC differs from TAI by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the leap second. The Global Positioning System broadcasts a very precise time signal based on UTC time.\nThe surface of the Earth is split into a number of time zones. Standard time or civil time in a time zone deviates a fixed, round amount, usually a whole number of hours, from some form of Universal Time, usually UTC. Most time zones are exactly one hour apart, and by convention compute their local time as an offset from UTC. For example, time zones at sea are based on UTC. In many locations (but not at sea) these offsets vary twice yearly due to daylight saving time transitions.\nSome other time standards are used mainly for scientific work. Terrestrial Time is a theoretical ideal scale realized by TAI. Geocentric Coordinate Time and Barycentric Coordinate Time are scales defined as coordinate times in the context of the general theory of relativity, with TCG applying to Earth\'s center and TCB to the solar system\'s barycenter. Barycentric Dynamical Time is an older relativistic scale related to TCB that is still in use.\n== Philosophy ==\n=== Religion ===\n==== Cyclical views of time ====\nMany ancient cultures, particularly in the East, had a cyclical view of time. In these traditions, time was often seen as a recurring pattern of ages or cycles, where events and phenomena repeated themselves in a predictable manner. One of the most famous examples of this concept is found in Hindu philosophy, where time is depicted as a wheel called the "Kalachakra" or "Wheel of Time." According to this belief, the universe undergoes endless cycles of creation, preservation, and destruction.\nSimilarly, in other ancient cultures such as those of the Mayans, Aztecs, and Chinese, there were also beliefs in cyclical time, often associated with astronomical observations and calendars. These cultures developed complex systems to track time, seasons, and celestial movements, reflecting their understanding of cyclical patterns in nature and the universe.\nThe cyclical view of time contrasts with the linear concept of time more common in Western thought, where time is seen as progressing in a straight line from past to future without repetition.\n==== Time in Abrahamic religions ====\nIn general, the Islamic and Judeo-Christian world-view regards time as linear and directional, beginning with the act of creation by God. The traditional Christian view sees time ending, teleologically, with the eschatological end of the present order of things, the "end time". Though some Christian theologians (such as Augustine of Hippo and Aquinas) believe that God is outside of time, seeing all events simultaneously, that time did not exist before God, and that God created time.\nIn the Old Testament book Ecclesiastes, traditionally ascribed to Solomon (970–928 BC), time is depicted as cyclical and beyond human control. The book wrote that there is an appropriate season or time for every activity.\n==== Time in Greek mythology ====\nThe Greek language denotes two distinct principles, Chronos and Kairos. The former refers to numeric, or chronological, time. The latter, literally "the right or opportune moment", relates specifically to metaphysical or Divine time. In theology, Kairos is qualitative, as opposed to quantitative.\nIn Greek mythology, Chronos (ancient Greek: Χρόνος) is identified as the personification of time. His name in Greek means "time" and is alternatively spelled Chronus (Latin spelling) or Khronos. Chronos is usually portrayed as an old, wise man with a long, gray beard, such as "Father Time". Some English words whose etymological root is khronos/chronos include chronology, chronometer, chronic, anachronism, synchronise, and chronicle.\n==== Time in Kabbalah & Rabbinical thought ====\nRabbis sometimes saw time like "an accordion that was expanded and collapsed at will." According to Kabbalists, "time" is a paradox and an illusion.\n==== Time in Advaita Vedanta ====\nAccording to Advaita Vedanta, time is integral to the phenomenal world, which lacks independent reality. Time and the phenomenal world are products of maya, influenced by our senses, concepts, and imaginations. The phenomenal world, including time, is seen as impermanent and characterized by plurality, suffering, conflict, and division. Since phenomenal existence is dominated by temporality (kala), everything within time is subject to change and decay. Overcoming pain and death requires knowledge that transcends temporal existence and reveals its eternal foundation.\n=== In Western philosophy ===\nTwo contrasting viewpoints on time divide prominent philosophers. One view is that time is part of the fundamental structure of the universe—a dimension independent of events, in which events occur in sequence. Isaac Newton subscribed to this realist view, and hence it is sometimes referred to as Newtonian time.\nThe opposing view is that time does not refer to any kind of "container" that events and objects "move through", nor to any entity that "flows", but that it is instead part of a fundamental intellectual structure (together with space and number) within which humans sequence and compare events. This second view, in the tradition of Gottfried Leibniz and Immanuel Kant, holds that time is neither an event nor a thing, and thus is not itself measurable nor can it be travelled. Furthermore, it may be that there is a subjective component to time, but whether or not time itself is "felt", as a sensation, or is a judgment, is a matter of debate.', '=== Arrow of time ===\nUnlike space, where an object can travel in the opposite directions (and in 3 dimensions), time appears to have only one dimension and only one direction—the past lies behind, fixed and immutable, while the future lies ahead and is not necessarily fixed. Yet most laws of physics allow any process to proceed both forward and in reverse. There are only a few physical phenomena that violate the reversibility of time. This time directionality is known as the arrow of time. Acknowledged examples of the arrow of time are:\nRadiative arrow of time, manifested in waves (e.g., light and sound) travelling only expanding (rather than focusing) in time (see light cone);\nEntropic arrow of time: according to the second law of thermodynamics an isolated system evolves toward a larger disorder rather than orders spontaneously;\nQuantum arrow time, which is related to irreversibility of measurement in quantum mechanics according to the Copenhagen interpretation of quantum mechanics;\nWeak arrow of time: preference for a certain time direction of weak force in particle physics (see violation of CP symmetry);\nCosmological arrow of time, which follows the accelerated expansion of the Universe after the Big Bang.\nThe relationships between these different arrows of time is a hotly debated topic in theoretical physics.\nThe second law of thermodynamics states that entropy must increase over time. Brian Greene theorizes that, according to the equations, the change in entropy occurs symmetrically whether going forward or backward in time. So entropy tends to increase in either direction, and our current low-entropy universe is a statistical aberration, in a similar manner as tossing a coin often enough that eventually heads will result ten times in a row. However, this theory is not supported empirically in local experiment.\n=== Classical mechanics ===\nIn non-relativistic classical mechanics, Newton\'s concept of "relative, apparent, and common time" can be used in the formulation of a prescription for the synchronization of clocks. Events seen by two different observers in motion relative to each other produce a mathematical concept of time that works sufficiently well for describing the everyday phenomena of most people\'s experience. In the late nineteenth century, physicists encountered problems with the classical understanding of time, in connection with the behavior of electricity and magnetism. The 1860s Maxwell\'s equations described that light always travels at a constant speed (in a vacuum). However, classical mechanics assumed that motion was measured relative to a fixed reference frame. The Michelson–Morley experiment contradicted the assumption. Einstein later proposed a method of synchronizing clocks using the constant, finite speed of light as the maximum signal velocity. This led directly to the conclusion that observers in motion relative to one another measure different elapsed times for the same event.\n=== Spacetime ===\nTime has historically been closely related with space, the two together merging into spacetime in Einstein\'s special relativity and general relativity. According to these theories, the concept of time depends on the spatial reference frame of the observer, and the human perception, as well as the measurement by instruments such as clocks, are different for observers in relative motion. For example, if a spaceship carrying a clock flies through space at (very nearly) the speed of light, its crew does not notice a change in the speed of time on board their vessel because everything traveling at the same speed slows down at the same rate (including the clock, the crew\'s thought processes, and the functions of their bodies). However, to a stationary observer watching the spaceship fly by, the spaceship appears flattened in the direction it is traveling and the clock on board the spaceship appears to move very slowly.\nOn the other hand, the crew on board the spaceship also perceives the observer as slowed down and flattened along the spaceship\'s direction of travel, because both are moving at very nearly the speed of light relative to each other. Because the outside universe appears flattened to the spaceship, the crew perceives themselves as quickly traveling between regions of space that (to the stationary observer) are many light years apart. This is reconciled by the fact that the crew\'s perception of time is different from the stationary observer\'s; what seems like seconds to the crew might be hundreds of years to the stationary observer. In either case, however, causality remains unchanged: the past is the set of events that can send light signals to an entity and the future is the set of events to which an entity can send light signals.\n=== Dilation ===\nEinstein showed in his thought experiments that people travelling at different speeds, while agreeing on cause and effect, measure different time separations between events, and can even observe different chronological orderings between non-causally related events. Though these effects are typically minute in the human experience, the effect becomes much more pronounced for objects moving at speeds approaching the speed of light. Subatomic particles exist for a well-known average fraction of a second in a lab relatively at rest, but when travelling close to the speed of light they are measured to travel farther and exist for much longer than when at rest.\nAccording to the special theory of relativity, in the high-speed particle\'s frame of reference, it exists, on the average, for a standard amount of time known as its mean lifetime, and the distance it travels in that time is zero, because its velocity is zero. Relative to a frame of reference at rest, time seems to "slow down" for the particle. Relative to the high-speed particle, distances seem to shorten. Einstein showed how both temporal and spatial dimensions can be altered (or "warped") by high-speed motion.\nEinstein (The Meaning of Relativity): "Two events taking place at the points A and B of a system K are simultaneous if they appear at the same instant when observed from the middle point, M, of the interval AB. Time is then defined as the ensemble of the indications of similar clocks, at rest relative to K, which register the same simultaneously." Einstein wrote in his book, Relativity, that simultaneity is also relative, i.e., two events that appear simultaneous to an observer in a particular inertial reference frame need not be judged as simultaneous by a second observer in a different inertial frame of reference.\nAccording to general relativity, time also runs slower in stronger gravitational fields; this is gravitational time dilation. The effect of the dilation becomes more noticeable in a mass-dense object. A famous example of time dilation is a thought experiment of a subject approaching the event horizon of a black hole. As a consequence of how gravitational fields warp spacetime, the subject will experience gravitational time dilation. From the perspective of the subject itself, they will experience time normally. Meanwhile, an observer from the outside will see the subject move closer to the black hole until the extreme, in which the subject appears \'frozen\' in time and eventually fade to nothingness due to the diminishing amount of light returning.\n=== Relativistic versus Newtonian ===\nThe animations visualise the different treatments of time in the Newtonian and the relativistic descriptions. At the heart of these differences are the Galilean and Lorentz transformations applicable in the Newtonian and relativistic theories, respectively. In the figures, the vertical direction indicates time. The horizontal direction indicates distance (only one spatial dimension is taken into account), and the thick dashed curve is the spacetime trajectory ("world line") of the observer. The small dots indicate specific (past and future) events in spacetime. The slope of the world line (deviation from being vertical) gives the relative velocity to the observer.\nIn the Newtonian description these changes are such that time is absolute: the movements of the observer do not influence whether an event occurs in the \'now\' (i.e., whether an event passes the horizontal line through the observer). However, in the relativistic description the observability of events is absolute: the movements of the observer do not influence whether an event passes the "light cone" of the observer. Notice that with the change from a Newtonian to a relativistic description, the concept of absolute time is no longer applicable: events move up and down in the figure depending on the acceleration of the observer.\n=== Quantization ===\nTime quantization refers to the theory that time has the smallest possible unit. Time quantization is a hypothetical concept. In the modern established physical theories like the Standard Model of particle physics and general relativity time is not quantized. Planck time (~ 5.4 × 10−44 seconds) is the unit of time in the system of natural units known as Planck units. Current established physical theories are believed to fail at this time scale, and many physicists expect that the Planck time might be the smallest unit of time that could ever be measured, even in principle. Though tentative physical theories that attempt to describe phenomena at this scale exist; an example is loop quantum gravity. Loop quantum gravity suggests that time is quantized; if gravity is quantized, spacetime is also quantized.\n== Travel ==', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Ultraviolet radiation, also known as simply UV, is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights.\nThe photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms.:\u200a25–26\u200a  Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature.:\u200a28\u200a  Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact.\nFor humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength "extreme" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life.\nThe lower wavelength limit of the visible spectrum is conventionally taken as 400 nm.  Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range.  Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.\n== Visibility ==\nUltraviolet rays are not usable for normal human vision.\nThe lens of the human eye and surgically implanted lens produced since 1986 blocks most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy.\nPeople lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet.  Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds "true" UV vision.\n== History and discovery ==\n"Ultraviolet" means "beyond violet" (from Latin ultra, "beyond"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light.\nUV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them "(de-)oxidizing rays" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from "heat rays", discovered the previous year at the other end of the visible spectrum. The simpler term "chemical rays" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them "tithonic rays"). The terms "chemical rays" and "heat rays" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established.\nThe discovery of the ultraviolet radiation with wavelengths below 200 nm, named "vacuum ultraviolet" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided "unanimously" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen.\n== Subtypes ==\nThe electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348:\nSeveral solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum.\nVacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers.\nTechnology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes.\nExtreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer  satellite.\nSome sources use the distinction of "hard UV" and "soft UV". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with "hard UV" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions.\n== Solar ultraviolet ==\nVery hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth\'s atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum.\nThe atmosphere blocks about 77% of the Sun\'s UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth\'s surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth\'s surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On "partly cloudy" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB.']

Question: What is the relationship between Coordinated Universal Time (UTC) and Universal Time (UT1)?

Choices:
Choice A) UTC and Universal Time (UT1) are identical time scales that are used interchangeably in science and engineering.
Choice B) UTC is a time scale that is completely independent of Universal Time (UT1). UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the "leap second".
Choice C) UTC is an atomic time scale designed to approximate Universal Time (UT1), but it differs from UT1 by a non-integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the "leap second".
Choice D) UTC is an atomic time scale designed to approximate Universal Time (UT1), but it differs from UT1 by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the "leap second".
Choice E) UTC is a time scale that is based on the irregularities in Earth's rotation and is completely independent of Universal Time (UT1).

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ["== Importance ==\nFrom the perspective of a planetary geologist, the atmosphere acts to shape a planetary surface. Wind picks up dust and other particles which, when they collide with the terrain, erode the relief and leave deposits (eolian processes). Frost and precipitations, which depend on the atmospheric composition, also influence the relief. Climate changes can influence a planet's geological history. Conversely, studying the surface of the Earth leads to an understanding of the atmosphere and climate of other planets.\nFor a meteorologist, the composition of the Earth's atmosphere is a factor affecting the climate and its variations.\nFor a biologist or paleontologist, the Earth's atmospheric composition is closely dependent on the appearance of life and its evolution.\n== See also ==\nAtmometer (evaporimeter)\nAtmospheric pressure\nInternational Standard Atmosphere\nKármán line\nSky\n== References ==\n== Further reading ==\nSanchez-Lavega, Agustin (2010). An Introduction to Planetary Atmospheres. Taylor & Francis. ISBN 978-1420067323.\n== External links ==\nProperties of atmospheric strata – The flight environment of the atmosphere\nAtmosphere – Everything you need to know", 'An atmosphere (from Ancient Greek  ἀτμός (atmós) \'vapour, steam\' and  σφαῖρα (sphaîra) \'sphere\') is a layer of gases that envelop an astronomical object, held in place by the gravity of the object. A planet retains an atmosphere when the gravity is great and the temperature of the atmosphere is low. A stellar atmosphere is the outer region of a star, which includes the layers above the opaque photosphere; stars of low temperature might have outer atmospheres containing compound molecules.\nThe atmosphere of Earth is composed of nitrogen (78%), oxygen (21%), argon (0.9%), carbon dioxide (0.04%) and trace gases. Most organisms use oxygen for respiration; lightning and bacteria perform nitrogen fixation which produces ammonia that is used to make nucleotides and amino acids; plants, algae, and cyanobacteria use carbon dioxide for photosynthesis. The layered composition of the atmosphere minimises the harmful effects of sunlight, ultraviolet radiation, solar wind, and cosmic rays and thus protects the organisms from genetic damage. The current composition of the atmosphere of the Earth is the product of billions of years of biochemical modification of the paleoatmosphere by living organisms.\n== Occurrence and compositions ==\n=== Origins ===\nAtmospheres are clouds of gas bound to and engulfing an astronomical focal point of sufficiently dominating mass, adding to its mass, possibly escaping from it or collapsing into it.\nBecause of the latter, such planetary nucleus can develop from interstellar molecular clouds or protoplanetary disks into rocky astronomical objects with varyingly thick atmospheres, gas giants or fusors.\nComposition and thickness is originally determined by the stellar nebula\'s chemistry and temperature, but can also by a product processes within the astronomical body outgasing a different atmosphere.\n=== Compositions ===\nThe atmospheres of the planets Venus and Mars are principally composed of carbon dioxide and nitrogen, argon and oxygen.\nThe composition of Earth\'s atmosphere is determined by the by-products of the life that it sustains. Dry air (mixture of gases) from Earth\'s atmosphere contains 78.08% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and traces of hydrogen, helium, and other "noble" gases (by volume), but generally a variable amount of water vapor is also present, on average about 1% at sea level.\nThe low temperatures and higher gravity of the Solar System\'s giant planets—Jupiter, Saturn, Uranus and Neptune—allow them more readily to retain gases with low molecular masses. These planets have hydrogen–helium atmospheres, with trace amounts of more complex compounds.\nTwo satellites of the outer planets possess significant atmospheres. Titan, a moon of Saturn, and Triton, a moon of Neptune, have atmospheres mainly of nitrogen. When in the part of its orbit closest to the Sun, Pluto has an atmosphere of nitrogen and methane similar to Triton\'s, but these gases are frozen when it is farther from the Sun.\nOther bodies within the Solar System have extremely thin atmospheres not in equilibrium. These include the Moon (sodium gas), Mercury (sodium gas), Europa (oxygen), Io (sulfur), and Enceladus (water vapor).\nThe first exoplanet whose atmospheric composition was determined is HD 209458b, a gas giant with a close orbit around a star in the constellation Pegasus. Its atmosphere is heated to temperatures over 1,000 K, and is steadily escaping into space. Hydrogen, oxygen, carbon and sulfur have been detected in the planet\'s inflated atmosphere.\n=== Atmospheres in the Solar System ===\nAtmosphere of the Sun\nAtmosphere of Mercury\nAtmosphere of Venus\nAtmosphere of Earth\nAtmosphere of the Moon\nAtmosphere of Mars\nAtmosphere of Ceres\nAtmosphere of Jupiter\nAtmosphere of Io\nAtmosphere of Callisto\nAtmosphere of Europa\nAtmosphere of Ganymede\nAtmosphere of Saturn\nAtmosphere of Titan\nAtmosphere of Enceladus\nAtmosphere of Uranus\nAtmosphere of Titania\nAtmosphere of Neptune\nAtmosphere of Triton\nAtmosphere of Pluto\n== Structure of atmosphere ==\n=== Earth ===\nThe atmosphere of Earth is composed of layers with different properties, such as specific gaseous composition, temperature, and pressure.\nThe troposphere is the lowest layer of the atmosphere. This extends from the planetary surface to the bottom of the stratosphere. The troposphere contains 75–80% of the mass of the atmosphere, and is the atmospheric layer wherein the weather occurs; the height of the troposphere varies between 17 km at the equator and 7.0 km at the poles.\nThe stratosphere extends from the top of the troposphere to the bottom of the mesosphere, and contains the ozone layer, at an altitude between 15 km and 35 km. It is the atmospheric layer that absorbs most of the ultraviolet radiation that Earth receives from the Sun.\nThe mesosphere ranges from 50 km to 85 km and is the layer wherein most meteors are incinerated before reaching the surface.\nThe thermosphere extends from an altitude of 85 km to the base of the exosphere at 690 km and contains the ionosphere, where solar radiation ionizes the atmosphere. The density of the ionosphere is greater at short distances from the planetary surface in the daytime and decreases as the ionosphere rises at night-time, thereby allowing a greater range of radio frequencies to travel greater distances.\nThe exosphere begins at 690 to 1,000 km from the surface, and extends to roughly 10,000 km, where it interacts with the magnetosphere of Earth.\n== Pressure ==\nAtmospheric pressure is the force (per unit-area) perpendicular to a unit-area of planetary surface, as determined by the weight of the vertical column of atmospheric gases. In said atmospheric model, the atmospheric pressure, the weight of the mass of the gas, decreases at high altitude because of the diminishing mass of the gas above the point of barometric measurement. The units of air pressure are based upon the standard atmosphere (atm), which is 101,325 Pa (equivalent to 760 Torr or 14.696 psi). The height at which the atmospheric pressure declines by a factor of e (an irrational number equal to 2.71828) is called the scale height (H). For an atmosphere of uniform temperature, the scale height is proportional to the atmospheric temperature and is inversely proportional to the product of the mean molecular mass of dry air, and the local acceleration of gravity at the point of barometric measurement.\n== Escape ==\nSurface gravity differs significantly among the planets. For example, the large gravitational force of the giant planet Jupiter retains light gases such as hydrogen and helium that escape from objects with lower gravity. Secondly, the distance from the Sun determines the energy available to heat atmospheric gas to the point where some fraction of its molecules\' thermal motion exceed the planet\'s escape velocity, allowing those to escape a planet\'s gravitational grasp. Thus, distant and cold Titan, Triton, and Pluto are able to retain their atmospheres despite their relatively low gravities.\nSince a collection of gas molecules may be moving at a wide range of velocities, there will always be some fast enough to produce a slow leakage of gas into space. Lighter molecules move faster than heavier ones with the same thermal kinetic energy, and so gases of low molecular weight are lost more rapidly than those of high molecular weight. It is thought that Venus and Mars may have lost much of their water when, after being photodissociated into hydrogen and oxygen by solar ultraviolet radiation, the hydrogen escaped. Earth\'s magnetic field helps to prevent this, as, normally, the solar wind would greatly enhance the escape of hydrogen. However, over the past 3 billion years Earth may have lost gases through the magnetic polar regions due to auroral activity, including a net 2% of its atmospheric oxygen. The net effect, taking the most important escape processes into account, is that an intrinsic magnetic field does not protect a planet from atmospheric escape and that for some magnetizations the presence of a magnetic field works to increase the escape rate.\nOther mechanisms that can cause atmosphere depletion are solar wind-induced sputtering, impact erosion, weathering, and sequestration—sometimes referred to as "freezing out"—into the regolith and polar caps.\n== Terrain ==\nAtmospheres have dramatic effects on the surfaces of rocky bodies. Objects that have no atmosphere, or that have only an exosphere, have terrain that is covered in craters. Without an atmosphere, the planet has no protection from meteoroids, and all of them collide with the surface as meteorites and create craters.\nFor planets with a significant atmosphere, most meteoroids burn up as meteors before hitting a planet\'s surface. When meteoroids do impact, the effects are often erased by the action of wind.\nWind erosion is a significant factor in shaping the terrain of rocky planets with atmospheres, and over time can erase the effects of both craters and volcanoes. In addition, since liquids cannot exist without pressure, an atmosphere allows liquid to be present at the surface, resulting in lakes, rivers and oceans. Earth and Titan are known to have liquids at their surface and terrain on the planet suggests that Mars had liquid on its surface in the past.\n=== Outside the Solar System ===\nAtmosphere of HD 209458 b\n== Circulation ==\nThe circulation of the atmosphere occurs due to thermal differences when convection becomes a more efficient transporter of heat than thermal radiation. On planets where the primary heat source is solar radiation, excess heat in the tropics is transported to higher latitudes. When a planet generates a significant amount of heat internally, such as is the case for Jupiter, convection in the atmosphere can transport thermal energy from the higher temperature interior up to the surface.\n== Importance ==', 'In 1912, Vesto M. Slipher made spectrographic studies of the brightest spiral nebulae to determine their composition. Slipher discovered that the spiral nebulae have high Doppler shifts, indicating that they are moving at a rate exceeding the velocity of the stars he had measured. He found that the majority of these nebulae are moving away from us.\nIn 1917, Heber Doust Curtis observed nova S Andromedae within the "Great Andromeda Nebula", as the Andromeda Galaxy, Messier object M31, was then known. Searching the photographic record, he found 11 more novae. Curtis noticed that these novae were, on average, 10 magnitudes fainter than those that occurred within this galaxy. As a result, he was able to come up with a distance estimate of 150,000 parsecs. He became a proponent of the so-called "island universes" hypothesis, which holds that spiral nebulae are actually independent galaxies.\nIn 1920 a debate took place between Harlow Shapley and Heber Curtis, the Great Debate, concerning the nature of the Milky Way, spiral nebulae, and the dimensions of the universe. To support his claim that the Great Andromeda Nebula is an external galaxy, Curtis noted the appearance of dark lanes resembling the dust clouds in the Milky Way, as well as the significant Doppler shift.\nIn 1922, the Estonian astronomer Ernst Öpik gave a distance determination that supported the theory that the Andromeda Nebula is indeed a distant extra-galactic object. Using the new 100-inch Mt. Wilson telescope, Edwin Hubble was able to resolve the outer parts of some spiral nebulae as collections of individual stars and identified some Cepheid variables, thus allowing him to estimate the distance to the nebulae: they were far too distant to be part of the Milky Way. In 1926 Hubble produced a classification of galactic morphology that is used to this day.\n=== Multi-wavelength observation ===\nAdvances in astronomy have always been driven by technology. After centuries of success in optical astronomy, recent decades have seen major progress in other regions of the electromagnetic spectrum.\nThe dust present in the interstellar medium is opaque to visual light. It is more transparent to far-infrared, which can be used to observe the interior regions of giant molecular clouds and galactic cores in great detail. Infrared is also used to observe distant, red-shifted galaxies that were formed much earlier. Water vapor and carbon dioxide absorb a number of useful portions of the infrared spectrum, so high-altitude or space-based telescopes are used for infrared astronomy.\nThe first non-visual study of galaxies, particularly active galaxies, was made using radio frequencies. The Earth\'s atmosphere is nearly transparent to radio between 5 MHz and 30 GHz. The ionosphere blocks signals below this range. Large radio interferometers have been used to map the active jets emitted from active nuclei.\nUltraviolet and X-ray telescopes can observe highly energetic galactic phenomena. Ultraviolet flares are sometimes observed when a star in a distant galaxy is torn apart from the tidal forces of a nearby black hole. The distribution of hot gas in galactic clusters can be mapped by X-rays. The existence of supermassive black holes at the cores of galaxies was confirmed through X-ray astronomy.\n=== Modern research ===\nIn 1944, Hendrik van de Hulst predicted that microwave radiation with wavelength of 21 cm would be detectable from interstellar atomic hydrogen gas; and in 1951 it was observed. This radiation is not affected by dust absorption, and so its Doppler shift can be used to map the motion of the gas in this galaxy. These observations led to the hypothesis of a rotating bar structure in the center of this galaxy. With improved radio telescopes, hydrogen gas could also be traced in other galaxies.\nIn the 1970s, Vera Rubin uncovered a discrepancy between observed galactic rotation speed and that predicted by the visible mass of stars and gas. Today, the galaxy rotation problem is thought to be explained by the presence of large quantities of unseen dark matter.\nBeginning in the 1990s, the Hubble Space Telescope yielded improved observations. Among other things, its data helped establish that the missing dark matter in this galaxy could not consist solely of inherently faint and small stars. The Hubble Deep Field, an extremely long exposure of a relatively empty part of the sky, provided evidence that there are about 125 billion (1.25×1011) galaxies in the observable universe. Improved technology in detecting the spectra invisible to humans (radio telescopes, infrared cameras, and x-ray telescopes) allows detection of other galaxies that are not detected by Hubble. Particularly, surveys in the Zone of Avoidance (the region of sky blocked at visible-light wavelengths by the Milky Way) have revealed a number of new galaxies.\nA 2016 study published in The Astrophysical Journal, led by Christopher Conselice of the University of Nottingham, analyzed many sources of data to estimate that the observable universe (up to z=8) contained at least two trillion (2×1012) galaxies, a factor of 10 more than are directly observed in Hubble images.:\u200a12\u200a However, later observations with the New Horizons space probe from outside the zodiacal light observed less cosmic optical light than Conselice while still suggesting that direct observations are missing galaxies.\n== Types and morphology ==\nGalaxies come in three main types: ellipticals, spirals, and irregulars. A slightly more extensive description of galaxy types based on their appearance is given by the Hubble sequence. Since the Hubble sequence is entirely based upon visual morphological type (shape), it may miss certain important characteristics of galaxies such as star formation rate in starburst galaxies and activity in the cores of active galaxies.\nMany galaxies are thought to contain a supermassive black hole at their center. This includes the Milky Way, whose core region is called the Galactic Center.\n=== Ellipticals ===\nThe Hubble classification system rates elliptical galaxies on the basis of their ellipticity, ranging from E0, being nearly spherical, up to E7, which is highly elongated. These galaxies have an ellipsoidal profile, giving them an elliptical appearance regardless of the viewing angle. Their appearance shows little structure and they typically have relatively little interstellar matter. Consequently, these galaxies also have a low portion of open clusters and a reduced rate of new star formation. Instead, they are dominated by generally older, more evolved stars that are orbiting the common center of gravity in random directions. The stars contain low abundances of heavy elements because star formation ceases after the initial burst. In this sense they have some similarity to the much smaller globular clusters.\n==== Type-cD galaxies ====\nThe largest galaxies are the type-cD galaxies.\nFirst described in 1964 by a paper by Thomas A. Matthews and others, they are a subtype of the more general class of D galaxies, which are giant elliptical galaxies, except that they are much larger. They are popularly known as the supergiant elliptical galaxies and constitute the largest and most luminous galaxies known. These galaxies feature a central elliptical nucleus with an extensive, faint halo of stars extending to megaparsec scales. The profile of their surface brightnesses as a function of their radius (or distance from their cores) falls off more slowly than their smaller counterparts.\nThe formation of these cD galaxies remains an active area of research, but the leading model is that they are the result of the mergers of smaller galaxies in the environments of dense clusters, or even those outside of clusters with random overdensities. These processes are the mechanisms that drive the formation of fossil groups or fossil clusters, where a large, relatively isolated, supergiant elliptical resides in the middle of the cluster and are surrounded by an extensive cloud of X-rays as the residue of these galactic collisions. Another older model posits the phenomenon of cooling flow, where the heated gases in clusters collapses towards their centers as they cool, forming stars in the process, a phenomenon observed in clusters such as Perseus, and more recently in the Phoenix Cluster.\n==== Shell galaxy ====\nA shell galaxy is a type of elliptical galaxy where the stars in its halo are arranged in concentric shells. About one-tenth of elliptical galaxies have a shell-like structure, which has never been observed in spiral galaxies. These structures are thought to develop when a larger galaxy absorbs a smaller companion galaxy—that as the two galaxy centers approach, they start to oscillate around a center point, and the oscillation creates gravitational ripples forming the shells of stars, similar to ripples spreading on water. For example, galaxy NGC 3923 has over 20 shells.\n=== Spirals ===\nSpiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept.', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', '=== Progenitor ===\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic type Ic-BL events that are associated with long-duration gamma-ray bursts.\n== External impact ==\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\n=== Source of heavy elements ===\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\n=== Role in stellar evolution ===\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n=== Cosmic rays ===\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\n=== Gravitational waves ===', "==== Red-giant-branch phase ====\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star.  For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower 12C/13C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\nThe helium core continues to grow on the red-giant branch.  It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase.  The star increases in luminosity towards the tip of the red-giant branch.  Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n==== Horizontal branch ====\nIn the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 108 times the luminosity of the Sun for a few days and 1011 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning.  These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n==== Asymptotic-giant-branch phase ====\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time).  Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star.  Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically.  This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface.  This is known as the second dredge up, and in some stars there may even be a third dredge up.  In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra.  A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\nAnother well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths.  These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity.  These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n==== Post-AGB ====\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n=== Massive stars ===\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants.  Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.\n==== Supergiant evolution ====\nExtremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\nThe core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core.  In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process.  At the end of helium fusion, the core of a star consists primarily of carbon and oxygen.  In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium.  Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.\nThe exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉.  After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse.  Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning.  For a range of stars of approximately 8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova.", "When both rates of movement are known, the space velocity of the star relative to the Sun or the galaxy can be computed. Among nearby stars, it has been found that younger population I stars have generally lower velocities than older, population II stars. The latter have elliptical orbits that are inclined to the plane of the galaxy. A comparison of the kinematics of nearby stars has allowed astronomers to trace their origin to common points in giant molecular clouds; such groups with common points of origin are referred to as stellar associations.\n=== Magnetic field ===\nThe magnetic field of a star is generated within regions of the interior where convective circulation occurs. This movement of conductive plasma functions like a dynamo, wherein the movement of electrical charges induce magnetic fields, as does a mechanical dynamo. Those magnetic fields have a great range that extend throughout and beyond the star. The strength of the magnetic field varies with the mass and composition of the star, and the amount of magnetic surface activity depends upon the star's rate of rotation. This surface activity produces starspots, which are regions of strong magnetic fields and lower than normal surface temperatures. Coronal loops are arching magnetic field flux lines that rise from a star's surface into the star's outer atmosphere, its corona. The coronal loops can be seen due to the plasma they conduct along their length. Stellar flares are bursts of high-energy particles that are emitted due to the same magnetic activity.\nYoung, rapidly rotating stars tend to have high levels of surface activity because of their magnetic field. The magnetic field can act upon a star's stellar wind, functioning as a brake to gradually slow the rate of rotation with time. Thus, older stars such as the Sun have a much slower rate of rotation and a lower level of surface activity. The activity levels of slowly rotating stars tend to vary in a cyclical manner and can shut down altogether for periods of time. During the Maunder Minimum, for example, the Sun underwent a 70-year period with almost no sunspot activity.\n=== Mass ===\nStars have masses ranging from less than half the solar mass to over 200 solar masses (see List of most massive stars). One of the most massive stars known is Eta Carinae, which, with 100–150 times as much mass as the Sun, will have a lifespan of only several million years. Studies of the most massive open clusters suggests 150 M☉ as a rough upper limit for stars in the current era of the universe. This represents an empirical value for the theoretical limit on the mass of forming stars due to increasing radiation pressure on the accreting gas cloud. Several stars in the R136 cluster in the Large Magellanic Cloud have been measured with larger masses, but it has been determined that they could have been created through the collision and merger of massive stars in close binary systems, sidestepping the 150 M☉ limit on massive star formation.\nThe first stars to form after the Big Bang may have been larger, up to 300 M☉, due to the complete absence of elements heavier than lithium in their composition. This generation of supermassive population III stars is likely to have existed in the very early universe (i.e., they are observed to have a high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life. In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at z = 6.60.\nWith a mass only 80 times that of Jupiter (MJ), 2MASS J0523-1403 is the smallest known star undergoing nuclear fusion in its core. For stars with metallicity similar to the Sun, the theoretical minimum mass the star can have and still undergo fusion at the core, is estimated to be about 75 MJ. When the metallicity is very low, the minimum star size seems to be about 8.3% of the solar mass, or about 87 MJ. Smaller bodies called brown dwarfs, occupy a poorly defined grey area between stars and gas giants.\nThe combination of the radius and the mass of a star determines its surface gravity. Giant stars have a much lower surface gravity than do main sequence stars, while the opposite is the case for degenerate, compact stars such as white dwarfs. The surface gravity can influence the appearance of a star's spectrum, with higher gravity causing a broadening of the absorption lines.\n=== Rotation ===\nThe rotation rate of stars can be determined through spectroscopic measurement, or more exactly determined by tracking their starspots. Young stars can have a rotation greater than 100 km/s at the equator. The B-class star Achernar, for example, has an equatorial velocity of about 225 km/s or greater, causing its equator to bulge outward and giving it an equatorial diameter that is more than 50% greater than between the poles. This rate of rotation is just below the critical velocity of 300 km/s at which speed the star would break apart. By contrast, the Sun rotates once every 25–35 days depending on latitude, with an equatorial velocity of 1.93 km/s. A main sequence star's magnetic field and the stellar wind serve to slow its rotation by a significant amount as it evolves on the main sequence.\nDegenerate stars have contracted into a compact mass, resulting in a rapid rate of rotation. However they have relatively low rates of rotation compared to what would be expected by conservation of angular momentum—the tendency of a rotating body to compensate for a contraction in size by increasing its rate of spin. A large portion of the star's angular momentum is dissipated as a result of mass loss through the stellar wind. In spite of this, the rate of rotation for a pulsar can be very rapid. The pulsar at the heart of the Crab nebula, for example, rotates 30 times per second. The rotation rate of the pulsar will gradually slow due to the emission of radiation.\n=== Temperature ===\nThe surface temperature of a main sequence star is determined by the rate of energy production of its core and by its radius, and is often estimated from the star's color index. The temperature is normally given in terms of an effective temperature, which is the temperature of an idealized black body that radiates its energy at the same luminosity per surface area as the star. The effective temperature is only representative of the surface, as the temperature increases toward the core. The temperature in the core region of a star is several million kelvins.\nThe stellar temperature will determine the rate of ionization of various elements, resulting in characteristic absorption lines in the spectrum. The surface temperature of a star, along with its visual absolute magnitude and absorption features, is used to classify a star (see classification below).\nMassive main sequence stars can have surface temperatures of 50,000 K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600 K; but they have a high luminosity due to their large exterior surface area.\n== Radiation ==\nThe energy produced by stars, a product of nuclear fusion, radiates to space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind, which streams from the outer layers as electrically charged protons and alpha and beta particles. A steady stream of almost massless neutrinos emanate directly from the star's core.\nThe production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers.\nThe color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the star's outer layers, including its photosphere. Besides visible light, stars emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics.\nUsing the stellar spectrum, astronomers can determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is found, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.) With these parameters, astronomers can estimate the age of the star.\n=== Luminosity ===\nThe luminosity of a star is the amount of light and other forms of radiant energy it radiates per unit of time. It has units of power. The luminosity of a star is determined by its radius and surface temperature. Many stars do not radiate uniformly across their entire surface. The rapidly rotating star Vega, for example, has a higher energy flux (power per unit area) at its poles than along its equator.", "A supernova explosion blows away the star's outer layers, leaving a remnant such as the Crab Nebula. The core is compressed into a neutron star, which sometimes manifests itself as a pulsar or X-ray burster. In the case of the largest stars, the remnant is a black hole greater than 4 M☉. In a neutron star the matter is in a state known as neutron-degenerate matter, with a more exotic form of degenerate matter, QCD matter, possibly present in the core.\nThe blown-off outer layers of dying stars include heavy elements, which may be recycled during the formation of new stars. These heavy elements allow the formation of rocky planets. The outflow from supernovae and the stellar wind of large stars play an important part in shaping the interstellar medium.\n==== Binary stars ====\nBinary stars' evolution may significantly differ from that of single stars of the same mass. For example, when any star expands to become a red giant, it may overflow its Roche lobe, the surrounding region where material is gravitationally bound to it; if stars in a binary system are close enough, some of that material may overflow to the other star, yielding phenomena including contact binaries, common-envelope binaries, cataclysmic variables, blue stragglers, and type Ia supernovae. Mass transfer leads to cases such as the Algol paradox, where the most-evolved star in a system is the least massive.\nThe evolution of binary star and higher-order star systems is intensely researched since so many stars have been found to be members of binary systems. Around half of Sun-like stars, and an even higher proportion of more massive stars, form in multiple systems, and this may greatly influence such phenomena as novae and supernovae, the formation of certain types of star, and the enrichment of space with nucleosynthesis products.\nThe influence of binary star evolution on the formation of evolved massive stars such as luminous blue variables, Wolf–Rayet stars, and the progenitors of certain classes of core collapse supernova is still disputed. Single massive stars may be unable to expel their outer layers fast enough to form the types and numbers of evolved stars that are observed, or to produce progenitors that would explode as the supernovae that are observed. Mass transfer through gravitational stripping in binary systems is seen by some astronomers as the solution to that problem.\n== Distribution ==\nStars are not spread uniformly across the universe but are normally grouped into galaxies along with interstellar gas and dust. A typical large galaxy like the Milky Way contains hundreds of billions of stars. There are more than 2 trillion (1012) galaxies, though most are less than 10% the mass of the Milky Way. Overall, there are likely to be between 1022 and 1024 stars, which are more stars than all the grains of sand on planet Earth. Most stars are within galaxies, but between 10 and 50% of the starlight in large galaxy clusters may come from stars outside of any galaxy.\nA multi-star system consists of two or more gravitationally bound stars that orbit each other. The simplest and most common multi-star system is a binary star, but systems of three or more stars exist. For reasons of orbital stability, such multi-star systems are often organized into hierarchical sets of binary stars. Larger groups are called star clusters. These range from loose stellar associations with only a few stars to open clusters with dozens to thousands of stars, up to enormous globular clusters with hundreds of thousands of stars. Such systems orbit their host galaxy. The stars in an open or globular cluster all formed from the same giant molecular cloud, so all members normally have similar ages and compositions.\nMany stars are observed, and most or all may have originally formed in gravitationally bound, multiple-star systems. This is particularly true for very massive O and B class stars, 80% of which are believed to be part of multiple-star systems. The proportion of single star systems increases with decreasing star mass, so that only 25% of red dwarfs are known to have stellar companions. As 85% of all stars are red dwarfs, more than two thirds of stars in the Milky Way are likely single red dwarfs. In a 2017 study of the Perseus molecular cloud, astronomers found that most of the newly formed stars are in binary systems. In the model that best explained the data, all stars initially formed as binaries, though some binaries later split up and leave single stars behind.\nThe nearest star to the Earth, apart from the Sun, is Proxima Centauri, 4.2465 light-years (40.175 trillion kilometres) away. Travelling at the orbital speed of the Space Shuttle, 8 kilometres per second (29,000 kilometres per hour), it would take about 150,000 years to arrive. This is typical of stellar separations in galactic discs. Stars can be much closer to each other in the centres of galaxies and in globular clusters, or much farther apart in galactic halos.\nDue to the relatively vast distances between stars outside the galactic nucleus, collisions between stars are thought to be rare. In denser regions such as the core of globular clusters or the galactic center, collisions can be more common. Such collisions can produce what are known as blue stragglers. These abnormal stars have a higher surface temperature and thus are bluer than stars at the main sequence turnoff in the cluster to which they belong; in standard stellar evolution, blue stragglers would already have evolved off the main sequence and thus would not be seen in the cluster.\n== Characteristics ==\nAlmost everything about a star is determined by its initial mass, including such characteristics as luminosity, size, evolution, lifespan, and its eventual fate.\n=== Age ===\nMost stars are between 1 billion and 10 billion years old. Some stars may even be close to 13.8 billion years old—the observed age of the universe. The oldest star yet discovered, HD 140283, nicknamed Methuselah star, is an estimated 14.46 ± 0.8 billion years old. (Due to the uncertainty in the value, this age for the star does not conflict with the age of the universe, determined by the Planck satellite as 13.799 ± 0.021).\nThe more massive the star, the shorter its lifespan, primarily because massive stars have greater pressure on their cores, causing them to burn hydrogen more rapidly. The most massive stars last an average of a few million years, while stars of minimum mass (red dwarfs) burn their fuel very slowly and can last tens to hundreds of billions of years.\n=== Chemical composition ===\nWhen stars form in the present Milky Way galaxy, they are composed of about 71% hydrogen and 27% helium, as measured by mass, with a small fraction of heavier elements. Typically the portion of heavy elements is measured in terms of the iron content of the stellar atmosphere, as iron is a common element and its absorption lines are relatively easy to measure. The portion of heavier elements may be an indicator of the likelihood that the star has a planetary system.\nAs of 2005 the star with the lowest iron content ever measured is the dwarf HE1327-2326, with only 1/200,000th the iron content of the Sun. By contrast, the super-metal-rich star μ Leonis has nearly double the abundance of iron as the Sun, while the planet-bearing star 14 Herculis has nearly triple the iron. Chemically peculiar stars show unusual abundances of certain elements in their spectrum; especially chromium and rare earth elements. Stars with cooler outer atmospheres, including the Sun, can form various diatomic and polyatomic molecules.\n=== Diameter ===\nDue to their great distance from the Earth, all stars except the Sun appear to the unaided eye as shining points in the night sky that twinkle because of the effect of the Earth's atmosphere. The Sun is close enough to the Earth to appear as a disk instead, and to provide daylight. Other than the Sun, the star with the largest apparent size is R Doradus, with an angular diameter of only 0.057 arcseconds.\nThe disks of most stars are much too small in angular size to be observed with current ground-based optical telescopes, so interferometer telescopes are required to produce images of these objects. Another technique for measuring the angular size of stars is through occultation. By precisely measuring the drop in brightness of a star as it is occulted by the Moon (or the rise in brightness when it reappears), the star's angular diameter can be computed.\nStars range in size from neutron stars, which vary anywhere from 20 to 40 km (25 mi) in diameter, to supergiants like Betelgeuse in the Orion constellation, which has a diameter about 640 times that of the Sun with a much lower density.\n=== Kinematics ===\nThe motion of a star relative to the Sun can provide useful information about the origin and age of a star, as well as the structure and evolution of the surrounding galaxy. The components of motion of a star consist of the radial velocity toward or away from the Sun, and the traverse angular movement, which is called its proper motion.\nRadial velocity is measured by the doppler shift of the star's spectral lines and is given in units of km/s. The proper motion of a star, its parallax, is determined by precise astrometric measurements in units of milli-arc seconds (mas) per year. With knowledge of the star's parallax and its distance, the proper motion velocity can be calculated. Together with the radial velocity, the total velocity can be calculated. Stars with high rates of proper motion are likely to be relatively close to the Sun, making them good candidates for parallax measurements.", 'The occurrence of convection in the outer envelope of a main sequence star depends on the star\'s mass. Stars with several times the mass of the Sun have a convection zone deep within the interior and a radiative zone in the outer layers. Smaller stars such as the Sun are just the opposite, with the convective zone located in the outer layers. Red dwarf stars with less than 0.4 M☉ are convective throughout, which prevents the accumulation of a helium core. For most stars the convective zones will vary over time as the star ages and the constitution of the interior is modified.\nThe photosphere is that portion of a star that is visible to an observer. This is the layer at which the plasma of the star becomes transparent to photons of light. From here, the energy generated at the core becomes free to propagate into space. It is within the photosphere that sun spots, regions of lower than average temperature, appear.\nAbove the level of the photosphere is the stellar atmosphere. In a main sequence star such as the Sun, the lowest level of the atmosphere, just above the photosphere, is the thin chromosphere region, where spicules appear and stellar flares begin. Above this is the transition region, where the temperature rapidly increases within a distance of only 100 km (62 mi). Beyond this is the corona, a volume of super-heated plasma that can extend outward to several million kilometres. The existence of a corona appears to be dependent on a convective zone in the outer layers of the star. Despite its high temperature, the corona emits very little light, due to its low gas density. The corona region of the Sun is normally only visible during a solar eclipse.\nFrom the corona, a stellar wind of plasma particles expands outward from the star, until it interacts with the interstellar medium. For the Sun, the influence of its solar wind extends throughout a bubble-shaped region called the heliosphere.\n== Nuclear fusion reaction pathways ==\nWhen nuclei fuse, the mass of the fused product is less than the mass of the original parts. This lost mass is converted to electromagnetic energy, according to the mass–energy equivalence relationship\n{\\displaystyle E=mc^{2}}\n. A variety of nuclear fusion reactions take place in the cores of stars, that depend upon their mass and composition.\nThe hydrogen fusion process is temperature-sensitive, so a moderate increase in the core temperature will result in a significant increase in the fusion rate. As a result, the core temperature of main sequence stars only varies from 4 million kelvin for a small M-class star to 40 million kelvin for a massive O-class star.\nIn the Sun, with a 16-million-kelvin core, hydrogen fuses to form helium in the proton–proton chain reaction:\n41H → 22H + 2e+ + 2νe(2 x 0.4 MeV)\n2e+ + 2e− → 2γ (2 x 1.0 MeV)\n21H + 22H → 23He + 2γ (2 x 5.5 MeV)\n23He → 4He + 21H (12.9 MeV)\nThere are a couple other paths, in which 3He and 4He combine to form 7Be, which eventually (with the addition of another proton) yields two 4He, a gain of one.\nAll these reactions result in the overall reaction:\n41H → 4He + 2γ + 2νe (26.7 MeV)\nwhere γ is a gamma ray photon, νe is a neutrino, and H and He are isotopes of hydrogen and helium, respectively. The energy released by this reaction is in millions of electron volts. Each individual reaction produces only a tiny amount of energy, but because enormous numbers of these reactions occur constantly, they produce all the energy necessary to sustain the star\'s radiation output. In comparison, the combustion of two hydrogen gas molecules with one oxygen gas molecule releases only 5.7 eV.\nIn more massive stars, helium is produced in a cycle of reactions catalyzed by carbon called the carbon-nitrogen-oxygen cycle.\nIn evolved stars with cores at 100 million kelvin and masses between 0.5 and 10 M☉, helium can be transformed into carbon in the triple-alpha process that uses the intermediate element beryllium:\n4He + 4He + 92 keV → 8*Be\n4He + 8*Be + 67 keV → 12*C\n12*C → 12C + γ + 7.4 MeV\nFor an overall reaction of:\n34He → 12C + γ + 7.2 MeV\nIn massive stars, heavier elements can be burned in a contracting core through the neon-burning process and oxygen-burning process. The final stage in the stellar nucleosynthesis process is the silicon-burning process that results in the production of the stable isotope iron-56. Any further fusion would be an endothermic process that consumes energy, and so further energy can only be produced through gravitational collapse.\n== See also ==\nList of proper names of stars\nOutline of astronomy\nSidereal time\nStar clocks\nStar count\nStars in fiction\n== References ==\n== External links ==\n"How To Decipher Classification Codes". Astronomical Society of South Australia. Retrieved 20 August 2010.\nKaler, James. "Portraits of Stars and their Constellations". University of Illinois. Retrieved 20 August 2010.\nPickover, Cliff (2001). The Stars of Heaven. Oxford University Press. ISBN 978-0-19-514874-9.\nPrialnick, Dina; et al. (2001). "Stars: Stellar Atmospheres, Structure, & Evolution". University of St. Andrews. Archived from the original on 11 February 2021. Retrieved 20 August 2010.\n"Query star by identifier, coordinates or reference code". SIMBAD. Centre de Données astronomiques de Strasbourg. Retrieved 20 August 2010.']

Question: What is a planetary system?

Choices:
Choice A) A system of planets that are all located in the same solar system.
Choice B) A system of planets that are all the same size and shape.
Choice C) Any set of gravitationally bound non-stellar objects in or out of orbit around a star or star system.
Choice D) A system of planets that are all located in the same galaxy.
Choice E) A system of planets that are all made of gas.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'Quark', 'Fusion powers stars and produces most elements lighter than cobalt in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 616 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.645% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and the result is fusion; this is an exothermic process.\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is 13.6 eV—less than one-millionth of the 17.6 MeV released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though individual fission reactions are generally much more energetic than individual fusion ones, which are themselves millions of times more energetic than chemical reactions. Via the mass–energy equivalence, fusion yields a 0.7% efficiency of reactant mass into energy. This can be only be exceeded by the extreme cases of the accretion process involving neutron stars or black holes, approaching 40% efficiency, and antimatter annihilation at 100% efficiency. (The complete conversion of one gram of matter would expel 9×1013 joules of energy.)\n== In astrophysics ==\nFusion is responsible for the astrophysical production of the majority of elements lighter than iron. This includes most types of Big Bang nucleosynthesis and stellar nucleosynthesis. Non-fusion processes that contribute include the s-process and r-process in neutron merger and supernova nucleosynthesis, responsible for elements heavier than iron.\n=== Stars ===\nAn important fusion process is the stellar nucleosynthesis that powers stars, including the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounts for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was unknown; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein\'s equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy had not yet been discovered, nor even that stars are largely composed of hydrogen (see metallicity). Eddington\'s paper reasoned that:\nThe leading theory of stellar energy, the contraction hypothesis, should cause the rotation of a star to visibly speed up due to conservation of angular momentum. But observations of Cepheid variable stars showed this was not happening.\nThe only other known plausible source of energy was conversion of matter to energy; Einstein had shown some years earlier that a small amount of matter was equivalent to a large amount of energy.\nFrancis Aston had also recently shown that the mass of a helium atom was about 0.8% less than the mass of the four hydrogen atoms which would, combined, form a helium atom (according to the then-prevailing theory of atomic structure which held atomic weight to be the distinguishing property between elements; work by Henry Moseley and Antonius van den Broek would later show that nucleic charge was the distinguishing property and that a helium nucleus, therefore, consisted of two hydrogen nuclei plus additional mass). This suggested that if such a combination could happen, it would release considerable energy as a byproduct.\nIf a star contained just 5% of fusible hydrogen, it would suffice to explain how stars got their energy. (It is now known that most \'ordinary\' stars are usually made of around 70% to 75% hydrogen)\nFurther elements might also be fused, and other scientists had speculated that stars were the "crucible" in which light elements combined to create heavy elements, but without more accurate measurements of their atomic masses nothing more could be said at the time.\nAll of these speculations were proven correct in the following decades.\nThe primary source of solar energy, and that of similar size stars, is the fusion of hydrogen to form helium (the proton–proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to fuse heavier elements. In massive cores, silicon-burning is the final fusion cycle, leading to a build-up of iron and nickel nuclei.\nNuclear binding energy makes the production of elements heavier than nickel via fusion energetically unfavorable. These elements are produced in non-fusion processes: the s-process, r-process, and the variety of processes that can produce p-nuclei. Such processes occur in giant star shells, or supernovae, or neutron star mergers.\n=== Brown dwarfs ===\nBrown dwarfs fuse deuterium and in very high mass cases also fuse lithium.\n=== White dwarfs ===\nCarbon-oxygen white dwarfs, which accrete matter either from an active stellar companion or white dwarf merger, approach the Chandrasekhar limit of 1.44 solar masses. Immediately prior, carbon burning fusion begins, destroying the Earth-sized dwarf within one second, in a Type Ia supernova.\nMuch more rarely, helium white dwarfs may merge, which does not cause an explosion but begins helium burning in an extreme type of helium star.\n=== Neutron stars ===\nSome neutron stars accrete hydrogen and helium from an active stellar companion. Periodically, the helium accretion reaches a critical level, and a thermonuclear burn wave propagates across the surface, on the timescale of one second.\n=== Black hole accretion disks ===\nSimilar to stellar fusion, extreme conditions within black hole accretion disks can allow fusion reactions. Calculations show the most energetic reactions occur around lower stellar mass black holes, below 10 solar masses, compared to those above 100. Beyond five Schwarzschild radii, carbon-burning and fusion of helium-3 dominates the reactions. Within this distance, around lower mass black holes, fusion of nitrogen, oxygen, neon, and magnesium can occur. In the extreme limit, the silicon-burning process can begin with the fusion of silicon and selenium nuclei.\n=== Big Bang ===\nFrom the period approximately 10 seconds to 20 minutes after the Big Bang, the universe cooled from over 100 keV to 1 keV. This allowed the combination of protons and neutrons in deuterium nuclei, and beginning a rapid fusion chain into tritium and helium-3 and ending in predominantly helium-4, with a minimal fraction of lithium, beryllium, and boron nuclei.\n== Requirements ==\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbors due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface-area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.', 'The neutron is a subatomic particle, symbol n or n0, that has no electric charge, and a mass slightly greater than that of a proton. The neutron was discovered by James Chadwick in 1932, leading to the discovery of nuclear fission in 1938, the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).\nNeutrons are found, together with a similar number of protons in the nuclei of atoms. Atoms of a chemical element that differ only in neutron number are called isotopes. Free neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes. Neutron stars, formed from massive collapsing stars, consist of neutrons at the density of atomic nuclei but a total mass more than the Sun.\nNeutron properties and interactions are described by nuclear physics.  Neutrons are not elementary particles; each is composed of three quarks. A free neutron spontaneously decays to a proton, an electron, and an antineutrino, with a mean lifetime of about 15 minutes.\nThe neutron is essential to the production of nuclear power.\nDedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments.  Free neutrons do not directly ionize atoms, but they do indirectly cause ionizing radiation, so they can be a biological hazard, depending on dose. A small natural "neutron background" flux of free neutrons exists on Earth, caused by cosmic rays, and by the natural radioactivity of spontaneously fissionable elements in the Earth\'s crust.\n== Discovery ==\nThe story of the discovery of the neutron and its properties is central to the extraordinary developments in atomic physics that occurred in the first half of the 20th century, leading ultimately to the atomic bomb in 1945. The name derives from the Latin root for neutralis (neuter) and the Greek suffix -on (a suffix used in the names of subatomic particles, e.g. electron and proton)\nand references to the word neutron can be found in the literature as early as 1899 in connection with discussion on the nature of the atom.\nIn the 1911 Rutherford model, the atom consisted of a small positively charged massive nucleus surrounded by a much larger cloud of negatively charged electrons. In 1920, Ernest Rutherford suggested that the nucleus consisted of positive protons and neutrally charged particles, suggested to be a proton and an electron bound in some way. Electrons were assumed to reside within the nucleus because it was known that beta radiation consisted of electrons emitted from the nucleus. About the time Rutherford suggested the neutral proton-electron composite, several other publications appeared making similar suggestions, and in 1921 the American chemist W. D. Harkins first named the hypothetical particle a "neutron".\nThroughout the 1920s, physicists assumed that the atomic nucleus was composed of protons and "nuclear electrons". Beginning in 1928, it became clear that this model was inconsistent with the then-new quantum theory. Confined to a volume the size of an nucleus, an electron consistent with the Heisenberg uncertainty relation of quantum mechanics would have an energy exceeding the binding energy of the nucleus. The energy was so large that according to the Klein paradox, discovered by Oskar Klein in 1928, an electron would escape the confinement of a nucleus. Furthermore, the observed properties of atoms and molecules were inconsistent with the nuclear spin expected from the proton–electron hypothesis. Protons and electrons both carry an intrinsic spin of \u20601/2\u2060ħ, and the isotopes of the same species were found to have either integer or fractional spin. By the hypothesis, isotopes would be composed of the same number of protons, but differing numbers of neutral bound proton+electron "particles". This physical picture was a contradiction, since there is no way to arrange the spins of an electron and a proton in a bound state to get a fractional spin.\nIn 1931, Walther Bothe and Herbert Becker found that if alpha particle radiation from polonium fell on beryllium, boron, or lithium, an unusually penetrating radiation was produced. The radiation was not influenced by an electric field, so Bothe and Becker assumed it was gamma radiation. The following year Irène Joliot-Curie and Frédéric Joliot-Curie in Paris showed that if this "gamma" radiation fell on paraffin, or any other hydrogen-containing compound, it ejected protons of very high energy. Neither Rutherford nor James Chadwick at the Cavendish Laboratory in Cambridge were convinced by the gamma ray interpretation. Chadwick quickly performed a series of experiments that showed that the new radiation consisted of uncharged particles with about the same mass as the proton. These properties matched Rutherford\'s hypothesized neutron. Chadwick won the 1935 Nobel Prize in Physics for this discovery.\nModels for an atomic nucleus consisting of protons and neutrons were quickly developed by Werner Heisenberg and others. The proton–neutron model explained the puzzle of nuclear spins. The origins of beta radiation were explained by Enrico Fermi in 1934 by the process of beta decay, in which the neutron decays to a proton by creating an electron and a then-undiscovered neutrino. In 1935, Chadwick and his doctoral student Maurice Goldhaber reported the first accurate measurement of the mass of the neutron.\nBy 1934, Fermi had bombarded heavier elements with neutrons to induce radioactivity in elements of high atomic number. In 1938, Fermi received the Nobel Prize in Physics "for his demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons". In December 1938 Otto Hahn, Lise Meitner, and Fritz Strassmann discovered nuclear fission, or the fractionation of uranium nuclei into lighter elements, induced by neutron bombardment. In 1945 Hahn received the 1944 Nobel Prize in Chemistry "for his discovery of the fission of heavy atomic nuclei".\nThe discovery of nuclear fission would lead to the development of nuclear power and the atomic bomb by the end of World War II. It was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, in a cascade known as a nuclear chain reaction.:\u200a460–461\u200a These events and findings led Fermi to construct the Chicago Pile-1 at the University of Chicago in 1942, the first self-sustaining nuclear reactor. Just three years later the Manhattan Project was able to test the first atomic bomb, the Trinity nuclear test in July 1945.\n== Occurrence ==\n=== Atomic nucleus ===\nAn atomic nucleus is formed by a number of protons, Z (the atomic number), and a number of neutrons, N (the neutron number), bound together by the nuclear force. Protons and neutrons each have a mass of approximately one dalton. The atomic number determines the chemical properties of the atom, and the neutron number determines the isotope or nuclide.:\u200a4\u200a The terms isotope and nuclide are often used synonymously, but they refer to chemical and nuclear properties, respectively.:\u200a4\u200a Isotopes are nuclides with the same atomic number, but different neutron number. Nuclides with the same neutron number, but different atomic number, are called isotones. The atomic mass number, A, is equal to the sum of atomic and neutron numbers. Nuclides with the same atomic mass number, but different atomic and neutron numbers, are called isobars.  The mass of a nucleus is always slightly less than the sum of its proton and neutron masses: the difference in mass represents the mass equivalent to nuclear binding energy, the energy which would need to be added to take the nucleus apart.:\u200a822\nThe nucleus of the most common isotope of the hydrogen atom (with the chemical symbol 1H) is a lone proton.:\u200a20\u200a The nuclei of the heavy hydrogen isotopes deuterium (D or 2H) and tritium (T or 3H) contain one proton bound to one and two neutrons, respectively.:\u200a20\u200a All other types of atomic nuclei are composed of two or more protons and various numbers of neutrons. The most common nuclide of the common chemical element lead, 208Pb, has 82 protons and 126 neutrons, for example. The table of nuclides comprises all the known nuclides. Even though it is not a chemical element, the neutron is included in this table.\nProtons and neutrons behave almost identically under the influence of the nuclear force within the nucleus. They are therefore both referred to collectively as nucleons.  The concept of isospin, in which the proton and neutron are viewed as two quantum states of the same particle, is used to model the interactions of nucleons by the nuclear or weak forces.:\u200a141\nNeutrons are a necessary constituent of any atomic nucleus that contains more than one proton. As a result of their positive charges, interacting protons have a mutual electromagnetic repulsion that is stronger than their attractive nuclear interaction, so proton-only nuclei are unstable (see diproton and neutron–proton ratio). Neutrons bind with protons and one another in the nucleus via the nuclear force, effectively moderating the repulsive forces between the protons and stabilizing the nucleus.:\u200a461\u200a Heavy nuclei carry a large positive charge, hence they require "extra" neutrons to be stable.:\u200a461', '{\\displaystyle L_{z}=m_{\\ell }\\hbar }\nThe values of mℓ range from −ℓ to ℓ, with integer intervals.\nThe s subshell (ℓ = 0) contains only one orbital, and therefore the mℓ of an electron in an s orbital will always be 0. The p subshell (ℓ = 1) contains three orbitals, so the mℓ of an electron in a p orbital will be −1, 0, or 1. The d subshell (ℓ = 2) contains five orbitals, with mℓ values of −2, −1, 0, 1, and 2.\n=== Spin magnetic quantum number ===\nThe spin magnetic quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis:\n{\\displaystyle S_{z}=m_{s}\\hbar }\nIn general, the values of ms range from −s to s, where s is the spin quantum number, associated with the magnitude of particle\'s intrinsic spin angular momentum:\n{\\displaystyle m_{s}=-s,-s+1,-s+2,\\cdots ,s-2,s-1,s}\nAn electron state has spin number s = \u20601/2\u2060, consequently ms will be +\u20601/2\u2060 ("spin up") or −\u20601/2\u2060 "spin down" states. Since electron are fermions they obey the Pauli exclusion principle: each electron state must have different quantum numbers.  Therefore, every orbital will be occupied with at most two electrons, one for each spin state.\n=== The Aufbau principle and Hund\'s Rules ===\nA multi-electron atom can be modeled qualitatively as a hydrogen like atom with higher nuclear charge and correspondingly more electrons. The occupation of the electron states in such an atom can be predicted by the Aufbau principle and Hund\'s empirical rules for the quantum numbers.  The Aufbau principle fills orbitals based on their principal and azimuthal quantum numbers (lowest n + l first, with lowest n breaking ties; Hund\'s rule favors unpaired electrons in the outermost orbital). These rules are empirical but they can be related to electron physics.:\u200a10\u200a:\u200a260\n== Spin-orbit coupled systems ==\nWhen one takes the spin–orbit interaction into consideration, the L and S operators no longer commute with the Hamiltonian, and the eigenstates of the system no longer have well-defined orbital angular momentum and spin. Thus another set of quantum numbers should be used. This set includes\nThe total angular momentum quantum number:\n{\\displaystyle j=|\\ell \\pm s|,}\nwhich gives the total angular momentum through the relation\n{\\displaystyle J^{2}=\\hbar ^{2}j(j+1).}\nThe projection of the total angular momentum along a specified axis:\n{\\displaystyle m_{j}=-j,-j+1,-j+2,\\cdots ,j-2,j-1,j}\nanalogous to the above and satisfies both\n{\\displaystyle m_{j}=m_{\\ell }+m_{s},}\nand\n{\\displaystyle |m_{\\ell }+m_{s}|\\leq j.}\nParityThis is the eigenvalue under reflection: positive (+1) for states which came from even ℓ and negative (−1) for states which came from odd ℓ. The former is also known as even parity and the latter as odd parity, and is given by\n{\\displaystyle P=(-1)^{\\ell }.}\nFor example, consider the following 8 states, defined by their quantum numbers:\nThe quantum states in the system can be described as linear combination of these 8 states. However, in the presence of spin–orbit interaction, if one wants to describe the same system by 8 states that are eigenvectors of the Hamiltonian (i.e. each represents a state that does not mix with others over time), we should consider the following 8 states:\n== Atomic nuclei ==\nIn nuclei, the entire assembly of protons and neutrons (nucleons) has a resultant angular momentum due to the angular momenta of each nucleon, usually denoted I. If the total angular momentum of a neutron is jn = ℓ + s and for a proton is jp = ℓ + s (where s for protons and neutrons happens to be \u20601/2\u2060 again (see note)), then the nuclear angular momentum quantum numbers I are given by:\n{\\displaystyle I=|j_{n}-j_{p}|,|j_{n}-j_{p}|+1,|j_{n}-j_{p}|+2,\\cdots ,(j_{n}+j_{p})-2,(j_{n}+j_{p})-1,(j_{n}+j_{p})}\nNote: The orbital angular momenta of the nuclear (and atomic) states are all integer multiples of ħ while the intrinsic angular momentum of the neutron and  proton are half-integer multiples.  It should be immediately apparent that the combination of the intrinsic spins of the nucleons with their orbital motion will always give half-integer values for the total spin, I, of any odd-A nucleus and integer values for any even-A nucleus.\nParity with the number I is used to label nuclear angular momentum states, examples for some isotopes of hydrogen (H), carbon (C), and sodium (Na) are;\nThe reason for the unusual fluctuations in I, even by differences of just one nucleon, are due to the odd and even numbers of protons and neutrons – pairs of nucleons have a total angular momentum of zero (just like electrons in orbitals), leaving an odd or even number of unpaired nucleons. The property of nuclear spin is an important factor for the operation of NMR spectroscopy in organic chemistry, and MRI in nuclear medicine, due to the nuclear magnetic moment interacting with an external magnetic field.\n== Elementary particles ==\nElementary particles contain many quantum numbers which are usually said to be intrinsic to them. However, it should be understood that the elementary particles are quantum states of the standard model of particle physics, and hence the quantum numbers of these particles bear the same relation to the Hamiltonian of this model as the quantum numbers of the Bohr atom does to its Hamiltonian. In other words, each quantum number denotes a symmetry of the problem. It is more useful in quantum field theory to distinguish between spacetime and internal symmetries.\nTypical quantum numbers related to spacetime symmetries are spin (related to rotational symmetry), the parity, C-parity and T-parity (related to the Poincaré symmetry of spacetime). Typical internal symmetries are lepton number and baryon number or the electric charge. (For a full list of quantum numbers of this kind see the article on flavour.)\n== Multiplicative quantum numbers ==\nMost conserved quantum numbers are additive, so in an elementary particle reaction, the sum of the quantum numbers should be the same before and after the reaction. However, some, usually called a parity, are multiplicative; i.e., their product is conserved. All multiplicative quantum numbers belong to a symmetry (like parity) in which applying the symmetry transformation twice is equivalent to doing nothing (involution).\n== See also ==\nElectron configuration\n== References ==\n== Further reading ==\nDirac, Paul A. M. (1982). Principles of Quantum Mechanics. Oxford University Press. ISBN 0-19-852011-5.\nGriffiths, David J. (2004). Introduction to Quantum Mechanics (2nd ed.). Prentice Hall. ISBN 0-13-805326-X.\nHalzen, Francis & Martin, Alan D. (1984). Quarks and Leptons: An Introductory Course in Modern Particle Physics. John Wiley & Sons. ISBN 0-471-88741-2.\nEisberg, Robert Martin; Resnick, Robert (1985). Quantum Physics of Atoms, Molecules, Solids, Nuclei and Particles (2nd ed.). John Wiley & Sons. ISBN 978-0-471-87373-0 – via Internet Archive.', '=== Progenitor ===\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic type Ic-BL events that are associated with long-duration gamma-ray bursts.\n== External impact ==\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\n=== Source of heavy elements ===\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\n=== Role in stellar evolution ===\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n=== Cosmic rays ===\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\n=== Gravitational waves ===', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'While supersymmetry has not been discovered at high energy, see Section Supersymmetry in particle physics, supersymmetry was found to be effectively realized at the intermediate energy of hadronic physics where baryons and mesons are superpartners. An exception is the pion that appears as a zero mode in the mass spectrum and thus protected by the supersymmetry: It has no baryonic partner. The realization of this effective supersymmetry is readily explained in quark–diquark models: Because two different color charges close together (e.g., blue and red) appear under coarse resolution as the corresponding anti-color (e.g. anti-green), a diquark cluster viewed with coarse resolution (i.e., at the energy-momentum scale used to study hadron structure) effectively appears as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves likes a meson.\n=== Supersymmetry in condensed matter physics ===\nSUSY concepts have provided useful extensions to the WKB approximation. Additionally, SUSY has been applied to disorder averaged systems both quantum and non-quantum (through statistical mechanics), the Fokker–Planck equation being an example of a non-quantum theory. The \'supersymmetry\' in all these systems arises from the fact that one is modelling one particle and as such the \'statistics\' do not matter. The use of the supersymmetry method provides a mathematical rigorous alternative to the replica trick, but only in non-interacting systems, which attempts to address the so-called \'problem of the denominator\' under disorder averaging. For more on the applications of supersymmetry in condensed matter physics see Efetov (1997).\nIn 2021, a group of researchers showed that, in theory,\n{\\displaystyle N=(0,1)}\nSUSY could be realised at the edge of a Moore–Read quantum Hall state. However, to date, no experiments have been done yet to realise it at an edge of a Moore–Read state. In 2022, a different group of researchers created a computer simulation of atoms in 1 dimensions that had supersymmetric topological quasiparticles.\n=== Supersymmetry in optics ===\nIn 2013, integrated optics was found to provide a fertile ground on which certain ramifications of SUSY can be explored in readily-accessible laboratory settings. Making use of the analogous mathematical structure of the quantum-mechanical Schrödinger equation and the wave equation governing the evolution of light in one-dimensional settings, one may interpret the refractive index distribution of a structure as a potential landscape in which optical wave packets propagate. In this manner, a new class of functional optical structures with possible applications in phase matching, mode conversion and space-division multiplexing becomes possible. SUSY transformations have been also proposed as a way to address inverse scattering problems in optics and as a one-dimensional transformation optics.\n=== Supersymmetry in dynamical systems ===\nAll stochastic (partial) differential equations, the models for all types of continuous time dynamical systems, possess topological supersymmetry. In the operator representation of stochastic evolution, the topological supersymmetry is the exterior derivative which is commutative with the stochastic evolution operator defined as the stochastically averaged pullback induced on differential forms by SDE-defined diffeomorphisms of the phase space. The topological sector of the so-emerging supersymmetric theory of stochastic dynamics can be recognized as the Witten-type topological field theory.\nThe meaning of the topological supersymmetry in dynamical systems is the preservation of the phase space continuity—infinitely close points will remain close during continuous time evolution even in the presence of noise. When the topological supersymmetry is broken spontaneously, this property is violated in the limit of the infinitely long temporal evolution and the model can be said to exhibit (the stochastic generalization of) the butterfly effect. From a more general perspective, spontaneous breakdown of the topological supersymmetry is the theoretical essence of the ubiquitous dynamical phenomenon variously known as chaos, turbulence, self-organized criticality etc. The Goldstone theorem explains the associated emergence of the long-range dynamical behavior that manifests itself as \u20601/f\u2060 noise, butterfly effect, and the scale-free statistics of sudden (instantonic) processes, such as earthquakes, neuroavalanches, and solar flares, known as the Zipf\'s law and the Richter scale.\n=== Supersymmetry in mathematics ===\nSUSY is also sometimes studied mathematically for its intrinsic properties. This is because it describes complex fields satisfying a property known as holomorphy, which allows holomorphic quantities to be exactly computed. This makes supersymmetric models useful "toy models" of more realistic theories. A prime example of this has been the demonstration of S-duality in four-dimensional gauge theories that interchanges particles and monopoles.\nThe proof of the Atiyah–Singer index theorem is much simplified by the use of supersymmetric quantum mechanics.\n=== Supersymmetry in string theory ===\nSupersymmetry is an integral part of string theory, a possible theory of everything. There are two types of string theory, supersymmetric string theory or superstring theory, and non-supersymmetric string theory. By definition of superstring theory, supersymmetry is required in superstring theory at some level. However, even in non-supersymmetric string theory, a type of supersymmetry called misaligned supersymmetry is still required in the theory in order to ensure no physical tachyons appear. Any string theories without some kind of supersymmetry, such as bosonic string theory and the\n{\\displaystyle E_{7}\\times E_{7}}\n16\n{\\displaystyle SU(16)}\n, and\n{\\displaystyle E_{8}}\nheterotic string theories, will have a tachyon and therefore the spacetime vacuum itself would be unstable and would decay into some tachyon-free string theory usually in a lower spacetime dimension. There is no experimental evidence that either supersymmetry or misaligned supersymmetry holds in our universe, and many physicists have moved on from supersymmetry and string theory entirely due to the non-detection of supersymmetry at the LHC.\nDespite the null results for supersymmetry at the LHC so far, some particle physicists have nevertheless moved to string theory in order to resolve the naturalness crisis for certain supersymmetric extensions of the Standard Model. According to the particle physicists, there exists a concept of "stringy naturalness" in string theory, where the string theory landscape could have a power law statistical pull on soft SUSY breaking terms to large values (depending on the number of hidden sector SUSY breaking fields contributing to the soft terms). If this is coupled with an anthropic requirement that contributions to the weak scale not exceed a factor between 2 and 5 from its measured value (as argued by Agrawal et al.), then the Higgs mass is pulled up to the vicinity of 125 GeV while most sparticles are pulled to values beyond the current reach of LHC. (The Higgs was determined to have a mass of 125 GeV ±0.15 GeV in 2022.) An exception occurs for higgsinos which gain mass not from SUSY breaking but rather from whatever mechanism solves the SUSY mu problem. Light higgsino pair production in association with hard initial state jet radiation leads to a soft opposite-sign dilepton plus jet plus missing transverse energy signal.\n== Supersymmetry in particle physics ==\nIn particle physics, a supersymmetric extension of the Standard Model is a possible candidate for undiscovered particle physics, and seen by some physicists as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to be incomplete and where limitations of current theories are well established. In particular, one supersymmetric extension of the Standard Model, the Minimal Supersymmetric Standard Model (MSSM), became popular in theoretical particle physics, as the Minimal Supersymmetric Standard Model is the simplest supersymmetric extension of the Standard Model that could resolve major hierarchy problems within the Standard Model, by guaranteeing that quadratic divergences of all orders will cancel out in perturbation theory. If a supersymmetric extension of the Standard Model is correct, superpartners of the existing elementary particles would be new and undiscovered particles and supersymmetry is expected to be spontaneously broken.\nThere is no experimental evidence that a supersymmetric extension to the Standard Model is correct, or whether or not other extensions to current models might be more accurate. It is only since around 2010 that particle accelerators specifically designed to study physics beyond the Standard Model have become operational (i.e. the Large Hadron Collider (LHC)), and it is not known where exactly to look, nor the energies required for a successful search. However, the negative results from the LHC since 2010 have already ruled out some supersymmetric extensions to the Standard Model, and many physicists believe that the Minimal Supersymmetric Standard Model, while not ruled out, is no longer able to fully resolve the hierarchy problem.\n=== Supersymmetric extensions of the Standard Model ===', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  ħωk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, θk†, θk, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the θk, and the Fock space is built by applying all products of creation operators θk† to |0⟩.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at φ = 0, so that the vacuum minimizing the Hamiltonian satisfies ⟨φ⟩ = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  φ = v .  This occurs for example, if V(φ) = gφ4 − 2m2φ2 with g > 0 and m2 > 0, for which the minimum energy is found at v = ±m/√g. The value of v in one of these vacua may be considered as condensate of the field φ. Canonical quantization then can be carried out for the shifted field  φ(x,t) − v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator  [A, B]  expressed in the phase space formulation is iħ{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  ħ-deformations are highly nonunique—quantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  ħ → 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an ħ-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional Schrödinger Hilbert space. If the chosen variables are complex, we get something like the Segal–Bargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.']

Question: What is the relationship between chemical potential and quarks/antiquarks?

Choices:
Choice A) Chemical potential, represented by μ, is a measure of the imbalance between quarks and antiquarks in a system. Higher μ indicates a stronger bias favoring quarks over antiquarks.
Choice B) Chemical potential, represented by μ, is a measure of the balance between quarks and antiquarks in a system. Higher μ indicates an equal number of quarks and antiquarks.
Choice C) Chemical potential, represented by μ, is a measure of the imbalance between quarks and antiquarks in a system. Higher μ indicates a stronger bias favoring antiquarks over quarks.
Choice D) Chemical potential, represented by μ, is a measure of the density of antiquarks in a system. Higher μ indicates a higher density of antiquarks.
Choice E) Chemical potential, represented by μ, is a measure of the density of quarks in a system. Higher μ indicates a higher density of quarks.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.', 'Accretion of material onto the protostar continues partially from the newly formed circumstellar disc. When the density and temperature are high enough, deuterium fusion begins, and the outward pressure of the resultant radiation slows (but does not stop) the collapse. Material comprising the cloud continues to "rain" onto the protostar. In this stage bipolar jets are produced called Herbig–Haro objects. This is probably the means by which excess angular momentum of the infalling material is expelled, allowing the star to continue to form.\nWhen the surrounding gas and dust envelope disperses and accretion process stops, the star is considered a pre-main-sequence star (PMS star). The energy source of these objects is (gravitational contraction)Kelvin–Helmholtz mechanism, as opposed to hydrogen burning in main sequence stars. The PMS star follows a Hayashi track on the Hertzsprung–Russell (H–R) diagram. The contraction will proceed until the Hayashi limit is reached, and thereafter contraction will continue on a Kelvin–Helmholtz timescale with the temperature remaining stable. Stars with less than 0.5 M☉ thereafter join the main sequence. For more massive PMS stars, at the end of the Hayashi track they will slowly collapse in near hydrostatic equilibrium, following the Henyey track.\nFinally, hydrogen begins to fuse in the core of the star, and the rest of the enveloping material is cleared away. This ends the protostellar phase and begins the star\'s main sequence phase on the H–R diagram.\nThe stages of the process are well defined in stars with masses around 1 M☉ or less. In high mass stars, the length of the star formation process is comparable to the other timescales of their evolution, much shorter, and the process is not so well defined. The later evolution of stars is studied in stellar evolution.\n== Observations ==\nKey elements of star formation are only available by observing in wavelengths other than the optical. The protostellar stage of stellar existence is almost invariably hidden away deep inside dense clouds of gas and dust left over from the GMC. Often, these star-forming cocoons known as Bok globules, can be seen in silhouette against bright emission from surrounding gas. Early stages of a star\'s life can be seen in infrared light, which penetrates the dust more easily than visible light.\nObservations from the Wide-field Infrared Survey Explorer (WISE) have thus been especially important for unveiling numerous galactic protostars and their parent star clusters.  Examples of such embedded star clusters are FSR 1184, FSR 1190, Camargo 14, Camargo 74, Majaess 64, and Majaess 98.\nThe structure of the molecular cloud and the effects of the protostar can be observed in near-IR extinction maps (where the number of stars are counted per unit area and compared to a nearby zero extinction area of sky), continuum dust emission and rotational transitions of CO and other molecules; these last two are observed in the millimeter and submillimeter range. The radiation from the protostar and early star has to be observed in infrared astronomy wavelengths, as the extinction caused by the rest of the cloud in which the star is forming is usually too big to allow us to observe it in the visual part of the spectrum. This presents considerable difficulties as the Earth\'s atmosphere is almost entirely opaque from 20μm to 850μm, with narrow windows at 200μm and 450μm. Even outside this range, atmospheric subtraction techniques must be used.\nX-ray observations have proven useful for studying young stars, since X-ray emission from these objects is about 100–100,000 times stronger than X-ray emission from main-sequence stars. The earliest detections of X-rays from T Tauri stars were made by the Einstein X-ray Observatory. For low-mass stars X-rays are generated by the heating of the stellar corona through magnetic reconnection, while for high-mass O and early B-type stars X-rays are generated through supersonic shocks in the stellar winds. Photons in the soft X-ray energy range covered by the Chandra X-ray Observatory and XMM-Newton may penetrate the interstellar medium with only moderate absorption due to gas, making the X-ray a useful wavelength for seeing the stellar populations within molecular clouds. X-ray emission as evidence of stellar youth makes this band particularly useful for performing censuses of stars in star-forming regions, given that not all young stars have infrared excesses. X-ray observations have provided near-complete censuses of all stellar-mass objects in the Orion Nebula Cluster and Taurus Molecular Cloud.\nThe formation of individual stars can only be directly observed in the Milky Way Galaxy, but in distant galaxies star formation has been detected through its unique spectral signature.\nInitial research indicates star-forming clumps start as giant, dense areas in turbulent gas-rich matter in young galaxies, live about 500 million years, and may migrate to the center of a galaxy, creating the central bulge of a galaxy.\nOn February 21, 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\nIn February 2018, astronomers reported, for the first time, a signal of the reionization epoch, an indirect detection of light from the earliest stars formed - about 180 million years after the Big Bang.\nAn article published on October 22, 2019, reported on the detection of 3MM-1, a massive star-forming galaxy about 12.5 billion light-years away that is obscured by clouds of dust. At a mass of about 1010.8 solar masses, it showed a star formation rate about 100 times as high as in the Milky Way.\n=== Notable pathfinder objects ===\nMWC 349 was first discovered in 1978, and is estimated to be only 1,000 years old.\nVLA 1623 – The first exemplar Class 0 protostar, a type of embedded protostar that has yet to accrete the majority of its mass. Found in 1993, is possibly younger than 10,000 years.\nL1014 – An extremely faint embedded object representative of a new class of sources that are only now being detected with the newest telescopes. Their status is still undetermined, they could be the youngest low-mass Class 0 protostars yet seen or even very low-mass evolved objects (like brown dwarfs or even rogue planets).\nGCIRS 8* – The youngest known main sequence star in the Galactic Center region, discovered in August 2006. It is estimated to be 3.5 million years old.\n== Low mass and high mass star formation ==\nStars of different masses are thought to form by slightly different mechanisms.  The theory of low-mass star formation, which is well-supported by observation, suggests that low-mass stars form by the gravitational collapse of rotating density enhancements within molecular clouds.  As described above, the collapse of a rotating cloud of gas and dust leads to the formation of an accretion disk through which matter is channeled onto a central protostar.  For stars with masses higher than about 8 M☉, however, the mechanism of star formation is not well understood.\nMassive stars emit copious quantities of radiation which pushes against infalling material.  In the past, it was thought that this radiation pressure might be substantial enough to halt accretion onto the massive protostar and prevent the formation of stars with masses more than a few tens of solar masses. Recent theoretical work has shown that the production of a jet and outflow clears a cavity through which much of the radiation from a massive protostar can escape without hindering accretion through the disk and onto the protostar. Present thinking is that massive stars may therefore be able to form by a mechanism similar to that by which low mass stars form.\nThere is mounting evidence that at least some massive protostars are indeed surrounded by accretion disks.  Disk accretion in high-mass protostars, similar to their low-mass counterparts, is expected to exhibit bursts of episodic accretion as a result of a gravitationally instability leading to clumpy and in-continuous accretion rates. Recent evidence of accretion bursts in high-mass protostars has indeed been confirmed observationally. Several other theories of massive star formation remain to be tested observationally.  Of these, perhaps the most prominent is the theory of competitive accretion, which suggests that massive protostars are "seeded" by low-mass protostars which compete with other protostars to draw in matter from the entire parent molecular cloud, instead of simply from a small local region.\nAnother theory of massive star formation suggests that massive stars may form by the coalescence of two or more stars of lower mass.\n== Filamentary nature of star formation ==\nRecent studies have emphasized the role of filamentary structures in molecular clouds as the initial conditions for star formation. Findings from the Herschel Space Observatory highlight the ubiquitous nature of these filaments in the cold interstellar medium (ISM). The spatial relationship between cores and filaments indicates that the majority of prestellar cores are located within 0.1 pc of supercritical filaments. This supports the hypothesis that filamentary structures act as pathways for the accumulation of gas and dust, leading to core formation.', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', '=== Progenitor ===\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic type Ic-BL events that are associated with long-duration gamma-ray bursts.\n== External impact ==\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\n=== Source of heavy elements ===\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\n=== Role in stellar evolution ===\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n=== Cosmic rays ===\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\n=== Gravitational waves ===', 'Star formation', "==== Red-giant-branch phase ====\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star.  For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower 12C/13C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\nThe helium core continues to grow on the red-giant branch.  It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase.  The star increases in luminosity towards the tip of the red-giant branch.  Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n==== Horizontal branch ====\nIn the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 108 times the luminosity of the Sun for a few days and 1011 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning.  These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n==== Asymptotic-giant-branch phase ====\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time).  Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star.  Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically.  This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface.  This is known as the second dredge up, and in some stars there may even be a third dredge up.  In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra.  A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\nAnother well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths.  These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity.  These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n==== Post-AGB ====\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n=== Massive stars ===\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants.  Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.\n==== Supergiant evolution ====\nExtremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\nThe core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core.  In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process.  At the end of helium fusion, the core of a star consists primarily of carbon and oxygen.  In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium.  Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.\nThe exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉.  After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse.  Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning.  For a range of stars of approximately 8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova.", "In more massive stars, the fusion of neon proceeds without a runaway deflagration.  This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements.  Surrounding the core are shells of lighter elements still undergoing fusion.  The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged.  The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope.  The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars.  Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself.  The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.\n==== Supernova ====\nWhen the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole.  Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\n== Stellar remnants ==\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n=== White and black dwarfs ===\nFor a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years.\nThe chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n=== Neutron stars ===\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n=== Black holes ===\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉.", 'Petrosian magnitudes have the advantage of being redshift and distance independent, allowing the measurement of the galaxy\'s apparent size since the Petrosian radius is defined in terms of the galaxy\'s overall luminous flux.\nA critique of an earlier version of this method has been issued by the Infrared Processing and Analysis Center, with the method causing a magnitude of error (upwards to 10%) of the values than using isophotal diameter. The use of Petrosian magnitudes also have the disadvantage of missing most of the light outside the Petrosian aperture, which is defined relative to the galaxy\'s overall brightness profile, especially for elliptical galaxies, with higher signal-to-noise ratios on higher distances and redshifts. A correction for this method has been issued by Graham et al. in 2005, based on the assumption that galaxies follow Sérsic\'s law.\n=== Near-infrared method ===\nThis method has been used by 2MASS as an adaptation from the previously used methods of isophotal measurement. Since 2MASS operates in the near infrared, which has the advantage of being able to recognize dimmer, cooler, and older stars, it has a different form of approach compared to other methods that normally use B-filter. The detail of the method used by 2MASS has been described thoroughly in a document by Jarrett et al., with the survey measuring several parameters.\nThe standard aperture ellipse (area of detection) is defined by the infrared isophote at the Ks band (roughly 2.2 μm wavelength) of 20 mag/arcsec2. Gathering the overall luminous flux of the galaxy has been employed by at least four methods: the first being a circular aperture extending 7 arcseconds from the center, an isophote at 20 mag/arcsec2, a "total" aperture defined by the radial light distribution that covers the supposed extent of the galaxy, and the Kron aperture (defined as 2.5 times the first-moment radius, an integration of the flux of the "total" aperture).\n== Larger-scale structures ==\nDeep-sky surveys show that galaxies are often found in groups and clusters. Solitary galaxies that have not significantly interacted with other galaxies of comparable mass in the past few billion years are relatively scarce. Only about 5% of the galaxies surveyed are isolated in this sense. However, they may have interacted and even merged with other galaxies in the past, and may still be orbited by smaller satellite galaxies.\nOn the largest scale, the universe is continually expanding, resulting in an average increase in the separation between individual galaxies (see Hubble\'s law). Associations of galaxies can overcome this expansion on a local scale through their mutual gravitational attraction. These associations formed early, as clumps of dark matter pulled their respective galaxies together. Nearby groups later merged to form larger-scale clusters. This ongoing merging process, as well as an influx of infalling gas, heats the intergalactic gas in a cluster to very high temperatures of 30–100 megakelvins. About 70–80% of a cluster\'s mass is in the form of dark matter, with 10–30% consisting of this heated gas and the remaining few percent in the form of galaxies.\nMost galaxies are gravitationally bound to a number of other galaxies. These form a fractal-like hierarchical distribution of clustered structures, with the smallest such associations being termed groups. A group of galaxies is the most common type of galactic cluster; these formations contain the majority of galaxies (as well as most of the baryonic mass) in the universe. To remain gravitationally bound to such a group, each member galaxy must have a sufficiently low velocity to prevent it from escaping (see Virial theorem). If there is insufficient kinetic energy, however, the group may evolve into a smaller number of galaxies through mergers.\nClusters of galaxies consist of hundreds to thousands of galaxies bound together by gravity. Clusters of galaxies are often dominated by a single giant elliptical galaxy, known as the brightest cluster galaxy, which, over time, tidally destroys its satellite galaxies and adds their mass to its own.\nSuperclusters contain tens of thousands of galaxies, which are found in clusters, groups and sometimes individually. At the supercluster scale, galaxies are arranged into sheets and filaments surrounding vast empty voids. Above this scale, the universe appears to be the same in all directions (isotropic and homogeneous), though this notion has been challenged in recent years by numerous findings of large-scale structures that appear to be exceeding this scale. The Hercules–Corona Borealis Great Wall, currently the largest structure in the universe found so far, is 10 billion light-years (three gigaparsecs) in length.\nThe Milky Way galaxy is a member of an association named the Local Group, a relatively small group of galaxies that has a diameter of approximately one megaparsec. The Milky Way and the Andromeda Galaxy are the two brightest galaxies within the group; many of the other member galaxies are dwarf companions of these two. The Local Group itself is a part of a cloud-like structure within the Virgo Supercluster, a large, extended structure of groups and clusters of galaxies centered on the Virgo Cluster. In turn, the Virgo Supercluster is a portion of the Laniakea Supercluster.\n== Magnetic fields ==\nGalaxies have magnetic fields of their own. A galaxy\'s magnetic field influences its dynamics in multiple ways, including affecting the formation of spiral arms and transporting angular momentum in gas clouds. The latter effect is particularly important, as it is a necessary factor for the gravitational collapse of those clouds, and thus for star formation.\nThe typical average equipartition strength for spiral galaxies is about 10 μG (microgauss) or 1 nT (nanotesla). By comparison, the Earth\'s magnetic field has an average strength of about 0.3 G (Gauss) or 30 μT (microtesla). Radio-faint galaxies like M 31 and M33, the Milky Way\'s neighbors, have weaker fields (about 5 μG), while gas-rich galaxies with high star-formation rates, like M 51, M 83 and NGC 6946, have 15 μG on average. In prominent spiral arms, the field strength can be up to 25 μG, in regions where cold gas and dust are also concentrated. The strongest total equipartition fields (50–100 μG) were found in starburst galaxies—for example, in M 82 and the Antennae; and in nuclear starburst regions, such as the centers of NGC 1097 and other barred galaxies.\n== Formation and evolution ==\n=== Formation ===\nCurrent models of the formation of galaxies in the early universe are based on the ΛCDM model. About 300,000 years after the Big Bang, atoms of hydrogen and helium began to form, in an event called recombination. Nearly all the hydrogen was neutral (non-ionized) and readily absorbed light, and no stars had yet formed. As a result, this period has been called the "dark ages". It was from density fluctuations (or anisotropic irregularities) in this primordial matter that larger structures began to appear. As a result, masses of baryonic matter started to condense within cold dark matter halos. These primordial structures allowed gasses to condense in to protogalaxies, large scale gas clouds that were precursors to the first galaxies.:\u200a6\nAs gas falls in to the gravity of the dark matter halos, its pressure and temperature rise. To condense further, the gas must radiate energy. This process was slow in the early universe dominated by hydrogen atoms and molecules which are inefficient radiators compared to heavier elements. As clumps of gas aggregate forming rotating disks, temperatures and pressures continue to increase. Some places within the disk reach high enough density to form stars.\nOnce protogalaxies began to form and contract, the first halo stars, called Population III stars, appeared within them. These were composed of primordial gas, almost entirely of hydrogen and helium.\nEmission from the first stars heats the remaining gas helping to trigger additional star formation; the ultraviolet light emission from the first generation of stars re-ionized the surrounding neutral hydrogen in expanding spheres eventually reaching the entire universe, an event called reionization. The most massive stars collapse in violent supernova explosions releasing heavy elements ("metals") into the interstellar medium.:\u200a14\u200a This metal content is incorporated into population II stars.\nTheoretical models for early galaxy formation have been verified and informed by a large number and variety of sophisticated astronomical observations.:\u200a43\u200a The photometric observations generally need spectroscopic confirmation due the large number mechanisms that can introduce systematic errors. For example, a high redshift (z ~ 16) photometric observation by James Webb Space Telescope (JWST) was later corrected to be closer to z ~ 5.\nNevertheless, confirmed observations from the JWST and other observatories are accumulating, allowing systematic comparison of early galaxies to predictions of theory.\nEvidence for individual Population III stars in early galaxies is even more challenging. Even seemingly confirmed spectroscopic evidence may turn out to have other origins. For example, astronomers reported HeII emission evidence for Population III stars in the Cosmos Redshift 7 galaxy, with a redshift value of 6.60. Subsequent observations found metallic emission lines, OIII, inconsistent with an early-galaxy star.:\u200a108\n=== Evolution ===\nOnce stars begin to form, emit radiation, and in some cases explode, the process of galaxy formation becomes very complex, involving interactions between the forces of gravity, radiation, and thermal energy. Many details are still poorly understood.', 'Stellar evolution is the process by which a star changes over the course of time. Depending on the mass of the star, its lifetime can range from a few million years for the most massive to trillions of years for the least massive, which is considerably longer than the current age of the universe. The table shows the lifetimes of stars as a function of their masses. All stars are formed from collapsing clouds of gas and dust, often called nebulae or molecular clouds.  Over the course of millions of years, these protostars settle down into a state of equilibrium, becoming what is known as a main-sequence star.\nNuclear fusion powers a star for most of its existence. Initially the energy is generated by the fusion of hydrogen atoms at the core of the main-sequence star. Later, as the preponderance of atoms at the core becomes helium, stars like the Sun begin to fuse hydrogen along a spherical shell surrounding the core. This process causes the star to gradually grow in size, passing through the subgiant stage until it reaches the red-giant phase. Stars with at least half the mass of the Sun can also begin to generate energy through the fusion of helium at their core, whereas more-massive stars can fuse heavier elements along a series of concentric shells. Once a star like the Sun has exhausted its nuclear fuel, its core collapses into a dense white dwarf and the outer layers are expelled as a planetary nebula. Stars with around ten or more times the mass of the Sun can explode in a supernova as their inert iron cores collapse into an extremely dense neutron star or black hole. Although the universe is not old enough for any of the smallest red dwarfs to have reached the end of their existence, stellar models suggest they will slowly become brighter and hotter before running out of hydrogen fuel and becoming low-mass white dwarfs.\nStellar evolution is not studied by observing the life of a single star, as most stellar changes occur too slowly to be detected, even over many centuries. Instead, astrophysicists come to understand how stars evolve by observing numerous stars at various points in their lifetime, and by simulating stellar structure using computer models.\n== Star formation ==\n=== Protostar ===\nStellar evolution starts with the gravitational collapse of a giant molecular cloud. Typical giant molecular clouds are roughly 100 light-years (9.5×1014 km) across and contain up to 6,000,000 solar masses (1.2×1037 kg). As it collapses, a giant molecular cloud breaks into smaller and smaller pieces. In each of these fragments, the collapsing gas releases gravitational potential energy as heat. As its temperature and pressure increase, a fragment condenses into a rotating ball of superhot gas known as a protostar.  Filamentary structures are truly ubiquitous in the molecular cloud. Dense molecular filaments will fragment into gravitationally bound cores, which are the precursors of stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed fragmentation manner of the filaments. In supercritical filaments, observations have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded two protostars with gas outflows.\nA protostar continues to grow by accretion of gas and dust from the molecular cloud, becoming a pre-main-sequence star as it reaches its final mass. Further development is determined by its mass. Mass is typically compared to the mass of the Sun: 1.0 M☉ (2.0×1030 kg) means 1 solar mass.\nProtostars are encompassed in dust, and are thus more readily visible at infrared wavelengths.\nObservations from the Wide-field Infrared Survey Explorer (WISE) have been especially important for unveiling numerous galactic protostars and their parent star clusters.\n=== Brown dwarfs and sub-stellar objects ===\nProtostars with masses less than roughly 0.08 M☉ (1.6×1029 kg) never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (MJ), 2.5 × 1028 kg, or 0.0125 M☉). Objects smaller than 13 MJ are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets). Both types, deuterium-burning and not, shine dimly and fade away slowly, cooling gradually over hundreds of millions of years.\n=== Main sequence stellar mass objects ===\nFor a more-massive protostar, the core temperature will eventually reach 10 million kelvin, initiating the proton–proton chain reaction and allowing hydrogen to fuse, first to deuterium and then to helium. In stars of slightly over 1 M☉ (2.0×1030 kg), the carbon–nitrogen–oxygen fusion reaction (CNO cycle) contributes a large portion of the energy generation. The onset of nuclear fusion leads relatively quickly to a hydrostatic equilibrium in which energy released by the core maintains a high gas pressure, balancing the weight of the star\'s matter and preventing further gravitational collapse. The star thus evolves rapidly to a stable state, beginning the main-sequence phase of its evolution.\nA new star will sit at a specific point on the main sequence of the Hertzsprung–Russell diagram, with the main-sequence spectral type depending upon the mass of the star. Small, relatively cold, low-mass red dwarfs fuse hydrogen slowly and will remain on the main sequence for hundreds of billions of years or longer, whereas massive, hot O-type stars will leave the main sequence after just a few million years. A mid-sized yellow dwarf star, like the Sun, will remain on the main sequence for about 10 billion years. The Sun is thought to be in the middle of its main sequence lifespan.\n=== Planetary system ===\nA star may gain a protoplanetary disk, which furthermore can develop into a planetary system.\n== Mature stars ==\nEventually the star\'s core exhausts its supply of hydrogen and the star begins to evolve off the main sequence. Without the outward radiation pressure generated by the fusion of hydrogen to counteract the force of gravity, the core contracts until either electron degeneracy pressure becomes sufficient to oppose gravity or the core becomes hot enough (around 100 MK) for helium fusion to begin. Which of these happens first depends upon the star\'s mass.\n=== Low-mass stars ===\nWhat happens after a low-mass star ceases to produce energy through fusion has not been directly observed; the universe is around 13.8 billion years old, which is less time (by several orders of magnitude, in some cases) than it takes for fusion to cease in such stars.\nRecent astrophysical models suggest that red dwarfs of 0.1 M☉ may stay on the main sequence for some six to twelve trillion years, gradually increasing in both temperature and luminosity, and take several hundred billion years more to collapse, slowly, into a white dwarf.  Such stars will not become red giants as the whole star is a convection zone and it will not develop a degenerate helium core with a shell burning hydrogen.  Instead, hydrogen fusion will proceed until almost the whole star is helium.\nSlightly more massive stars do expand into red giants, but their helium cores are not massive enough to reach the temperatures required for helium fusion so they never reach the tip of the red-giant branch.  When hydrogen shell burning finishes, these stars move directly off the red-giant branch like a post-asymptotic-giant-branch (AGB) star, but at lower luminosity, to become a white dwarf.  A star with an initial mass about 0.6 M☉ will be able to reach temperatures high enough to fuse helium, and these "mid-sized" stars go on to further stages of evolution beyond the red-giant branch.\n=== Mid-sized stars ===\nStars of roughly 0.6–10 M☉ become red giants, which are large non-main-sequence stars of stellar classification K or M. Red giants lie along the right edge of the Hertzsprung–Russell diagram due to their red color and large luminosity. Examples include Aldebaran in the constellation Taurus and Arcturus in the constellation of Boötes.\nMid-sized stars are red giants during two different phases of their post-main-sequence evolution: red-giant-branch stars, with inert cores made of helium and hydrogen-burning shells, and asymptotic-giant-branch stars, with inert cores made of carbon and helium-burning shells inside the hydrogen-burning shells.  Between these two phases, stars spend a period on the horizontal branch with a helium-fusing core.  Many of these helium-fusing stars cluster towards the cool end of the horizontal branch as K-type giants and are referred to as red clump giants.\n==== Subgiant phase ====\nWhen a star exhausts the hydrogen in its core, it leaves the main sequence and begins to fuse hydrogen in a shell outside the core.  The core increases in mass as the shell produces more helium.  Depending on the mass of the helium core, this continues for several million to one or two billion years, with the star expanding and cooling at a similar or slightly lower luminosity to its main sequence state.  Eventually either the core becomes degenerate, in stars around the mass of the sun, or the outer layers cool sufficiently to become opaque, in more massive stars.  Either of these changes cause the hydrogen shell to increase in temperature and the luminosity of the star to increase, at which point the star expands onto the red-giant branch.\n==== Red-giant-branch phase ====', '=== Spirals ===\nSpiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept.\nSpiral galaxies consist of a rotating disk of stars and interstellar medium, along with a central bulge of generally older stars. Extending outward from the bulge are relatively bright arms. In the Hubble classification scheme, spiral galaxies are listed as type S, followed by a letter (a, b, or c) which indicates the degree of tightness of the spiral arms and the size of the central bulge. An Sa galaxy has tightly wound, poorly defined arms and possesses a relatively large core region. At the other extreme, an Sc galaxy has open, well-defined arms and a small core region. A galaxy with poorly defined arms is sometimes referred to as a flocculent spiral galaxy; in contrast to the grand design spiral galaxy that has prominent and well-defined spiral arms. The speed in which a galaxy rotates is thought to correlate with the flatness of the disc as some spiral galaxies have thick bulges, while others are thin and dense.\nIn spiral galaxies, the spiral arms do have the shape of approximate logarithmic spirals, a pattern that can be theoretically shown to result from a disturbance in a uniformly rotating mass of stars. Like the stars, the spiral arms rotate around the center, but they do so with constant angular velocity. The spiral arms are thought to be areas of high-density matter, or "density waves". As stars move through an arm, the space velocity of each stellar system is modified by the gravitational force of the higher density. (The velocity returns to normal after the stars depart on the other side of the arm.) This effect is akin to a "wave" of slowdowns moving along a highway full of moving cars. The arms are visible because the high density facilitates star formation, and therefore they harbor many bright and young stars.\n==== Barred spiral galaxy ====\nA majority of spiral galaxies, including the Milky Way galaxy, have a linear, bar-shaped band of stars that extends outward to either side of the core, then merges into the spiral arm structure. In the Hubble classification scheme, these are designated by an SB, followed by a lower-case letter (a, b or c) which indicates the form of the spiral arms (in the same manner as the categorization of normal spiral galaxies). Bars are thought to be temporary structures that can occur as a result of a density wave radiating outward from the core, or else due to a tidal interaction with another galaxy. Many barred spiral galaxies are active, possibly as a result of gas being channeled into the core along the arms.\nOur own galaxy, the Milky Way, is a large disk-shaped barred-spiral galaxy about 30 kiloparsecs in diameter and a kiloparsec thick. It contains about two hundred billion (2×1011) stars and has a total mass of about six hundred billion (6×1011) times the mass of the Sun.\n==== Super-luminous spiral ====\nRecently, researchers described galaxies called super-luminous spirals. They are very large with an upward diameter of 437,000 light-years (compared to the Milky Way\'s 87,400 light-year diameter). With a mass of 340 billion solar masses, they generate a significant amount of ultraviolet and mid-infrared light. They are thought to have an increased star formation rate around 30 times faster than the Milky Way.\n=== Other morphologies ===\nPeculiar galaxies are galactic formations that develop unusual properties due to tidal interactions with other galaxies.\nA ring galaxy has a ring-like structure of stars and interstellar medium surrounding a bare core. A ring galaxy is thought to occur when a smaller galaxy passes through the core of a spiral galaxy. Such an event may have affected the Andromeda Galaxy, as it displays a multi-ring-like structure when viewed in infrared radiation.\nA lenticular galaxy is an intermediate form that has properties of both elliptical and spiral galaxies. These are categorized as Hubble type S0, and they possess ill-defined spiral arms with an elliptical halo of stars (barred lenticular galaxies receive Hubble classification SB0).\nIrregular galaxies are galaxies that can not be readily classified into an elliptical or spiral morphology.\nAn Irr-I galaxy has some structure but does not align cleanly with the Hubble classification scheme.\nIrr-II galaxies do not possess any structure that resembles a Hubble classification, and may have been disrupted. Nearby examples of (dwarf) irregular galaxies include the Magellanic Clouds.\nA dark or "ultra diffuse" galaxy is an extremely-low-luminosity galaxy. It may be the same size as the Milky Way, but have a visible star count only one percent of the Milky Way\'s. Multiple mechanisms for producing this type of galaxy have been proposed, and it is possible that different dark galaxies formed by different means. One candidate explanation for the low luminosity is that the galaxy lost its star-forming gas at an early stage, resulting in old stellar populations.\n=== Dwarfs ===\nDespite the prominence of large elliptical and spiral galaxies, most galaxies are dwarf galaxies. They are relatively small when compared with other galactic formations, being about one hundredth the size of the Milky Way, with only a few billion stars. Blue compact dwarf galaxies contains large clusters of young, hot, massive stars. Ultra-compact dwarf galaxies have been discovered that are only 100 parsecs across.\nMany dwarf galaxies may orbit a single larger galaxy; the Milky Way has at least a dozen such satellites, with an estimated 300–500 yet to be discovered.\nMost of the information we have about dwarf galaxies come from observations of the local group, containing two spiral galaxies, the Milky Way and Andromeda, and many dwarf galaxies. These dwarf galaxies are classified as either irregular or dwarf elliptical/dwarf spheroidal galaxies.\nA study of 27 Milky Way neighbors found that in all dwarf galaxies, the central mass is approximately 10 million solar masses, regardless of whether it has thousands or millions of stars. This suggests that galaxies are largely formed by dark matter, and that the minimum size may indicate a form of warm dark matter incapable of gravitational coalescence on a smaller scale.\n== Variants ==\n=== Interacting ===\nInteractions between galaxies are relatively frequent, and they can play an important role in galactic evolution. Near misses between galaxies result in warping distortions due to tidal interactions, and may cause some exchange of gas and dust.\nCollisions occur when two galaxies pass directly through each other and have sufficient relative momentum not to merge. The stars of interacting galaxies usually do not collide, but the gas and dust within the two forms interacts, sometimes triggering star formation. A collision can severely distort the galaxies\' shapes, forming bars, rings or tail-like structures.\nAt the extreme of interactions are galactic mergers, where the galaxies\' relative momentums are insufficient to allow them to pass through each other. Instead, they gradually merge to form a single, larger galaxy. Mergers can result in significant changes to the galaxies\' original morphology. If one of the galaxies is much more massive than the other, the result is known as cannibalism, where the more massive larger galaxy remains relatively undisturbed, and the smaller one is torn apart. The Milky Way galaxy is currently in the process of cannibalizing the Sagittarius Dwarf Elliptical Galaxy and the Canis Major Dwarf Galaxy.\n=== Starburst ===\nStars are created within galaxies from a reserve of cold gas that forms giant molecular clouds. Some galaxies have been observed to form stars at an exceptional rate, which is known as a starburst. If they continue to do so, they would consume their reserve of gas in a time span less than the galaxy\'s lifespan. Hence starburst activity usually lasts only about ten million years, a relatively brief period in a galaxy\'s history. Starburst galaxies were more common during the universe\'s early history, but still contribute an estimated 15% to total star production.\nStarburst galaxies are characterized by dusty concentrations of gas and the appearance of newly formed stars, including massive stars that ionize the surrounding clouds to create H II regions. These stars produce supernova explosions, creating expanding remnants that interact powerfully with the surrounding gas. These outbursts trigger a chain reaction of star-building that spreads throughout the gaseous region. Only when the available gas is nearly consumed or dispersed does the activity end.\nStarbursts are often associated with merging or interacting galaxies. The prototype example of such a starburst-forming interaction is M82, which experienced a close encounter with the larger M81. Irregular galaxies often exhibit spaced knots of starburst activity.\n=== Radio galaxy ===\nA radio galaxy is a galaxy with giant regions of radio emission extending well beyond its visible structure. These energetic radio lobes are powered by jets from its active galactic nucleus. Radio galaxies are classified according to their Fanaroff–Riley classification. The FR I class have lower radio luminosity and exhibit structures which are more elongated; the FR II class are higher radio luminosity. The correlation of radio luminosity and structure suggests that the sources in these two types of galaxies may differ.']

Question: What is the reason for the formation of stars exclusively within molecular clouds?

Choices:
Choice A) The formation of stars occurs exclusively outside of molecular clouds.
Choice B) The low temperatures and high densities of molecular clouds cause the gravitational force to exceed the internal pressures that are acting "outward" to prevent a collapse.
Choice C) The low temperatures and low densities of molecular clouds cause the gravitational force to be less than the internal pressures that are acting "outward" to prevent a collapse.
Choice D) The high temperatures and low densities of molecular clouds cause the gravitational force to exceed the internal pressures that are acting "outward" to prevent a collapse.
Choice E) The high temperatures and high densities of molecular clouds cause the gravitational force to be less than the internal pressures that are acting "outward" to prevent a collapse.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ["Maxwell's demon", 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Jordy, W. H. (1952). Henry Adams: Scientific Historian. New Haven. ISBN 978-0-685-26683-0. {{cite book}}: ISBN / Date incompatibility (help)\nKhan, Salman. "Maxwell\'s Demon". Archived from the original on 2010-03-17.\nMaroney, O. J. E. (2009) ""Information Processing and Thermodynamic Entropy" The Stanford Encyclopedia of Philosophy (Autumn 2009 Edition)\nMaxwell, J. C. (1871). Theory of Heat. London, New York [etc.] Longmans, Green., reprinted (2001) New York: Dover, ISBN 0-486-41735-2\nNorton, J. (2005). "Eaters of the lotus: Landauer\'s principle and the return of Maxwell\'s demon" (PDF). Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics. 36 (2): 375–411. Bibcode:2005SHPMP..36..375N. CiteSeerX 10.1.1.468.3017. doi:10.1016/j.shpsb.2004.12.002. S2CID 21104635. Archived (PDF) from the original on 2006-09-01.\nRaizen, Mark G. (2011) "Demons, Entropy, and the Quest for Absolute Zero", Scientific American, March, pp54-59\nReaney, Patricia. "Scientists build nanomachine", Reuters, February 1, 2007\nRubi, J Miguel, "Does Nature Break the Second Law of Thermodynamics?"; Scientific American, October 2008 :\nSplasho (2008) – Historical development of Maxwell\'s demon\nWeiss, Peter. "Breaking the Law – Can quantum mechanics + thermodynamics = perpetual motion?", Science News, October 7, 2000', 'However, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".\nIn October 2000 astronomers Iván Almár and Jill Tarter presented a paper to The SETI Permanent Study Group in Rio de Janeiro, Brazil which proposed a scale (modelled after the Torino scale) which is an ordinal scale between zero and ten that quantifies the impact of any public announcement regarding evidence of extraterrestrial intelligence; the Rio scale has since inspired the 2005 San Marino Scale (in regard to the risks of transmissions from Earth) and the 2010 London Scale (in regard to the detection of extraterrestrial life). The Rio scale itself was revised in 2018.\nThe SETI Institute does not officially recognize the Wow! signal as of extraterrestrial origin as it was unable to be verified, although in a 2020 Twitter post the organization stated that \'\'an astronomer might have pinpointed the host star\'\'. The SETI Institute has also publicly denied that the candidate signal Radio source SHGb02+14a is of extraterrestrial origin. Although other volunteering projects such as Zooniverse credit users for discoveries, there is currently no crediting or early notification by SETI@Home following the discovery of a signal.\nSome people, including Steven M. Greer, have expressed cynicism that the general public might not be informed in the event of a genuine discovery of extraterrestrial intelligence due to significant vested interests. Some, such as Bruce Jakosky have also argued that the official disclosure of extraterrestrial life may have far reaching and as yet undetermined implications for society, particularly for the world\'s religions.\n== Active SETI ==\nActive SETI, also known as messaging to extraterrestrial intelligence (METI), consists of sending signals into space in the hope that they will be detected by an alien intelligence.\n=== Realized interstellar radio message projects ===\nIn November 1974, a largely symbolic attempt was made at the Arecibo Observatory to send a message to other worlds. Known as the Arecibo Message, it was sent towards the globular cluster M13, which is 25,000 light-years from Earth. Further IRMs Cosmic Call, Teen Age Message, Cosmic Call 2, and A Message From Earth were transmitted in 1999, 2001, 2003 and 2008 from the Evpatoria Planetary Radar.\n=== Debate ===\nWhether or not to attempt to contact extraterrestrials has attracted significant academic debate in the fields of space ethics and space policy. Physicist Stephen Hawking, in his book A Brief History of Time, suggests that "alerting" extraterrestrial intelligences to our existence is foolhardy, citing humankind\'s history of treating its own kind harshly in meetings of civilizations with a significant technology gap, e.g., the extermination of Tasmanian aborigines. He suggests, in view of this history, that we "lay low". In one response to Hawking, in September 2016, astronomer Seth Shostak sought to allay such concerns. Astronomer Jill Tarter also disagrees with Hawking, arguing that aliens developed and long-lived enough to communicate and travel across interstellar distances would have evolved a cooperative and less violent intelligence. She however thinks it is too soon for humans to attempt active SETI and that humans should be more advanced technologically first but keep listening in the meantime.\n== Criticism ==\nAs various SETI projects have progressed, some have criticized early claims by researchers as being too "euphoric". For example, Peter Schenkel, while remaining a supporter of SETI projects, wrote in 2006 that:\n[i]n light of new findings and insights, it seems appropriate to put excessive euphoria to rest and to take a more down-to-earth view [...] We should quietly admit that the early estimates—that there may be a million, a hundred thousand, or ten thousand advanced extraterrestrial civilizations in our galaxy—may no longer be tenable.\nCritics claim that the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability, as explained in a 2009 editorial in Nature, which said:\nSeti... has always sat at the edge of mainstream astronomy. This is partly because, no matter how scientifically rigorous its practitioners try to be, SETI can\'t escape an association with UFO believers and other such crackpots. But it is also because SETI is arguably not a falsifiable experiment. Regardless of how exhaustively the Galaxy is searched, the null result of radio silence doesn\'t rule out the existence of alien civilizations. It means only that those civilizations might not be using radio to communicate.\nNature added that SETI was "marked by a hope, bordering on faith" that aliens were aiming signals at us, that a hypothetical alien SETI project looking at Earth with "similar faith" would be "sorely disappointed", despite our many untargeted radar and TV signals, and our few targeted Active SETI radio signals denounced by those fearing aliens, and that it had difficulties attracting even sympathetic working scientists and government funding because it was "an effort so likely to turn up nothing".\nHowever, Nature also added, "Nonetheless, a small SETI effort is well worth supporting, especially given the enormous implications if it did succeed" and that "happily, a handful of wealthy technologists and other private donors have proved willing to provide that support".\nSupporters of the Rare Earth Hypothesis argue that advanced lifeforms are likely to be very rare, and that, if that is so, then SETI efforts will be futile. However, the Rare Earth Hypothesis itself faces many criticisms.\nIn 1993, Roy Mash stated that "Arguments favoring the existence of extraterrestrial intelligence nearly always contain an overt appeal to big numbers, often combined with a covert reliance on generalization from a single instance" and concluded that "the dispute between believers and skeptics is seen to boil down to a conflict of intuitions which can barely be engaged, let alone resolved, given our present state of knowledge". In response, in 2012, Milan M. Ćirković, then research professor at the Astronomical Observatory of Belgrade and a research associate of the Future of Humanity Institute at the University of Oxford, said that Mash was unrealistically over-reliant on excessive abstraction that ignored the empirical information available to modern SETI researchers.\nGeorge Basalla, Emeritus Professor of History at the University of Delaware, is a critic of SETI who argued in 2006 that "extraterrestrials discussed by scientists are as imaginary as the spirits and gods of religion or myth", and was in turn criticized by Milan M. Ćirković for, among other things, being unable to distinguish between "SETI believers" and "scientists engaged in SETI", who are often sceptical (especially about quick detection), such as Freeman Dyson and, at least in their later years, Iosif Shklovsky and Sebastian von Hoerner, and for ignoring the difference between the knowledge underlying the arguments of modern scientists and those of ancient Greek thinkers.\nMassimo Pigliucci, Professor of Philosophy at CUNY – City College, asked in 2010 whether SETI is "uncomfortably close to the status of pseudoscience" due to the lack of any clear point at which negative results cause the hypothesis of Extraterrestrial Intelligence to be abandoned, before eventually concluding that SETI is "almost-science", which is described by Milan M. Ćirković as Pigliucci putting SETI in "the illustrious company of string theory, interpretations of quantum mechanics, evolutionary psychology and history (of the \'synthetic\' kind done recently by Jared Diamond)", while adding that his justification for doing so with SETI "is weak, outdated, and reflecting particular philosophical prejudices similar to the ones described above in Mash and Basalla".\nRichard Carrigan, a particle physicist at the Fermi National Accelerator Laboratory near Chicago, Illinois, suggested that passive SETI could also be dangerous and that a signal released onto the Internet could act as a computer virus. Computer security expert Bruce Schneier dismissed this possibility as a "bizarre movie-plot threat".\n=== Ufology ===\nUfologist Stanton Friedman has often criticized SETI researchers for, among other reasons, what he sees as their unscientific criticisms of Ufology, but, unlike SETI, Ufology has generally not been embraced by academia as a scientific field of study, and it is usually characterized as a partial or total pseudoscience. In a 2016 interview, Jill Tarter pointed out that it is still a misconception that SETI and UFOs are related. She states, "SETI uses the tools of the astronomer to attempt to find evidence of somebody else\'s technology coming from a great distance. If we ever claim detection of a signal, we will provide evidence and data that can be independently confirmed. UFOs—none of the above." The Galileo Project headed by Harvard astronomer Avi Loeb is one of the few scientific efforts to study UFOs or UAPs. Loeb criticized that the study of UAP is often dismissed and not sufficiently studied by scientists and should shift from "occupying the talking points of national security administrators and politicians" to the realm of science. The Galileo Project\'s position after the publication of the 2021 UFO Report by the U.S. Intelligence community is that the scientific community needs to "systematically, scientifically and transparently look for potential evidence of extraterrestrial technological equipment".\n== See also ==\n== References ==\n== Further reading ==', 'Search for extraterrestrial intelligence', "Planck's law", 'In October 2019, Breakthrough Listen started a collaboration with scientists from the TESS team (Transiting Exoplanet Survey Satellite) to look for signs of advanced extraterrestrial life. Thousands of new planets found by TESS will be scanned for technosignatures by Breakthrough Listen partner facilities across the globe. Data from TESS monitoring of stars will also be searched for anomalies.\n=== FAST ===\nChina\'s 500 meter Aperture Spherical Telescope (FAST) lists detecting interstellar communication signals as part of its science mission. It is funded by the National Development and Reform Commission (NDRC) and managed by the National Astronomical observatories (NAOC) of the Chinese Academy of Sciences (CAS). FAST is the first radio observatory built with SETI as a core scientific goal. FAST consists of a fixed 500 m (1,600 ft) diameter spherical dish constructed in a natural depression sinkhole caused by karst processes in the region. It is the world\'s largest filled-aperture radio telescope.\nAccording to its website, FAST can search to 28 light-years, and is able to reach 1,400 stars. If the transmitter\'s radiated power were to be increased to 1,000,000 MW, FAST would be able to reach one million stars. This is compared to the former Arecibo 305 meter telescope detection distance of 18 light-years.\nOn 14 June 2022, astronomers, working with China\'s FAST telescope, reported the possibility of having detected artificial (presumably alien) signals, but cautioned that further studies were required to determine if a natural radio interference may be the source. More recently, on 18 June 2022, Dan Werthimer, chief scientist for several SETI-related projects, reportedly noted, "These signals are from radio interference; they are due to radio pollution from earthlings, not from E.T.".\n=== UCLA ===\nSince 2016, University of California Los Angeles (UCLA) undergraduate and graduate students have been participating in radio searches for technosignatures with the Green Bank Telescope. Targets include the Kepler field, TRAPPIST-1, and solar-type stars. The search is sensitive to Arecibo-class transmitters located within 420 light years of Earth and to transmitters that are 1,000 times more powerful than Arecibo located within 13,000 light years of Earth.\n== Community SETI projects ==\n=== SETI@home ===\nThe SETI@home project used volunteer computing to analyze signals acquired by the SERENDIP project.\nSETI@home was conceived by David Gedye along with Craig Kasnoff and is a popular volunteer computing project that was launched by the Berkeley SETI Research Center at the University of California, Berkeley, in May 1999. It was originally funded by The Planetary Society and Paramount Pictures, and later by the state of California. The project is run by director David P. Anderson and chief scientist Dan Werthimer. Any individual could become involved with SETI research by downloading the Berkeley Open Infrastructure for Network Computing (BOINC) software program, attaching to the SETI@home project, and allowing the program to run as a background process that uses idle computer power. The SETI@home program itself ran signal analysis on a "work unit" of data recorded from the central 2.5 MHz wide band of the SERENDIP IV instrument. After computation on the work unit was complete, the results were then automatically reported back to SETI@home servers at University of California, Berkeley. By June 28, 2009, the SETI@home project had over 180,000 active participants volunteering a total of over 290,000 computers. These computers gave SETI@home an average computational power of 617 teraFLOPS. In 2004 radio source SHGb02+14a set off speculation in the media that a signal had been detected but researchers noted the frequency drifted rapidly and the detection on three SETI@home computers fell within random chance.\nBy 2010, after 10 years of data collection, SETI@home had listened to that one frequency at every point of over 67 percent of the sky observable from Arecibo with at least three scans (out of the goal of nine scans), which covers about 20 percent of the full celestial sphere. On March 31, 2020, with 91,454 active users, the project stopped sending out new work to SETI@home users, bringing this particular SETI effort to an indefinite hiatus.\n=== SETI Net ===\nSETI Network was the only fully operational private search system. The SETI Net station consisted of off-the-shelf, consumer-grade electronics to minimize cost and to allow this design to be replicated as simply as possible. It had a 3-meter parabolic antenna that could be directed in azimuth and elevation, an LNA that covered 100 MHz of the 1420 MHz spectrum, a receiver to reproduce the wideband audio, and a standard personal computer as the control device and for deploying the detection algorithms. The antenna could be pointed and locked to one sky location in Ra and DEC which enabling the system to integrate on it for long periods. The Wow! signal area was monitored for many long periods. All search data was collected and is available on the Internet archive.\nSETI Net started operation in the early 1980s as a way to learn about the science of the search, and developed several software packages for the amateur SETI community. It provided an astronomical clock, a file manager to keep track of SETI data files, a spectrum analyzer optimized for amateur SETI, remote control of the station from the Internet, and other packages.\nSETI Net went dark and was decommissioned on 2021-12-04. The collected data is available on their website.\n=== The SETI League and Project Argus ===\nFounded in 1994 in response to the United States Congress cancellation of the NASA SETI program, The SETI League, Incorporated is a membership-supported nonprofit organization with 1,500 members in 62 countries. This grass-roots alliance of amateur and professional radio astronomers is headed by executive director emeritus H. Paul Shuch, the engineer credited with developing the world\'s first commercial home satellite TV receiver. Many SETI League members are licensed radio amateurs and microwave experimenters. Others are digital signal processing experts and computer enthusiasts.\nThe SETI League pioneered the conversion of backyard satellite TV dishes 3 to 5 m (10–16 ft) in diameter into research-grade radio telescopes of modest sensitivity. The organization concentrates on coordinating a global network of small, amateur-built radio telescopes under Project Argus, an all-sky survey seeking to achieve real-time coverage of the entire sky. Project Argus was conceived as a continuation of the all-sky survey component of the late NASA SETI program (the targeted search having been continued by the SETI Institute\'s Project Phoenix). There are currently 143 Project Argus radio telescopes operating in 27 countries. Project Argus instruments typically exhibit sensitivity on the order of 10−23 Watts/square metre, or roughly equivalent to that achieved by the Ohio State University Big Ear radio telescope in 1977, when it detected the landmark "Wow!" candidate signal.\nThe name "Argus" derives from the mythical Greek guard-beast who had 100 eyes, and could see in all directions at once. In the SETI context, the name has been used for radio telescopes in fiction (Arthur C. Clarke, "Imperial Earth"; Carl Sagan, "Contact"), was the name initially used for the NASA study ultimately known as "Cyclops," and is the name given to an omnidirectional radio telescope design being developed at the Ohio State University.\n== Optical experiments ==\nWhile most SETI sky searches have studied the radio spectrum, some SETI researchers have considered the possibility that alien civilizations might be using powerful lasers for interstellar communications at optical wavelengths. The idea was first suggested by R. N. Schwartz and Charles Hard Townes in a 1961 paper published in the journal Nature titled "Interstellar and Interplanetary Communication by Optical Masers". However, the 1971 Cyclops study discounted the possibility of optical SETI, reasoning that construction of a laser system that could outshine the bright central star of a remote star system would be too difficult. In 1983, Townes published a detailed study of the idea in the United States journal Proceedings of the National Academy of Sciences, which was met with interest by the SETI community.\nThere are two problems with optical SETI. The first problem is that lasers are highly "monochromatic", that is, they emit light only on one frequency, making it troublesome to figure out what frequency to look for. However, emitting light in narrow pulses results in a broad spectrum of emission; the spread in frequency becomes higher as the pulse width becomes narrower, making it easier to detect an emission.\nThe other problem is that while radio transmissions can be broadcast in all directions, lasers are highly directional. Interstellar gas and dust is almost transparent to near infrared, so these signals can be seen from greater distances, but the extraterrestrial laser signals would need to be transmitted in the direction of Earth in order to be detected.\nOptical SETI supporters have conducted paper studies of the effectiveness of using contemporary high-energy lasers and a ten-meter diameter mirror as an interstellar beacon. The analysis shows that an infrared pulse from a laser, focused into a narrow beam by such a mirror, would appear thousands of times brighter than the Sun to a distant civilization in the beam\'s line of fire. The Cyclops study proved incorrect in suggesting a laser beam would be inherently hard to see.', '=== Supersymmetric extensions of the Standard Model ===\nIncorporating supersymmetry into the Standard Model requires doubling the number of particles since there is no way that any of the particles in the Standard Model can be superpartners of each other. With the addition of new particles, there are many possible new interactions. The simplest possible supersymmetric model consistent with the Standard Model is the Minimal Supersymmetric Standard Model (MSSM) which can include the necessary additional new particles that are able to be superpartners of those in the Standard Model.\nOne of the original motivations for the Minimal Supersymmetric Standard Model came from the hierarchy problem. Due to the quadratically divergent contributions to the Higgs mass squared in the Standard Model, the quantum mechanical interactions of the Higgs boson causes a large renormalization of the Higgs mass and unless there is an accidental cancellation, the natural size of the Higgs mass is the greatest scale possible. Furthermore, the electroweak scale receives enormous Planck-scale quantum corrections. The observed hierarchy between the electroweak scale and the Planck scale must be achieved with extraordinary fine tuning. This problem is known as the hierarchy problem.\nSupersymmetry close to the electroweak scale, such as in the Minimal Supersymmetric Standard Model, would solve the hierarchy problem that afflicts the Standard Model. It would reduce the size of the quantum corrections by having automatic cancellations between fermionic and bosonic Higgs interactions, and Planck-scale quantum corrections cancel between partners and superpartners (owing to a minus sign associated with fermionic loops). The hierarchy between the electroweak scale and the Planck scale would be achieved in a natural manner, without extraordinary fine-tuning. If supersymmetry were restored at the weak scale, then the Higgs mass would be related to supersymmetry breaking which can be induced from small non-perturbative effects explaining the vastly different scales in the weak interactions and gravitational interactions.\nAnother motivation for the Minimal Supersymmetric Standard Model comes from grand unification, the idea that the gauge symmetry groups should unify at high-energy. In the Standard Model, however, the weak, strong and electromagnetic gauge couplings fail to unify at high energy. In particular, the renormalization group evolution of the three gauge coupling constants of the Standard Model is somewhat sensitive to the present particle content of the theory. These coupling constants do not quite meet together at a common energy scale if we run the renormalization group using the Standard Model. After incorporating minimal SUSY at the electroweak scale, the running of the gauge couplings are modified, and joint convergence of the gauge coupling constants is projected to occur at approximately 1016 GeV. The modified running also provides a natural mechanism for radiative electroweak symmetry breaking.\nIn many supersymmetric extensions of the Standard Model, such as the Minimal Supersymmetric Standard Model, there is a heavy stable particle (such as the neutralino) which could serve as a weakly interacting massive particle (WIMP) dark matter candidate. The existence of a supersymmetric dark matter candidate is related closely to R-parity. Supersymmetry at the electroweak scale (augmented with a discrete symmetry) typically provides a candidate dark matter particle at a mass scale consistent with thermal relic abundance calculations.\nThe standard paradigm for incorporating supersymmetry into a realistic theory is to have the underlying dynamics of the theory be supersymmetric, but the ground state of the theory does not respect the symmetry and supersymmetry is broken spontaneously. The supersymmetry break can not be done permanently by the particles of the MSSM as they currently appear. This means that there is a new sector of the theory that is responsible for the breaking. The only constraint on this new sector is that it must break supersymmetry permanently and must give superparticles TeV scale masses. There are many models that can do this and most of their details do not matter. In order to parameterize the relevant features of supersymmetry breaking, arbitrary soft SUSY breaking terms are added to the theory which temporarily break SUSY explicitly but could never arise from a complete theory of supersymmetry breaking.\nAll of these supersymmetric partners (sparticles) are hypothetical and have not been observed experimentally. They are predicted by various supersymmetric extensions of the Standard Model.\n=== Searches and constraints for supersymmetry ===\nSUSY extensions of the standard model are constrained by a variety of experiments, including measurements of low-energy observables – for example, the anomalous magnetic moment of the muon at Fermilab; the WMAP dark matter density measurement and direct detection experiments – for example, XENON-100 and LUX; and by particle collider experiments, including B-physics, Higgs phenomenology and direct searches for superpartners (sparticles), at the Large Electron–Positron Collider, Tevatron and the LHC. In fact, CERN publicly states that if a supersymmetric model of the Standard Model "is correct, supersymmetric particles should appear in collisions at the LHC."\nHistorically, the tightest limits were from direct production at colliders. The first mass limits for squarks and gluinos were made at CERN by the UA1 experiment and the UA2 experiment at the Super Proton Synchrotron. LEP later set very strong limits, which in 2006 were extended by the D0 experiment at the Tevatron. From 2003 to 2015, WMAP\'s and Planck\'s dark matter density measurements have strongly constrained supersymmetric extensions of the Standard Model, which, if they explain dark matter, have to be tuned to invoke a particular mechanism to sufficiently reduce the neutralino density.\nPrior to the beginning of the LHC, in 2009, fits of available data to CMSSM and NUHM1 indicated that squarks and gluinos were most likely to have masses in the 500 to 800 GeV range, though values as high as 2.5 TeV were allowed with low probabilities. Neutralinos and sleptons were expected to be quite light, with the lightest neutralino and the lightest stau most likely to be found between 100 and 150 GeV.\nThe first runs of the LHC surpassed existing experimental limits from the Large Electron–Positron Collider and Tevatron and partially excluded the aforementioned expected ranges. In 2011–12, the LHC discovered a Higgs boson with a mass of about 125 GeV, and with couplings to fermions and bosons which are consistent with the Standard Model. The MSSM predicts that the mass of the lightest Higgs boson should not be much higher than the mass of the Z boson, and, in the absence of fine tuning (with the supersymmetry breaking scale on the order of 1 TeV), should not exceed 135 GeV. The LHC found no previously unknown particles other than the Higgs boson which was already suspected to exist as part of the Standard Model, and therefore no evidence for any supersymmetric extension of the Standard Model.\nIndirect methods include the search for a permanent electric dipole moment (EDM) in the known Standard Model particles, which can arise when the Standard Model particle interacts with the supersymmetric particles. The current best constraint on the electron electric dipole moment put it to be smaller than 10−28 e·cm, equivalent to a sensitivity to new physics at the TeV scale and matching that of the current best particle colliders. A permanent EDM in any fundamental particle points towards time-reversal violating physics, and therefore also CP-symmetry violation via the CPT theorem. Such EDM experiments are also much more scalable than conventional particle accelerators and offer a practical alternative to detecting physics beyond the standard model as accelerator experiments become increasingly costly and complicated to maintain. The current best limit for the electron\'s EDM has already reached a sensitivity to rule out so called \'naive\' versions of supersymmetric extensions of the Standard Model.\nResearch in the late 2010s and early 2020s from experimental data on the cosmological constant, LIGO noise, and pulsar timing, suggests it\'s very unlikely that there are any new particles with masses much higher than those which can be found in the standard model or the LHC. However, this research has also indicated that quantum gravity or perturbative quantum field theory will become strongly coupled before 1 PeV, leading to other new physics in the TeVs.\n=== Current status ===\nThe negative findings in the experiments disappointed many physicists, who believed that supersymmetric extensions of the Standard Model (and other theories relying upon it) were by far the most promising theories for "new" physics beyond the Standard Model, and had hoped for signs of unexpected results from the experiments. In particular, the LHC result seems problematic for the Minimal Supersymmetric Standard Model, as the value of 125 GeV is relatively large for the model and can only be achieved with large radiative loop corrections from top squarks, which many theorists consider to be "unnatural" (see naturalness and fine tuning).', 'While supersymmetry has not been discovered at high energy, see Section Supersymmetry in particle physics, supersymmetry was found to be effectively realized at the intermediate energy of hadronic physics where baryons and mesons are superpartners. An exception is the pion that appears as a zero mode in the mass spectrum and thus protected by the supersymmetry: It has no baryonic partner. The realization of this effective supersymmetry is readily explained in quark–diquark models: Because two different color charges close together (e.g., blue and red) appear under coarse resolution as the corresponding anti-color (e.g. anti-green), a diquark cluster viewed with coarse resolution (i.e., at the energy-momentum scale used to study hadron structure) effectively appears as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves likes a meson.\n=== Supersymmetry in condensed matter physics ===\nSUSY concepts have provided useful extensions to the WKB approximation. Additionally, SUSY has been applied to disorder averaged systems both quantum and non-quantum (through statistical mechanics), the Fokker–Planck equation being an example of a non-quantum theory. The \'supersymmetry\' in all these systems arises from the fact that one is modelling one particle and as such the \'statistics\' do not matter. The use of the supersymmetry method provides a mathematical rigorous alternative to the replica trick, but only in non-interacting systems, which attempts to address the so-called \'problem of the denominator\' under disorder averaging. For more on the applications of supersymmetry in condensed matter physics see Efetov (1997).\nIn 2021, a group of researchers showed that, in theory,\n{\\displaystyle N=(0,1)}\nSUSY could be realised at the edge of a Moore–Read quantum Hall state. However, to date, no experiments have been done yet to realise it at an edge of a Moore–Read state. In 2022, a different group of researchers created a computer simulation of atoms in 1 dimensions that had supersymmetric topological quasiparticles.\n=== Supersymmetry in optics ===\nIn 2013, integrated optics was found to provide a fertile ground on which certain ramifications of SUSY can be explored in readily-accessible laboratory settings. Making use of the analogous mathematical structure of the quantum-mechanical Schrödinger equation and the wave equation governing the evolution of light in one-dimensional settings, one may interpret the refractive index distribution of a structure as a potential landscape in which optical wave packets propagate. In this manner, a new class of functional optical structures with possible applications in phase matching, mode conversion and space-division multiplexing becomes possible. SUSY transformations have been also proposed as a way to address inverse scattering problems in optics and as a one-dimensional transformation optics.\n=== Supersymmetry in dynamical systems ===\nAll stochastic (partial) differential equations, the models for all types of continuous time dynamical systems, possess topological supersymmetry. In the operator representation of stochastic evolution, the topological supersymmetry is the exterior derivative which is commutative with the stochastic evolution operator defined as the stochastically averaged pullback induced on differential forms by SDE-defined diffeomorphisms of the phase space. The topological sector of the so-emerging supersymmetric theory of stochastic dynamics can be recognized as the Witten-type topological field theory.\nThe meaning of the topological supersymmetry in dynamical systems is the preservation of the phase space continuity—infinitely close points will remain close during continuous time evolution even in the presence of noise. When the topological supersymmetry is broken spontaneously, this property is violated in the limit of the infinitely long temporal evolution and the model can be said to exhibit (the stochastic generalization of) the butterfly effect. From a more general perspective, spontaneous breakdown of the topological supersymmetry is the theoretical essence of the ubiquitous dynamical phenomenon variously known as chaos, turbulence, self-organized criticality etc. The Goldstone theorem explains the associated emergence of the long-range dynamical behavior that manifests itself as \u20601/f\u2060 noise, butterfly effect, and the scale-free statistics of sudden (instantonic) processes, such as earthquakes, neuroavalanches, and solar flares, known as the Zipf\'s law and the Richter scale.\n=== Supersymmetry in mathematics ===\nSUSY is also sometimes studied mathematically for its intrinsic properties. This is because it describes complex fields satisfying a property known as holomorphy, which allows holomorphic quantities to be exactly computed. This makes supersymmetric models useful "toy models" of more realistic theories. A prime example of this has been the demonstration of S-duality in four-dimensional gauge theories that interchanges particles and monopoles.\nThe proof of the Atiyah–Singer index theorem is much simplified by the use of supersymmetric quantum mechanics.\n=== Supersymmetry in string theory ===\nSupersymmetry is an integral part of string theory, a possible theory of everything. There are two types of string theory, supersymmetric string theory or superstring theory, and non-supersymmetric string theory. By definition of superstring theory, supersymmetry is required in superstring theory at some level. However, even in non-supersymmetric string theory, a type of supersymmetry called misaligned supersymmetry is still required in the theory in order to ensure no physical tachyons appear. Any string theories without some kind of supersymmetry, such as bosonic string theory and the\n{\\displaystyle E_{7}\\times E_{7}}\n16\n{\\displaystyle SU(16)}\n, and\n{\\displaystyle E_{8}}\nheterotic string theories, will have a tachyon and therefore the spacetime vacuum itself would be unstable and would decay into some tachyon-free string theory usually in a lower spacetime dimension. There is no experimental evidence that either supersymmetry or misaligned supersymmetry holds in our universe, and many physicists have moved on from supersymmetry and string theory entirely due to the non-detection of supersymmetry at the LHC.\nDespite the null results for supersymmetry at the LHC so far, some particle physicists have nevertheless moved to string theory in order to resolve the naturalness crisis for certain supersymmetric extensions of the Standard Model. According to the particle physicists, there exists a concept of "stringy naturalness" in string theory, where the string theory landscape could have a power law statistical pull on soft SUSY breaking terms to large values (depending on the number of hidden sector SUSY breaking fields contributing to the soft terms). If this is coupled with an anthropic requirement that contributions to the weak scale not exceed a factor between 2 and 5 from its measured value (as argued by Agrawal et al.), then the Higgs mass is pulled up to the vicinity of 125 GeV while most sparticles are pulled to values beyond the current reach of LHC. (The Higgs was determined to have a mass of 125 GeV ±0.15 GeV in 2022.) An exception occurs for higgsinos which gain mass not from SUSY breaking but rather from whatever mechanism solves the SUSY mu problem. Light higgsino pair production in association with hard initial state jet radiation leads to a soft opposite-sign dilepton plus jet plus missing transverse energy signal.\n== Supersymmetry in particle physics ==\nIn particle physics, a supersymmetric extension of the Standard Model is a possible candidate for undiscovered particle physics, and seen by some physicists as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to be incomplete and where limitations of current theories are well established. In particular, one supersymmetric extension of the Standard Model, the Minimal Supersymmetric Standard Model (MSSM), became popular in theoretical particle physics, as the Minimal Supersymmetric Standard Model is the simplest supersymmetric extension of the Standard Model that could resolve major hierarchy problems within the Standard Model, by guaranteeing that quadratic divergences of all orders will cancel out in perturbation theory. If a supersymmetric extension of the Standard Model is correct, superpartners of the existing elementary particles would be new and undiscovered particles and supersymmetry is expected to be spontaneously broken.\nThere is no experimental evidence that a supersymmetric extension to the Standard Model is correct, or whether or not other extensions to current models might be more accurate. It is only since around 2010 that particle accelerators specifically designed to study physics beyond the Standard Model have become operational (i.e. the Large Hadron Collider (LHC)), and it is not known where exactly to look, nor the energies required for a successful search. However, the negative results from the LHC since 2010 have already ruled out some supersymmetric extensions to the Standard Model, and many physicists believe that the Minimal Supersymmetric Standard Model, while not ruled out, is no longer able to fully resolve the hierarchy problem.\n=== Supersymmetric extensions of the Standard Model ===', 'Fusion powers stars and produces most elements lighter than cobalt in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 616 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.645% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and the result is fusion; this is an exothermic process.\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is 13.6 eV—less than one-millionth of the 17.6 MeV released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though individual fission reactions are generally much more energetic than individual fusion ones, which are themselves millions of times more energetic than chemical reactions. Via the mass–energy equivalence, fusion yields a 0.7% efficiency of reactant mass into energy. This can be only be exceeded by the extreme cases of the accretion process involving neutron stars or black holes, approaching 40% efficiency, and antimatter annihilation at 100% efficiency. (The complete conversion of one gram of matter would expel 9×1013 joules of energy.)\n== In astrophysics ==\nFusion is responsible for the astrophysical production of the majority of elements lighter than iron. This includes most types of Big Bang nucleosynthesis and stellar nucleosynthesis. Non-fusion processes that contribute include the s-process and r-process in neutron merger and supernova nucleosynthesis, responsible for elements heavier than iron.\n=== Stars ===\nAn important fusion process is the stellar nucleosynthesis that powers stars, including the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounts for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was unknown; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein\'s equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy had not yet been discovered, nor even that stars are largely composed of hydrogen (see metallicity). Eddington\'s paper reasoned that:\nThe leading theory of stellar energy, the contraction hypothesis, should cause the rotation of a star to visibly speed up due to conservation of angular momentum. But observations of Cepheid variable stars showed this was not happening.\nThe only other known plausible source of energy was conversion of matter to energy; Einstein had shown some years earlier that a small amount of matter was equivalent to a large amount of energy.\nFrancis Aston had also recently shown that the mass of a helium atom was about 0.8% less than the mass of the four hydrogen atoms which would, combined, form a helium atom (according to the then-prevailing theory of atomic structure which held atomic weight to be the distinguishing property between elements; work by Henry Moseley and Antonius van den Broek would later show that nucleic charge was the distinguishing property and that a helium nucleus, therefore, consisted of two hydrogen nuclei plus additional mass). This suggested that if such a combination could happen, it would release considerable energy as a byproduct.\nIf a star contained just 5% of fusible hydrogen, it would suffice to explain how stars got their energy. (It is now known that most \'ordinary\' stars are usually made of around 70% to 75% hydrogen)\nFurther elements might also be fused, and other scientists had speculated that stars were the "crucible" in which light elements combined to create heavy elements, but without more accurate measurements of their atomic masses nothing more could be said at the time.\nAll of these speculations were proven correct in the following decades.\nThe primary source of solar energy, and that of similar size stars, is the fusion of hydrogen to form helium (the proton–proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to fuse heavier elements. In massive cores, silicon-burning is the final fusion cycle, leading to a build-up of iron and nickel nuclei.\nNuclear binding energy makes the production of elements heavier than nickel via fusion energetically unfavorable. These elements are produced in non-fusion processes: the s-process, r-process, and the variety of processes that can produce p-nuclei. Such processes occur in giant star shells, or supernovae, or neutron star mergers.\n=== Brown dwarfs ===\nBrown dwarfs fuse deuterium and in very high mass cases also fuse lithium.\n=== White dwarfs ===\nCarbon-oxygen white dwarfs, which accrete matter either from an active stellar companion or white dwarf merger, approach the Chandrasekhar limit of 1.44 solar masses. Immediately prior, carbon burning fusion begins, destroying the Earth-sized dwarf within one second, in a Type Ia supernova.\nMuch more rarely, helium white dwarfs may merge, which does not cause an explosion but begins helium burning in an extreme type of helium star.\n=== Neutron stars ===\nSome neutron stars accrete hydrogen and helium from an active stellar companion. Periodically, the helium accretion reaches a critical level, and a thermonuclear burn wave propagates across the surface, on the timescale of one second.\n=== Black hole accretion disks ===\nSimilar to stellar fusion, extreme conditions within black hole accretion disks can allow fusion reactions. Calculations show the most energetic reactions occur around lower stellar mass black holes, below 10 solar masses, compared to those above 100. Beyond five Schwarzschild radii, carbon-burning and fusion of helium-3 dominates the reactions. Within this distance, around lower mass black holes, fusion of nitrogen, oxygen, neon, and magnesium can occur. In the extreme limit, the silicon-burning process can begin with the fusion of silicon and selenium nuclei.\n=== Big Bang ===\nFrom the period approximately 10 seconds to 20 minutes after the Big Bang, the universe cooled from over 100 keV to 1 keV. This allowed the combination of protons and neutrons in deuterium nuclei, and beginning a rapid fusion chain into tritium and helium-3 and ending in predominantly helium-4, with a minimal fraction of lithium, beryllium, and boron nuclei.\n== Requirements ==\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbors due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface-area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.']

Question: What is the Maxwell's Demon thought experiment?

Choices:
Choice A) A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with different gases at equal temperatures. The demon selectively allows molecules to pass from one side to the other, causing an increase in temperature in one part and a decrease in temperature in the other, contrary to the second law of thermodynamics.
Choice B) A thought experiment in which a demon guards a macroscopic trapdoor in a wall separating two parts of a container filled with different gases at different temperatures. The demon selectively allows molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, in accordance with the second law of thermodynamics.
Choice C) A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with the same gas at equal temperatures. The demon selectively allows faster-than-average molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, contrary to the second law of thermodynamics.
Choice D) A thought experiment in which a demon guards a macroscopic trapdoor in a wall separating two parts of a container filled with the same gas at equal temperatures. The demon selectively allows faster-than-average molecules to pass from one side to the other, causing an increase in temperature in one part and a decrease in temperature in the other, contrary to the second law of thermodynamics.
Choice E) A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with the same gas at different temperatures. The demon selectively allows slower-than-average molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, in accordance with the second law of thermodynamics.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Crystallography', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', 'An early use of the piezoelectricity of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. The quartz clock is a familiar device using the mineral. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.\nAlmost all the industrial demand for quartz crystal (used primarily in electronics) is met with synthetic quartz produced by the hydrothermal process. However, synthetic crystals are less prized for use as gemstones. The popularity of crystal healing has increased the demand for natural quartz crystals, which are now often mined in developing countries using primitive mining methods, sometimes involving child labor.\n== See also ==\nFused quartz\nList of minerals\nQuartz fiber\nQuartz reef mining\nQuartzolite\nShocked quartz\n== References ==\n== External links ==\nQuartz varieties, properties, crystal morphology. Photos and illustrations\nGilbert Hart, "Nomenclature of Silica", American Mineralogist, Volume 12, pp. 383–395. 1927\n"The Quartz Watch – Inventors". The Lemelson Center, National Museum of American History, Smithsonian Institution. Archived from the original on 7 January 2009.\nTerminology used to describe the characteristics of quartz crystals when used as oscillators\nQuartz use as prehistoric stone tool raw material', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word κρύσταλλος (krústallos; "clear ice, rock-crystal"), and γράφειν (gráphein; "to write"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well new developments in these fields.\n== History and timeline ==\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the Davisson–Germer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n== Methodology ==\nCrystallographic methods depend mainly on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. Crystallographers often explicitly state the type of beam used, as in the terms X-ray diffraction, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.\nX-rays interact with the spatial distribution of electrons in the sample.\nNeutrons are scattered by the atomic nuclei through the strong nuclear forces, but in addition the magnetic moment of neutrons is non-zero, so they are also scattered by magnetic fields. When neutrons are scattered from hydrogen-containing materials, they produce diffraction patterns with high noise levels, which can sometimes be resolved by substituting deuterium for hydrogen.\nElectrons are charged particles and therefore interact with the total charge distribution of both the atomic nuclei and the electrons of the sample.:\u200aChpt 4\nIt is hard to focus x-rays or neutrons, but since electrons are charged they can be focused and are used in electron microscope to produce magnified images. There are many ways that transmission electron microscopy and related techniques such as scanning transmission electron microscopy, high-resolution electron microscopy can be used to obtain images with in many cases atomic resolution from which crystallographic information can be obtained. There are also other methods such as low-energy electron diffraction, low-energy electron microscopy and reflection high-energy electron diffraction which can be used to obtain crystallographic information about surfaces.\n== Applications in various areas ==\n=== Materials science ===\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Most materials do not occur as a single crystal, but are poly-crystalline in nature (they exist as an aggregate of small crystals with different orientations). As such, powder diffraction techniques, which take diffraction patterns of samples with a large number of crystals, play an important role in structural determination.\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements. Crystallographic studies help elucidate the relationship between a material\'s structure and its properties, aiding in developing new materials with tailored characteristics. This understanding is crucial in various fields, including metallurgy, geology, and materials science. Advancements in crystallographic techniques, such as electron diffraction and X-ray crystallography, continue to expand our understanding of material behavior at the atomic level.\nIn another example, iron transforms from a body-centered cubic (bcc) structure called ferrite to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material\'s properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which structures are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory.\n=== Biology ===\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. The double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol, Pymol or VMD can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron diffraction has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n== Notation ==\nCoordinates in square brackets such as [100] denote a direction vector (in real space).\nCoordinates in angle brackets or chevrons such as <100> denote a family of directions which are related by symmetry operations. In the cubic crystal system for example, <100> would mean [100], [010], [001] or the negative of any of those directions.\nMiller indices in parentheses such as (100) denote a plane of the crystal structure, and regular repetitions of that plane with a particular spacing. In the cubic system, the normal to the (hkl) plane is the direction [hkl], but in lower-symmetry cases, the normal to (hkl) is not parallel to [hkl].\nIndices in curly brackets or braces such as {100} denote a family of planes and their normals. In cubic materials the symmetry makes them equivalent, just as the way angle brackets denote a family of directions. In non-cubic materials, <hkl> is not necessarily perpendicular to {hkl}.\n== Reference literature ==\nThe International Tables for Crystallography is an eight-book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure through x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and do not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:', "== Importance ==\nFrom the perspective of a planetary geologist, the atmosphere acts to shape a planetary surface. Wind picks up dust and other particles which, when they collide with the terrain, erode the relief and leave deposits (eolian processes). Frost and precipitations, which depend on the atmospheric composition, also influence the relief. Climate changes can influence a planet's geological history. Conversely, studying the surface of the Earth leads to an understanding of the atmosphere and climate of other planets.\nFor a meteorologist, the composition of the Earth's atmosphere is a factor affecting the climate and its variations.\nFor a biologist or paleontologist, the Earth's atmospheric composition is closely dependent on the appearance of life and its evolution.\n== See also ==\nAtmometer (evaporimeter)\nAtmospheric pressure\nInternational Standard Atmosphere\nKármán line\nSky\n== References ==\n== Further reading ==\nSanchez-Lavega, Agustin (2010). An Introduction to Planetary Atmospheres. Taylor & Francis. ISBN 978-1420067323.\n== External links ==\nProperties of atmospheric strata – The flight environment of the atmosphere\nAtmosphere – Everything you need to know", '=== Progenitor ===\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic type Ic-BL events that are associated with long-duration gamma-ray bursts.\n== External impact ==\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\n=== Source of heavy elements ===\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\n=== Role in stellar evolution ===\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n=== Cosmic rays ===\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\n=== Gravitational waves ===', 'Quartz', 'Petrosian magnitudes have the advantage of being redshift and distance independent, allowing the measurement of the galaxy\'s apparent size since the Petrosian radius is defined in terms of the galaxy\'s overall luminous flux.\nA critique of an earlier version of this method has been issued by the Infrared Processing and Analysis Center, with the method causing a magnitude of error (upwards to 10%) of the values than using isophotal diameter. The use of Petrosian magnitudes also have the disadvantage of missing most of the light outside the Petrosian aperture, which is defined relative to the galaxy\'s overall brightness profile, especially for elliptical galaxies, with higher signal-to-noise ratios on higher distances and redshifts. A correction for this method has been issued by Graham et al. in 2005, based on the assumption that galaxies follow Sérsic\'s law.\n=== Near-infrared method ===\nThis method has been used by 2MASS as an adaptation from the previously used methods of isophotal measurement. Since 2MASS operates in the near infrared, which has the advantage of being able to recognize dimmer, cooler, and older stars, it has a different form of approach compared to other methods that normally use B-filter. The detail of the method used by 2MASS has been described thoroughly in a document by Jarrett et al., with the survey measuring several parameters.\nThe standard aperture ellipse (area of detection) is defined by the infrared isophote at the Ks band (roughly 2.2 μm wavelength) of 20 mag/arcsec2. Gathering the overall luminous flux of the galaxy has been employed by at least four methods: the first being a circular aperture extending 7 arcseconds from the center, an isophote at 20 mag/arcsec2, a "total" aperture defined by the radial light distribution that covers the supposed extent of the galaxy, and the Kron aperture (defined as 2.5 times the first-moment radius, an integration of the flux of the "total" aperture).\n== Larger-scale structures ==\nDeep-sky surveys show that galaxies are often found in groups and clusters. Solitary galaxies that have not significantly interacted with other galaxies of comparable mass in the past few billion years are relatively scarce. Only about 5% of the galaxies surveyed are isolated in this sense. However, they may have interacted and even merged with other galaxies in the past, and may still be orbited by smaller satellite galaxies.\nOn the largest scale, the universe is continually expanding, resulting in an average increase in the separation between individual galaxies (see Hubble\'s law). Associations of galaxies can overcome this expansion on a local scale through their mutual gravitational attraction. These associations formed early, as clumps of dark matter pulled their respective galaxies together. Nearby groups later merged to form larger-scale clusters. This ongoing merging process, as well as an influx of infalling gas, heats the intergalactic gas in a cluster to very high temperatures of 30–100 megakelvins. About 70–80% of a cluster\'s mass is in the form of dark matter, with 10–30% consisting of this heated gas and the remaining few percent in the form of galaxies.\nMost galaxies are gravitationally bound to a number of other galaxies. These form a fractal-like hierarchical distribution of clustered structures, with the smallest such associations being termed groups. A group of galaxies is the most common type of galactic cluster; these formations contain the majority of galaxies (as well as most of the baryonic mass) in the universe. To remain gravitationally bound to such a group, each member galaxy must have a sufficiently low velocity to prevent it from escaping (see Virial theorem). If there is insufficient kinetic energy, however, the group may evolve into a smaller number of galaxies through mergers.\nClusters of galaxies consist of hundreds to thousands of galaxies bound together by gravity. Clusters of galaxies are often dominated by a single giant elliptical galaxy, known as the brightest cluster galaxy, which, over time, tidally destroys its satellite galaxies and adds their mass to its own.\nSuperclusters contain tens of thousands of galaxies, which are found in clusters, groups and sometimes individually. At the supercluster scale, galaxies are arranged into sheets and filaments surrounding vast empty voids. Above this scale, the universe appears to be the same in all directions (isotropic and homogeneous), though this notion has been challenged in recent years by numerous findings of large-scale structures that appear to be exceeding this scale. The Hercules–Corona Borealis Great Wall, currently the largest structure in the universe found so far, is 10 billion light-years (three gigaparsecs) in length.\nThe Milky Way galaxy is a member of an association named the Local Group, a relatively small group of galaxies that has a diameter of approximately one megaparsec. The Milky Way and the Andromeda Galaxy are the two brightest galaxies within the group; many of the other member galaxies are dwarf companions of these two. The Local Group itself is a part of a cloud-like structure within the Virgo Supercluster, a large, extended structure of groups and clusters of galaxies centered on the Virgo Cluster. In turn, the Virgo Supercluster is a portion of the Laniakea Supercluster.\n== Magnetic fields ==\nGalaxies have magnetic fields of their own. A galaxy\'s magnetic field influences its dynamics in multiple ways, including affecting the formation of spiral arms and transporting angular momentum in gas clouds. The latter effect is particularly important, as it is a necessary factor for the gravitational collapse of those clouds, and thus for star formation.\nThe typical average equipartition strength for spiral galaxies is about 10 μG (microgauss) or 1 nT (nanotesla). By comparison, the Earth\'s magnetic field has an average strength of about 0.3 G (Gauss) or 30 μT (microtesla). Radio-faint galaxies like M 31 and M33, the Milky Way\'s neighbors, have weaker fields (about 5 μG), while gas-rich galaxies with high star-formation rates, like M 51, M 83 and NGC 6946, have 15 μG on average. In prominent spiral arms, the field strength can be up to 25 μG, in regions where cold gas and dust are also concentrated. The strongest total equipartition fields (50–100 μG) were found in starburst galaxies—for example, in M 82 and the Antennae; and in nuclear starburst regions, such as the centers of NGC 1097 and other barred galaxies.\n== Formation and evolution ==\n=== Formation ===\nCurrent models of the formation of galaxies in the early universe are based on the ΛCDM model. About 300,000 years after the Big Bang, atoms of hydrogen and helium began to form, in an event called recombination. Nearly all the hydrogen was neutral (non-ionized) and readily absorbed light, and no stars had yet formed. As a result, this period has been called the "dark ages". It was from density fluctuations (or anisotropic irregularities) in this primordial matter that larger structures began to appear. As a result, masses of baryonic matter started to condense within cold dark matter halos. These primordial structures allowed gasses to condense in to protogalaxies, large scale gas clouds that were precursors to the first galaxies.:\u200a6\nAs gas falls in to the gravity of the dark matter halos, its pressure and temperature rise. To condense further, the gas must radiate energy. This process was slow in the early universe dominated by hydrogen atoms and molecules which are inefficient radiators compared to heavier elements. As clumps of gas aggregate forming rotating disks, temperatures and pressures continue to increase. Some places within the disk reach high enough density to form stars.\nOnce protogalaxies began to form and contract, the first halo stars, called Population III stars, appeared within them. These were composed of primordial gas, almost entirely of hydrogen and helium.\nEmission from the first stars heats the remaining gas helping to trigger additional star formation; the ultraviolet light emission from the first generation of stars re-ionized the surrounding neutral hydrogen in expanding spheres eventually reaching the entire universe, an event called reionization. The most massive stars collapse in violent supernova explosions releasing heavy elements ("metals") into the interstellar medium.:\u200a14\u200a This metal content is incorporated into population II stars.\nTheoretical models for early galaxy formation have been verified and informed by a large number and variety of sophisticated astronomical observations.:\u200a43\u200a The photometric observations generally need spectroscopic confirmation due the large number mechanisms that can introduce systematic errors. For example, a high redshift (z ~ 16) photometric observation by James Webb Space Telescope (JWST) was later corrected to be closer to z ~ 5.\nNevertheless, confirmed observations from the JWST and other observatories are accumulating, allowing systematic comparison of early galaxies to predictions of theory.\nEvidence for individual Population III stars in early galaxies is even more challenging. Even seemingly confirmed spectroscopic evidence may turn out to have other origins. For example, astronomers reported HeII emission evidence for Population III stars in the Cosmos Redshift 7 galaxy, with a redshift value of 6.60. Subsequent observations found metallic emission lines, OIII, inconsistent with an early-galaxy star.:\u200a108\n=== Evolution ===\nOnce stars begin to form, emit radiation, and in some cases explode, the process of galaxy formation becomes very complex, involving interactions between the forces of gravity, radiation, and thermal energy. Many details are still poorly understood.']

Question: What are the four qualitative levels of crystallinity described by geologists?

Choices:
Choice A) Holocrystalline, hypocrystalline, hypercrystalline, and holohyaline
Choice B) Holocrystalline, hypocrystalline, hypohyaline, and holohyaline
Choice C) Holocrystalline, hypohyaline, hypercrystalline, and holohyaline
Choice D) Holocrystalline, hypocrystalline, hypercrystalline, and hyperhyaline
Choice E) Holocrystalline, hypocrystalline, hypohyaline, and hyperhyaline

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Shower-curtain effect\n\nThe shower-curtain effect in physics describes the phenomenon of a shower curtain being blown inward when a shower is running. The problem of identifying the cause of this effect has been featured in Scientific American magazine, with several theories given to explain the phenomenon but no definite conclusion.\nThe shower-curtain effect may also be used to describe the observation of how nearby phase front distortions of an optical wave are more severe than remote distortions of the same amplitude.\n== Hypotheses ==\n=== Buoyancy hypothesis ===\nAlso called chimney effect or stack effect, observes that warm air (from the hot shower) rises out over the shower curtain as cooler air (near the floor) pushes in under the curtain to replace the rising air.  By pushing the curtain in towards the shower, the (short range) vortex and Coandă effects become more significant. However, the shower-curtain effect persists when cold water is used, implying that this is not the sole mechanism.\n=== Bernoulli effect hypothesis ===\nThe most popular explanation given for the shower-curtain effect is Bernoulli\'s principle.  Bernoulli\'s principle states that an increase in velocity results in a decrease in pressure.  This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water.  This movement would be parallel to the plane of the shower curtain.  If air is moving across the inside surface of the shower curtain, Bernoulli\'s principle says the air pressure there will drop.  This would result in a pressure differential between the inside and outside, causing the curtain to move inward.  It would be strongest when the gap between the bather and the curtain is smallest, resulting in the curtain attaching to the bather.\n=== Horizontal vortex hypothesis ===\nA computer simulation of a typical bathroom found that none of the above theories pan out in their analysis, but instead found that the spray from the shower-head drives a horizontal vortex. This vortex has a low-pressure zone in the centre, which sucks the curtain.\nDavid Schmidt of the University of Massachusetts was awarded the 2001 Ig Nobel Prize in Physics for his partial solution to the question of why shower curtains billow inwards. He used a computational fluid dynamics code to achieve the results.  Professor Schmidt is adamant that this was done "for fun" in his own free time without the use of grants.\n=== Coandă effect ===\nThe Coandă effect, also known as "boundary layer attachment", is the tendency of a moving fluid to adhere to an adjacent wall.\n=== Condensation ===\nA hot shower will produce steam that condenses on the shower side of the curtain, lowering the pressure there.  In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.\n=== Air pressure ===\nColder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.\n== Solutions ==\nMany shower curtains come with features to reduce the shower-curtain effect. They may have adhesive suction cups on the bottom edges of the curtain, which are then pushed onto the sides of the shower when in use. Others may have magnets at the bottom, though these are not effective on acrylic or fiberglass tubs.\nIt is possible to use a telescopic shower curtain rod to block the curtain on its lower part and to prevent it from sucking inside.\nHanging the curtain rod higher or lower, or especially further away from the shower head, can reduce the effect. A convex shower rod can also be used to hold the curtain against the inside wall of a tub.\nA weight can be attached to a long string and the string attached to the curtain rod in the middle of the curtain (on the inside). Hanging the weight low against the curtain just above the rim of the shower pan or tub makes it an effective billowing deterrent without allowing the weight to hit the pan or tub and damage it.\nThere are a few alternative solutions that either attach to the shower curtain directly, attach to the shower rod or attach to the wall.\n== References ==\n== External links ==\nScientific American: Why does the shower curtain move toward the water?\nWhy does the shower curtain blow up and in instead of down and out?\nVideo demonstration of how this phenomenon could be solved.\nThe Straight Dope: Why does the shower curtain blow in despite the water pushing it out (revisited)?\n2001 Ig Nobel Prize Winners\nFluent NEWS: Shower Curtain Grabs Scientist – But He Lives to Tell Why\nArggh, Why Does the Shower Curtain Attack Me? by Joe Palca. All Things Considered, National Public Radio.  November 4, 2006. (audio)\nExperimental Investigation of the Influence of the Relative Position of the Scattering Layer on Image Quality: the Shower Curtain Effect\nThe shower curtain effect; ESA', 'In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.  The main forms of precipitation include drizzle, rain, Rain and snow mixed ("sleet" in Commonwealth usage), snow, ice pellets, graupel and hail. Precipitation occurs when a portion of the atmosphere becomes saturated with water vapor (reaching 100% relative humidity), so that the water condenses and "precipitates" or falls. Thus, fog and mist are not precipitation; their water vapor does not condense sufficiently to precipitate, so fog and mist do not fall. (Such a non-precipitating combination is a colloid.) Two processes, possibly acting together, can lead to air becoming saturated with water vapor: cooling the air or adding water vapor to the air. Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called showers.\nMoisture that is lifted or otherwise forced to rise over a layer of sub-freezing air at the surface may be condensed by the low temperature into clouds and rain. This process is typically active when freezing rain occurs. A stationary front is often present near the area of freezing rain and serves as the focus for forcing moist air to rise. Provided there is necessary and sufficient atmospheric moisture content, the moisture within the rising air will condense into clouds, namely nimbostratus and cumulonimbus if significant precipitation is involved. Eventually, the cloud droplets will grow large enough to form raindrops and descend toward the Earth where they will freeze on contact with exposed objects. Where relatively warm water bodies are present, for example due to water evaporation from lakes, lake-effect snowfall becomes a concern downwind of the warm lakes within the cold cyclonic flow around the backside of extratropical cyclones. Lake-effect snowfall can be locally heavy.  Thundersnow is possible within a cyclone\'s comma head and within lake effect precipitation bands. In mountainous areas, heavy precipitation is possible where upslope flow is maximized within windward sides of the terrain at elevation. On the leeward side of mountains, desert climates can exist due to the dry air caused by compressional heating. Most precipitation occurs within the tropics and is caused by convection.\nPrecipitation is a major component of the water cycle, and is responsible for depositing most of the fresh water on the planet. Approximately 505,000 cubic kilometres (121,000 cu mi) of water falls as precipitation each year: 398,000 cubic kilometres (95,000 cu mi) over oceans and 107,000 cubic kilometres (26,000 cu mi) over land. Given the Earth\'s surface area, that means the globally averaged annual precipitation is 990 millimetres (39 in), but over land it is only 715 millimetres (28.1 in). Climate classification systems such as the Köppen climate classification system use average annual rainfall to help differentiate between differing climate regimes. Global warming is already causing changes to weather, increasing precipitation in some geographies, and reducing it in others, resulting in additional extreme weather.\nPrecipitation may occur on other celestial bodies. Saturn\'s largest satellite, Titan, hosts methane precipitation as a slow-falling drizzle, which has been observed as rain puddles at its equator and polar regions.\n== Types ==\nMechanisms of producing precipitation include convective, stratiform, and orographic rainfall.  Convective processes involve strong vertical motions that can cause the overturning of the atmosphere in that location within an hour and cause heavy precipitation, while stratiform processes involve weaker upward motions and less intense precipitation.  Precipitation can be divided into three categories, based on whether it falls as liquid water, liquid water that freezes on contact with the surface, or ice. Mixtures of different types of precipitation, including types in different categories, can fall simultaneously. Liquid forms of precipitation include rain and drizzle. Rain or drizzle that freezes on contact within a subfreezing air mass is called "freezing rain" or "freezing drizzle". Frozen forms of precipitation include snow, ice needles, ice pellets, hail, and graupel.\n=== Measurement ===\nLiquid precipitation\nRainfall (including drizzle and rain) is usually measured using a rain gauge and expressed in units of millimeters (mm) of height or depth. Equivalently, it can be expressed as a physical quantity with dimension of volume of water per collection area, in units of liters per square meter (L/m2); as 1L = 1dm3 = 1mm·m2, the units of area (m2) cancel out, resulting in simply "mm". This also corresponds to an area density expressed in kg/m2, if assuming that 1 liter of water has a mass of 1 kg (water density), which is acceptable for most practical purposes. The corresponding English unit used is usually inches. In Australia before metrication, rainfall was also measured in "points", each of which was defined as one-hundredth of an inch.\nSolid precipitation\nA snow gauge is usually used to measure the amount of solid precipitation. Snowfall is usually measured in centimeters by letting snow fall into a container and then measure the height. The snow can then optionally be melted to obtain a water equivalent measurement in millimeters like for liquid precipitation. The relationship between snow height and water equivalent depends on the water content of the snow; the water equivalent can thus only provide a rough estimate of snow depth. Other forms of solid precipitation, such as snow pellets and hail or even rain and snow mixed ("sleet" in Commonwealth usage), can also be melted and measured as their respective water equivalents, usually expressed in millimeters as for liquid precipitation.\n== Air becomes saturated ==\n=== Cooling air to its dew point ===\nThe dew point is the temperature to which a parcel of air must be cooled in order to become saturated, and (unless super-saturation occurs) condenses to water.  Water vapor normally begins to condense on condensation nuclei such as dust, ice, and salt in order to form clouds. The cloud condensation nuclei concentration will determine the cloud microphysics. An elevated portion of a frontal zone forces broad areas of lift, which form cloud decks such as altostratus or cirrostratus.  Stratus is a stable cloud deck which tends to form when a cool, stable air mass is trapped underneath a warm air mass. It can also form due to the lifting of advection fog during breezy conditions.\nThere are four main mechanisms for cooling the air to its dew point: adiabatic cooling, conductive cooling, radiational cooling, and evaporative cooling. Adiabatic cooling occurs when air rises and expands. The air can rise due to convection, large-scale atmospheric motions, or a physical barrier such as a mountain (orographic lift). Conductive cooling occurs when the air comes into contact with a colder surface, usually by being blown from one surface to another, for example from a liquid water surface to colder land. Radiational cooling occurs due to the emission of infrared radiation, either by the air or by the surface underneath.  Evaporative cooling occurs when moisture is added to the air through evaporation, which forces the air temperature to cool to its wet-bulb temperature, or until it reaches saturation.\n=== Adding moisture to the air ===\nThe main ways water vapor is added to the air are: wind convergence into areas of upward motion, precipitation or virga falling from above, daytime heating evaporating water from the surface of oceans, water bodies or wet land, transpiration from plants, cool or dry air moving over warmer water, and lifting air over mountains.\n== Forms of precipitation ==\n=== Raindrops ===\nCoalescence occurs when water droplets fuse to create larger water droplets, or when water droplets freeze onto an ice crystal, which is known as the Bergeron process. The fall rate of very small droplets is negligible, hence clouds do not fall out of the sky; precipitation will only occur when these coalesce into larger drops. droplets with different size will have different terminal velocity that cause droplets collision and producing larger droplets, Turbulence will enhance the collision process. As these larger water droplets descend, coalescence continues, so that drops become heavy enough to overcome air resistance and fall as rain.\nRaindrops have sizes ranging from 5.1 to 20 millimetres (0.20 to 0.79 in) mean diameter, above which they tend to break up. Smaller drops are called cloud droplets, and their shape is spherical. As a raindrop increases in size, its shape becomes more oblate, with its largest cross-section facing the oncoming airflow. Contrary to the cartoon pictures of raindrops, their shape does not resemble a teardrop. Intensity and duration of rainfall are usually inversely related, i.e., high intensity storms are likely to be of short duration and low intensity storms can have a long duration.  Rain drops associated with melting hail tend to be larger than other rain drops.  The METAR code for rain is RA, while the coding for rain showers is SHRA.\n=== Ice pellets ===\nIce pellets ("sleet" in US usage) are a form of precipitation consisting of small, translucent balls of ice. Ice pellets are usually (but not always) smaller than hailstones.  They often bounce when they hit the ground, and generally do not freeze into a solid mass unless mixed with freezing rain. The METAR code for ice pellets is PL.', 'Ultraviolet radiation, also known as simply UV, is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights.\nThe photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms.:\u200a25–26\u200a  Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature.:\u200a28\u200a  Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact.\nFor humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength "extreme" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life.\nThe lower wavelength limit of the visible spectrum is conventionally taken as 400 nm.  Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range.  Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.\n== Visibility ==\nUltraviolet rays are not usable for normal human vision.\nThe lens of the human eye and surgically implanted lens produced since 1986 blocks most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy.\nPeople lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet.  Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds "true" UV vision.\n== History and discovery ==\n"Ultraviolet" means "beyond violet" (from Latin ultra, "beyond"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light.\nUV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them "(de-)oxidizing rays" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from "heat rays", discovered the previous year at the other end of the visible spectrum. The simpler term "chemical rays" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them "tithonic rays"). The terms "chemical rays" and "heat rays" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established.\nThe discovery of the ultraviolet radiation with wavelengths below 200 nm, named "vacuum ultraviolet" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided "unanimously" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen.\n== Subtypes ==\nThe electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348:\nSeveral solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum.\nVacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers.\nTechnology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes.\nExtreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer  satellite.\nSome sources use the distinction of "hard UV" and "soft UV". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with "hard UV" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions.\n== Solar ultraviolet ==\nVery hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth\'s atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum.\nThe atmosphere blocks about 77% of the Sun\'s UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth\'s surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth\'s surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On "partly cloudy" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB.', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'In the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein\'s proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal. This has at times been called Planck\'s "second theory".\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his \'third theory\', that both emission and absorption of light were quantal.\nThe colourful term "ultraviolet catastrophe" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck\'s thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of "catastrophe". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called \'photons\', and by Rayleigh and by Jeans.\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν. In contrast to Planck\'s and Einstein\'s formulas, Bohr\'s formula referred explicitly and categorically to energy levels of atoms. Bohr\'s formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck\'s model, the frequency\n{\\displaystyle \\nu }\nhas no immediate relation to frequencies that might describe those quantum states themselves.\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck\'s law. The actual word \'photon\' was invented still later, by G.N. Lewis in 1926, who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word \'photon\' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein\'s 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\nUltimately, Planck\'s law of black-body radiation contributed to Einstein\'s concept of quanta of light carrying linear momentum, which became the fundamental basis for the development of quantum mechanics.\nThe above-mentioned linearity of Planck\'s mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg\'s original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg\'s theory accounted for Bohr\'s above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states. At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg\'s paper and recognized the matrix character of Heisenberg\'s theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg\'s original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics. Heisenberg\'s explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck\'s oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = \u2060h/2π\u2060, and ω = 2πν denotes angular frequency, and less often the equivalent formula E = hν. This statement about a really existing and propagating light quantum, based on Einstein\'s, has a physical meaning different from that of Planck\'s above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\nAn article by Helge Kragh published in Physics World gives an account of this history.\n== See also ==\nEmissivity\nRadiance\nSakuma–Hattori equation\n== References ==\n=== Bibliography ===\n== External links ==\nSummary of Radiation\nRadiation of a Blackbody – interactive simulation to play with Planck\'s law\nScienceworld entry on Planck\'s Law', 'An atmosphere (from Ancient Greek  ἀτμός (atmós) \'vapour, steam\' and  σφαῖρα (sphaîra) \'sphere\') is a layer of gases that envelop an astronomical object, held in place by the gravity of the object. A planet retains an atmosphere when the gravity is great and the temperature of the atmosphere is low. A stellar atmosphere is the outer region of a star, which includes the layers above the opaque photosphere; stars of low temperature might have outer atmospheres containing compound molecules.\nThe atmosphere of Earth is composed of nitrogen (78%), oxygen (21%), argon (0.9%), carbon dioxide (0.04%) and trace gases. Most organisms use oxygen for respiration; lightning and bacteria perform nitrogen fixation which produces ammonia that is used to make nucleotides and amino acids; plants, algae, and cyanobacteria use carbon dioxide for photosynthesis. The layered composition of the atmosphere minimises the harmful effects of sunlight, ultraviolet radiation, solar wind, and cosmic rays and thus protects the organisms from genetic damage. The current composition of the atmosphere of the Earth is the product of billions of years of biochemical modification of the paleoatmosphere by living organisms.\n== Occurrence and compositions ==\n=== Origins ===\nAtmospheres are clouds of gas bound to and engulfing an astronomical focal point of sufficiently dominating mass, adding to its mass, possibly escaping from it or collapsing into it.\nBecause of the latter, such planetary nucleus can develop from interstellar molecular clouds or protoplanetary disks into rocky astronomical objects with varyingly thick atmospheres, gas giants or fusors.\nComposition and thickness is originally determined by the stellar nebula\'s chemistry and temperature, but can also by a product processes within the astronomical body outgasing a different atmosphere.\n=== Compositions ===\nThe atmospheres of the planets Venus and Mars are principally composed of carbon dioxide and nitrogen, argon and oxygen.\nThe composition of Earth\'s atmosphere is determined by the by-products of the life that it sustains. Dry air (mixture of gases) from Earth\'s atmosphere contains 78.08% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and traces of hydrogen, helium, and other "noble" gases (by volume), but generally a variable amount of water vapor is also present, on average about 1% at sea level.\nThe low temperatures and higher gravity of the Solar System\'s giant planets—Jupiter, Saturn, Uranus and Neptune—allow them more readily to retain gases with low molecular masses. These planets have hydrogen–helium atmospheres, with trace amounts of more complex compounds.\nTwo satellites of the outer planets possess significant atmospheres. Titan, a moon of Saturn, and Triton, a moon of Neptune, have atmospheres mainly of nitrogen. When in the part of its orbit closest to the Sun, Pluto has an atmosphere of nitrogen and methane similar to Triton\'s, but these gases are frozen when it is farther from the Sun.\nOther bodies within the Solar System have extremely thin atmospheres not in equilibrium. These include the Moon (sodium gas), Mercury (sodium gas), Europa (oxygen), Io (sulfur), and Enceladus (water vapor).\nThe first exoplanet whose atmospheric composition was determined is HD 209458b, a gas giant with a close orbit around a star in the constellation Pegasus. Its atmosphere is heated to temperatures over 1,000 K, and is steadily escaping into space. Hydrogen, oxygen, carbon and sulfur have been detected in the planet\'s inflated atmosphere.\n=== Atmospheres in the Solar System ===\nAtmosphere of the Sun\nAtmosphere of Mercury\nAtmosphere of Venus\nAtmosphere of Earth\nAtmosphere of the Moon\nAtmosphere of Mars\nAtmosphere of Ceres\nAtmosphere of Jupiter\nAtmosphere of Io\nAtmosphere of Callisto\nAtmosphere of Europa\nAtmosphere of Ganymede\nAtmosphere of Saturn\nAtmosphere of Titan\nAtmosphere of Enceladus\nAtmosphere of Uranus\nAtmosphere of Titania\nAtmosphere of Neptune\nAtmosphere of Triton\nAtmosphere of Pluto\n== Structure of atmosphere ==\n=== Earth ===\nThe atmosphere of Earth is composed of layers with different properties, such as specific gaseous composition, temperature, and pressure.\nThe troposphere is the lowest layer of the atmosphere. This extends from the planetary surface to the bottom of the stratosphere. The troposphere contains 75–80% of the mass of the atmosphere, and is the atmospheric layer wherein the weather occurs; the height of the troposphere varies between 17 km at the equator and 7.0 km at the poles.\nThe stratosphere extends from the top of the troposphere to the bottom of the mesosphere, and contains the ozone layer, at an altitude between 15 km and 35 km. It is the atmospheric layer that absorbs most of the ultraviolet radiation that Earth receives from the Sun.\nThe mesosphere ranges from 50 km to 85 km and is the layer wherein most meteors are incinerated before reaching the surface.\nThe thermosphere extends from an altitude of 85 km to the base of the exosphere at 690 km and contains the ionosphere, where solar radiation ionizes the atmosphere. The density of the ionosphere is greater at short distances from the planetary surface in the daytime and decreases as the ionosphere rises at night-time, thereby allowing a greater range of radio frequencies to travel greater distances.\nThe exosphere begins at 690 to 1,000 km from the surface, and extends to roughly 10,000 km, where it interacts with the magnetosphere of Earth.\n== Pressure ==\nAtmospheric pressure is the force (per unit-area) perpendicular to a unit-area of planetary surface, as determined by the weight of the vertical column of atmospheric gases. In said atmospheric model, the atmospheric pressure, the weight of the mass of the gas, decreases at high altitude because of the diminishing mass of the gas above the point of barometric measurement. The units of air pressure are based upon the standard atmosphere (atm), which is 101,325 Pa (equivalent to 760 Torr or 14.696 psi). The height at which the atmospheric pressure declines by a factor of e (an irrational number equal to 2.71828) is called the scale height (H). For an atmosphere of uniform temperature, the scale height is proportional to the atmospheric temperature and is inversely proportional to the product of the mean molecular mass of dry air, and the local acceleration of gravity at the point of barometric measurement.\n== Escape ==\nSurface gravity differs significantly among the planets. For example, the large gravitational force of the giant planet Jupiter retains light gases such as hydrogen and helium that escape from objects with lower gravity. Secondly, the distance from the Sun determines the energy available to heat atmospheric gas to the point where some fraction of its molecules\' thermal motion exceed the planet\'s escape velocity, allowing those to escape a planet\'s gravitational grasp. Thus, distant and cold Titan, Triton, and Pluto are able to retain their atmospheres despite their relatively low gravities.\nSince a collection of gas molecules may be moving at a wide range of velocities, there will always be some fast enough to produce a slow leakage of gas into space. Lighter molecules move faster than heavier ones with the same thermal kinetic energy, and so gases of low molecular weight are lost more rapidly than those of high molecular weight. It is thought that Venus and Mars may have lost much of their water when, after being photodissociated into hydrogen and oxygen by solar ultraviolet radiation, the hydrogen escaped. Earth\'s magnetic field helps to prevent this, as, normally, the solar wind would greatly enhance the escape of hydrogen. However, over the past 3 billion years Earth may have lost gases through the magnetic polar regions due to auroral activity, including a net 2% of its atmospheric oxygen. The net effect, taking the most important escape processes into account, is that an intrinsic magnetic field does not protect a planet from atmospheric escape and that for some magnetizations the presence of a magnetic field works to increase the escape rate.\nOther mechanisms that can cause atmosphere depletion are solar wind-induced sputtering, impact erosion, weathering, and sequestration—sometimes referred to as "freezing out"—into the regolith and polar caps.\n== Terrain ==\nAtmospheres have dramatic effects on the surfaces of rocky bodies. Objects that have no atmosphere, or that have only an exosphere, have terrain that is covered in craters. Without an atmosphere, the planet has no protection from meteoroids, and all of them collide with the surface as meteorites and create craters.\nFor planets with a significant atmosphere, most meteoroids burn up as meteors before hitting a planet\'s surface. When meteoroids do impact, the effects are often erased by the action of wind.\nWind erosion is a significant factor in shaping the terrain of rocky planets with atmospheres, and over time can erase the effects of both craters and volcanoes. In addition, since liquids cannot exist without pressure, an atmosphere allows liquid to be present at the surface, resulting in lakes, rivers and oceans. Earth and Titan are known to have liquids at their surface and terrain on the planet suggests that Mars had liquid on its surface in the past.\n=== Outside the Solar System ===\nAtmosphere of HD 209458 b\n== Circulation ==\nThe circulation of the atmosphere occurs due to thermal differences when convection becomes a more efficient transporter of heat than thermal radiation. On planets where the primary heat source is solar radiation, excess heat in the tropics is transported to higher latitudes. When a planet generates a significant amount of heat internally, such as is the case for Jupiter, convection in the atmosphere can transport thermal energy from the higher temperature interior up to the surface.\n== Importance ==', 'UVC LEDs are relatively new to the commercial market and are gaining in popularity. Due to their monochromatic nature (±5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.\nDisinfection using UV radiation is commonly used in wastewater treatment applications and is finding an increased usage in municipal drinking water treatment. Many bottlers of spring water use UV disinfection equipment to sterilize their water. Solar water disinfection has been researched for cheaply treating contaminated water using natural sunlight. The UVA irradiation and increased water temperature kill organisms in the water.\nUltraviolet radiation is used in several food processes to kill unwanted microorganisms. UV can be used to pasteurize fruit juices by flowing the juice over a high-intensity ultraviolet source. The effectiveness of such a process depends on the UV absorbance of the juice.\nPulsed light (PL) is a technique of killing microorganisms on surfaces using pulses of an intense broad spectrum, rich in UVC between 200 and 280 nm. Pulsed light works with xenon flash lamps that can produce flashes several times per second. Disinfection robots use pulsed UV.\nThe antimicrobial effectiveness of filtered far-UVC (222\u2009nm) light on a range of pathogens, including bacteria and fungi showed inhibition of pathogen growth, and since it has lesser harmful effects, it provides essential insights for reliable disinfection in healthcare settings, such as hospitals and long-term care homes. UVC has also been shown to be effective at degrading SARS-CoV-2 virus.\n==== Biological ====\nSome animals, including birds, reptiles, and insects such as bees, can see near-ultraviolet wavelengths. Many fruits, flowers, and seeds stand out more strongly from the background in ultraviolet wavelengths as compared to human color vision. Scorpions glow or take on a yellow to green color under UV illumination, thus assisting in the control of these arachnids. Many birds have patterns in their plumage that are invisible at usual wavelengths but observable in ultraviolet, and the urine and other secretions of some animals, including dogs, cats, and human beings, are much easier to spot with ultraviolet. Urine trails of rodents can be detected by pest control technicians for proper treatment of infested dwellings.\nButterflies use ultraviolet as a communication system for sex recognition and mating behavior. For example, in the Colias eurytheme butterfly, males rely on visual cues to locate and identify females. Instead of using chemical stimuli to find mates, males are attracted to the ultraviolet-reflecting color of female hind wings. In Pieris napi butterflies it was shown that females in northern Finland with less UV-radiation present in the environment possessed stronger UV signals to attract their males than those occurring further south. This suggested that it was evolutionarily more difficult to increase the UV-sensitivity of the eyes of the males than to increase the UV-signals emitted by the females.\nMany insects use the ultraviolet wavelength emissions from celestial objects as references for flight navigation. A local ultraviolet emitter will normally disrupt the navigation process and will eventually attract the flying insect.\nThe green fluorescent protein (GFP) is often used in genetics as a marker. Many substances, such as proteins, have significant light absorption bands in the ultraviolet that are of interest in biochemistry and related fields. UV-capable spectrophotometers are common in such laboratories.\nUltraviolet traps called bug zappers are used to eliminate various small flying insects. They are attracted to the UV and are killed using an electric shock, or trapped once they come into contact with the device. Different designs of ultraviolet radiation traps are also used by entomologists for collecting nocturnal insects during faunistic survey studies.\n==== Therapy ====\nUltraviolet radiation is helpful in the treatment of skin conditions such as psoriasis and vitiligo. Exposure to UVA, while the skin is hyper-photosensitive, by taking psoralens is an effective treatment for psoriasis. Due to the potential of psoralens to cause damage to the liver, PUVA therapy may be used only a limited number of times over a patient\'s lifetime.\nUVB phototherapy does not require additional medications or topical preparations for the therapeutic benefit; only the exposure is needed. However, phototherapy can be effective when used in conjunction with certain topical treatments such as anthralin, coal tar, and vitamin A and D derivatives, or systemic treatments such as methotrexate and Soriatane.\n==== Herpetology ====\nReptiles need UVB for biosynthesis of vitamin D, and other metabolic processes. Specifically cholecalciferol (vitamin D3), which is needed for basic cellular / neural functioning as well as the utilization of calcium for bone and egg production. The UVA wavelength is also visible to many reptiles and might play a significant role in their ability survive in the wild as well as in visual communication between individuals. Therefore, in a typical reptile enclosure, a fluorescent UV a/b source (at the proper strength / spectrum for the species), must be available for many captive species to survive. Simple supplementation with cholecalciferol (Vitamin D3) will not be enough as there is a complete biosynthetic pathway that is "leapfrogged" (risks of possible overdoses), the intermediate molecules and metabolites also play important functions in the animals health. Natural sunlight in the right levels is always going to be superior to artificial sources, but this might not be possible for keepers in different parts of the world.\nIt is a known problem that high levels of output of the UVa part of the spectrum can both cause cellular and DNA damage to sensitive parts of their bodies – especially the eyes where blindness is the result of an improper UVa/b source use and placement photokeratitis. For many keepers there must also be a provision for an adequate heat source this has resulted in the marketing of heat and light "combination" products. Keepers should be careful of these "combination" light/ heat and UVa/b generators, they typically emit high levels of UVa with lower levels of UVb that are set and difficult to control so that animals can have their needs met. A better strategy is to use individual sources of these elements and so they can be placed and controlled by the keepers for the max benefit of the animals.\n== Evolutionary significance ==\nThe evolution of early reproductive proteins and enzymes is attributed in modern models of evolutionary theory to ultraviolet radiation. UVB causes thymine base pairs next to each other in genetic sequences to bond together into thymine dimers, a disruption in the strand that reproductive enzymes cannot copy. This leads to frameshifting during genetic replication and protein synthesis, usually killing the cell. Before formation of the UV-blocking ozone layer, when early prokaryotes approached the surface of the ocean, they almost invariably died out. The few that survived had developed enzymes that monitored the genetic material and removed thymine dimers by nucleotide excision repair enzymes. Many enzymes and proteins involved in modern mitosis and meiosis are similar to repair enzymes, and are believed to be evolved modifications of the enzymes originally used to overcome DNA damages caused by UV.\nElevated levels of ultraviolet radiation, in particular UV-B, have also been speculated as a cause of mass extinctions in the fossil record.\n== Photobiology ==\nPhotobiology is the scientific study of the beneficial and harmful interactions of non-ionizing radiation in living organisms, conventionally demarcated around 10 eV, the first ionization energy of oxygen. UV ranges roughly from 3 to 30 eV in energy. Hence photobiology entertains some, but not all, of the UV spectrum.\n== See also ==\n== References ==\n== Further reading ==\nAllen, Jeannie (6 September 2001). Ultraviolet Radiation: How it Affects Life on Earth. Earth Observatory. NASA, USA.\nHockberger, Philip E. (2002). "A History of Ultraviolet Photobiology for Humans, Animals and Microorganisms". Photochemistry and Photobiology. 76 (6): 561–569. doi:10.1562/0031-8655(2002)0760561AHOUPF2.0.CO2. PMID 12511035. S2CID 222100404.\nHu, S; Ma, F; Collado-Mesa, F; Kirsner, R. S. (July 2004). "UV radiation, latitude, and melanoma in US Hispanics and blacks". Arch. Dermatol. 140 (7): 819–824. doi:10.1001/archderm.140.7.819. PMID 15262692.\nStrauss, CEM; Funk, DJ (1991). "Broadly tunable difference-frequency generation of VUV using two-photon resonances in H2 and Kr". Optics Letters. 16 (15): 1192–4. Bibcode:1991OptL...16.1192S. doi:10.1364/ol.16.001192. PMID 19776917.\n== External links ==\nMedia related to Ultraviolet light at Wikimedia Commons\nThe dictionary definition of ultraviolet at Wiktionary', '== See also ==\n== References ==\n== Further reading ==\nAnderson, P.W., Basic Notions of Condensed Matter Physics, Perseus Publishing (1997).\nFaghri, A., and Zhang, Y., Fundamentals of Multiphase Heat Transfer and Flow, Springer Nature Switzerland AG, 2020.\nFisher, M.E. (1974). "The renormalization group in the theory of critical behavior". Rev. Mod. Phys. 46 (4): 597–616. Bibcode:1974RvMP...46..597F. doi:10.1103/revmodphys.46.597.\nGoldenfeld, N., Lectures on Phase Transitions and the Renormalization Group, Perseus Publishing (1992).\nIvancevic, Vladimir G; Ivancevic, Tijana T (2008), Chaos, Phase Transitions, Topology Change and Path Integrals, Berlin: Springer, ISBN 978-3-540-79356-4, retrieved 14 March 2013\nM.R. Khoshbin-e-Khoshnazar, Ice Phase Transition as a sample of finite system phase transition, (Physics Education (India) Volume 32. No. 2, Apr - Jun 2016)\nKleinert, H., Gauge Fields in Condensed Matter, Vol. I, "Superfluidity and Vortex lines; Disorder Fields, Phase Transitions", pp. 1–742, World Scientific (Singapore, 1989); Paperback ISBN 9971-5-0210-0 (physik.fu-berlin.de readable online)\nKleinert, Hagen; Verena Schulte-Frohlinde (2001). Critical Properties of φ4-Theories. World Scientific. ISBN 981-02-4659-5. Archived from the original on 26 February 2008. (readable online).\nKogut, J.; Wilson, K (1974). "The Renormalization Group and the epsilon-Expansion". Phys. Rep. 12 (2): 75–199. Bibcode:1974PhR....12...75W. doi:10.1016/0370-1573(74)90023-4.\nKrieger, Martin H., Constitutions of matter : mathematically modelling the most everyday of physical phenomena, University of Chicago Press, 1996. Contains a detailed pedagogical discussion of Onsager\'s solution of the 2-D Ising Model.\nLandau, L.D. and Lifshitz, E.M., Statistical Physics Part 1, vol. 5 of Course of Theoretical Physics, Pergamon Press, 3rd Ed. (1994).\nMussardo G., "Statistical Field Theory. An Introduction to Exactly Solved Models of Statistical Physics", Oxford University Press, 2010.\nSchroeder, Manfred R., Fractals, chaos, power laws : minutes from an infinite paradise, New York: W. H. Freeman, 1991.  Very well-written book in "semi-popular" style—not a textbook—aimed at an audience with some training in mathematics and the physical sciences.  Explains what scaling in phase transitions is all about, among other things.\nH. E. Stanley, Introduction to Phase Transitions and Critical Phenomena (Oxford University Press, Oxford and New York 1971).\nYeomans J. M., Statistical Mechanics of Phase Transitions, Oxford University Press, 1992.\n== External links ==\nMedia related to Phase changes at Wikimedia Commons\nInteractive Phase Transitions on lattices with Java applets\nUniversality classes from Sklogwiki', 'However, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".\nIn October 2000 astronomers Iván Almár and Jill Tarter presented a paper to The SETI Permanent Study Group in Rio de Janeiro, Brazil which proposed a scale (modelled after the Torino scale) which is an ordinal scale between zero and ten that quantifies the impact of any public announcement regarding evidence of extraterrestrial intelligence; the Rio scale has since inspired the 2005 San Marino Scale (in regard to the risks of transmissions from Earth) and the 2010 London Scale (in regard to the detection of extraterrestrial life). The Rio scale itself was revised in 2018.\nThe SETI Institute does not officially recognize the Wow! signal as of extraterrestrial origin as it was unable to be verified, although in a 2020 Twitter post the organization stated that \'\'an astronomer might have pinpointed the host star\'\'. The SETI Institute has also publicly denied that the candidate signal Radio source SHGb02+14a is of extraterrestrial origin. Although other volunteering projects such as Zooniverse credit users for discoveries, there is currently no crediting or early notification by SETI@Home following the discovery of a signal.\nSome people, including Steven M. Greer, have expressed cynicism that the general public might not be informed in the event of a genuine discovery of extraterrestrial intelligence due to significant vested interests. Some, such as Bruce Jakosky have also argued that the official disclosure of extraterrestrial life may have far reaching and as yet undetermined implications for society, particularly for the world\'s religions.\n== Active SETI ==\nActive SETI, also known as messaging to extraterrestrial intelligence (METI), consists of sending signals into space in the hope that they will be detected by an alien intelligence.\n=== Realized interstellar radio message projects ===\nIn November 1974, a largely symbolic attempt was made at the Arecibo Observatory to send a message to other worlds. Known as the Arecibo Message, it was sent towards the globular cluster M13, which is 25,000 light-years from Earth. Further IRMs Cosmic Call, Teen Age Message, Cosmic Call 2, and A Message From Earth were transmitted in 1999, 2001, 2003 and 2008 from the Evpatoria Planetary Radar.\n=== Debate ===\nWhether or not to attempt to contact extraterrestrials has attracted significant academic debate in the fields of space ethics and space policy. Physicist Stephen Hawking, in his book A Brief History of Time, suggests that "alerting" extraterrestrial intelligences to our existence is foolhardy, citing humankind\'s history of treating its own kind harshly in meetings of civilizations with a significant technology gap, e.g., the extermination of Tasmanian aborigines. He suggests, in view of this history, that we "lay low". In one response to Hawking, in September 2016, astronomer Seth Shostak sought to allay such concerns. Astronomer Jill Tarter also disagrees with Hawking, arguing that aliens developed and long-lived enough to communicate and travel across interstellar distances would have evolved a cooperative and less violent intelligence. She however thinks it is too soon for humans to attempt active SETI and that humans should be more advanced technologically first but keep listening in the meantime.\n== Criticism ==\nAs various SETI projects have progressed, some have criticized early claims by researchers as being too "euphoric". For example, Peter Schenkel, while remaining a supporter of SETI projects, wrote in 2006 that:\n[i]n light of new findings and insights, it seems appropriate to put excessive euphoria to rest and to take a more down-to-earth view [...] We should quietly admit that the early estimates—that there may be a million, a hundred thousand, or ten thousand advanced extraterrestrial civilizations in our galaxy—may no longer be tenable.\nCritics claim that the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability, as explained in a 2009 editorial in Nature, which said:\nSeti... has always sat at the edge of mainstream astronomy. This is partly because, no matter how scientifically rigorous its practitioners try to be, SETI can\'t escape an association with UFO believers and other such crackpots. But it is also because SETI is arguably not a falsifiable experiment. Regardless of how exhaustively the Galaxy is searched, the null result of radio silence doesn\'t rule out the existence of alien civilizations. It means only that those civilizations might not be using radio to communicate.\nNature added that SETI was "marked by a hope, bordering on faith" that aliens were aiming signals at us, that a hypothetical alien SETI project looking at Earth with "similar faith" would be "sorely disappointed", despite our many untargeted radar and TV signals, and our few targeted Active SETI radio signals denounced by those fearing aliens, and that it had difficulties attracting even sympathetic working scientists and government funding because it was "an effort so likely to turn up nothing".\nHowever, Nature also added, "Nonetheless, a small SETI effort is well worth supporting, especially given the enormous implications if it did succeed" and that "happily, a handful of wealthy technologists and other private donors have proved willing to provide that support".\nSupporters of the Rare Earth Hypothesis argue that advanced lifeforms are likely to be very rare, and that, if that is so, then SETI efforts will be futile. However, the Rare Earth Hypothesis itself faces many criticisms.\nIn 1993, Roy Mash stated that "Arguments favoring the existence of extraterrestrial intelligence nearly always contain an overt appeal to big numbers, often combined with a covert reliance on generalization from a single instance" and concluded that "the dispute between believers and skeptics is seen to boil down to a conflict of intuitions which can barely be engaged, let alone resolved, given our present state of knowledge". In response, in 2012, Milan M. Ćirković, then research professor at the Astronomical Observatory of Belgrade and a research associate of the Future of Humanity Institute at the University of Oxford, said that Mash was unrealistically over-reliant on excessive abstraction that ignored the empirical information available to modern SETI researchers.\nGeorge Basalla, Emeritus Professor of History at the University of Delaware, is a critic of SETI who argued in 2006 that "extraterrestrials discussed by scientists are as imaginary as the spirits and gods of religion or myth", and was in turn criticized by Milan M. Ćirković for, among other things, being unable to distinguish between "SETI believers" and "scientists engaged in SETI", who are often sceptical (especially about quick detection), such as Freeman Dyson and, at least in their later years, Iosif Shklovsky and Sebastian von Hoerner, and for ignoring the difference between the knowledge underlying the arguments of modern scientists and those of ancient Greek thinkers.\nMassimo Pigliucci, Professor of Philosophy at CUNY – City College, asked in 2010 whether SETI is "uncomfortably close to the status of pseudoscience" due to the lack of any clear point at which negative results cause the hypothesis of Extraterrestrial Intelligence to be abandoned, before eventually concluding that SETI is "almost-science", which is described by Milan M. Ćirković as Pigliucci putting SETI in "the illustrious company of string theory, interpretations of quantum mechanics, evolutionary psychology and history (of the \'synthetic\' kind done recently by Jared Diamond)", while adding that his justification for doing so with SETI "is weak, outdated, and reflecting particular philosophical prejudices similar to the ones described above in Mash and Basalla".\nRichard Carrigan, a particle physicist at the Fermi National Accelerator Laboratory near Chicago, Illinois, suggested that passive SETI could also be dangerous and that a signal released onto the Internet could act as a computer virus. Computer security expert Bruce Schneier dismissed this possibility as a "bizarre movie-plot threat".\n=== Ufology ===\nUfologist Stanton Friedman has often criticized SETI researchers for, among other reasons, what he sees as their unscientific criticisms of Ufology, but, unlike SETI, Ufology has generally not been embraced by academia as a scientific field of study, and it is usually characterized as a partial or total pseudoscience. In a 2016 interview, Jill Tarter pointed out that it is still a misconception that SETI and UFOs are related. She states, "SETI uses the tools of the astronomer to attempt to find evidence of somebody else\'s technology coming from a great distance. If we ever claim detection of a signal, we will provide evidence and data that can be independently confirmed. UFOs—none of the above." The Galileo Project headed by Harvard astronomer Avi Loeb is one of the few scientific efforts to study UFOs or UAPs. Loeb criticized that the study of UAP is often dismissed and not sufficiently studied by scientists and should shift from "occupying the talking points of national security administrators and politicians" to the realm of science. The Galileo Project\'s position after the publication of the 2021 UFO Report by the U.S. Intelligence community is that the scientific community needs to "systematically, scientifically and transparently look for potential evidence of extraterrestrial technological equipment".\n== See also ==\n== References ==\n== Further reading ==']

Question: What is the most popular explanation for the shower-curtain effect?

Choices:
Choice A) The pressure differential between the inside and outside of the shower
Choice B) The decrease in velocity resulting in an increase in pressure
Choice C) The movement of air across the outside surface of the shower curtain
Choice D) The use of cold water
Choice E) Bernoulli's principle

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Crossover study\n\nIn medicine, a crossover study or crossover trial is a longitudinal study in which subjects receive a sequence of different treatments (or exposures). While crossover studies can be observational studies, many important crossover studies are controlled experiments, which are discussed in this article. Crossover designs are common for experiments in many scientific disciplines, for example psychology, pharmaceutical science, and medicine.\nRandomized, controlled crossover experiments are especially important in health care. In a randomized clinical trial, the subjects are randomly assigned to different arms of the study which receive different treatments. When the trial has a repeated measures design, the same measures are collected multiple times for each subject. A crossover trial has a repeated measures design in which each patient is assigned to a sequence of two or more treatments, of which one may be a standard treatment or a placebo.\nNearly all crossover are designed to have "balance", whereby all subjects receive the same number of treatments and participate for the same number of periods. In most crossover trials each subject receives all treatments, in a random order.\nStatisticians suggest that designs should have four periods, which is more efficient than the two-period design, even if the study must be truncated to three periods. However, the two-period design is often taught in non-statistical textbooks, partly because of its simplicity.\n== Analysis ==\nThe data is analyzed using the statistical method that was specified in the clinical trial protocol, which must have been approved by the appropriate institutional review boards and regulatory agencies before the trial can begin. Most clinical trials are analyzed using repeated-measurements ANOVA (analysis of variance) or mixed models that include random effects.\nIn most longitudinal studies of human subjects, patients may withdraw from the trial or become "lost to follow-up". There are statistical methods for dealing with such missing-data and "censoring" problems. An important method analyzes the data according to the principle of the intention to treat.\n== Advantages ==\nA crossover study has two advantages over both a parallel study and a non-crossover longitudinal study. First, the influence of confounding covariates is reduced because each crossover patient serves as their own control. In a randomized non-crossover study it is often the case that different treatment-groups are found to be unbalanced on some covariates. In a controlled, randomized crossover designs, such imbalances are implausible (unless covariates were to change systematically during the study).\nSecond, optimal crossover designs are statistically efficient, and so require fewer subjects than do non-crossover designs (even other repeated measures designs).\nOptimal crossover designs are discussed in the graduate textbook by Jones and Kenward and in the review article by Stufken. Crossover designs are discussed along with more general repeated-measurements designs in the graduate textbook by Vonesh and Chinchilli.\n== Limitations and disadvantages ==\nThese studies are often done to improve the symptoms of patients with chronic conditions. For curative treatments or rapidly changing conditions, cross-over trials may be infeasible or unethical.\nCrossover studies often have two problems:\nFirst is the issue of "order" effects, because it is possible that the order in which treatments are administered may affect the outcome. An example might be a drug with many adverse effects given first, making patients taking a second, less harmful medicine, more sensitive to any adverse effect.\nSecond is the issue of "carry-over" between treatments, which confounds the estimates of the treatment effects. In practice, "carry-over" effects can be avoided with a sufficiently long "wash-out" period between treatments. However, planning for sufficiently long wash-out periods requires expert knowledge of the dynamics of the treatment, which is often unknown.\n== See also ==\nDesign of experiments\nGlossary of experimental design\nRandomized controlled trial\nSurvival analysis\nN of 1 trial\nSingle-subject design\n== Notes ==\n== References ==\nM. Bose and A. Dey (2009).  Optimal Crossover Designs.  World Scientific.  ISBN 978-9812818423\nD. E. Johnson (2010).  Crossover experiments.  WIREs Comp Stat, 2: 620-625.  [1]\nJones, Byron; Kenward, Michael G. (2014). Design and Analysis of Cross-Over Trials (Third ed.). London: Chapman and Hall. ISBN 978-0412606403.{{cite book}}:  CS1 maint: publisher location (link)\nK.-J. Lui, (2016).  Crossover Designs: Testing, Estimation, and Sample Size. Wiley.\nNajafi Mehdi, (2004).  Statistical Questions in Evidence Based Medicine.  New York: Oxford University Press.  ISBN 0-19-262992-1\nD. Raghavarao and L. Padgett (2014).  Repeated Measurements and Cross-Over Designs. Wiley.  ISBN 978-1-118-70925-2\nD. A. Ratkowsky, M. A. Evans, and J. R. Alldredge (1992).  Cross-Over Experiments: Design, Analysis, and Application. Marcel Dekker.  ISBN 978-0824788926\nSenn, S. (2002).  Cross-Over Trials in Clinical Research, Second edition. Wiley. ISBN 978-0-471-49653-3\nStufken, J. (1996). "Optimal Crossover Designs". In Ghosh, S.; Rao, C. R. (eds.). Design and Analysis of Experiments. Handbook of Statistics. Vol. 13. North-Holland. pp. 63–90. ISBN 978-0-444-82061-7.\nVonesh, Edward F.; Chinchilli, Vernon G. (1997). "Crossover Experiments". Linear and Nonlinear Models for the Analysis of Repeated Measurements. London: Chapman and Hall. pp. 111–202. ISBN 978-0824782481.{{cite book}}:  CS1 maint: publisher location (link)', '=== Supersymmetric extensions of the Standard Model ===\nIncorporating supersymmetry into the Standard Model requires doubling the number of particles since there is no way that any of the particles in the Standard Model can be superpartners of each other. With the addition of new particles, there are many possible new interactions. The simplest possible supersymmetric model consistent with the Standard Model is the Minimal Supersymmetric Standard Model (MSSM) which can include the necessary additional new particles that are able to be superpartners of those in the Standard Model.\nOne of the original motivations for the Minimal Supersymmetric Standard Model came from the hierarchy problem. Due to the quadratically divergent contributions to the Higgs mass squared in the Standard Model, the quantum mechanical interactions of the Higgs boson causes a large renormalization of the Higgs mass and unless there is an accidental cancellation, the natural size of the Higgs mass is the greatest scale possible. Furthermore, the electroweak scale receives enormous Planck-scale quantum corrections. The observed hierarchy between the electroweak scale and the Planck scale must be achieved with extraordinary fine tuning. This problem is known as the hierarchy problem.\nSupersymmetry close to the electroweak scale, such as in the Minimal Supersymmetric Standard Model, would solve the hierarchy problem that afflicts the Standard Model. It would reduce the size of the quantum corrections by having automatic cancellations between fermionic and bosonic Higgs interactions, and Planck-scale quantum corrections cancel between partners and superpartners (owing to a minus sign associated with fermionic loops). The hierarchy between the electroweak scale and the Planck scale would be achieved in a natural manner, without extraordinary fine-tuning. If supersymmetry were restored at the weak scale, then the Higgs mass would be related to supersymmetry breaking which can be induced from small non-perturbative effects explaining the vastly different scales in the weak interactions and gravitational interactions.\nAnother motivation for the Minimal Supersymmetric Standard Model comes from grand unification, the idea that the gauge symmetry groups should unify at high-energy. In the Standard Model, however, the weak, strong and electromagnetic gauge couplings fail to unify at high energy. In particular, the renormalization group evolution of the three gauge coupling constants of the Standard Model is somewhat sensitive to the present particle content of the theory. These coupling constants do not quite meet together at a common energy scale if we run the renormalization group using the Standard Model. After incorporating minimal SUSY at the electroweak scale, the running of the gauge couplings are modified, and joint convergence of the gauge coupling constants is projected to occur at approximately 1016 GeV. The modified running also provides a natural mechanism for radiative electroweak symmetry breaking.\nIn many supersymmetric extensions of the Standard Model, such as the Minimal Supersymmetric Standard Model, there is a heavy stable particle (such as the neutralino) which could serve as a weakly interacting massive particle (WIMP) dark matter candidate. The existence of a supersymmetric dark matter candidate is related closely to R-parity. Supersymmetry at the electroweak scale (augmented with a discrete symmetry) typically provides a candidate dark matter particle at a mass scale consistent with thermal relic abundance calculations.\nThe standard paradigm for incorporating supersymmetry into a realistic theory is to have the underlying dynamics of the theory be supersymmetric, but the ground state of the theory does not respect the symmetry and supersymmetry is broken spontaneously. The supersymmetry break can not be done permanently by the particles of the MSSM as they currently appear. This means that there is a new sector of the theory that is responsible for the breaking. The only constraint on this new sector is that it must break supersymmetry permanently and must give superparticles TeV scale masses. There are many models that can do this and most of their details do not matter. In order to parameterize the relevant features of supersymmetry breaking, arbitrary soft SUSY breaking terms are added to the theory which temporarily break SUSY explicitly but could never arise from a complete theory of supersymmetry breaking.\nAll of these supersymmetric partners (sparticles) are hypothetical and have not been observed experimentally. They are predicted by various supersymmetric extensions of the Standard Model.\n=== Searches and constraints for supersymmetry ===\nSUSY extensions of the standard model are constrained by a variety of experiments, including measurements of low-energy observables – for example, the anomalous magnetic moment of the muon at Fermilab; the WMAP dark matter density measurement and direct detection experiments – for example, XENON-100 and LUX; and by particle collider experiments, including B-physics, Higgs phenomenology and direct searches for superpartners (sparticles), at the Large Electron–Positron Collider, Tevatron and the LHC. In fact, CERN publicly states that if a supersymmetric model of the Standard Model "is correct, supersymmetric particles should appear in collisions at the LHC."\nHistorically, the tightest limits were from direct production at colliders. The first mass limits for squarks and gluinos were made at CERN by the UA1 experiment and the UA2 experiment at the Super Proton Synchrotron. LEP later set very strong limits, which in 2006 were extended by the D0 experiment at the Tevatron. From 2003 to 2015, WMAP\'s and Planck\'s dark matter density measurements have strongly constrained supersymmetric extensions of the Standard Model, which, if they explain dark matter, have to be tuned to invoke a particular mechanism to sufficiently reduce the neutralino density.\nPrior to the beginning of the LHC, in 2009, fits of available data to CMSSM and NUHM1 indicated that squarks and gluinos were most likely to have masses in the 500 to 800 GeV range, though values as high as 2.5 TeV were allowed with low probabilities. Neutralinos and sleptons were expected to be quite light, with the lightest neutralino and the lightest stau most likely to be found between 100 and 150 GeV.\nThe first runs of the LHC surpassed existing experimental limits from the Large Electron–Positron Collider and Tevatron and partially excluded the aforementioned expected ranges. In 2011–12, the LHC discovered a Higgs boson with a mass of about 125 GeV, and with couplings to fermions and bosons which are consistent with the Standard Model. The MSSM predicts that the mass of the lightest Higgs boson should not be much higher than the mass of the Z boson, and, in the absence of fine tuning (with the supersymmetry breaking scale on the order of 1 TeV), should not exceed 135 GeV. The LHC found no previously unknown particles other than the Higgs boson which was already suspected to exist as part of the Standard Model, and therefore no evidence for any supersymmetric extension of the Standard Model.\nIndirect methods include the search for a permanent electric dipole moment (EDM) in the known Standard Model particles, which can arise when the Standard Model particle interacts with the supersymmetric particles. The current best constraint on the electron electric dipole moment put it to be smaller than 10−28 e·cm, equivalent to a sensitivity to new physics at the TeV scale and matching that of the current best particle colliders. A permanent EDM in any fundamental particle points towards time-reversal violating physics, and therefore also CP-symmetry violation via the CPT theorem. Such EDM experiments are also much more scalable than conventional particle accelerators and offer a practical alternative to detecting physics beyond the standard model as accelerator experiments become increasingly costly and complicated to maintain. The current best limit for the electron\'s EDM has already reached a sensitivity to rule out so called \'naive\' versions of supersymmetric extensions of the Standard Model.\nResearch in the late 2010s and early 2020s from experimental data on the cosmological constant, LIGO noise, and pulsar timing, suggests it\'s very unlikely that there are any new particles with masses much higher than those which can be found in the standard model or the LHC. However, this research has also indicated that quantum gravity or perturbative quantum field theory will become strongly coupled before 1 PeV, leading to other new physics in the TeVs.\n=== Current status ===\nThe negative findings in the experiments disappointed many physicists, who believed that supersymmetric extensions of the Standard Model (and other theories relying upon it) were by far the most promising theories for "new" physics beyond the Standard Model, and had hoped for signs of unexpected results from the experiments. In particular, the LHC result seems problematic for the Minimal Supersymmetric Standard Model, as the value of 125 GeV is relatively large for the model and can only be achieved with large radiative loop corrections from top squarks, which many theorists consider to be "unnatural" (see naturalness and fine tuning).', 'In October 2019, Breakthrough Listen started a collaboration with scientists from the TESS team (Transiting Exoplanet Survey Satellite) to look for signs of advanced extraterrestrial life. Thousands of new planets found by TESS will be scanned for technosignatures by Breakthrough Listen partner facilities across the globe. Data from TESS monitoring of stars will also be searched for anomalies.\n=== FAST ===\nChina\'s 500 meter Aperture Spherical Telescope (FAST) lists detecting interstellar communication signals as part of its science mission. It is funded by the National Development and Reform Commission (NDRC) and managed by the National Astronomical observatories (NAOC) of the Chinese Academy of Sciences (CAS). FAST is the first radio observatory built with SETI as a core scientific goal. FAST consists of a fixed 500 m (1,600 ft) diameter spherical dish constructed in a natural depression sinkhole caused by karst processes in the region. It is the world\'s largest filled-aperture radio telescope.\nAccording to its website, FAST can search to 28 light-years, and is able to reach 1,400 stars. If the transmitter\'s radiated power were to be increased to 1,000,000 MW, FAST would be able to reach one million stars. This is compared to the former Arecibo 305 meter telescope detection distance of 18 light-years.\nOn 14 June 2022, astronomers, working with China\'s FAST telescope, reported the possibility of having detected artificial (presumably alien) signals, but cautioned that further studies were required to determine if a natural radio interference may be the source. More recently, on 18 June 2022, Dan Werthimer, chief scientist for several SETI-related projects, reportedly noted, "These signals are from radio interference; they are due to radio pollution from earthlings, not from E.T.".\n=== UCLA ===\nSince 2016, University of California Los Angeles (UCLA) undergraduate and graduate students have been participating in radio searches for technosignatures with the Green Bank Telescope. Targets include the Kepler field, TRAPPIST-1, and solar-type stars. The search is sensitive to Arecibo-class transmitters located within 420 light years of Earth and to transmitters that are 1,000 times more powerful than Arecibo located within 13,000 light years of Earth.\n== Community SETI projects ==\n=== SETI@home ===\nThe SETI@home project used volunteer computing to analyze signals acquired by the SERENDIP project.\nSETI@home was conceived by David Gedye along with Craig Kasnoff and is a popular volunteer computing project that was launched by the Berkeley SETI Research Center at the University of California, Berkeley, in May 1999. It was originally funded by The Planetary Society and Paramount Pictures, and later by the state of California. The project is run by director David P. Anderson and chief scientist Dan Werthimer. Any individual could become involved with SETI research by downloading the Berkeley Open Infrastructure for Network Computing (BOINC) software program, attaching to the SETI@home project, and allowing the program to run as a background process that uses idle computer power. The SETI@home program itself ran signal analysis on a "work unit" of data recorded from the central 2.5 MHz wide band of the SERENDIP IV instrument. After computation on the work unit was complete, the results were then automatically reported back to SETI@home servers at University of California, Berkeley. By June 28, 2009, the SETI@home project had over 180,000 active participants volunteering a total of over 290,000 computers. These computers gave SETI@home an average computational power of 617 teraFLOPS. In 2004 radio source SHGb02+14a set off speculation in the media that a signal had been detected but researchers noted the frequency drifted rapidly and the detection on three SETI@home computers fell within random chance.\nBy 2010, after 10 years of data collection, SETI@home had listened to that one frequency at every point of over 67 percent of the sky observable from Arecibo with at least three scans (out of the goal of nine scans), which covers about 20 percent of the full celestial sphere. On March 31, 2020, with 91,454 active users, the project stopped sending out new work to SETI@home users, bringing this particular SETI effort to an indefinite hiatus.\n=== SETI Net ===\nSETI Network was the only fully operational private search system. The SETI Net station consisted of off-the-shelf, consumer-grade electronics to minimize cost and to allow this design to be replicated as simply as possible. It had a 3-meter parabolic antenna that could be directed in azimuth and elevation, an LNA that covered 100 MHz of the 1420 MHz spectrum, a receiver to reproduce the wideband audio, and a standard personal computer as the control device and for deploying the detection algorithms. The antenna could be pointed and locked to one sky location in Ra and DEC which enabling the system to integrate on it for long periods. The Wow! signal area was monitored for many long periods. All search data was collected and is available on the Internet archive.\nSETI Net started operation in the early 1980s as a way to learn about the science of the search, and developed several software packages for the amateur SETI community. It provided an astronomical clock, a file manager to keep track of SETI data files, a spectrum analyzer optimized for amateur SETI, remote control of the station from the Internet, and other packages.\nSETI Net went dark and was decommissioned on 2021-12-04. The collected data is available on their website.\n=== The SETI League and Project Argus ===\nFounded in 1994 in response to the United States Congress cancellation of the NASA SETI program, The SETI League, Incorporated is a membership-supported nonprofit organization with 1,500 members in 62 countries. This grass-roots alliance of amateur and professional radio astronomers is headed by executive director emeritus H. Paul Shuch, the engineer credited with developing the world\'s first commercial home satellite TV receiver. Many SETI League members are licensed radio amateurs and microwave experimenters. Others are digital signal processing experts and computer enthusiasts.\nThe SETI League pioneered the conversion of backyard satellite TV dishes 3 to 5 m (10–16 ft) in diameter into research-grade radio telescopes of modest sensitivity. The organization concentrates on coordinating a global network of small, amateur-built radio telescopes under Project Argus, an all-sky survey seeking to achieve real-time coverage of the entire sky. Project Argus was conceived as a continuation of the all-sky survey component of the late NASA SETI program (the targeted search having been continued by the SETI Institute\'s Project Phoenix). There are currently 143 Project Argus radio telescopes operating in 27 countries. Project Argus instruments typically exhibit sensitivity on the order of 10−23 Watts/square metre, or roughly equivalent to that achieved by the Ohio State University Big Ear radio telescope in 1977, when it detected the landmark "Wow!" candidate signal.\nThe name "Argus" derives from the mythical Greek guard-beast who had 100 eyes, and could see in all directions at once. In the SETI context, the name has been used for radio telescopes in fiction (Arthur C. Clarke, "Imperial Earth"; Carl Sagan, "Contact"), was the name initially used for the NASA study ultimately known as "Cyclops," and is the name given to an omnidirectional radio telescope design being developed at the Ohio State University.\n== Optical experiments ==\nWhile most SETI sky searches have studied the radio spectrum, some SETI researchers have considered the possibility that alien civilizations might be using powerful lasers for interstellar communications at optical wavelengths. The idea was first suggested by R. N. Schwartz and Charles Hard Townes in a 1961 paper published in the journal Nature titled "Interstellar and Interplanetary Communication by Optical Masers". However, the 1971 Cyclops study discounted the possibility of optical SETI, reasoning that construction of a laser system that could outshine the bright central star of a remote star system would be too difficult. In 1983, Townes published a detailed study of the idea in the United States journal Proceedings of the National Academy of Sciences, which was met with interest by the SETI community.\nThere are two problems with optical SETI. The first problem is that lasers are highly "monochromatic", that is, they emit light only on one frequency, making it troublesome to figure out what frequency to look for. However, emitting light in narrow pulses results in a broad spectrum of emission; the spread in frequency becomes higher as the pulse width becomes narrower, making it easier to detect an emission.\nThe other problem is that while radio transmissions can be broadcast in all directions, lasers are highly directional. Interstellar gas and dust is almost transparent to near infrared, so these signals can be seen from greater distances, but the extraterrestrial laser signals would need to be transmitted in the direction of Earth in order to be detected.\nOptical SETI supporters have conducted paper studies of the effectiveness of using contemporary high-energy lasers and a ten-meter diameter mirror as an interstellar beacon. The analysis shows that an infrared pulse from a laser, focused into a narrow beam by such a mirror, would appear thousands of times brighter than the Sun to a distant civilization in the beam\'s line of fire. The Cyclops study proved incorrect in suggesting a laser beam would be inherently hard to see.', '=== MOP and Project Phoenix ===\nIn 1978, the NASA SETI program had been heavily criticized by Senator William Proxmire, and funding for SETI research was removed from the NASA budget by Congress in 1981; however, funding was restored in 1982, after Carl Sagan talked with Proxmire and convinced him of the program\'s value. In 1992, the U.S. government funded an operational SETI program, in the form of the NASA Microwave Observing Program (MOP). MOP was planned as a long-term effort to conduct a general survey of the sky and also carry out targeted searches of 800 specific nearby stars. MOP was to be performed by radio antennas associated with the NASA Deep Space Network, as well as the 140-foot (43 m) radio telescope of the National Radio Astronomy Observatory at Green Bank, West Virginia and the 1,000-foot (300 m) radio telescope at the Arecibo Observatory in Puerto Rico. The signals were to be analyzed by spectrum analyzers, each with a capacity of 15 million channels. These spectrum analyzers could be grouped together to obtain greater capacity. Those used in the targeted search had a bandwidth of 1 hertz per channel, while those used in the sky survey had a bandwidth of 30 hertz per channel.\nMOP drew the attention of the United States Congress, where the program met opposition and canceled one year after its start. SETI advocates continued without government funding, and in 1995 the nonprofit SETI Institute of Mountain View, California resurrected the MOP program under the name of Project "Phoenix", backed by private sources of funding. In 2012 it cost around $2 million per year to maintain SETI research at the SETI Institute and around 10 times that to support different SETI activities globally. Project Phoenix, under the direction of Jill Tarter, was a continuation of the targeted search program from MOP and studied roughly 1,000 nearby Sun-like stars until approximately 2015. From 1995 through March 2004, Phoenix conducted observations at the 64-meter (210 ft) Parkes radio telescope in Australia, the 140-foot (43 m) radio telescope of the National Radio Astronomy Observatory in Green Bank, West Virginia, and the 1,000-foot (300 m) radio telescope at the Arecibo Observatory in Puerto Rico. The project observed the equivalent of 800 stars over the available channels in the frequency range from 1200 to 3000 MHz. The search was sensitive enough to pick up transmitters with 1 GW EIRP to a distance of about 200 light-years.\n== Ongoing radio searches ==\nMany radio frequencies penetrate Earth\'s atmosphere quite well, and this led to radio telescopes that investigate the cosmos using large radio antennas. Furthermore, human endeavors emit considerable electromagnetic radiation as a byproduct of communications such as television and radio. These signals would be easy to recognize as artificial due to their repetitive nature and narrow bandwidths. Earth has been sending radio waves from broadcasts into space for over 100 years. These signals have reached over 1,000 stars, most notably Vega, Aldebaran, Barnard\'s Star, Sirius, and Proxima Centauri. If intelligent alien life exists on any planet orbiting these nearby stars, these signals could be heard and deciphered, even though some of the signal is garbled by the Earth\'s ionosphere.\nMany international radio telescopes are currently being used for radio SETI searches, including the Low Frequency Array (LOFAR) in Europe, the Murchison Widefield Array (MWA) in Australia, and the Lovell Telescope in the United Kingdom.\n=== Allen Telescope Array ===\nThe SETI Institute collaborated with the Radio Astronomy Laboratory at the Berkeley SETI Research Center to develop a specialized radio telescope array for SETI studies, similar to a mini-cyclops array. Formerly known as the One Hectare Telescope (1HT), the concept was renamed the "Allen Telescope Array" (ATA) after the project\'s benefactor, Paul Allen. Its sensitivity is designed to be equivalent to a single large dish more than 100 meters in diameter, if fully completed. Presently, the array has 42 operational dishes at the Hat Creek Radio Observatory in rural northern California.\nThe full array (ATA-350) is planned to consist of 350 or more offset-Gregorian radio dishes, each 6.1 meters (20 feet) in diameter. These dishes are the largest producible with commercially available satellite television dish technology. The ATA was planned for a 2007 completion date, at a cost of US$25 million. The SETI Institute provided money for building the ATA while University of California, Berkeley designed the telescope and provided operational funding. The first portion of the array (ATA-42) became operational in October 2007 with 42 antennas. The DSP system planned for ATA-350 is extremely ambitious. Completion of the full 350 element array will depend on funding and the technical results from ATA-42.\nATA-42 (ATA) is designed to allow multiple observers simultaneous access to the interferometer output at the same time. Typically, the ATA snapshot imager (used for astronomical surveys and SETI) is run in parallel with a beamforming system (used primarily for SETI). ATA also supports observations in multiple synthesized pencil beams at once, through a technique known as "multibeaming". Multibeaming provides an effective filter for identifying false positives in SETI, since a very distant transmitter must appear at only one point on the sky.\nSETI Institute\'s Center for SETI Research (CSR) uses ATA in the search for extraterrestrial intelligence, observing 12 hours a day, 7 days a week. From 2007 to 2015, ATA identified hundreds of millions of technological signals. So far, all these signals have been assigned the status of noise or radio frequency interference because a) they appear to be generated by satellites or Earth-based transmitters, or b) they disappeared before the threshold time limit of ~1 hour. Researchers in CSR are working on ways to reduce the threshold time limit, and to expand ATA\'s capabilities for detection of signals that may have embedded messages.\nBerkeley astronomers used the ATA to pursue several science topics, some of which might have transient SETI signals, until 2011, when the collaboration between the University of California, Berkeley and the SETI Institute was terminated.\nCNET published an article and pictures about the Allen Telescope Array (ATA) on December 12, 2008.\nIn April 2011, the ATA entered an 8-month "hibernation" due to funding shortfalls. Regular operation of the ATA resumed on December 5, 2011.\nIn 2012, the ATA was revitalized with a $3.6 million donation by Franklin Antonio, co-founder and Chief Scientist of QUALCOMM Incorporated. This gift supported upgrades of all the receivers on the ATA dishes to have (2× to 10× over the range 1–8 GHz) greater sensitivity than before and supporting observations over a wider frequency range from 1–18 GHz, though initially the radio frequency electronics only go to 12 GHz. As of July 2013, the first of these receivers was installed and proven, with full installation on all 42 antennas being expected for June 2017. ATA is well suited to the search for extraterrestrial intelligence (SETI) and to discovery of astronomical radio sources, such as heretofore unexplained non-repeating, possibly extragalactic, pulses known as fast radio bursts or FRBs.\n=== SERENDIP ===\nSERENDIP (Search for Extraterrestrial Radio Emissions from Nearby Developed Intelligent Populations) is a SETI program launched in 1979 by the Berkeley SETI Research Center. SERENDIP takes advantage of ongoing "mainstream" radio telescope observations as a "piggy-back" or "commensal" program, using large radio telescopes including the NRAO 90m telescope at Green Bank and, formerly, the Arecibo 305m telescope. Rather than having its own observation program, SERENDIP analyzes deep space radio telescope data that it obtains while other astronomers are using the telescopes. The most recently deployed SERENDIP spectrometer, SERENDIP VI, was installed at both the Arecibo Telescope and the Green Bank Telescope in 2014–2015.\n=== Breakthrough Listen ===\nBreakthrough Listen is a ten-year initiative with $100 million funding begun in July 2015 to actively search for intelligent extraterrestrial communications in the universe, in a substantially expanded way, using resources that had not previously been extensively used for the purpose. It has been described as the most comprehensive search for alien communications to date. The science program for Breakthrough Listen is based at Berkeley SETI Research Center, located in the Astronomy Department at the University of California, Berkeley.\nAnnounced in July 2015, the project is observing for thousands of hours every year on two major radio telescopes, the Green Bank Observatory in West Virginia, and the Parkes Observatory in Australia. Previously, only about 24 to 36 hours of telescope time per year were used in the search for alien life. Furthermore, the Automated Planet Finder at Lick Observatory is searching for optical signals coming from laser transmissions. The massive data rates from the radio telescopes (24 GB/s at Green Bank) necessitated the construction of dedicated hardware at the telescopes to perform the bulk of the analysis. Some of the data are also analyzed by volunteers in the SETI@home volunteer computing network. Founder of modern SETI Frank Drake was one of the scientists on the project\'s advisory committee.\nIn October 2019, Breakthrough Listen started a collaboration with scientists from the TESS team (Transiting Exoplanet Survey Satellite) to look for signs of advanced extraterrestrial life. Thousands of new planets found by TESS will be scanned for technosignatures by Breakthrough Listen partner facilities across the globe. Data from TESS monitoring of stars will also be searched for anomalies.\n=== FAST ===', 'Shower-curtain effect\n\nThe shower-curtain effect in physics describes the phenomenon of a shower curtain being blown inward when a shower is running. The problem of identifying the cause of this effect has been featured in Scientific American magazine, with several theories given to explain the phenomenon but no definite conclusion.\nThe shower-curtain effect may also be used to describe the observation of how nearby phase front distortions of an optical wave are more severe than remote distortions of the same amplitude.\n== Hypotheses ==\n=== Buoyancy hypothesis ===\nAlso called chimney effect or stack effect, observes that warm air (from the hot shower) rises out over the shower curtain as cooler air (near the floor) pushes in under the curtain to replace the rising air.  By pushing the curtain in towards the shower, the (short range) vortex and Coandă effects become more significant. However, the shower-curtain effect persists when cold water is used, implying that this is not the sole mechanism.\n=== Bernoulli effect hypothesis ===\nThe most popular explanation given for the shower-curtain effect is Bernoulli\'s principle.  Bernoulli\'s principle states that an increase in velocity results in a decrease in pressure.  This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water.  This movement would be parallel to the plane of the shower curtain.  If air is moving across the inside surface of the shower curtain, Bernoulli\'s principle says the air pressure there will drop.  This would result in a pressure differential between the inside and outside, causing the curtain to move inward.  It would be strongest when the gap between the bather and the curtain is smallest, resulting in the curtain attaching to the bather.\n=== Horizontal vortex hypothesis ===\nA computer simulation of a typical bathroom found that none of the above theories pan out in their analysis, but instead found that the spray from the shower-head drives a horizontal vortex. This vortex has a low-pressure zone in the centre, which sucks the curtain.\nDavid Schmidt of the University of Massachusetts was awarded the 2001 Ig Nobel Prize in Physics for his partial solution to the question of why shower curtains billow inwards. He used a computational fluid dynamics code to achieve the results.  Professor Schmidt is adamant that this was done "for fun" in his own free time without the use of grants.\n=== Coandă effect ===\nThe Coandă effect, also known as "boundary layer attachment", is the tendency of a moving fluid to adhere to an adjacent wall.\n=== Condensation ===\nA hot shower will produce steam that condenses on the shower side of the curtain, lowering the pressure there.  In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.\n=== Air pressure ===\nColder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.\n== Solutions ==\nMany shower curtains come with features to reduce the shower-curtain effect. They may have adhesive suction cups on the bottom edges of the curtain, which are then pushed onto the sides of the shower when in use. Others may have magnets at the bottom, though these are not effective on acrylic or fiberglass tubs.\nIt is possible to use a telescopic shower curtain rod to block the curtain on its lower part and to prevent it from sucking inside.\nHanging the curtain rod higher or lower, or especially further away from the shower head, can reduce the effect. A convex shower rod can also be used to hold the curtain against the inside wall of a tub.\nA weight can be attached to a long string and the string attached to the curtain rod in the middle of the curtain (on the inside). Hanging the weight low against the curtain just above the rim of the shower pan or tub makes it an effective billowing deterrent without allowing the weight to hit the pan or tub and damage it.\nThere are a few alternative solutions that either attach to the shower curtain directly, attach to the shower rod or attach to the wall.\n== References ==\n== External links ==\nScientific American: Why does the shower curtain move toward the water?\nWhy does the shower curtain blow up and in instead of down and out?\nVideo demonstration of how this phenomenon could be solved.\nThe Straight Dope: Why does the shower curtain blow in despite the water pushing it out (revisited)?\n2001 Ig Nobel Prize Winners\nFluent NEWS: Shower Curtain Grabs Scientist – But He Lives to Tell Why\nArggh, Why Does the Shower Curtain Attack Me? by Joe Palca. All Things Considered, National Public Radio.  November 4, 2006. (audio)\nExperimental Investigation of the Influence of the Relative Position of the Scattering Layer on Image Quality: the Shower Curtain Effect\nThe shower curtain effect; ESA', 'While supersymmetry has not been discovered at high energy, see Section Supersymmetry in particle physics, supersymmetry was found to be effectively realized at the intermediate energy of hadronic physics where baryons and mesons are superpartners. An exception is the pion that appears as a zero mode in the mass spectrum and thus protected by the supersymmetry: It has no baryonic partner. The realization of this effective supersymmetry is readily explained in quark–diquark models: Because two different color charges close together (e.g., blue and red) appear under coarse resolution as the corresponding anti-color (e.g. anti-green), a diquark cluster viewed with coarse resolution (i.e., at the energy-momentum scale used to study hadron structure) effectively appears as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves likes a meson.\n=== Supersymmetry in condensed matter physics ===\nSUSY concepts have provided useful extensions to the WKB approximation. Additionally, SUSY has been applied to disorder averaged systems both quantum and non-quantum (through statistical mechanics), the Fokker–Planck equation being an example of a non-quantum theory. The \'supersymmetry\' in all these systems arises from the fact that one is modelling one particle and as such the \'statistics\' do not matter. The use of the supersymmetry method provides a mathematical rigorous alternative to the replica trick, but only in non-interacting systems, which attempts to address the so-called \'problem of the denominator\' under disorder averaging. For more on the applications of supersymmetry in condensed matter physics see Efetov (1997).\nIn 2021, a group of researchers showed that, in theory,\n{\\displaystyle N=(0,1)}\nSUSY could be realised at the edge of a Moore–Read quantum Hall state. However, to date, no experiments have been done yet to realise it at an edge of a Moore–Read state. In 2022, a different group of researchers created a computer simulation of atoms in 1 dimensions that had supersymmetric topological quasiparticles.\n=== Supersymmetry in optics ===\nIn 2013, integrated optics was found to provide a fertile ground on which certain ramifications of SUSY can be explored in readily-accessible laboratory settings. Making use of the analogous mathematical structure of the quantum-mechanical Schrödinger equation and the wave equation governing the evolution of light in one-dimensional settings, one may interpret the refractive index distribution of a structure as a potential landscape in which optical wave packets propagate. In this manner, a new class of functional optical structures with possible applications in phase matching, mode conversion and space-division multiplexing becomes possible. SUSY transformations have been also proposed as a way to address inverse scattering problems in optics and as a one-dimensional transformation optics.\n=== Supersymmetry in dynamical systems ===\nAll stochastic (partial) differential equations, the models for all types of continuous time dynamical systems, possess topological supersymmetry. In the operator representation of stochastic evolution, the topological supersymmetry is the exterior derivative which is commutative with the stochastic evolution operator defined as the stochastically averaged pullback induced on differential forms by SDE-defined diffeomorphisms of the phase space. The topological sector of the so-emerging supersymmetric theory of stochastic dynamics can be recognized as the Witten-type topological field theory.\nThe meaning of the topological supersymmetry in dynamical systems is the preservation of the phase space continuity—infinitely close points will remain close during continuous time evolution even in the presence of noise. When the topological supersymmetry is broken spontaneously, this property is violated in the limit of the infinitely long temporal evolution and the model can be said to exhibit (the stochastic generalization of) the butterfly effect. From a more general perspective, spontaneous breakdown of the topological supersymmetry is the theoretical essence of the ubiquitous dynamical phenomenon variously known as chaos, turbulence, self-organized criticality etc. The Goldstone theorem explains the associated emergence of the long-range dynamical behavior that manifests itself as \u20601/f\u2060 noise, butterfly effect, and the scale-free statistics of sudden (instantonic) processes, such as earthquakes, neuroavalanches, and solar flares, known as the Zipf\'s law and the Richter scale.\n=== Supersymmetry in mathematics ===\nSUSY is also sometimes studied mathematically for its intrinsic properties. This is because it describes complex fields satisfying a property known as holomorphy, which allows holomorphic quantities to be exactly computed. This makes supersymmetric models useful "toy models" of more realistic theories. A prime example of this has been the demonstration of S-duality in four-dimensional gauge theories that interchanges particles and monopoles.\nThe proof of the Atiyah–Singer index theorem is much simplified by the use of supersymmetric quantum mechanics.\n=== Supersymmetry in string theory ===\nSupersymmetry is an integral part of string theory, a possible theory of everything. There are two types of string theory, supersymmetric string theory or superstring theory, and non-supersymmetric string theory. By definition of superstring theory, supersymmetry is required in superstring theory at some level. However, even in non-supersymmetric string theory, a type of supersymmetry called misaligned supersymmetry is still required in the theory in order to ensure no physical tachyons appear. Any string theories without some kind of supersymmetry, such as bosonic string theory and the\n{\\displaystyle E_{7}\\times E_{7}}\n16\n{\\displaystyle SU(16)}\n, and\n{\\displaystyle E_{8}}\nheterotic string theories, will have a tachyon and therefore the spacetime vacuum itself would be unstable and would decay into some tachyon-free string theory usually in a lower spacetime dimension. There is no experimental evidence that either supersymmetry or misaligned supersymmetry holds in our universe, and many physicists have moved on from supersymmetry and string theory entirely due to the non-detection of supersymmetry at the LHC.\nDespite the null results for supersymmetry at the LHC so far, some particle physicists have nevertheless moved to string theory in order to resolve the naturalness crisis for certain supersymmetric extensions of the Standard Model. According to the particle physicists, there exists a concept of "stringy naturalness" in string theory, where the string theory landscape could have a power law statistical pull on soft SUSY breaking terms to large values (depending on the number of hidden sector SUSY breaking fields contributing to the soft terms). If this is coupled with an anthropic requirement that contributions to the weak scale not exceed a factor between 2 and 5 from its measured value (as argued by Agrawal et al.), then the Higgs mass is pulled up to the vicinity of 125 GeV while most sparticles are pulled to values beyond the current reach of LHC. (The Higgs was determined to have a mass of 125 GeV ±0.15 GeV in 2022.) An exception occurs for higgsinos which gain mass not from SUSY breaking but rather from whatever mechanism solves the SUSY mu problem. Light higgsino pair production in association with hard initial state jet radiation leads to a soft opposite-sign dilepton plus jet plus missing transverse energy signal.\n== Supersymmetry in particle physics ==\nIn particle physics, a supersymmetric extension of the Standard Model is a possible candidate for undiscovered particle physics, and seen by some physicists as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to be incomplete and where limitations of current theories are well established. In particular, one supersymmetric extension of the Standard Model, the Minimal Supersymmetric Standard Model (MSSM), became popular in theoretical particle physics, as the Minimal Supersymmetric Standard Model is the simplest supersymmetric extension of the Standard Model that could resolve major hierarchy problems within the Standard Model, by guaranteeing that quadratic divergences of all orders will cancel out in perturbation theory. If a supersymmetric extension of the Standard Model is correct, superpartners of the existing elementary particles would be new and undiscovered particles and supersymmetry is expected to be spontaneously broken.\nThere is no experimental evidence that a supersymmetric extension to the Standard Model is correct, or whether or not other extensions to current models might be more accurate. It is only since around 2010 that particle accelerators specifically designed to study physics beyond the Standard Model have become operational (i.e. the Large Hadron Collider (LHC)), and it is not known where exactly to look, nor the energies required for a successful search. However, the negative results from the LHC since 2010 have already ruled out some supersymmetric extensions to the Standard Model, and many physicists believe that the Minimal Supersymmetric Standard Model, while not ruled out, is no longer able to fully resolve the hierarchy problem.\n=== Supersymmetric extensions of the Standard Model ===', 'However, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".\nIn October 2000 astronomers Iván Almár and Jill Tarter presented a paper to The SETI Permanent Study Group in Rio de Janeiro, Brazil which proposed a scale (modelled after the Torino scale) which is an ordinal scale between zero and ten that quantifies the impact of any public announcement regarding evidence of extraterrestrial intelligence; the Rio scale has since inspired the 2005 San Marino Scale (in regard to the risks of transmissions from Earth) and the 2010 London Scale (in regard to the detection of extraterrestrial life). The Rio scale itself was revised in 2018.\nThe SETI Institute does not officially recognize the Wow! signal as of extraterrestrial origin as it was unable to be verified, although in a 2020 Twitter post the organization stated that \'\'an astronomer might have pinpointed the host star\'\'. The SETI Institute has also publicly denied that the candidate signal Radio source SHGb02+14a is of extraterrestrial origin. Although other volunteering projects such as Zooniverse credit users for discoveries, there is currently no crediting or early notification by SETI@Home following the discovery of a signal.\nSome people, including Steven M. Greer, have expressed cynicism that the general public might not be informed in the event of a genuine discovery of extraterrestrial intelligence due to significant vested interests. Some, such as Bruce Jakosky have also argued that the official disclosure of extraterrestrial life may have far reaching and as yet undetermined implications for society, particularly for the world\'s religions.\n== Active SETI ==\nActive SETI, also known as messaging to extraterrestrial intelligence (METI), consists of sending signals into space in the hope that they will be detected by an alien intelligence.\n=== Realized interstellar radio message projects ===\nIn November 1974, a largely symbolic attempt was made at the Arecibo Observatory to send a message to other worlds. Known as the Arecibo Message, it was sent towards the globular cluster M13, which is 25,000 light-years from Earth. Further IRMs Cosmic Call, Teen Age Message, Cosmic Call 2, and A Message From Earth were transmitted in 1999, 2001, 2003 and 2008 from the Evpatoria Planetary Radar.\n=== Debate ===\nWhether or not to attempt to contact extraterrestrials has attracted significant academic debate in the fields of space ethics and space policy. Physicist Stephen Hawking, in his book A Brief History of Time, suggests that "alerting" extraterrestrial intelligences to our existence is foolhardy, citing humankind\'s history of treating its own kind harshly in meetings of civilizations with a significant technology gap, e.g., the extermination of Tasmanian aborigines. He suggests, in view of this history, that we "lay low". In one response to Hawking, in September 2016, astronomer Seth Shostak sought to allay such concerns. Astronomer Jill Tarter also disagrees with Hawking, arguing that aliens developed and long-lived enough to communicate and travel across interstellar distances would have evolved a cooperative and less violent intelligence. She however thinks it is too soon for humans to attempt active SETI and that humans should be more advanced technologically first but keep listening in the meantime.\n== Criticism ==\nAs various SETI projects have progressed, some have criticized early claims by researchers as being too "euphoric". For example, Peter Schenkel, while remaining a supporter of SETI projects, wrote in 2006 that:\n[i]n light of new findings and insights, it seems appropriate to put excessive euphoria to rest and to take a more down-to-earth view [...] We should quietly admit that the early estimates—that there may be a million, a hundred thousand, or ten thousand advanced extraterrestrial civilizations in our galaxy—may no longer be tenable.\nCritics claim that the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability, as explained in a 2009 editorial in Nature, which said:\nSeti... has always sat at the edge of mainstream astronomy. This is partly because, no matter how scientifically rigorous its practitioners try to be, SETI can\'t escape an association with UFO believers and other such crackpots. But it is also because SETI is arguably not a falsifiable experiment. Regardless of how exhaustively the Galaxy is searched, the null result of radio silence doesn\'t rule out the existence of alien civilizations. It means only that those civilizations might not be using radio to communicate.\nNature added that SETI was "marked by a hope, bordering on faith" that aliens were aiming signals at us, that a hypothetical alien SETI project looking at Earth with "similar faith" would be "sorely disappointed", despite our many untargeted radar and TV signals, and our few targeted Active SETI radio signals denounced by those fearing aliens, and that it had difficulties attracting even sympathetic working scientists and government funding because it was "an effort so likely to turn up nothing".\nHowever, Nature also added, "Nonetheless, a small SETI effort is well worth supporting, especially given the enormous implications if it did succeed" and that "happily, a handful of wealthy technologists and other private donors have proved willing to provide that support".\nSupporters of the Rare Earth Hypothesis argue that advanced lifeforms are likely to be very rare, and that, if that is so, then SETI efforts will be futile. However, the Rare Earth Hypothesis itself faces many criticisms.\nIn 1993, Roy Mash stated that "Arguments favoring the existence of extraterrestrial intelligence nearly always contain an overt appeal to big numbers, often combined with a covert reliance on generalization from a single instance" and concluded that "the dispute between believers and skeptics is seen to boil down to a conflict of intuitions which can barely be engaged, let alone resolved, given our present state of knowledge". In response, in 2012, Milan M. Ćirković, then research professor at the Astronomical Observatory of Belgrade and a research associate of the Future of Humanity Institute at the University of Oxford, said that Mash was unrealistically over-reliant on excessive abstraction that ignored the empirical information available to modern SETI researchers.\nGeorge Basalla, Emeritus Professor of History at the University of Delaware, is a critic of SETI who argued in 2006 that "extraterrestrials discussed by scientists are as imaginary as the spirits and gods of religion or myth", and was in turn criticized by Milan M. Ćirković for, among other things, being unable to distinguish between "SETI believers" and "scientists engaged in SETI", who are often sceptical (especially about quick detection), such as Freeman Dyson and, at least in their later years, Iosif Shklovsky and Sebastian von Hoerner, and for ignoring the difference between the knowledge underlying the arguments of modern scientists and those of ancient Greek thinkers.\nMassimo Pigliucci, Professor of Philosophy at CUNY – City College, asked in 2010 whether SETI is "uncomfortably close to the status of pseudoscience" due to the lack of any clear point at which negative results cause the hypothesis of Extraterrestrial Intelligence to be abandoned, before eventually concluding that SETI is "almost-science", which is described by Milan M. Ćirković as Pigliucci putting SETI in "the illustrious company of string theory, interpretations of quantum mechanics, evolutionary psychology and history (of the \'synthetic\' kind done recently by Jared Diamond)", while adding that his justification for doing so with SETI "is weak, outdated, and reflecting particular philosophical prejudices similar to the ones described above in Mash and Basalla".\nRichard Carrigan, a particle physicist at the Fermi National Accelerator Laboratory near Chicago, Illinois, suggested that passive SETI could also be dangerous and that a signal released onto the Internet could act as a computer virus. Computer security expert Bruce Schneier dismissed this possibility as a "bizarre movie-plot threat".\n=== Ufology ===\nUfologist Stanton Friedman has often criticized SETI researchers for, among other reasons, what he sees as their unscientific criticisms of Ufology, but, unlike SETI, Ufology has generally not been embraced by academia as a scientific field of study, and it is usually characterized as a partial or total pseudoscience. In a 2016 interview, Jill Tarter pointed out that it is still a misconception that SETI and UFOs are related. She states, "SETI uses the tools of the astronomer to attempt to find evidence of somebody else\'s technology coming from a great distance. If we ever claim detection of a signal, we will provide evidence and data that can be independently confirmed. UFOs—none of the above." The Galileo Project headed by Harvard astronomer Avi Loeb is one of the few scientific efforts to study UFOs or UAPs. Loeb criticized that the study of UAP is often dismissed and not sufficiently studied by scientists and should shift from "occupying the talking points of national security administrators and politicians" to the realm of science. The Galileo Project\'s position after the publication of the 2021 UFO Report by the U.S. Intelligence community is that the scientific community needs to "systematically, scientifically and transparently look for potential evidence of extraterrestrial technological equipment".\n== See also ==\n== References ==\n== Further reading ==', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.']

Question: What is a crossover experiment?

Choices:
Choice A) An experiment that involves crossing over two different types of materials to create a new material.
Choice B) A type of experiment used to distinguish between different mechanisms proposed for a chemical reaction, such as intermolecular vs. intramolecular mechanisms.
Choice C) An experiment that involves crossing over two different types of organisms to create a hybrid.
Choice D) An experiment that involves crossing over two different types of cells to create a new cell.
Choice E) An experiment that involves crossing over two different chemicals to create a new substance.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Supersymmetry', 'While supersymmetry has not been discovered at high energy, see Section Supersymmetry in particle physics, supersymmetry was found to be effectively realized at the intermediate energy of hadronic physics where baryons and mesons are superpartners. An exception is the pion that appears as a zero mode in the mass spectrum and thus protected by the supersymmetry: It has no baryonic partner. The realization of this effective supersymmetry is readily explained in quark–diquark models: Because two different color charges close together (e.g., blue and red) appear under coarse resolution as the corresponding anti-color (e.g. anti-green), a diquark cluster viewed with coarse resolution (i.e., at the energy-momentum scale used to study hadron structure) effectively appears as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves likes a meson.\n=== Supersymmetry in condensed matter physics ===\nSUSY concepts have provided useful extensions to the WKB approximation. Additionally, SUSY has been applied to disorder averaged systems both quantum and non-quantum (through statistical mechanics), the Fokker–Planck equation being an example of a non-quantum theory. The \'supersymmetry\' in all these systems arises from the fact that one is modelling one particle and as such the \'statistics\' do not matter. The use of the supersymmetry method provides a mathematical rigorous alternative to the replica trick, but only in non-interacting systems, which attempts to address the so-called \'problem of the denominator\' under disorder averaging. For more on the applications of supersymmetry in condensed matter physics see Efetov (1997).\nIn 2021, a group of researchers showed that, in theory,\n{\\displaystyle N=(0,1)}\nSUSY could be realised at the edge of a Moore–Read quantum Hall state. However, to date, no experiments have been done yet to realise it at an edge of a Moore–Read state. In 2022, a different group of researchers created a computer simulation of atoms in 1 dimensions that had supersymmetric topological quasiparticles.\n=== Supersymmetry in optics ===\nIn 2013, integrated optics was found to provide a fertile ground on which certain ramifications of SUSY can be explored in readily-accessible laboratory settings. Making use of the analogous mathematical structure of the quantum-mechanical Schrödinger equation and the wave equation governing the evolution of light in one-dimensional settings, one may interpret the refractive index distribution of a structure as a potential landscape in which optical wave packets propagate. In this manner, a new class of functional optical structures with possible applications in phase matching, mode conversion and space-division multiplexing becomes possible. SUSY transformations have been also proposed as a way to address inverse scattering problems in optics and as a one-dimensional transformation optics.\n=== Supersymmetry in dynamical systems ===\nAll stochastic (partial) differential equations, the models for all types of continuous time dynamical systems, possess topological supersymmetry. In the operator representation of stochastic evolution, the topological supersymmetry is the exterior derivative which is commutative with the stochastic evolution operator defined as the stochastically averaged pullback induced on differential forms by SDE-defined diffeomorphisms of the phase space. The topological sector of the so-emerging supersymmetric theory of stochastic dynamics can be recognized as the Witten-type topological field theory.\nThe meaning of the topological supersymmetry in dynamical systems is the preservation of the phase space continuity—infinitely close points will remain close during continuous time evolution even in the presence of noise. When the topological supersymmetry is broken spontaneously, this property is violated in the limit of the infinitely long temporal evolution and the model can be said to exhibit (the stochastic generalization of) the butterfly effect. From a more general perspective, spontaneous breakdown of the topological supersymmetry is the theoretical essence of the ubiquitous dynamical phenomenon variously known as chaos, turbulence, self-organized criticality etc. The Goldstone theorem explains the associated emergence of the long-range dynamical behavior that manifests itself as \u20601/f\u2060 noise, butterfly effect, and the scale-free statistics of sudden (instantonic) processes, such as earthquakes, neuroavalanches, and solar flares, known as the Zipf\'s law and the Richter scale.\n=== Supersymmetry in mathematics ===\nSUSY is also sometimes studied mathematically for its intrinsic properties. This is because it describes complex fields satisfying a property known as holomorphy, which allows holomorphic quantities to be exactly computed. This makes supersymmetric models useful "toy models" of more realistic theories. A prime example of this has been the demonstration of S-duality in four-dimensional gauge theories that interchanges particles and monopoles.\nThe proof of the Atiyah–Singer index theorem is much simplified by the use of supersymmetric quantum mechanics.\n=== Supersymmetry in string theory ===\nSupersymmetry is an integral part of string theory, a possible theory of everything. There are two types of string theory, supersymmetric string theory or superstring theory, and non-supersymmetric string theory. By definition of superstring theory, supersymmetry is required in superstring theory at some level. However, even in non-supersymmetric string theory, a type of supersymmetry called misaligned supersymmetry is still required in the theory in order to ensure no physical tachyons appear. Any string theories without some kind of supersymmetry, such as bosonic string theory and the\n{\\displaystyle E_{7}\\times E_{7}}\n16\n{\\displaystyle SU(16)}\n, and\n{\\displaystyle E_{8}}\nheterotic string theories, will have a tachyon and therefore the spacetime vacuum itself would be unstable and would decay into some tachyon-free string theory usually in a lower spacetime dimension. There is no experimental evidence that either supersymmetry or misaligned supersymmetry holds in our universe, and many physicists have moved on from supersymmetry and string theory entirely due to the non-detection of supersymmetry at the LHC.\nDespite the null results for supersymmetry at the LHC so far, some particle physicists have nevertheless moved to string theory in order to resolve the naturalness crisis for certain supersymmetric extensions of the Standard Model. According to the particle physicists, there exists a concept of "stringy naturalness" in string theory, where the string theory landscape could have a power law statistical pull on soft SUSY breaking terms to large values (depending on the number of hidden sector SUSY breaking fields contributing to the soft terms). If this is coupled with an anthropic requirement that contributions to the weak scale not exceed a factor between 2 and 5 from its measured value (as argued by Agrawal et al.), then the Higgs mass is pulled up to the vicinity of 125 GeV while most sparticles are pulled to values beyond the current reach of LHC. (The Higgs was determined to have a mass of 125 GeV ±0.15 GeV in 2022.) An exception occurs for higgsinos which gain mass not from SUSY breaking but rather from whatever mechanism solves the SUSY mu problem. Light higgsino pair production in association with hard initial state jet radiation leads to a soft opposite-sign dilepton plus jet plus missing transverse energy signal.\n== Supersymmetry in particle physics ==\nIn particle physics, a supersymmetric extension of the Standard Model is a possible candidate for undiscovered particle physics, and seen by some physicists as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to be incomplete and where limitations of current theories are well established. In particular, one supersymmetric extension of the Standard Model, the Minimal Supersymmetric Standard Model (MSSM), became popular in theoretical particle physics, as the Minimal Supersymmetric Standard Model is the simplest supersymmetric extension of the Standard Model that could resolve major hierarchy problems within the Standard Model, by guaranteeing that quadratic divergences of all orders will cancel out in perturbation theory. If a supersymmetric extension of the Standard Model is correct, superpartners of the existing elementary particles would be new and undiscovered particles and supersymmetry is expected to be spontaneously broken.\nThere is no experimental evidence that a supersymmetric extension to the Standard Model is correct, or whether or not other extensions to current models might be more accurate. It is only since around 2010 that particle accelerators specifically designed to study physics beyond the Standard Model have become operational (i.e. the Large Hadron Collider (LHC)), and it is not known where exactly to look, nor the energies required for a successful search. However, the negative results from the LHC since 2010 have already ruled out some supersymmetric extensions to the Standard Model, and many physicists believe that the Minimal Supersymmetric Standard Model, while not ruled out, is no longer able to fully resolve the hierarchy problem.\n=== Supersymmetric extensions of the Standard Model ===', 'In response to the so-called "naturalness crisis" in the Minimal Supersymmetric Standard Model, some researchers have abandoned naturalness and the original motivation to solve the hierarchy problem naturally with supersymmetry, while other researchers have moved on to other supersymmetric models such as split supersymmetry. Still others have moved to string theory as a result of the naturalness crisis. Former enthusiastic supporter Mikhail Shifman went as far as urging the theoretical community to search for new ideas and accept that supersymmetry was a failed theory in particle physics. However, some researchers suggested that this "naturalness" crisis was premature because various calculations were too optimistic about the limits of masses which would allow a supersymmetric extension of the Standard Model as a solution.\n== General supersymmetry ==\nSupersymmetry appears in many related contexts of theoretical physics. It is possible to have multiple supersymmetries and also have supersymmetric extra dimensions.\n=== Extended supersymmetry ===\nIt is possible to have more than one kind of supersymmetry transformation. Theories with more than one supersymmetry transformation are known as extended supersymmetric theories. The more supersymmetry a theory has, the more constrained are the field content and interactions. Typically the number of copies of a supersymmetry is a power of 2 (1, 2, 4, 8...). In four dimensions, a spinor has four degrees of freedom and thus the minimal number of supersymmetry generators is four in four dimensions and having eight copies of supersymmetry means that there are 32 supersymmetry generators.\nThe maximal number of supersymmetry generators possible is 32. Theories with more than 32 supersymmetry generators automatically have massless fields with spin greater than 2. It is not known how to make massless fields with spin greater than two interact, so the maximal number of supersymmetry generators considered is 32. This is due to the Weinberg–Witten theorem. This corresponds to an N = 8 supersymmetry theory. Theories with 32 supersymmetries automatically have a graviton.\nFor four dimensions there are the following theories, with the corresponding multiplets (CPT adds a copy, whenever they are not invariant under such symmetry):\n=== Supersymmetry in alternate numbers of dimensions ===\nIt is possible to have supersymmetry in dimensions other than four. Because the properties of spinors change drastically between different dimensions, each dimension has its characteristic. In d dimensions, the size of spinors is approximately 2d/2 or 2(d − 1)/2. Since the maximum number of supersymmetries is 32, the greatest number of dimensions in which a supersymmetric theory can exist is eleven.\n=== Fractional supersymmetry ===\nFractional supersymmetry is a generalization of the notion of supersymmetry in which the minimal positive amount of spin does not have to be \u20601/2\u2060 but can be an arbitrary \u20601/N\u2060 for integer value of N. Such a generalization is possible in two or fewer spacetime dimensions.\n== See also ==\n== References ==\n== Further reading ==\n=== Theoretical introductions, free and online ===\n=== Monographs ===\n=== On experiments ===\n== External links ==\nSupersymmetry – European Organization for Nuclear Research (CERN)\nThe status of supersymmetry – Symmetry Magazine (Fermilab/SLAC), January 12, 2021\nAs Supersymmetry Fails Tests, Physicists Seek New Ideas – Quanta Magazine, November 20, 2012\nWhat is Supersymmetry? – Fermilab, May 21, 2013\nWhy Supersymmetry? – Fermilab, May 31, 2013\nThe Standard Model and Supersymmetry – World Science Festival, March 4, 2015\nSUSY running out of hiding places – BBC, December 11, 2012', '=== Supersymmetric extensions of the Standard Model ===\nIncorporating supersymmetry into the Standard Model requires doubling the number of particles since there is no way that any of the particles in the Standard Model can be superpartners of each other. With the addition of new particles, there are many possible new interactions. The simplest possible supersymmetric model consistent with the Standard Model is the Minimal Supersymmetric Standard Model (MSSM) which can include the necessary additional new particles that are able to be superpartners of those in the Standard Model.\nOne of the original motivations for the Minimal Supersymmetric Standard Model came from the hierarchy problem. Due to the quadratically divergent contributions to the Higgs mass squared in the Standard Model, the quantum mechanical interactions of the Higgs boson causes a large renormalization of the Higgs mass and unless there is an accidental cancellation, the natural size of the Higgs mass is the greatest scale possible. Furthermore, the electroweak scale receives enormous Planck-scale quantum corrections. The observed hierarchy between the electroweak scale and the Planck scale must be achieved with extraordinary fine tuning. This problem is known as the hierarchy problem.\nSupersymmetry close to the electroweak scale, such as in the Minimal Supersymmetric Standard Model, would solve the hierarchy problem that afflicts the Standard Model. It would reduce the size of the quantum corrections by having automatic cancellations between fermionic and bosonic Higgs interactions, and Planck-scale quantum corrections cancel between partners and superpartners (owing to a minus sign associated with fermionic loops). The hierarchy between the electroweak scale and the Planck scale would be achieved in a natural manner, without extraordinary fine-tuning. If supersymmetry were restored at the weak scale, then the Higgs mass would be related to supersymmetry breaking which can be induced from small non-perturbative effects explaining the vastly different scales in the weak interactions and gravitational interactions.\nAnother motivation for the Minimal Supersymmetric Standard Model comes from grand unification, the idea that the gauge symmetry groups should unify at high-energy. In the Standard Model, however, the weak, strong and electromagnetic gauge couplings fail to unify at high energy. In particular, the renormalization group evolution of the three gauge coupling constants of the Standard Model is somewhat sensitive to the present particle content of the theory. These coupling constants do not quite meet together at a common energy scale if we run the renormalization group using the Standard Model. After incorporating minimal SUSY at the electroweak scale, the running of the gauge couplings are modified, and joint convergence of the gauge coupling constants is projected to occur at approximately 1016 GeV. The modified running also provides a natural mechanism for radiative electroweak symmetry breaking.\nIn many supersymmetric extensions of the Standard Model, such as the Minimal Supersymmetric Standard Model, there is a heavy stable particle (such as the neutralino) which could serve as a weakly interacting massive particle (WIMP) dark matter candidate. The existence of a supersymmetric dark matter candidate is related closely to R-parity. Supersymmetry at the electroweak scale (augmented with a discrete symmetry) typically provides a candidate dark matter particle at a mass scale consistent with thermal relic abundance calculations.\nThe standard paradigm for incorporating supersymmetry into a realistic theory is to have the underlying dynamics of the theory be supersymmetric, but the ground state of the theory does not respect the symmetry and supersymmetry is broken spontaneously. The supersymmetry break can not be done permanently by the particles of the MSSM as they currently appear. This means that there is a new sector of the theory that is responsible for the breaking. The only constraint on this new sector is that it must break supersymmetry permanently and must give superparticles TeV scale masses. There are many models that can do this and most of their details do not matter. In order to parameterize the relevant features of supersymmetry breaking, arbitrary soft SUSY breaking terms are added to the theory which temporarily break SUSY explicitly but could never arise from a complete theory of supersymmetry breaking.\nAll of these supersymmetric partners (sparticles) are hypothetical and have not been observed experimentally. They are predicted by various supersymmetric extensions of the Standard Model.\n=== Searches and constraints for supersymmetry ===\nSUSY extensions of the standard model are constrained by a variety of experiments, including measurements of low-energy observables – for example, the anomalous magnetic moment of the muon at Fermilab; the WMAP dark matter density measurement and direct detection experiments – for example, XENON-100 and LUX; and by particle collider experiments, including B-physics, Higgs phenomenology and direct searches for superpartners (sparticles), at the Large Electron–Positron Collider, Tevatron and the LHC. In fact, CERN publicly states that if a supersymmetric model of the Standard Model "is correct, supersymmetric particles should appear in collisions at the LHC."\nHistorically, the tightest limits were from direct production at colliders. The first mass limits for squarks and gluinos were made at CERN by the UA1 experiment and the UA2 experiment at the Super Proton Synchrotron. LEP later set very strong limits, which in 2006 were extended by the D0 experiment at the Tevatron. From 2003 to 2015, WMAP\'s and Planck\'s dark matter density measurements have strongly constrained supersymmetric extensions of the Standard Model, which, if they explain dark matter, have to be tuned to invoke a particular mechanism to sufficiently reduce the neutralino density.\nPrior to the beginning of the LHC, in 2009, fits of available data to CMSSM and NUHM1 indicated that squarks and gluinos were most likely to have masses in the 500 to 800 GeV range, though values as high as 2.5 TeV were allowed with low probabilities. Neutralinos and sleptons were expected to be quite light, with the lightest neutralino and the lightest stau most likely to be found between 100 and 150 GeV.\nThe first runs of the LHC surpassed existing experimental limits from the Large Electron–Positron Collider and Tevatron and partially excluded the aforementioned expected ranges. In 2011–12, the LHC discovered a Higgs boson with a mass of about 125 GeV, and with couplings to fermions and bosons which are consistent with the Standard Model. The MSSM predicts that the mass of the lightest Higgs boson should not be much higher than the mass of the Z boson, and, in the absence of fine tuning (with the supersymmetry breaking scale on the order of 1 TeV), should not exceed 135 GeV. The LHC found no previously unknown particles other than the Higgs boson which was already suspected to exist as part of the Standard Model, and therefore no evidence for any supersymmetric extension of the Standard Model.\nIndirect methods include the search for a permanent electric dipole moment (EDM) in the known Standard Model particles, which can arise when the Standard Model particle interacts with the supersymmetric particles. The current best constraint on the electron electric dipole moment put it to be smaller than 10−28 e·cm, equivalent to a sensitivity to new physics at the TeV scale and matching that of the current best particle colliders. A permanent EDM in any fundamental particle points towards time-reversal violating physics, and therefore also CP-symmetry violation via the CPT theorem. Such EDM experiments are also much more scalable than conventional particle accelerators and offer a practical alternative to detecting physics beyond the standard model as accelerator experiments become increasingly costly and complicated to maintain. The current best limit for the electron\'s EDM has already reached a sensitivity to rule out so called \'naive\' versions of supersymmetric extensions of the Standard Model.\nResearch in the late 2010s and early 2020s from experimental data on the cosmological constant, LIGO noise, and pulsar timing, suggests it\'s very unlikely that there are any new particles with masses much higher than those which can be found in the standard model or the LHC. However, this research has also indicated that quantum gravity or perturbative quantum field theory will become strongly coupled before 1 PeV, leading to other new physics in the TeVs.\n=== Current status ===\nThe negative findings in the experiments disappointed many physicists, who believed that supersymmetric extensions of the Standard Model (and other theories relying upon it) were by far the most promising theories for "new" physics beyond the Standard Model, and had hoped for signs of unexpected results from the experiments. In particular, the LHC result seems problematic for the Minimal Supersymmetric Standard Model, as the value of 125 GeV is relatively large for the model and can only be achieved with large radiative loop corrections from top squarks, which many theorists consider to be "unnatural" (see naturalness and fine tuning).', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions, which groups by the nature of the outcome being considered (discrete, absolutely continuous, multivariate, etc.)\nAll of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution.\n=== Linear growth (e.g. errors, offsets) ===\nNormal distribution (Gaussian distribution), for a single such quantity; the most commonly used absolutely continuous distribution\n=== Exponential growth (e.g. prices, incomes, populations) ===\nLog-normal distribution, for a single such quantity whose log is normally distributed\nPareto distribution, for a single such quantity whose log is exponentially distributed; the prototypical power law distribution\n=== Uniformly distributed quantities ===\nDiscrete uniform distribution, for a finite set of values (e.g. the outcome of a fair dice)\nContinuous uniform distribution, for absolutely continuously distributed values\n=== Bernoulli trials (yes/no events, with a given probability) ===\nBasic distributions:\nBernoulli distribution, for the outcome of a single Bernoulli trial (e.g. success/failure, yes/no)\nBinomial distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed total number of independent occurrences\nNegative binomial distribution, for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs\nGeometric distribution, for binomial-type observations but where the quantity of interest is the number of failures before the first success; a special case of the negative binomial distribution\nRelated to sampling schemes over a finite population:\nHypergeometric distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, using sampling without replacement\nBeta-binomial distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, sampling using a Pólya urn model (in some sense, the "opposite" of sampling without replacement)\n=== Categorical outcomes (events with K possible outcomes) ===\nCategorical distribution, for a single categorical outcome (e.g. yes/no/maybe in a survey); a generalization of the Bernoulli distribution\nMultinomial distribution, for the number of each type of categorical outcome, given a fixed number of total outcomes; a generalization of the binomial distribution\nMultivariate hypergeometric distribution, similar to the multinomial distribution, but using sampling without replacement; a generalization of the hypergeometric distribution\n=== Poisson process (events that occur independently with a given rate) ===\nPoisson distribution, for the number of occurrences of a Poisson-type event in a given period of time\nExponential distribution, for the time before the next Poisson-type event occurs\nGamma distribution, for the time before the next k Poisson-type events occur\n=== Absolute values of vectors with normally distributed components ===\nRayleigh distribution, for the distribution of vector magnitudes with Gaussian distributed orthogonal components. Rayleigh distributions are found in RF signals with Gaussian real and imaginary components.\nRice distribution, a generalization of the Rayleigh distributions for where there is a stationary background signal component. Found in Rician fading of radio signals due to multipath propagation and in MR images with noise corruption on non-zero NMR signals.\n=== Normally distributed quantities operated with sum of squares ===\nChi-squared distribution, the distribution of a sum of squared standard normal variables; useful e.g. for inference regarding the sample variance of normally distributed samples (see chi-squared test)\nStudent\'s t distribution, the distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable; useful for inference regarding the mean of normally distributed samples with unknown variance (see Student\'s t-test)\nF-distribution, the distribution of the ratio of two scaled chi squared variables; useful e.g. for inferences that involve comparing variances or involving R-squared (the squared correlation coefficient)\n=== As conjugate prior distributions in Bayesian inference ===\nBeta distribution, for a single probability (real number between 0 and 1); conjugate to the Bernoulli distribution and binomial distribution\nGamma distribution, for a non-negative scaling parameter; conjugate to the rate parameter of a Poisson distribution or exponential distribution, the precision (inverse variance) of a normal distribution, etc.\nDirichlet distribution, for a vector of probabilities that must sum to 1; conjugate to the categorical distribution and multinomial distribution; generalization of the beta distribution\nWishart distribution, for a symmetric non-negative definite matrix; conjugate to the inverse of the covariance matrix of a multivariate normal distribution; generalization of the gamma distribution\n=== Some specialized applications of probability distributions ===\nThe cache language models and other statistical language models used in natural language processing to assign probabilities to the occurrence of particular words and word sequences do so by means of probability distributions.\nIn quantum mechanics, the probability density of finding the particle at a given point is proportional to the square of the magnitude of the particle\'s wavefunction at that point (see Born rule). Therefore, the probability distribution function of the position of a particle is described by\n{\\textstyle P_{a\\leq x\\leq b}(t)=\\int _{a}^{b}dx\\,|\\Psi (x,t)|^{2}}\n, probability that the particle\'s position x will be in the interval a ≤ x ≤ b in dimension one, and a similar triple integral in dimension three. This is a key principle of quantum mechanics.\nProbabilistic load flow in power-flow study explains the uncertainties of input variables as probability distribution and provides the power flow calculation also in term of probability distribution.\nPrediction of natural phenomena occurrences based on previous frequency distributions such as tropical cyclones, hail, time in between events, etc.\n== Fitting ==\n== See also ==\nConditional probability distribution\nEmpirical probability distribution\nHistogram\nJoint probability distribution\nProbability measure\nQuasiprobability distribution\nRiemann–Stieltjes integral application to probability theory\n=== Lists ===\nList of probability distributions\nList of statistical topics\n== References ==\n=== Citations ===\n=== Sources ===\n== External links ==\n"Probability distribution", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nField Guide to Continuous Probability Distributions, Gavin E. Crooks.\nDistinguishing probability measure, function and distribution, Math Stack Exchange', 'However, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".\nIn October 2000 astronomers Iván Almár and Jill Tarter presented a paper to The SETI Permanent Study Group in Rio de Janeiro, Brazil which proposed a scale (modelled after the Torino scale) which is an ordinal scale between zero and ten that quantifies the impact of any public announcement regarding evidence of extraterrestrial intelligence; the Rio scale has since inspired the 2005 San Marino Scale (in regard to the risks of transmissions from Earth) and the 2010 London Scale (in regard to the detection of extraterrestrial life). The Rio scale itself was revised in 2018.\nThe SETI Institute does not officially recognize the Wow! signal as of extraterrestrial origin as it was unable to be verified, although in a 2020 Twitter post the organization stated that \'\'an astronomer might have pinpointed the host star\'\'. The SETI Institute has also publicly denied that the candidate signal Radio source SHGb02+14a is of extraterrestrial origin. Although other volunteering projects such as Zooniverse credit users for discoveries, there is currently no crediting or early notification by SETI@Home following the discovery of a signal.\nSome people, including Steven M. Greer, have expressed cynicism that the general public might not be informed in the event of a genuine discovery of extraterrestrial intelligence due to significant vested interests. Some, such as Bruce Jakosky have also argued that the official disclosure of extraterrestrial life may have far reaching and as yet undetermined implications for society, particularly for the world\'s religions.\n== Active SETI ==\nActive SETI, also known as messaging to extraterrestrial intelligence (METI), consists of sending signals into space in the hope that they will be detected by an alien intelligence.\n=== Realized interstellar radio message projects ===\nIn November 1974, a largely symbolic attempt was made at the Arecibo Observatory to send a message to other worlds. Known as the Arecibo Message, it was sent towards the globular cluster M13, which is 25,000 light-years from Earth. Further IRMs Cosmic Call, Teen Age Message, Cosmic Call 2, and A Message From Earth were transmitted in 1999, 2001, 2003 and 2008 from the Evpatoria Planetary Radar.\n=== Debate ===\nWhether or not to attempt to contact extraterrestrials has attracted significant academic debate in the fields of space ethics and space policy. Physicist Stephen Hawking, in his book A Brief History of Time, suggests that "alerting" extraterrestrial intelligences to our existence is foolhardy, citing humankind\'s history of treating its own kind harshly in meetings of civilizations with a significant technology gap, e.g., the extermination of Tasmanian aborigines. He suggests, in view of this history, that we "lay low". In one response to Hawking, in September 2016, astronomer Seth Shostak sought to allay such concerns. Astronomer Jill Tarter also disagrees with Hawking, arguing that aliens developed and long-lived enough to communicate and travel across interstellar distances would have evolved a cooperative and less violent intelligence. She however thinks it is too soon for humans to attempt active SETI and that humans should be more advanced technologically first but keep listening in the meantime.\n== Criticism ==\nAs various SETI projects have progressed, some have criticized early claims by researchers as being too "euphoric". For example, Peter Schenkel, while remaining a supporter of SETI projects, wrote in 2006 that:\n[i]n light of new findings and insights, it seems appropriate to put excessive euphoria to rest and to take a more down-to-earth view [...] We should quietly admit that the early estimates—that there may be a million, a hundred thousand, or ten thousand advanced extraterrestrial civilizations in our galaxy—may no longer be tenable.\nCritics claim that the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability, as explained in a 2009 editorial in Nature, which said:\nSeti... has always sat at the edge of mainstream astronomy. This is partly because, no matter how scientifically rigorous its practitioners try to be, SETI can\'t escape an association with UFO believers and other such crackpots. But it is also because SETI is arguably not a falsifiable experiment. Regardless of how exhaustively the Galaxy is searched, the null result of radio silence doesn\'t rule out the existence of alien civilizations. It means only that those civilizations might not be using radio to communicate.\nNature added that SETI was "marked by a hope, bordering on faith" that aliens were aiming signals at us, that a hypothetical alien SETI project looking at Earth with "similar faith" would be "sorely disappointed", despite our many untargeted radar and TV signals, and our few targeted Active SETI radio signals denounced by those fearing aliens, and that it had difficulties attracting even sympathetic working scientists and government funding because it was "an effort so likely to turn up nothing".\nHowever, Nature also added, "Nonetheless, a small SETI effort is well worth supporting, especially given the enormous implications if it did succeed" and that "happily, a handful of wealthy technologists and other private donors have proved willing to provide that support".\nSupporters of the Rare Earth Hypothesis argue that advanced lifeforms are likely to be very rare, and that, if that is so, then SETI efforts will be futile. However, the Rare Earth Hypothesis itself faces many criticisms.\nIn 1993, Roy Mash stated that "Arguments favoring the existence of extraterrestrial intelligence nearly always contain an overt appeal to big numbers, often combined with a covert reliance on generalization from a single instance" and concluded that "the dispute between believers and skeptics is seen to boil down to a conflict of intuitions which can barely be engaged, let alone resolved, given our present state of knowledge". In response, in 2012, Milan M. Ćirković, then research professor at the Astronomical Observatory of Belgrade and a research associate of the Future of Humanity Institute at the University of Oxford, said that Mash was unrealistically over-reliant on excessive abstraction that ignored the empirical information available to modern SETI researchers.\nGeorge Basalla, Emeritus Professor of History at the University of Delaware, is a critic of SETI who argued in 2006 that "extraterrestrials discussed by scientists are as imaginary as the spirits and gods of religion or myth", and was in turn criticized by Milan M. Ćirković for, among other things, being unable to distinguish between "SETI believers" and "scientists engaged in SETI", who are often sceptical (especially about quick detection), such as Freeman Dyson and, at least in their later years, Iosif Shklovsky and Sebastian von Hoerner, and for ignoring the difference between the knowledge underlying the arguments of modern scientists and those of ancient Greek thinkers.\nMassimo Pigliucci, Professor of Philosophy at CUNY – City College, asked in 2010 whether SETI is "uncomfortably close to the status of pseudoscience" due to the lack of any clear point at which negative results cause the hypothesis of Extraterrestrial Intelligence to be abandoned, before eventually concluding that SETI is "almost-science", which is described by Milan M. Ćirković as Pigliucci putting SETI in "the illustrious company of string theory, interpretations of quantum mechanics, evolutionary psychology and history (of the \'synthetic\' kind done recently by Jared Diamond)", while adding that his justification for doing so with SETI "is weak, outdated, and reflecting particular philosophical prejudices similar to the ones described above in Mash and Basalla".\nRichard Carrigan, a particle physicist at the Fermi National Accelerator Laboratory near Chicago, Illinois, suggested that passive SETI could also be dangerous and that a signal released onto the Internet could act as a computer virus. Computer security expert Bruce Schneier dismissed this possibility as a "bizarre movie-plot threat".\n=== Ufology ===\nUfologist Stanton Friedman has often criticized SETI researchers for, among other reasons, what he sees as their unscientific criticisms of Ufology, but, unlike SETI, Ufology has generally not been embraced by academia as a scientific field of study, and it is usually characterized as a partial or total pseudoscience. In a 2016 interview, Jill Tarter pointed out that it is still a misconception that SETI and UFOs are related. She states, "SETI uses the tools of the astronomer to attempt to find evidence of somebody else\'s technology coming from a great distance. If we ever claim detection of a signal, we will provide evidence and data that can be independently confirmed. UFOs—none of the above." The Galileo Project headed by Harvard astronomer Avi Loeb is one of the few scientific efforts to study UFOs or UAPs. Loeb criticized that the study of UAP is often dismissed and not sufficiently studied by scientists and should shift from "occupying the talking points of national security administrators and politicians" to the realm of science. The Galileo Project\'s position after the publication of the 2021 UFO Report by the U.S. Intelligence community is that the scientific community needs to "systematically, scientifically and transparently look for potential evidence of extraterrestrial technological equipment".\n== See also ==\n== References ==\n== Further reading ==', '== See also ==\n== References ==\n== Further reading ==\nAnderson, P.W., Basic Notions of Condensed Matter Physics, Perseus Publishing (1997).\nFaghri, A., and Zhang, Y., Fundamentals of Multiphase Heat Transfer and Flow, Springer Nature Switzerland AG, 2020.\nFisher, M.E. (1974). "The renormalization group in the theory of critical behavior". Rev. Mod. Phys. 46 (4): 597–616. Bibcode:1974RvMP...46..597F. doi:10.1103/revmodphys.46.597.\nGoldenfeld, N., Lectures on Phase Transitions and the Renormalization Group, Perseus Publishing (1992).\nIvancevic, Vladimir G; Ivancevic, Tijana T (2008), Chaos, Phase Transitions, Topology Change and Path Integrals, Berlin: Springer, ISBN 978-3-540-79356-4, retrieved 14 March 2013\nM.R. Khoshbin-e-Khoshnazar, Ice Phase Transition as a sample of finite system phase transition, (Physics Education (India) Volume 32. No. 2, Apr - Jun 2016)\nKleinert, H., Gauge Fields in Condensed Matter, Vol. I, "Superfluidity and Vortex lines; Disorder Fields, Phase Transitions", pp. 1–742, World Scientific (Singapore, 1989); Paperback ISBN 9971-5-0210-0 (physik.fu-berlin.de readable online)\nKleinert, Hagen; Verena Schulte-Frohlinde (2001). Critical Properties of φ4-Theories. World Scientific. ISBN 981-02-4659-5. Archived from the original on 26 February 2008. (readable online).\nKogut, J.; Wilson, K (1974). "The Renormalization Group and the epsilon-Expansion". Phys. Rep. 12 (2): 75–199. Bibcode:1974PhR....12...75W. doi:10.1016/0370-1573(74)90023-4.\nKrieger, Martin H., Constitutions of matter : mathematically modelling the most everyday of physical phenomena, University of Chicago Press, 1996. Contains a detailed pedagogical discussion of Onsager\'s solution of the 2-D Ising Model.\nLandau, L.D. and Lifshitz, E.M., Statistical Physics Part 1, vol. 5 of Course of Theoretical Physics, Pergamon Press, 3rd Ed. (1994).\nMussardo G., "Statistical Field Theory. An Introduction to Exactly Solved Models of Statistical Physics", Oxford University Press, 2010.\nSchroeder, Manfred R., Fractals, chaos, power laws : minutes from an infinite paradise, New York: W. H. Freeman, 1991.  Very well-written book in "semi-popular" style—not a textbook—aimed at an audience with some training in mathematics and the physical sciences.  Explains what scaling in phase transitions is all about, among other things.\nH. E. Stanley, Introduction to Phase Transitions and Critical Phenomena (Oxford University Press, Oxford and New York 1971).\nYeomans J. M., Statistical Mechanics of Phase Transitions, Oxford University Press, 1992.\n== External links ==\nMedia related to Phase changes at Wikimedia Commons\nInteractive Phase Transitions on lattices with Java applets\nUniversality classes from Sklogwiki']

Question: What is the interpretation of supersymmetry in stochastic supersymmetric theory?

Choices:
Choice A) Supersymmetry is a type of hydromagnetic dynamo that arises when the magnetic field becomes strong enough to affect the fluid motions.
Choice B) Supersymmetry is a measure of the amplitude of the dynamo in the induction equation of the kinematic approximation.
Choice C) Supersymmetry is a measure of the strength of the magnetic field in the induction equation of the kinematic dynamo.
Choice D) Supersymmetry is a property of deterministic chaos that arises from the continuity of the flow in the model's phase space.
Choice E) Supersymmetry is an intrinsic property of all stochastic differential equations, and it preserves continuity in the model's phase space via continuous time flows.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'In October 2019, Breakthrough Listen started a collaboration with scientists from the TESS team (Transiting Exoplanet Survey Satellite) to look for signs of advanced extraterrestrial life. Thousands of new planets found by TESS will be scanned for technosignatures by Breakthrough Listen partner facilities across the globe. Data from TESS monitoring of stars will also be searched for anomalies.\n=== FAST ===\nChina\'s 500 meter Aperture Spherical Telescope (FAST) lists detecting interstellar communication signals as part of its science mission. It is funded by the National Development and Reform Commission (NDRC) and managed by the National Astronomical observatories (NAOC) of the Chinese Academy of Sciences (CAS). FAST is the first radio observatory built with SETI as a core scientific goal. FAST consists of a fixed 500 m (1,600 ft) diameter spherical dish constructed in a natural depression sinkhole caused by karst processes in the region. It is the world\'s largest filled-aperture radio telescope.\nAccording to its website, FAST can search to 28 light-years, and is able to reach 1,400 stars. If the transmitter\'s radiated power were to be increased to 1,000,000 MW, FAST would be able to reach one million stars. This is compared to the former Arecibo 305 meter telescope detection distance of 18 light-years.\nOn 14 June 2022, astronomers, working with China\'s FAST telescope, reported the possibility of having detected artificial (presumably alien) signals, but cautioned that further studies were required to determine if a natural radio interference may be the source. More recently, on 18 June 2022, Dan Werthimer, chief scientist for several SETI-related projects, reportedly noted, "These signals are from radio interference; they are due to radio pollution from earthlings, not from E.T.".\n=== UCLA ===\nSince 2016, University of California Los Angeles (UCLA) undergraduate and graduate students have been participating in radio searches for technosignatures with the Green Bank Telescope. Targets include the Kepler field, TRAPPIST-1, and solar-type stars. The search is sensitive to Arecibo-class transmitters located within 420 light years of Earth and to transmitters that are 1,000 times more powerful than Arecibo located within 13,000 light years of Earth.\n== Community SETI projects ==\n=== SETI@home ===\nThe SETI@home project used volunteer computing to analyze signals acquired by the SERENDIP project.\nSETI@home was conceived by David Gedye along with Craig Kasnoff and is a popular volunteer computing project that was launched by the Berkeley SETI Research Center at the University of California, Berkeley, in May 1999. It was originally funded by The Planetary Society and Paramount Pictures, and later by the state of California. The project is run by director David P. Anderson and chief scientist Dan Werthimer. Any individual could become involved with SETI research by downloading the Berkeley Open Infrastructure for Network Computing (BOINC) software program, attaching to the SETI@home project, and allowing the program to run as a background process that uses idle computer power. The SETI@home program itself ran signal analysis on a "work unit" of data recorded from the central 2.5 MHz wide band of the SERENDIP IV instrument. After computation on the work unit was complete, the results were then automatically reported back to SETI@home servers at University of California, Berkeley. By June 28, 2009, the SETI@home project had over 180,000 active participants volunteering a total of over 290,000 computers. These computers gave SETI@home an average computational power of 617 teraFLOPS. In 2004 radio source SHGb02+14a set off speculation in the media that a signal had been detected but researchers noted the frequency drifted rapidly and the detection on three SETI@home computers fell within random chance.\nBy 2010, after 10 years of data collection, SETI@home had listened to that one frequency at every point of over 67 percent of the sky observable from Arecibo with at least three scans (out of the goal of nine scans), which covers about 20 percent of the full celestial sphere. On March 31, 2020, with 91,454 active users, the project stopped sending out new work to SETI@home users, bringing this particular SETI effort to an indefinite hiatus.\n=== SETI Net ===\nSETI Network was the only fully operational private search system. The SETI Net station consisted of off-the-shelf, consumer-grade electronics to minimize cost and to allow this design to be replicated as simply as possible. It had a 3-meter parabolic antenna that could be directed in azimuth and elevation, an LNA that covered 100 MHz of the 1420 MHz spectrum, a receiver to reproduce the wideband audio, and a standard personal computer as the control device and for deploying the detection algorithms. The antenna could be pointed and locked to one sky location in Ra and DEC which enabling the system to integrate on it for long periods. The Wow! signal area was monitored for many long periods. All search data was collected and is available on the Internet archive.\nSETI Net started operation in the early 1980s as a way to learn about the science of the search, and developed several software packages for the amateur SETI community. It provided an astronomical clock, a file manager to keep track of SETI data files, a spectrum analyzer optimized for amateur SETI, remote control of the station from the Internet, and other packages.\nSETI Net went dark and was decommissioned on 2021-12-04. The collected data is available on their website.\n=== The SETI League and Project Argus ===\nFounded in 1994 in response to the United States Congress cancellation of the NASA SETI program, The SETI League, Incorporated is a membership-supported nonprofit organization with 1,500 members in 62 countries. This grass-roots alliance of amateur and professional radio astronomers is headed by executive director emeritus H. Paul Shuch, the engineer credited with developing the world\'s first commercial home satellite TV receiver. Many SETI League members are licensed radio amateurs and microwave experimenters. Others are digital signal processing experts and computer enthusiasts.\nThe SETI League pioneered the conversion of backyard satellite TV dishes 3 to 5 m (10–16 ft) in diameter into research-grade radio telescopes of modest sensitivity. The organization concentrates on coordinating a global network of small, amateur-built radio telescopes under Project Argus, an all-sky survey seeking to achieve real-time coverage of the entire sky. Project Argus was conceived as a continuation of the all-sky survey component of the late NASA SETI program (the targeted search having been continued by the SETI Institute\'s Project Phoenix). There are currently 143 Project Argus radio telescopes operating in 27 countries. Project Argus instruments typically exhibit sensitivity on the order of 10−23 Watts/square metre, or roughly equivalent to that achieved by the Ohio State University Big Ear radio telescope in 1977, when it detected the landmark "Wow!" candidate signal.\nThe name "Argus" derives from the mythical Greek guard-beast who had 100 eyes, and could see in all directions at once. In the SETI context, the name has been used for radio telescopes in fiction (Arthur C. Clarke, "Imperial Earth"; Carl Sagan, "Contact"), was the name initially used for the NASA study ultimately known as "Cyclops," and is the name given to an omnidirectional radio telescope design being developed at the Ohio State University.\n== Optical experiments ==\nWhile most SETI sky searches have studied the radio spectrum, some SETI researchers have considered the possibility that alien civilizations might be using powerful lasers for interstellar communications at optical wavelengths. The idea was first suggested by R. N. Schwartz and Charles Hard Townes in a 1961 paper published in the journal Nature titled "Interstellar and Interplanetary Communication by Optical Masers". However, the 1971 Cyclops study discounted the possibility of optical SETI, reasoning that construction of a laser system that could outshine the bright central star of a remote star system would be too difficult. In 1983, Townes published a detailed study of the idea in the United States journal Proceedings of the National Academy of Sciences, which was met with interest by the SETI community.\nThere are two problems with optical SETI. The first problem is that lasers are highly "monochromatic", that is, they emit light only on one frequency, making it troublesome to figure out what frequency to look for. However, emitting light in narrow pulses results in a broad spectrum of emission; the spread in frequency becomes higher as the pulse width becomes narrower, making it easier to detect an emission.\nThe other problem is that while radio transmissions can be broadcast in all directions, lasers are highly directional. Interstellar gas and dust is almost transparent to near infrared, so these signals can be seen from greater distances, but the extraterrestrial laser signals would need to be transmitted in the direction of Earth in order to be detected.\nOptical SETI supporters have conducted paper studies of the effectiveness of using contemporary high-energy lasers and a ten-meter diameter mirror as an interstellar beacon. The analysis shows that an infrared pulse from a laser, focused into a narrow beam by such a mirror, would appear thousands of times brighter than the Sun to a distant civilization in the beam\'s line of fire. The Cyclops study proved incorrect in suggesting a laser beam would be inherently hard to see.', 'Shower-curtain effect\n\nThe shower-curtain effect in physics describes the phenomenon of a shower curtain being blown inward when a shower is running. The problem of identifying the cause of this effect has been featured in Scientific American magazine, with several theories given to explain the phenomenon but no definite conclusion.\nThe shower-curtain effect may also be used to describe the observation of how nearby phase front distortions of an optical wave are more severe than remote distortions of the same amplitude.\n== Hypotheses ==\n=== Buoyancy hypothesis ===\nAlso called chimney effect or stack effect, observes that warm air (from the hot shower) rises out over the shower curtain as cooler air (near the floor) pushes in under the curtain to replace the rising air.  By pushing the curtain in towards the shower, the (short range) vortex and Coandă effects become more significant. However, the shower-curtain effect persists when cold water is used, implying that this is not the sole mechanism.\n=== Bernoulli effect hypothesis ===\nThe most popular explanation given for the shower-curtain effect is Bernoulli\'s principle.  Bernoulli\'s principle states that an increase in velocity results in a decrease in pressure.  This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water.  This movement would be parallel to the plane of the shower curtain.  If air is moving across the inside surface of the shower curtain, Bernoulli\'s principle says the air pressure there will drop.  This would result in a pressure differential between the inside and outside, causing the curtain to move inward.  It would be strongest when the gap between the bather and the curtain is smallest, resulting in the curtain attaching to the bather.\n=== Horizontal vortex hypothesis ===\nA computer simulation of a typical bathroom found that none of the above theories pan out in their analysis, but instead found that the spray from the shower-head drives a horizontal vortex. This vortex has a low-pressure zone in the centre, which sucks the curtain.\nDavid Schmidt of the University of Massachusetts was awarded the 2001 Ig Nobel Prize in Physics for his partial solution to the question of why shower curtains billow inwards. He used a computational fluid dynamics code to achieve the results.  Professor Schmidt is adamant that this was done "for fun" in his own free time without the use of grants.\n=== Coandă effect ===\nThe Coandă effect, also known as "boundary layer attachment", is the tendency of a moving fluid to adhere to an adjacent wall.\n=== Condensation ===\nA hot shower will produce steam that condenses on the shower side of the curtain, lowering the pressure there.  In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.\n=== Air pressure ===\nColder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.\n== Solutions ==\nMany shower curtains come with features to reduce the shower-curtain effect. They may have adhesive suction cups on the bottom edges of the curtain, which are then pushed onto the sides of the shower when in use. Others may have magnets at the bottom, though these are not effective on acrylic or fiberglass tubs.\nIt is possible to use a telescopic shower curtain rod to block the curtain on its lower part and to prevent it from sucking inside.\nHanging the curtain rod higher or lower, or especially further away from the shower head, can reduce the effect. A convex shower rod can also be used to hold the curtain against the inside wall of a tub.\nA weight can be attached to a long string and the string attached to the curtain rod in the middle of the curtain (on the inside). Hanging the weight low against the curtain just above the rim of the shower pan or tub makes it an effective billowing deterrent without allowing the weight to hit the pan or tub and damage it.\nThere are a few alternative solutions that either attach to the shower curtain directly, attach to the shower rod or attach to the wall.\n== References ==\n== External links ==\nScientific American: Why does the shower curtain move toward the water?\nWhy does the shower curtain blow up and in instead of down and out?\nVideo demonstration of how this phenomenon could be solved.\nThe Straight Dope: Why does the shower curtain blow in despite the water pushing it out (revisited)?\n2001 Ig Nobel Prize Winners\nFluent NEWS: Shower Curtain Grabs Scientist – But He Lives to Tell Why\nArggh, Why Does the Shower Curtain Attack Me? by Joe Palca. All Things Considered, National Public Radio.  November 4, 2006. (audio)\nExperimental Investigation of the Influence of the Relative Position of the Scattering Layer on Image Quality: the Shower Curtain Effect\nThe shower curtain effect; ESA', 'An early use of the piezoelectricity of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. The quartz clock is a familiar device using the mineral. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.\nAlmost all the industrial demand for quartz crystal (used primarily in electronics) is met with synthetic quartz produced by the hydrothermal process. However, synthetic crystals are less prized for use as gemstones. The popularity of crystal healing has increased the demand for natural quartz crystals, which are now often mined in developing countries using primitive mining methods, sometimes involving child labor.\n== See also ==\nFused quartz\nList of minerals\nQuartz fiber\nQuartz reef mining\nQuartzolite\nShocked quartz\n== References ==\n== External links ==\nQuartz varieties, properties, crystal morphology. Photos and illustrations\nGilbert Hart, "Nomenclature of Silica", American Mineralogist, Volume 12, pp. 383–395. 1927\n"The Quartz Watch – Inventors". The Lemelson Center, National Museum of American History, Smithsonian Institution. Archived from the original on 7 January 2009.\nTerminology used to describe the characteristics of quartz crystals when used as oscillators\nQuartz use as prehistoric stone tool raw material', 'A star is a luminous spheroid of plasma held together by self-gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye at night; their immense distances from Earth make them appear as fixed points of light. The most prominent stars have been categorised into constellations and asterisms, and many of the brightest stars have proper names. Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations. The observable universe contains an estimated 1022 to 1024 stars. Only about 4,000 of these stars are visible to the naked eye—all within the Milky Way galaxy.\nA star\'s life begins with the gravitational collapse of a gaseous nebula of material largely comprising hydrogen, helium, and trace heavier elements. Its total mass mainly determines its evolution and eventual fate. A star shines for most of its active life due to the thermonuclear fusion of hydrogen into helium in its core. This process releases energy that traverses the star\'s interior and radiates into outer space. At the end of a star\'s lifetime, fusion ceases and its core becomes a stellar remnant: a white dwarf, a neutron star, or—if it is sufficiently massive—a black hole.\nStellar nucleosynthesis in stars or their remnants creates almost all naturally occurring chemical elements heavier than lithium. Stellar mass loss or supernova explosions return chemically enriched material to the interstellar medium. These elements are then recycled into new stars. Astronomers can determine stellar properties—including mass, age, metallicity (chemical composition), variability, distance, and motion through space—by carrying out observations of a star\'s apparent brightness, spectrum, and changes in its position in the sky over time.\nStars can form orbital systems with other astronomical objects, as in planetary systems and star systems with two or more stars. When two such stars orbit closely, their gravitational interaction can significantly impact their evolution. Stars can form part of a much larger gravitationally bound structure, such as a star cluster or a galaxy.\n== Etymology ==\nThe word "star" ultimately derives from the Proto-Indo-European root "h₂stḗr" also meaning star, but further analyzable as h₂eh₁s- ("to burn", also the source of the word "ash") + -tēr (agentive suffix). Compare Latin stella, Greek aster, German Stern. Some scholars believe the word is a borrowing from Akkadian "istar" (Venus). "Star" is cognate (shares the same root) with the following words: asterisk, asteroid, astral, constellation, Esther.\n== Observation history ==\nHistorically, stars have been important to civilizations throughout the world. They have been part of religious practices, divination rituals, mythology, used for celestial navigation and orientation, to mark the passage of seasons, and to define calendars.\nEarly astronomers recognized a difference between "fixed stars", whose position on the celestial sphere does not change, and "wandering stars" (planets), which move noticeably relative to the fixed stars over days or weeks. Many ancient astronomers believed that the stars were permanently affixed to a heavenly sphere and that they were immutable. By convention, astronomers grouped prominent stars into asterisms and constellations and used them to track the motions of the planets and the inferred position of the Sun. The motion of the Sun against the background stars (and the horizon) was used to create calendars, which could be used to regulate agricultural practices. The Gregorian calendar, currently used nearly everywhere in the world, is a solar calendar based on the angle of the Earth\'s rotational axis relative to its local star, the Sun.\nThe oldest accurately dated star chart was the result of ancient Egyptian astronomy in 1534 BC. The earliest known star catalogues were compiled by the ancient Babylonian astronomers of Mesopotamia in the late 2nd millennium BC, during the Kassite Period (c.\u20091531 BC – c.\u20091155 BC).\nThe first star catalogue in Greek astronomy was created by Aristillus in approximately 300 BC, with the help of Timocharis. The star catalog of Hipparchus (2nd century BC) included 1,020 stars, and was used to assemble Ptolemy\'s star catalogue. Hipparchus is known for the discovery of the first recorded nova (new star). Many of the constellations and star names in use today derive from Greek astronomy.\nDespite the apparent immutability of the heavens, Chinese astronomers were aware that new stars could appear. In 185 AD, they were the first to observe and write about a supernova, now known as SN 185. The brightest stellar event in recorded history was the SN 1006 supernova, which was observed in 1006 and written about by the Egyptian astronomer Ali ibn Ridwan and several Chinese astronomers. The SN 1054 supernova, which gave birth to the Crab Nebula, was also observed by Chinese and Islamic astronomers.\nMedieval Islamic astronomers gave Arabic names to many stars that are still used today and they invented numerous astronomical instruments that could compute the positions of the stars. They built the first large observatory research institutes, mainly to produce Zij star catalogues. Among these, the Book of Fixed Stars (964) was written by the Persian astronomer Abd al-Rahman al-Sufi, who observed a number of stars, star clusters (including the Omicron Velorum and Brocchi\'s Clusters) and galaxies (including the Andromeda Galaxy). According to A. Zahoor, in the 11th century, the Persian polymath scholar Abu Rayhan Biruni described the Milky Way galaxy as a multitude of fragments having the properties of nebulous stars, and gave the latitudes of various stars during a lunar eclipse in 1019.\nAccording to Josep Puig, the Andalusian astronomer Ibn Bajjah proposed that the Milky Way was made up of many stars that almost touched one another and appeared to be a continuous image due to the effect of refraction from sublunary material, citing his observation of the conjunction of Jupiter and Mars on 500 AH (1106/1107 AD) as evidence.\nEarly European astronomers such as Tycho Brahe identified new stars in the night sky (later termed novae), suggesting that the heavens were not immutable. In 1584, Giordano Bruno suggested that the stars were like the Sun, and may have other planets, possibly even Earth-like, in orbit around them, an idea that had been suggested earlier by the ancient Greek philosophers, Democritus and Epicurus, and by medieval Islamic cosmologists such as Fakhr al-Din al-Razi. By the following century, the idea of the stars being the same as the Sun was reaching a consensus among astronomers. To explain why these stars exerted no net gravitational pull on the Solar System, Isaac Newton suggested that the stars were equally distributed in every direction, an idea prompted by the theologian Richard Bentley.\nThe Italian astronomer Geminiano Montanari recorded observing variations in luminosity of the star Algol in 1667. Edmond Halley published the first measurements of the proper motion of a pair of nearby "fixed" stars, demonstrating that they had changed positions since the time of the ancient Greek astronomers Ptolemy and Hipparchus.\nWilliam Herschel was the first astronomer to attempt to determine the distribution of stars in the sky. During the 1780s, he established a series of gauges in 600 directions and counted the stars observed along each line of sight. From this, he deduced that the number of stars steadily increased toward one side of the sky, in the direction of the Milky Way core. His son John Herschel repeated this study in the southern hemisphere and found a corresponding increase in the same direction. In addition to his other accomplishments, William Herschel is noted for his discovery that some stars do not merely lie along the same line of sight, but are physical companions that form binary star systems.\nThe science of stellar spectroscopy was pioneered by Joseph von Fraunhofer and Angelo Secchi. By comparing the spectra of stars such as Sirius to the Sun, they found differences in the strength and number of their absorption lines—the dark lines in stellar spectra caused by the atmosphere\'s absorption of specific frequencies. In 1865, Secchi began classifying stars into spectral types. The modern version of the stellar classification scheme was developed by Annie J. Cannon during the early 1900s.\nThe first direct measurement of the distance to a star (61 Cygni at 11.4 light-years) was made in 1838 by Friedrich Bessel using the parallax technique. Parallax measurements demonstrated the vast separation of the stars in the heavens. Observation of double stars gained increasing importance during the 19th century. In 1834, Friedrich Bessel observed changes in the proper motion of the star Sirius and inferred a hidden companion. Edward Pickering discovered the first spectroscopic binary in 1899 when he observed the periodic splitting of the spectral lines of the star Mizar in a 104-day period. Detailed observations of many binary star systems were collected by astronomers such as Friedrich Georg Wilhelm von Struve and S. W. Burnham, allowing the masses of stars to be determined from computation of orbital elements. The first solution to the problem of deriving an orbit of binary stars from telescope observations was made by Felix Savary in 1827.\nThe twentieth century saw increasingly rapid advances in the scientific study of stars. The photograph became a valuable astronomical tool. Karl Schwarzschild discovered that the color of a star and, hence, its temperature, could be determined by comparing the visual magnitude against the photographic magnitude. The development of the photoelectric photometer allowed precise measurements of magnitude at multiple wavelength intervals. In 1921 Albert A. Michelson made the first measurements of a stellar diameter using an interferometer on the Hooker telescope at Mount Wilson Observatory.', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'However, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".\nIn October 2000 astronomers Iván Almár and Jill Tarter presented a paper to The SETI Permanent Study Group in Rio de Janeiro, Brazil which proposed a scale (modelled after the Torino scale) which is an ordinal scale between zero and ten that quantifies the impact of any public announcement regarding evidence of extraterrestrial intelligence; the Rio scale has since inspired the 2005 San Marino Scale (in regard to the risks of transmissions from Earth) and the 2010 London Scale (in regard to the detection of extraterrestrial life). The Rio scale itself was revised in 2018.\nThe SETI Institute does not officially recognize the Wow! signal as of extraterrestrial origin as it was unable to be verified, although in a 2020 Twitter post the organization stated that \'\'an astronomer might have pinpointed the host star\'\'. The SETI Institute has also publicly denied that the candidate signal Radio source SHGb02+14a is of extraterrestrial origin. Although other volunteering projects such as Zooniverse credit users for discoveries, there is currently no crediting or early notification by SETI@Home following the discovery of a signal.\nSome people, including Steven M. Greer, have expressed cynicism that the general public might not be informed in the event of a genuine discovery of extraterrestrial intelligence due to significant vested interests. Some, such as Bruce Jakosky have also argued that the official disclosure of extraterrestrial life may have far reaching and as yet undetermined implications for society, particularly for the world\'s religions.\n== Active SETI ==\nActive SETI, also known as messaging to extraterrestrial intelligence (METI), consists of sending signals into space in the hope that they will be detected by an alien intelligence.\n=== Realized interstellar radio message projects ===\nIn November 1974, a largely symbolic attempt was made at the Arecibo Observatory to send a message to other worlds. Known as the Arecibo Message, it was sent towards the globular cluster M13, which is 25,000 light-years from Earth. Further IRMs Cosmic Call, Teen Age Message, Cosmic Call 2, and A Message From Earth were transmitted in 1999, 2001, 2003 and 2008 from the Evpatoria Planetary Radar.\n=== Debate ===\nWhether or not to attempt to contact extraterrestrials has attracted significant academic debate in the fields of space ethics and space policy. Physicist Stephen Hawking, in his book A Brief History of Time, suggests that "alerting" extraterrestrial intelligences to our existence is foolhardy, citing humankind\'s history of treating its own kind harshly in meetings of civilizations with a significant technology gap, e.g., the extermination of Tasmanian aborigines. He suggests, in view of this history, that we "lay low". In one response to Hawking, in September 2016, astronomer Seth Shostak sought to allay such concerns. Astronomer Jill Tarter also disagrees with Hawking, arguing that aliens developed and long-lived enough to communicate and travel across interstellar distances would have evolved a cooperative and less violent intelligence. She however thinks it is too soon for humans to attempt active SETI and that humans should be more advanced technologically first but keep listening in the meantime.\n== Criticism ==\nAs various SETI projects have progressed, some have criticized early claims by researchers as being too "euphoric". For example, Peter Schenkel, while remaining a supporter of SETI projects, wrote in 2006 that:\n[i]n light of new findings and insights, it seems appropriate to put excessive euphoria to rest and to take a more down-to-earth view [...] We should quietly admit that the early estimates—that there may be a million, a hundred thousand, or ten thousand advanced extraterrestrial civilizations in our galaxy—may no longer be tenable.\nCritics claim that the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability, as explained in a 2009 editorial in Nature, which said:\nSeti... has always sat at the edge of mainstream astronomy. This is partly because, no matter how scientifically rigorous its practitioners try to be, SETI can\'t escape an association with UFO believers and other such crackpots. But it is also because SETI is arguably not a falsifiable experiment. Regardless of how exhaustively the Galaxy is searched, the null result of radio silence doesn\'t rule out the existence of alien civilizations. It means only that those civilizations might not be using radio to communicate.\nNature added that SETI was "marked by a hope, bordering on faith" that aliens were aiming signals at us, that a hypothetical alien SETI project looking at Earth with "similar faith" would be "sorely disappointed", despite our many untargeted radar and TV signals, and our few targeted Active SETI radio signals denounced by those fearing aliens, and that it had difficulties attracting even sympathetic working scientists and government funding because it was "an effort so likely to turn up nothing".\nHowever, Nature also added, "Nonetheless, a small SETI effort is well worth supporting, especially given the enormous implications if it did succeed" and that "happily, a handful of wealthy technologists and other private donors have proved willing to provide that support".\nSupporters of the Rare Earth Hypothesis argue that advanced lifeforms are likely to be very rare, and that, if that is so, then SETI efforts will be futile. However, the Rare Earth Hypothesis itself faces many criticisms.\nIn 1993, Roy Mash stated that "Arguments favoring the existence of extraterrestrial intelligence nearly always contain an overt appeal to big numbers, often combined with a covert reliance on generalization from a single instance" and concluded that "the dispute between believers and skeptics is seen to boil down to a conflict of intuitions which can barely be engaged, let alone resolved, given our present state of knowledge". In response, in 2012, Milan M. Ćirković, then research professor at the Astronomical Observatory of Belgrade and a research associate of the Future of Humanity Institute at the University of Oxford, said that Mash was unrealistically over-reliant on excessive abstraction that ignored the empirical information available to modern SETI researchers.\nGeorge Basalla, Emeritus Professor of History at the University of Delaware, is a critic of SETI who argued in 2006 that "extraterrestrials discussed by scientists are as imaginary as the spirits and gods of religion or myth", and was in turn criticized by Milan M. Ćirković for, among other things, being unable to distinguish between "SETI believers" and "scientists engaged in SETI", who are often sceptical (especially about quick detection), such as Freeman Dyson and, at least in their later years, Iosif Shklovsky and Sebastian von Hoerner, and for ignoring the difference between the knowledge underlying the arguments of modern scientists and those of ancient Greek thinkers.\nMassimo Pigliucci, Professor of Philosophy at CUNY – City College, asked in 2010 whether SETI is "uncomfortably close to the status of pseudoscience" due to the lack of any clear point at which negative results cause the hypothesis of Extraterrestrial Intelligence to be abandoned, before eventually concluding that SETI is "almost-science", which is described by Milan M. Ćirković as Pigliucci putting SETI in "the illustrious company of string theory, interpretations of quantum mechanics, evolutionary psychology and history (of the \'synthetic\' kind done recently by Jared Diamond)", while adding that his justification for doing so with SETI "is weak, outdated, and reflecting particular philosophical prejudices similar to the ones described above in Mash and Basalla".\nRichard Carrigan, a particle physicist at the Fermi National Accelerator Laboratory near Chicago, Illinois, suggested that passive SETI could also be dangerous and that a signal released onto the Internet could act as a computer virus. Computer security expert Bruce Schneier dismissed this possibility as a "bizarre movie-plot threat".\n=== Ufology ===\nUfologist Stanton Friedman has often criticized SETI researchers for, among other reasons, what he sees as their unscientific criticisms of Ufology, but, unlike SETI, Ufology has generally not been embraced by academia as a scientific field of study, and it is usually characterized as a partial or total pseudoscience. In a 2016 interview, Jill Tarter pointed out that it is still a misconception that SETI and UFOs are related. She states, "SETI uses the tools of the astronomer to attempt to find evidence of somebody else\'s technology coming from a great distance. If we ever claim detection of a signal, we will provide evidence and data that can be independently confirmed. UFOs—none of the above." The Galileo Project headed by Harvard astronomer Avi Loeb is one of the few scientific efforts to study UFOs or UAPs. Loeb criticized that the study of UAP is often dismissed and not sufficiently studied by scientists and should shift from "occupying the talking points of national security administrators and politicians" to the realm of science. The Galileo Project\'s position after the publication of the 2021 UFO Report by the U.S. Intelligence community is that the scientific community needs to "systematically, scientifically and transparently look for potential evidence of extraterrestrial technological equipment".\n== See also ==\n== References ==\n== Further reading ==', 'Time is the continuous progression of existence that occurs in an apparently irreversible succession from the past, through the present, and into the future. It is a component quantity of various measurements used to sequence events, to compare the duration of events (or the intervals between them), and to quantify rates of change of quantities in material reality or in the conscious experience. Time is often referred to as a fourth dimension, along with three spatial dimensions.\nTime is one of the seven fundamental physical quantities in both the International System of Units (SI) and International System of Quantities. The SI base unit of time is the second, which is defined by measuring the electronic transition frequency of caesium atoms. General relativity is the primary framework for understanding how spacetime works. Through advances in both theoretical and experimental investigations of spacetime, it has been shown that time can be distorted and dilated, particularly at the edges of black holes.\nThroughout history, time has been an important subject of study in religion, philosophy, and science. Temporal measurement has occupied scientists and technologists, and has been a prime motivation in navigation and astronomy. Time is also of significant social importance, having economic value ("time is money") as well as personal value, due to an awareness of the limited time in each day ("carpe diem") and in human life spans.\n== Definition ==\nThe concept of time can be complex. Multiple notions exist, and defining time in a manner applicable to all fields without circularity has consistently eluded scholars. Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems. Traditional definitions of time involved the observation of periodic motion such as the apparent motion of the sun across the sky, the phases of the moon, and the passage of a free-swinging pendulum. More modern systems include the Global Positioning System, other satellite systems, Coordinated Universal Time and mean solar time. Although these systems differ from one another, with careful measurements they can be synchronized.\nIn physics, time is a fundamental concept to define other quantities, such as velocity. To avoid a circular definition, time in physics is operationally defined as "what a clock reads", specifically a count of repeating events such as the SI second. Although this aids in practical measurements, it does not address the essence of time. Physicists developed the concept of the spacetime continuum, where events are assigned four coordinates: three for space and one for time. Events like particle collisions, supernovas, or rocket launches have coordinates that may vary for different observers, making concepts like "now" and "here" relative. In general relativity, these coordinates do not directly correspond to the causal structure of events. Instead, the spacetime interval is calculated and classified as either space-like or time-like, depending on whether an observer exists that would say the events are separated by space or by time. Since the time required for light to travel a specific distance is the same for all observers—a fact first publicly demonstrated by the Michelson–Morley experiment—all observers will consistently agree on this definition of time as a causal relation.\nGeneral relativity does not address the nature of time for extremely small intervals where quantum mechanics holds. In quantum mechanics, time is treated as a universal and absolute parameter, differing from general relativity\'s notion of independent clocks. The problem of time consists of reconciling these two theories. As of 2025, there is no generally accepted theory of quantum general relativity.\n== Measurement ==\nMethods of temporal measurement, or chronometry, generally take two forms. The first is a calendar, a mathematical tool for organising intervals of time on Earth, consulted for periods longer than a day. The second is a clock, a physical mechanism that indicates the passage of time, consulted for periods less than a day. The combined measurement marks a specific moment in time from a reference point, or epoch.\n=== History of the calendar ===\nArtifacts from the Paleolithic suggest that the moon was used to reckon time as early as 6,000 years ago. Lunar calendars were among the first to appear, with years of either 12 or 13 lunar months (either 354 or 384 days). Without intercalation to add days or months to some years, seasons quickly drift in a calendar based solely on twelve lunar months. Lunisolar calendars have a thirteenth month added to some years to make up for the difference between a full year (now known to be about 365.24 days) and a year of just twelve lunar months. The numbers twelve and thirteen came to feature prominently in many cultures, at least partly due to this relationship of months to years.\nOther early forms of calendars originated in Mesoamerica, particularly in ancient Mayan civilization, in which they developed the Maya calendar, consisting of multiple interrelated calendars. These calendars were religiously and astronomically based; the Haab\' calendar has 18 months in a year and 20 days in a month, plus five epagomenal days at the end of the year. In conjunction, the Maya also used a 260-day sacred calendar called the Tzolk\'in.\nThe reforms of Julius Caesar in 45 BC put the Roman world on a solar calendar. This Julian calendar was faulty in that its intercalation still allowed the astronomical solstices and equinoxes to advance against it by about 11 minutes per year. Pope Gregory XIII introduced a correction in 1582; the Gregorian calendar was only slowly adopted by different nations over a period of centuries, but it is now by far the most commonly used calendar around the world.\nDuring the French Revolution, a new clock and calendar were invented as part of the dechristianization of France and to create a more rational system in order to replace the Gregorian calendar. The French Republican Calendar\'s days consisted of ten hours of a hundred minutes of a hundred seconds, which marked a deviation from the base 12 (duodecimal) system used in many other devices by many cultures. The system was abolished in 1806.\n=== History of other devices ===\nA large variety of devices have been invented to measure time. The study of these devices is called horology. They can be driven by a variety of means, including gravity, springs, and various forms of electrical power, and regulated by a variety of means.\nA sundial is any device that uses the direction of sunlight to cast shadows from a gnomon onto a set of markings calibrated to indicate the local time, usually to the hour. The idea to separate the day into smaller parts is credited to Egyptians because of their sundials, which operated on a duodecimal system. The importance of the number 12 is due to the number of lunar cycles in a year and the number of stars used to count the passage of night. Obelisks made as a gnomon were built as early as c.\u20093500 BC. An Egyptian device that dates to c.\u20091500 BC, similar in shape to a bent T-square, also measured the passage of time from the shadow cast by its crossbar on a nonlinear rule. The T was oriented eastward in the mornings. At noon, the device was turned around so that it could cast its shadow in the evening direction.\nAlarm clocks reportedly first appeared in ancient Greece c.\u2009250 BC with a water clock made by Plato that would set off a whistle. The hydraulic alarm worked by gradually filling a series of vessels with water. After some time, the water emptied out of a siphon. Inventor Ctesibius revised Plato\'s design; the water clock uses a float as the power drive system and uses a sundial to correct the water flow rate.\nIn medieval philosophical writings, the atom was a unit of time referred to as the smallest possible division of time. The earliest known occurrence in English is in Byrhtferth\'s Enchiridion (a science text) of 1010–1012, where it was defined as 1/564 of a momentum (11⁄2 minutes), and thus equal to 15/94 of a second. It was used in the computus, the process of calculating the date of Easter. The most precise timekeeping device of the ancient world was the water clock, or clepsydra, one of which was found in the tomb of Egyptian pharaoh Amenhotep I. They could be used to measure the hours even at night but required manual upkeep to replenish the flow of water. The ancient Greeks and the people from Chaldea (southeastern Mesopotamia) regularly maintained timekeeping records as an essential part of their astronomical observations. Arab inventors and engineers, in particular, made improvements on the use of water clocks up to the Middle Ages. In the 11th century, Chinese inventors and engineers invented the first mechanical clocks driven by an escapement mechanism.\nIncense sticks and candles were, and are, commonly used to measure time in temples and churches across the globe. Water clocks, and, later, mechanical clocks, were used to mark the events of the abbeys and monasteries of the Middle Ages. The passage of the hours at sea can also be marked by bell. The hours were marked by bells in abbeys as well as at sea. Richard of Wallingford (1292–1336), abbot of St. Alban\'s abbey, famously built a mechanical clock as an astronomical orrery about 1330. The hourglass uses the flow of sand to measure the flow of time. They were also used in navigation. Ferdinand Magellan used 18 glasses on each ship for his circumnavigation of the globe (1522). The English word clock probably comes from the Middle Dutch word klocke which, in turn, derives from the medieval Latin word clocca, which ultimately derives from Celtic and is cognate with French, Latin, and German words that mean bell.']

Question: What is the relevant type of coherence for the Young's double-slit interferometer?

Choices:
Choice A) Visibility
Choice B) Coherence time
Choice C) Spatial coherence
Choice D) Coherence length
Choice E) Diameter of the coherence area (Ac)

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ["== Importance ==\nFrom the perspective of a planetary geologist, the atmosphere acts to shape a planetary surface. Wind picks up dust and other particles which, when they collide with the terrain, erode the relief and leave deposits (eolian processes). Frost and precipitations, which depend on the atmospheric composition, also influence the relief. Climate changes can influence a planet's geological history. Conversely, studying the surface of the Earth leads to an understanding of the atmosphere and climate of other planets.\nFor a meteorologist, the composition of the Earth's atmosphere is a factor affecting the climate and its variations.\nFor a biologist or paleontologist, the Earth's atmospheric composition is closely dependent on the appearance of life and its evolution.\n== See also ==\nAtmometer (evaporimeter)\nAtmospheric pressure\nInternational Standard Atmosphere\nKármán line\nSky\n== References ==\n== Further reading ==\nSanchez-Lavega, Agustin (2010). An Introduction to Planetary Atmospheres. Taylor & Francis. ISBN 978-1420067323.\n== External links ==\nProperties of atmospheric strata – The flight environment of the atmosphere\nAtmosphere – Everything you need to know", 'Atmosphere', 'In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.  The main forms of precipitation include drizzle, rain, Rain and snow mixed ("sleet" in Commonwealth usage), snow, ice pellets, graupel and hail. Precipitation occurs when a portion of the atmosphere becomes saturated with water vapor (reaching 100% relative humidity), so that the water condenses and "precipitates" or falls. Thus, fog and mist are not precipitation; their water vapor does not condense sufficiently to precipitate, so fog and mist do not fall. (Such a non-precipitating combination is a colloid.) Two processes, possibly acting together, can lead to air becoming saturated with water vapor: cooling the air or adding water vapor to the air. Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called showers.\nMoisture that is lifted or otherwise forced to rise over a layer of sub-freezing air at the surface may be condensed by the low temperature into clouds and rain. This process is typically active when freezing rain occurs. A stationary front is often present near the area of freezing rain and serves as the focus for forcing moist air to rise. Provided there is necessary and sufficient atmospheric moisture content, the moisture within the rising air will condense into clouds, namely nimbostratus and cumulonimbus if significant precipitation is involved. Eventually, the cloud droplets will grow large enough to form raindrops and descend toward the Earth where they will freeze on contact with exposed objects. Where relatively warm water bodies are present, for example due to water evaporation from lakes, lake-effect snowfall becomes a concern downwind of the warm lakes within the cold cyclonic flow around the backside of extratropical cyclones. Lake-effect snowfall can be locally heavy.  Thundersnow is possible within a cyclone\'s comma head and within lake effect precipitation bands. In mountainous areas, heavy precipitation is possible where upslope flow is maximized within windward sides of the terrain at elevation. On the leeward side of mountains, desert climates can exist due to the dry air caused by compressional heating. Most precipitation occurs within the tropics and is caused by convection.\nPrecipitation is a major component of the water cycle, and is responsible for depositing most of the fresh water on the planet. Approximately 505,000 cubic kilometres (121,000 cu mi) of water falls as precipitation each year: 398,000 cubic kilometres (95,000 cu mi) over oceans and 107,000 cubic kilometres (26,000 cu mi) over land. Given the Earth\'s surface area, that means the globally averaged annual precipitation is 990 millimetres (39 in), but over land it is only 715 millimetres (28.1 in). Climate classification systems such as the Köppen climate classification system use average annual rainfall to help differentiate between differing climate regimes. Global warming is already causing changes to weather, increasing precipitation in some geographies, and reducing it in others, resulting in additional extreme weather.\nPrecipitation may occur on other celestial bodies. Saturn\'s largest satellite, Titan, hosts methane precipitation as a slow-falling drizzle, which has been observed as rain puddles at its equator and polar regions.\n== Types ==\nMechanisms of producing precipitation include convective, stratiform, and orographic rainfall.  Convective processes involve strong vertical motions that can cause the overturning of the atmosphere in that location within an hour and cause heavy precipitation, while stratiform processes involve weaker upward motions and less intense precipitation.  Precipitation can be divided into three categories, based on whether it falls as liquid water, liquid water that freezes on contact with the surface, or ice. Mixtures of different types of precipitation, including types in different categories, can fall simultaneously. Liquid forms of precipitation include rain and drizzle. Rain or drizzle that freezes on contact within a subfreezing air mass is called "freezing rain" or "freezing drizzle". Frozen forms of precipitation include snow, ice needles, ice pellets, hail, and graupel.\n=== Measurement ===\nLiquid precipitation\nRainfall (including drizzle and rain) is usually measured using a rain gauge and expressed in units of millimeters (mm) of height or depth. Equivalently, it can be expressed as a physical quantity with dimension of volume of water per collection area, in units of liters per square meter (L/m2); as 1L = 1dm3 = 1mm·m2, the units of area (m2) cancel out, resulting in simply "mm". This also corresponds to an area density expressed in kg/m2, if assuming that 1 liter of water has a mass of 1 kg (water density), which is acceptable for most practical purposes. The corresponding English unit used is usually inches. In Australia before metrication, rainfall was also measured in "points", each of which was defined as one-hundredth of an inch.\nSolid precipitation\nA snow gauge is usually used to measure the amount of solid precipitation. Snowfall is usually measured in centimeters by letting snow fall into a container and then measure the height. The snow can then optionally be melted to obtain a water equivalent measurement in millimeters like for liquid precipitation. The relationship between snow height and water equivalent depends on the water content of the snow; the water equivalent can thus only provide a rough estimate of snow depth. Other forms of solid precipitation, such as snow pellets and hail or even rain and snow mixed ("sleet" in Commonwealth usage), can also be melted and measured as their respective water equivalents, usually expressed in millimeters as for liquid precipitation.\n== Air becomes saturated ==\n=== Cooling air to its dew point ===\nThe dew point is the temperature to which a parcel of air must be cooled in order to become saturated, and (unless super-saturation occurs) condenses to water.  Water vapor normally begins to condense on condensation nuclei such as dust, ice, and salt in order to form clouds. The cloud condensation nuclei concentration will determine the cloud microphysics. An elevated portion of a frontal zone forces broad areas of lift, which form cloud decks such as altostratus or cirrostratus.  Stratus is a stable cloud deck which tends to form when a cool, stable air mass is trapped underneath a warm air mass. It can also form due to the lifting of advection fog during breezy conditions.\nThere are four main mechanisms for cooling the air to its dew point: adiabatic cooling, conductive cooling, radiational cooling, and evaporative cooling. Adiabatic cooling occurs when air rises and expands. The air can rise due to convection, large-scale atmospheric motions, or a physical barrier such as a mountain (orographic lift). Conductive cooling occurs when the air comes into contact with a colder surface, usually by being blown from one surface to another, for example from a liquid water surface to colder land. Radiational cooling occurs due to the emission of infrared radiation, either by the air or by the surface underneath.  Evaporative cooling occurs when moisture is added to the air through evaporation, which forces the air temperature to cool to its wet-bulb temperature, or until it reaches saturation.\n=== Adding moisture to the air ===\nThe main ways water vapor is added to the air are: wind convergence into areas of upward motion, precipitation or virga falling from above, daytime heating evaporating water from the surface of oceans, water bodies or wet land, transpiration from plants, cool or dry air moving over warmer water, and lifting air over mountains.\n== Forms of precipitation ==\n=== Raindrops ===\nCoalescence occurs when water droplets fuse to create larger water droplets, or when water droplets freeze onto an ice crystal, which is known as the Bergeron process. The fall rate of very small droplets is negligible, hence clouds do not fall out of the sky; precipitation will only occur when these coalesce into larger drops. droplets with different size will have different terminal velocity that cause droplets collision and producing larger droplets, Turbulence will enhance the collision process. As these larger water droplets descend, coalescence continues, so that drops become heavy enough to overcome air resistance and fall as rain.\nRaindrops have sizes ranging from 5.1 to 20 millimetres (0.20 to 0.79 in) mean diameter, above which they tend to break up. Smaller drops are called cloud droplets, and their shape is spherical. As a raindrop increases in size, its shape becomes more oblate, with its largest cross-section facing the oncoming airflow. Contrary to the cartoon pictures of raindrops, their shape does not resemble a teardrop. Intensity and duration of rainfall are usually inversely related, i.e., high intensity storms are likely to be of short duration and low intensity storms can have a long duration.  Rain drops associated with melting hail tend to be larger than other rain drops.  The METAR code for rain is RA, while the coding for rain showers is SHRA.\n=== Ice pellets ===\nIce pellets ("sleet" in US usage) are a form of precipitation consisting of small, translucent balls of ice. Ice pellets are usually (but not always) smaller than hailstones.  They often bounce when they hit the ground, and generally do not freeze into a solid mass unless mixed with freezing rain. The METAR code for ice pellets is PL.', '== See also ==\n== Notes ==\n== References ==\n== Further reading ==\nTechnical (books & book-length reviews):\nBanik, Indranil; Zhao, Hongsheng (2022-06-27). "From Galactic Bars to the Hubble Tension: Weighing Up the Astrophysical Evidence for Milgromian Gravity". Symmetry. 14 (7): 1331. arXiv:2110.06936. Bibcode:2022Symm...14.1331B. doi:10.3390/sym14071331. ISSN 2073-8994.\nMerritt, David (2020). A Philosophical Approach to MOND: Assessing the Milgromian Research Program in Cosmology (Cambridge: Cambridge University Press), 282 pp. ISBN 9781108492690\nFamaey, Benoît; McGaugh, Stacy S. (2012). "Modified Newtonian Dynamics (MOND): Observational Phenomenology and Relativistic Extensions". Living Reviews in Relativity. 15 (1): 10. arXiv:1112.3960. Bibcode:2012LRR....15...10F. doi:10.12942/lrr-2012-10. PMC 5255531. PMID 28163623.\nTechnical (review articles):\nMcGaugh, Stacy S. (2015). "A tale of two paradigms: The mutual incommensurability of ΛCDM and MOND". Canadian Journal of Physics. 93 (2): 250–259. arXiv:1404.7525. Bibcode:2015CaJPh..93..250M. doi:10.1139/cjp-2014-0203. S2CID 51822163.\nMilgrom, Mordehai (2015). "MOND theory". Canadian Journal of Physics. 93 (2): 107–118. arXiv:1404.7661. Bibcode:2015CaJPh..93..107M. doi:10.1139/cjp-2014-0211. S2CID 119183394.\nKroupa, Pavel (2015). "Galaxies as simple dynamical systems: Observational data disfavor dark matter and stochastic star formation". Canadian Journal of Physics. 93 (2): 169–202. arXiv:1406.4860. Bibcode:2015CaJPh..93..169K. doi:10.1139/cjp-2014-0179. S2CID 118479184.\nMilgrom, Mordehai (2014). "The MOND paradigm of modified dynamics". Scholarpedia. 9 (6): 31410. Bibcode:2014SchpJ...931410M. doi:10.4249/scholarpedia.31410.\nScarpa, Riccardo (2006). "Modified Newtonian Dynamics, an Introductory Review". AIP Conference Proceedings. Vol. 822. AIP. pp. 253–265. arXiv:astro-ph/0601478. doi:10.1063/1.2189141.\nPopular:\nA non-Standard model, David Merritt, Aeon Magazine, July 2021\nDark matter critics focus on details, ignore big picture, Lee, 14 Nov 2012\nMilgrom, Mordehai (2009). "MOND: Time for a change of mind?". arXiv:0908.3842 [astro-ph.CO].\n"Dark matter" doubters not silenced yet Archived 2016-05-20 at the Wayback Machine, World Science, 2 Aug 2007\nDoes Dark Matter Really Exist?, Milgrom, Scientific American, Aug 2002\n== External links ==\nMedia related to Modified Newtonian Dynamic at Wikimedia Commons\nMordehai Milgrom\'s website\nLarge collection of lectures and talks on Youtube', '== See also ==\n== References ==\n== Further reading ==\nAnderson, P.W., Basic Notions of Condensed Matter Physics, Perseus Publishing (1997).\nFaghri, A., and Zhang, Y., Fundamentals of Multiphase Heat Transfer and Flow, Springer Nature Switzerland AG, 2020.\nFisher, M.E. (1974). "The renormalization group in the theory of critical behavior". Rev. Mod. Phys. 46 (4): 597–616. Bibcode:1974RvMP...46..597F. doi:10.1103/revmodphys.46.597.\nGoldenfeld, N., Lectures on Phase Transitions and the Renormalization Group, Perseus Publishing (1992).\nIvancevic, Vladimir G; Ivancevic, Tijana T (2008), Chaos, Phase Transitions, Topology Change and Path Integrals, Berlin: Springer, ISBN 978-3-540-79356-4, retrieved 14 March 2013\nM.R. Khoshbin-e-Khoshnazar, Ice Phase Transition as a sample of finite system phase transition, (Physics Education (India) Volume 32. No. 2, Apr - Jun 2016)\nKleinert, H., Gauge Fields in Condensed Matter, Vol. I, "Superfluidity and Vortex lines; Disorder Fields, Phase Transitions", pp. 1–742, World Scientific (Singapore, 1989); Paperback ISBN 9971-5-0210-0 (physik.fu-berlin.de readable online)\nKleinert, Hagen; Verena Schulte-Frohlinde (2001). Critical Properties of φ4-Theories. World Scientific. ISBN 981-02-4659-5. Archived from the original on 26 February 2008. (readable online).\nKogut, J.; Wilson, K (1974). "The Renormalization Group and the epsilon-Expansion". Phys. Rep. 12 (2): 75–199. Bibcode:1974PhR....12...75W. doi:10.1016/0370-1573(74)90023-4.\nKrieger, Martin H., Constitutions of matter : mathematically modelling the most everyday of physical phenomena, University of Chicago Press, 1996. Contains a detailed pedagogical discussion of Onsager\'s solution of the 2-D Ising Model.\nLandau, L.D. and Lifshitz, E.M., Statistical Physics Part 1, vol. 5 of Course of Theoretical Physics, Pergamon Press, 3rd Ed. (1994).\nMussardo G., "Statistical Field Theory. An Introduction to Exactly Solved Models of Statistical Physics", Oxford University Press, 2010.\nSchroeder, Manfred R., Fractals, chaos, power laws : minutes from an infinite paradise, New York: W. H. Freeman, 1991.  Very well-written book in "semi-popular" style—not a textbook—aimed at an audience with some training in mathematics and the physical sciences.  Explains what scaling in phase transitions is all about, among other things.\nH. E. Stanley, Introduction to Phase Transitions and Critical Phenomena (Oxford University Press, Oxford and New York 1971).\nYeomans J. M., Statistical Mechanics of Phase Transitions, Oxford University Press, 1992.\n== External links ==\nMedia related to Phase changes at Wikimedia Commons\nInteractive Phase Transitions on lattices with Java applets\nUniversality classes from Sklogwiki', 'An atmosphere (from Ancient Greek  ἀτμός (atmós) \'vapour, steam\' and  σφαῖρα (sphaîra) \'sphere\') is a layer of gases that envelop an astronomical object, held in place by the gravity of the object. A planet retains an atmosphere when the gravity is great and the temperature of the atmosphere is low. A stellar atmosphere is the outer region of a star, which includes the layers above the opaque photosphere; stars of low temperature might have outer atmospheres containing compound molecules.\nThe atmosphere of Earth is composed of nitrogen (78%), oxygen (21%), argon (0.9%), carbon dioxide (0.04%) and trace gases. Most organisms use oxygen for respiration; lightning and bacteria perform nitrogen fixation which produces ammonia that is used to make nucleotides and amino acids; plants, algae, and cyanobacteria use carbon dioxide for photosynthesis. The layered composition of the atmosphere minimises the harmful effects of sunlight, ultraviolet radiation, solar wind, and cosmic rays and thus protects the organisms from genetic damage. The current composition of the atmosphere of the Earth is the product of billions of years of biochemical modification of the paleoatmosphere by living organisms.\n== Occurrence and compositions ==\n=== Origins ===\nAtmospheres are clouds of gas bound to and engulfing an astronomical focal point of sufficiently dominating mass, adding to its mass, possibly escaping from it or collapsing into it.\nBecause of the latter, such planetary nucleus can develop from interstellar molecular clouds or protoplanetary disks into rocky astronomical objects with varyingly thick atmospheres, gas giants or fusors.\nComposition and thickness is originally determined by the stellar nebula\'s chemistry and temperature, but can also by a product processes within the astronomical body outgasing a different atmosphere.\n=== Compositions ===\nThe atmospheres of the planets Venus and Mars are principally composed of carbon dioxide and nitrogen, argon and oxygen.\nThe composition of Earth\'s atmosphere is determined by the by-products of the life that it sustains. Dry air (mixture of gases) from Earth\'s atmosphere contains 78.08% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and traces of hydrogen, helium, and other "noble" gases (by volume), but generally a variable amount of water vapor is also present, on average about 1% at sea level.\nThe low temperatures and higher gravity of the Solar System\'s giant planets—Jupiter, Saturn, Uranus and Neptune—allow them more readily to retain gases with low molecular masses. These planets have hydrogen–helium atmospheres, with trace amounts of more complex compounds.\nTwo satellites of the outer planets possess significant atmospheres. Titan, a moon of Saturn, and Triton, a moon of Neptune, have atmospheres mainly of nitrogen. When in the part of its orbit closest to the Sun, Pluto has an atmosphere of nitrogen and methane similar to Triton\'s, but these gases are frozen when it is farther from the Sun.\nOther bodies within the Solar System have extremely thin atmospheres not in equilibrium. These include the Moon (sodium gas), Mercury (sodium gas), Europa (oxygen), Io (sulfur), and Enceladus (water vapor).\nThe first exoplanet whose atmospheric composition was determined is HD 209458b, a gas giant with a close orbit around a star in the constellation Pegasus. Its atmosphere is heated to temperatures over 1,000 K, and is steadily escaping into space. Hydrogen, oxygen, carbon and sulfur have been detected in the planet\'s inflated atmosphere.\n=== Atmospheres in the Solar System ===\nAtmosphere of the Sun\nAtmosphere of Mercury\nAtmosphere of Venus\nAtmosphere of Earth\nAtmosphere of the Moon\nAtmosphere of Mars\nAtmosphere of Ceres\nAtmosphere of Jupiter\nAtmosphere of Io\nAtmosphere of Callisto\nAtmosphere of Europa\nAtmosphere of Ganymede\nAtmosphere of Saturn\nAtmosphere of Titan\nAtmosphere of Enceladus\nAtmosphere of Uranus\nAtmosphere of Titania\nAtmosphere of Neptune\nAtmosphere of Triton\nAtmosphere of Pluto\n== Structure of atmosphere ==\n=== Earth ===\nThe atmosphere of Earth is composed of layers with different properties, such as specific gaseous composition, temperature, and pressure.\nThe troposphere is the lowest layer of the atmosphere. This extends from the planetary surface to the bottom of the stratosphere. The troposphere contains 75–80% of the mass of the atmosphere, and is the atmospheric layer wherein the weather occurs; the height of the troposphere varies between 17 km at the equator and 7.0 km at the poles.\nThe stratosphere extends from the top of the troposphere to the bottom of the mesosphere, and contains the ozone layer, at an altitude between 15 km and 35 km. It is the atmospheric layer that absorbs most of the ultraviolet radiation that Earth receives from the Sun.\nThe mesosphere ranges from 50 km to 85 km and is the layer wherein most meteors are incinerated before reaching the surface.\nThe thermosphere extends from an altitude of 85 km to the base of the exosphere at 690 km and contains the ionosphere, where solar radiation ionizes the atmosphere. The density of the ionosphere is greater at short distances from the planetary surface in the daytime and decreases as the ionosphere rises at night-time, thereby allowing a greater range of radio frequencies to travel greater distances.\nThe exosphere begins at 690 to 1,000 km from the surface, and extends to roughly 10,000 km, where it interacts with the magnetosphere of Earth.\n== Pressure ==\nAtmospheric pressure is the force (per unit-area) perpendicular to a unit-area of planetary surface, as determined by the weight of the vertical column of atmospheric gases. In said atmospheric model, the atmospheric pressure, the weight of the mass of the gas, decreases at high altitude because of the diminishing mass of the gas above the point of barometric measurement. The units of air pressure are based upon the standard atmosphere (atm), which is 101,325 Pa (equivalent to 760 Torr or 14.696 psi). The height at which the atmospheric pressure declines by a factor of e (an irrational number equal to 2.71828) is called the scale height (H). For an atmosphere of uniform temperature, the scale height is proportional to the atmospheric temperature and is inversely proportional to the product of the mean molecular mass of dry air, and the local acceleration of gravity at the point of barometric measurement.\n== Escape ==\nSurface gravity differs significantly among the planets. For example, the large gravitational force of the giant planet Jupiter retains light gases such as hydrogen and helium that escape from objects with lower gravity. Secondly, the distance from the Sun determines the energy available to heat atmospheric gas to the point where some fraction of its molecules\' thermal motion exceed the planet\'s escape velocity, allowing those to escape a planet\'s gravitational grasp. Thus, distant and cold Titan, Triton, and Pluto are able to retain their atmospheres despite their relatively low gravities.\nSince a collection of gas molecules may be moving at a wide range of velocities, there will always be some fast enough to produce a slow leakage of gas into space. Lighter molecules move faster than heavier ones with the same thermal kinetic energy, and so gases of low molecular weight are lost more rapidly than those of high molecular weight. It is thought that Venus and Mars may have lost much of their water when, after being photodissociated into hydrogen and oxygen by solar ultraviolet radiation, the hydrogen escaped. Earth\'s magnetic field helps to prevent this, as, normally, the solar wind would greatly enhance the escape of hydrogen. However, over the past 3 billion years Earth may have lost gases through the magnetic polar regions due to auroral activity, including a net 2% of its atmospheric oxygen. The net effect, taking the most important escape processes into account, is that an intrinsic magnetic field does not protect a planet from atmospheric escape and that for some magnetizations the presence of a magnetic field works to increase the escape rate.\nOther mechanisms that can cause atmosphere depletion are solar wind-induced sputtering, impact erosion, weathering, and sequestration—sometimes referred to as "freezing out"—into the regolith and polar caps.\n== Terrain ==\nAtmospheres have dramatic effects on the surfaces of rocky bodies. Objects that have no atmosphere, or that have only an exosphere, have terrain that is covered in craters. Without an atmosphere, the planet has no protection from meteoroids, and all of them collide with the surface as meteorites and create craters.\nFor planets with a significant atmosphere, most meteoroids burn up as meteors before hitting a planet\'s surface. When meteoroids do impact, the effects are often erased by the action of wind.\nWind erosion is a significant factor in shaping the terrain of rocky planets with atmospheres, and over time can erase the effects of both craters and volcanoes. In addition, since liquids cannot exist without pressure, an atmosphere allows liquid to be present at the surface, resulting in lakes, rivers and oceans. Earth and Titan are known to have liquids at their surface and terrain on the planet suggests that Mars had liquid on its surface in the past.\n=== Outside the Solar System ===\nAtmosphere of HD 209458 b\n== Circulation ==\nThe circulation of the atmosphere occurs due to thermal differences when convection becomes a more efficient transporter of heat than thermal radiation. On planets where the primary heat source is solar radiation, excess heat in the tropics is transported to higher latitudes. When a planet generates a significant amount of heat internally, such as is the case for Jupiter, convection in the atmosphere can transport thermal energy from the higher temperature interior up to the surface.\n== Importance ==', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', '=== Arrow of time ===\nUnlike space, where an object can travel in the opposite directions (and in 3 dimensions), time appears to have only one dimension and only one direction—the past lies behind, fixed and immutable, while the future lies ahead and is not necessarily fixed. Yet most laws of physics allow any process to proceed both forward and in reverse. There are only a few physical phenomena that violate the reversibility of time. This time directionality is known as the arrow of time. Acknowledged examples of the arrow of time are:\nRadiative arrow of time, manifested in waves (e.g., light and sound) travelling only expanding (rather than focusing) in time (see light cone);\nEntropic arrow of time: according to the second law of thermodynamics an isolated system evolves toward a larger disorder rather than orders spontaneously;\nQuantum arrow time, which is related to irreversibility of measurement in quantum mechanics according to the Copenhagen interpretation of quantum mechanics;\nWeak arrow of time: preference for a certain time direction of weak force in particle physics (see violation of CP symmetry);\nCosmological arrow of time, which follows the accelerated expansion of the Universe after the Big Bang.\nThe relationships between these different arrows of time is a hotly debated topic in theoretical physics.\nThe second law of thermodynamics states that entropy must increase over time. Brian Greene theorizes that, according to the equations, the change in entropy occurs symmetrically whether going forward or backward in time. So entropy tends to increase in either direction, and our current low-entropy universe is a statistical aberration, in a similar manner as tossing a coin often enough that eventually heads will result ten times in a row. However, this theory is not supported empirically in local experiment.\n=== Classical mechanics ===\nIn non-relativistic classical mechanics, Newton\'s concept of "relative, apparent, and common time" can be used in the formulation of a prescription for the synchronization of clocks. Events seen by two different observers in motion relative to each other produce a mathematical concept of time that works sufficiently well for describing the everyday phenomena of most people\'s experience. In the late nineteenth century, physicists encountered problems with the classical understanding of time, in connection with the behavior of electricity and magnetism. The 1860s Maxwell\'s equations described that light always travels at a constant speed (in a vacuum). However, classical mechanics assumed that motion was measured relative to a fixed reference frame. The Michelson–Morley experiment contradicted the assumption. Einstein later proposed a method of synchronizing clocks using the constant, finite speed of light as the maximum signal velocity. This led directly to the conclusion that observers in motion relative to one another measure different elapsed times for the same event.\n=== Spacetime ===\nTime has historically been closely related with space, the two together merging into spacetime in Einstein\'s special relativity and general relativity. According to these theories, the concept of time depends on the spatial reference frame of the observer, and the human perception, as well as the measurement by instruments such as clocks, are different for observers in relative motion. For example, if a spaceship carrying a clock flies through space at (very nearly) the speed of light, its crew does not notice a change in the speed of time on board their vessel because everything traveling at the same speed slows down at the same rate (including the clock, the crew\'s thought processes, and the functions of their bodies). However, to a stationary observer watching the spaceship fly by, the spaceship appears flattened in the direction it is traveling and the clock on board the spaceship appears to move very slowly.\nOn the other hand, the crew on board the spaceship also perceives the observer as slowed down and flattened along the spaceship\'s direction of travel, because both are moving at very nearly the speed of light relative to each other. Because the outside universe appears flattened to the spaceship, the crew perceives themselves as quickly traveling between regions of space that (to the stationary observer) are many light years apart. This is reconciled by the fact that the crew\'s perception of time is different from the stationary observer\'s; what seems like seconds to the crew might be hundreds of years to the stationary observer. In either case, however, causality remains unchanged: the past is the set of events that can send light signals to an entity and the future is the set of events to which an entity can send light signals.\n=== Dilation ===\nEinstein showed in his thought experiments that people travelling at different speeds, while agreeing on cause and effect, measure different time separations between events, and can even observe different chronological orderings between non-causally related events. Though these effects are typically minute in the human experience, the effect becomes much more pronounced for objects moving at speeds approaching the speed of light. Subatomic particles exist for a well-known average fraction of a second in a lab relatively at rest, but when travelling close to the speed of light they are measured to travel farther and exist for much longer than when at rest.\nAccording to the special theory of relativity, in the high-speed particle\'s frame of reference, it exists, on the average, for a standard amount of time known as its mean lifetime, and the distance it travels in that time is zero, because its velocity is zero. Relative to a frame of reference at rest, time seems to "slow down" for the particle. Relative to the high-speed particle, distances seem to shorten. Einstein showed how both temporal and spatial dimensions can be altered (or "warped") by high-speed motion.\nEinstein (The Meaning of Relativity): "Two events taking place at the points A and B of a system K are simultaneous if they appear at the same instant when observed from the middle point, M, of the interval AB. Time is then defined as the ensemble of the indications of similar clocks, at rest relative to K, which register the same simultaneously." Einstein wrote in his book, Relativity, that simultaneity is also relative, i.e., two events that appear simultaneous to an observer in a particular inertial reference frame need not be judged as simultaneous by a second observer in a different inertial frame of reference.\nAccording to general relativity, time also runs slower in stronger gravitational fields; this is gravitational time dilation. The effect of the dilation becomes more noticeable in a mass-dense object. A famous example of time dilation is a thought experiment of a subject approaching the event horizon of a black hole. As a consequence of how gravitational fields warp spacetime, the subject will experience gravitational time dilation. From the perspective of the subject itself, they will experience time normally. Meanwhile, an observer from the outside will see the subject move closer to the black hole until the extreme, in which the subject appears \'frozen\' in time and eventually fade to nothingness due to the diminishing amount of light returning.\n=== Relativistic versus Newtonian ===\nThe animations visualise the different treatments of time in the Newtonian and the relativistic descriptions. At the heart of these differences are the Galilean and Lorentz transformations applicable in the Newtonian and relativistic theories, respectively. In the figures, the vertical direction indicates time. The horizontal direction indicates distance (only one spatial dimension is taken into account), and the thick dashed curve is the spacetime trajectory ("world line") of the observer. The small dots indicate specific (past and future) events in spacetime. The slope of the world line (deviation from being vertical) gives the relative velocity to the observer.\nIn the Newtonian description these changes are such that time is absolute: the movements of the observer do not influence whether an event occurs in the \'now\' (i.e., whether an event passes the horizontal line through the observer). However, in the relativistic description the observability of events is absolute: the movements of the observer do not influence whether an event passes the "light cone" of the observer. Notice that with the change from a Newtonian to a relativistic description, the concept of absolute time is no longer applicable: events move up and down in the figure depending on the acceleration of the observer.\n=== Quantization ===\nTime quantization refers to the theory that time has the smallest possible unit. Time quantization is a hypothetical concept. In the modern established physical theories like the Standard Model of particle physics and general relativity time is not quantized. Planck time (~ 5.4 × 10−44 seconds) is the unit of time in the system of natural units known as Planck units. Current established physical theories are believed to fail at this time scale, and many physicists expect that the Planck time might be the smallest unit of time that could ever be measured, even in principle. Though tentative physical theories that attempt to describe phenomena at this scale exist; an example is loop quantum gravity. Loop quantum gravity suggests that time is quantized; if gravity is quantized, spacetime is also quantized.\n== Travel ==', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.']

Question: What is the revised view of the atmosphere's nature based on the time-varying multistability that is associated with the modulation of large-scale processes and aggregated feedback of small-scale processes?

Choices:
Choice A) The atmosphere is a system that is only influenced by large-scale processes and does not exhibit any small-scale feedback.
Choice B) The atmosphere possesses both chaos and order, including emerging organized systems and time-varying forcing from recurrent seasons.
Choice C) The atmosphere is a system that is only influenced by small-scale processes and does not exhibit any large-scale modulation.
Choice D) The atmosphere is a completely chaotic system with no order or organization.
Choice E) The atmosphere is a completely ordered system with no chaos or randomness.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Polytope', 'for convex polyhedra to higher-dimensional polytopes:\n{\\displaystyle \\sum \\varphi =(-1)^{d-1}}\n== Generalisations of a polytope ==\n=== Infinite polytopes ===\nNot all manifolds are finite. Where a polytope is understood as a tiling or decomposition of a manifold,  this idea may be extended to infinite manifolds. plane tilings, space-filling (honeycombs) and hyperbolic tilings are in this sense polytopes, and are sometimes called apeirotopes because they have infinitely many cells.\nAmong these, there are regular forms including the regular skew polyhedra and the infinite series of tilings represented by the regular apeirogon, square tiling, cubic honeycomb, and so on.\n=== Abstract polytopes ===\nThe theory of abstract polytopes attempts to detach polytopes from the space containing them, considering their purely combinatorial properties. This allows the definition of the term to be extended to include objects for which it is difficult to define an intuitive underlying space, such as the 11-cell.\nAn abstract polytope is a partially ordered set of elements or members, which obeys certain rules. It is a purely algebraic structure, and the theory was developed in order to avoid some of the issues which make it difficult to reconcile the various geometric classes within a consistent mathematical framework. A geometric polytope is said to be a realization in some real space of the associated abstract polytope.\n=== Complex polytopes ===\nStructures analogous to polytopes exist in complex Hilbert spaces\n{\\displaystyle \\mathbb {C} ^{n}}\nwhere n real dimensions are accompanied by n imaginary ones. Regular complex polytopes are more appropriately treated as configurations.\n== Duality ==\nEvery n-polytope has a dual structure, obtained by interchanging its vertices for facets, edges for ridges, and so on generally interchanging its (j − 1)-dimensional elements for (n − j)-dimensional elements (for j = 1 to n − 1), while retaining the connectivity or incidence between elements.\nFor an abstract polytope, this simply reverses the ordering of the set. This reversal is seen in the Schläfli symbols for regular polytopes, where the symbol for the dual polytope is simply the reverse of the original. For example, {4, 3, 3} is dual to {3, 3, 4}.\nIn the case of a geometric polytope, some geometric rule for dualising is necessary, see for example the rules described for dual polyhedra. Depending on circumstance, the dual figure may or may not be another geometric polytope.\nIf the dual is reversed, then the original polytope is recovered. Thus, polytopes exist in dual pairs.\n=== Self-dual polytopes ===\nIf a polytope has the same number of vertices as facets, of edges as ridges, and so forth, and the same connectivities, then the dual figure will be similar to the original and the polytope is self-dual.\nSome common self-dual polytopes include:\nEvery regular n-simplex, in any number of dimensions, with Schläfli symbol {3n}. These include the equilateral triangle {3}, regular tetrahedron {3,3}, and 5-cell  {3,3,3}.\nEvery hypercubic honeycomb, in any number of dimensions. These include the apeirogon {∞}, square tiling {4,4} and cubic honeycomb {4,3,4}.\nNumerous compact, paracompact and noncompact hyperbolic tilings, such as the icosahedral honeycomb {3,5,3}, and order-5 pentagonal tiling {5,5}.\nIn 2 dimensions, all regular polygons (regular 2-polytopes)\nIn 3 dimensions, the canonical polygonal pyramids and elongated pyramids, and tetrahedrally diminished dodecahedron.\nIn 4 dimensions, the 24-cell, with Schläfli symbol {3,4,3}. Also the great 120-cell {5,5/2,5} and grand stellated 120-cell {5/2,5,5/2}.\n== History ==\nPolygons and polyhedra have been known since ancient times.\nAn early hint of higher dimensions came in 1827 when August Ferdinand Möbius discovered that two mirror-image solids can be superimposed by rotating one of them through a fourth mathematical dimension. By the 1850s, a handful of other mathematicians such as Arthur Cayley and Hermann Grassmann had also considered higher dimensions.\nLudwig Schläfli was the first to consider analogues of polygons and polyhedra in these higher spaces. He described the six convex regular 4-polytopes in 1852 but his work was not published until 1901, six years after his death. By 1854, Bernhard Riemann\'s Habilitationsschrift had firmly established the geometry of higher dimensions, and thus the concept of n-dimensional polytopes was made acceptable. Schläfli\'s polytopes were rediscovered many times in the following decades, even during his lifetime.\nIn 1882 Reinhold Hoppe, writing in German, coined the word polytop to refer to this more general concept of polygons and polyhedra. In due course Alicia Boole Stott, daughter of logician George Boole, introduced the anglicised polytope into the English language.:\u200avi\nIn 1895, Thorold Gosset not only rediscovered Schläfli\'s regular polytopes but also investigated the ideas of semiregular polytopes and space-filling tessellations in higher dimensions. Polytopes also began to be studied in non-Euclidean spaces such as hyperbolic space.\nAn important milestone was reached in 1948 with H. S. M. Coxeter\'s book Regular Polytopes, summarizing work to date and adding new findings of his own.\nMeanwhile, the French mathematician Henri Poincaré had developed the topological idea of a polytope as the piecewise decomposition (e.g. CW-complex) of a manifold. Branko Grünbaum published his influential work on Convex Polytopes in 1967.\nIn 1952 Geoffrey Colin Shephard generalised the idea as complex polytopes in complex space, where each real dimension has an imaginary one associated with it. Coxeter developed the theory further.\nThe conceptual issues raised by complex polytopes, non-convexity, duality and other phenomena led Grünbaum and others to the more general study of abstract combinatorial properties relating vertices, edges, faces and so on. A related idea was that of incidence complexes, which studied the incidence or connection of the various elements with one another. These developments led eventually to the theory of abstract polytopes as partially ordered sets, or posets, of such elements. Peter McMullen and Egon Schulte published their book Abstract Regular Polytopes in 2002.\nEnumerating the uniform polytopes, convex and nonconvex, in four or more dimensions remains an outstanding problem. The convex uniform 4-polytopes were fully enumerated by John Conway and Michael Guy using a computer in 1965; in higher dimensions this problem was still open as of 1997. The full enumeration for nonconvex uniform polytopes is not known in dimensions four and higher as of 2008.\nIn modern times, polytopes and related concepts have found many important applications in fields as diverse as computer graphics, optimization, search engines, cosmology, quantum mechanics and numerous other fields. In 2013 the amplituhedron was discovered as a simplifying construct in certain calculations of theoretical physics.\n== Applications ==\nIn the field of optimization, linear programming studies the maxima and minima of linear functions; these maxima and minima occur on the boundary of an n-dimensional polytope. In linear programming, polytopes occur in the use of generalized barycentric coordinates and slack variables.\nIn  twistor theory, a branch of theoretical physics, a polytope called the amplituhedron is used in to calculate the scattering amplitudes of subatomic particles when they collide. The construct is purely theoretical with no known physical manifestation, but is said to greatly simplify certain calculations.\n== See also ==\n== References ==\n=== Citations ===\n=== Bibliography ===\n== External links ==\nWeisstein, Eric W. "Polytope". MathWorld.\n"Math will rock your world" – application of polytopes to a database of articles used to support custom news feeds via the Internet – (Business Week Online)\nRegular and semi-regular convex polytopes a short historical overview:', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Point group\n\nIn geometry, a point group is a mathematical group of symmetry operations (isometries in a Euclidean space) that have a fixed point in common. The coordinate origin of the Euclidean space is conventionally taken to be a fixed point, and every point group in dimension d is then a subgroup of the orthogonal group O(d). Point groups are used to describe the symmetries of geometric figures and physical objects such as molecules.\nEach point group can be represented as sets of orthogonal matrices M that transform point x into point y according to y = Mx. Each element of a point group is either a rotation (determinant of M = 1), or it is a reflection or improper rotation (determinant of M = −1).\nThe geometric symmetries of crystals are described by space groups, which allow translations and contain point groups as subgroups. Discrete point groups in more than one dimension come in infinite families, but from the crystallographic restriction theorem and one of Bieberbach\'s theorems, each number of dimensions has only a finite number of point groups that are symmetric over some lattice or grid with that number of dimensions. These are the crystallographic point groups.\n== Chiral and achiral point groups, reflection groups ==\nPoint groups can be classified into chiral (or purely rotational) groups and achiral groups.\nThe chiral groups are subgroups of the special orthogonal group SO(d): they contain only orientation-preserving orthogonal transformations, i.e., those of determinant +1. The achiral groups contain also transformations of determinant −1. In an achiral group, the orientation-preserving transformations form a (chiral) subgroup of index 2.\nFinite Coxeter groups or reflection groups are those point groups that are generated purely by a set of reflectional mirrors passing through the same point. A rank n Coxeter group has n mirrors and is represented by a Coxeter–Dynkin diagram. Coxeter notation offers a bracketed notation equivalent to the Coxeter diagram, with markup symbols for rotational and other subsymmetry point groups. Reflection groups are necessarily achiral (except for the trivial group containing only the identity element).\n== List of point groups ==\n=== One dimension ===\nThere are only two one-dimensional point groups, the identity group and the reflection group.\n=== Two dimensions ===\nPoint groups in two dimensions, sometimes called rosette groups.\nThey come in two infinite families:\nCyclic groups Cn of n-fold rotation groups\nDihedral groups Dn of n-fold rotation and reflection groups\nApplying the crystallographic restriction theorem restricts n to values 1, 2, 3, 4, and 6 for both families, yielding 10 groups.\nThe subset of pure reflectional point groups, defined by 1 or 2 mirrors, can also be given by their Coxeter group and related polygons. These include 5 crystallographic groups. The symmetry of the reflectional groups can be doubled by an isomorphism, mapping both mirrors onto each other by a bisecting mirror, doubling the symmetry order.\n=== Three dimensions ===\nPoint groups in three dimensions, sometimes called molecular point groups after their wide use in studying symmetries of molecules.\nThey come in 7 infinite families of axial groups (also called prismatic), and 7 additional polyhedral groups (also called Platonic). In Schoenflies notation,\nAxial groups: Cn,  S2n, Cnh, Cnv, Dn, Dnd, Dnh\nPolyhedral groups: T, Td, Th, O, Oh, I, Ih\nApplying the crystallographic restriction theorem to these groups yields the 32 crystallographic point groups.\n==== Reflection groups ====\nThe reflection point groups, defined by 1 to 3 mirror planes, can also be given by their Coxeter group and related polyhedra. The [3,3] group can be doubled, written as [[3,3]], mapping the first and last mirrors onto each other, doubling the symmetry to 48, and isomorphic to the [4,3] group.\n=== Four dimensions ===\nThe four-dimensional point groups (chiral as well as achiral) are listed in Conway and Smith, Section 4, Tables 4.1–4.3.\nThe following list gives the four-dimensional reflection groups (excluding those that leave a subspace fixed and that are therefore lower-dimensional reflection groups). Each group is specified as a Coxeter group, and like the polyhedral groups of 3D, it can be named by its related convex regular 4-polytope. Related pure rotational groups exist for each with half the order, and can be represented by the bracket Coxeter notation with a \'+\' exponent, for example [3,3,3]+ has three 3-fold gyration points and symmetry order 60. Front-back symmetric groups like [3,3,3] and [3,4,3] can be doubled, shown as double brackets in Coxeter\'s notation, for example [[3,3,3]] with its order doubled to 240.\n=== Five dimensions ===\nThe following table gives the five-dimensional reflection groups (excluding those that are lower-dimensional reflection groups), by listing them as Coxeter groups. Related chiral groups exist for each with half the order, and can be represented by the bracket Coxeter notation with a \'+\' exponent, for example [3,3,3,3]+ has four 3-fold gyration points and symmetry order 360.\n=== Six dimensions ===\nThe following table gives the six-dimensional reflection groups (excluding those that are lower-dimensional reflection groups), by listing them as Coxeter groups. Related pure rotational groups exist for each with half the order, and can be represented by the bracket Coxeter notation with a \'+\' exponent, for example [3,3,3,3,3]+ has five 3-fold gyration points and symmetry order 2520.\n=== Seven dimensions ===\nThe following table gives the seven-dimensional reflection groups (excluding those that are lower-dimensional reflection groups), by listing them as Coxeter groups. Related chiral groups exist for each with half the order, defined by an even number of reflections, and can be represented by the bracket Coxeter notation with a \'+\' exponent, for example [3,3,3,3,3,3]+ has six 3-fold gyration points and symmetry order 20160.\n=== Eight dimensions ===\nThe following table gives the eight-dimensional reflection groups (excluding those that are lower-dimensional reflection groups), by listing them as Coxeter groups. Related chiral groups exist for each with half the order, defined by an even number of reflections, and can be represented by the bracket Coxeter notation with a \'+\' exponent, for example [3,3,3,3,3,3,3]+ has seven 3-fold gyration points and symmetry order 181440.\n== See also ==\nPoint groups in two dimensions\nPoint groups in three dimensions\nPoint groups in four dimensions\nCrystallography\nCrystallographic point group\nMolecular symmetry\nSpace group\nX-ray diffraction\nBravais lattice\nInfrared spectroscopy of metal carbonyls\n== References ==\n== Further reading ==\nH. S. M. Coxeter (1995), F. Arthur Sherk; Peter McMullen; Anthony C. Thompson; Asia Ivic Weiss (eds.), Kaleidoscopes: Selected Writings of H. S. M. Coxeter, Wiley-Interscience Publication, ISBN 978-0-471-01003-6\n(Paper 23) H. S. M. Coxeter, Regular and Semi-Regular Polytopes II, [Math. Zeit. 188 (1985) 559–591]\nH. S. M. Coxeter; W. O. J. Moser (1980), Generators and Relations for Discrete Groups (4th ed.), New York: Springer-Verlag\nN. W. Johnson (2018), "Chapter 11: Finite symmetry groups", Geometries and Transformations\n== External links ==\nWeb-based point group tutorial (needs Java and Flash)\nSubgroup enumeration (needs Java)\nThe Geometry Center: 2.1 Formulas for Symmetries in Cartesian Coordinates (two dimensions)\nThe Geometry Center: 10.1 Formulas for Symmetries in Cartesian Coordinates (three dimensions)', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', '== See also ==\n== References ==\n== Further reading ==\nAnderson, P.W., Basic Notions of Condensed Matter Physics, Perseus Publishing (1997).\nFaghri, A., and Zhang, Y., Fundamentals of Multiphase Heat Transfer and Flow, Springer Nature Switzerland AG, 2020.\nFisher, M.E. (1974). "The renormalization group in the theory of critical behavior". Rev. Mod. Phys. 46 (4): 597–616. Bibcode:1974RvMP...46..597F. doi:10.1103/revmodphys.46.597.\nGoldenfeld, N., Lectures on Phase Transitions and the Renormalization Group, Perseus Publishing (1992).\nIvancevic, Vladimir G; Ivancevic, Tijana T (2008), Chaos, Phase Transitions, Topology Change and Path Integrals, Berlin: Springer, ISBN 978-3-540-79356-4, retrieved 14 March 2013\nM.R. Khoshbin-e-Khoshnazar, Ice Phase Transition as a sample of finite system phase transition, (Physics Education (India) Volume 32. No. 2, Apr - Jun 2016)\nKleinert, H., Gauge Fields in Condensed Matter, Vol. I, "Superfluidity and Vortex lines; Disorder Fields, Phase Transitions", pp. 1–742, World Scientific (Singapore, 1989); Paperback ISBN 9971-5-0210-0 (physik.fu-berlin.de readable online)\nKleinert, Hagen; Verena Schulte-Frohlinde (2001). Critical Properties of φ4-Theories. World Scientific. ISBN 981-02-4659-5. Archived from the original on 26 February 2008. (readable online).\nKogut, J.; Wilson, K (1974). "The Renormalization Group and the epsilon-Expansion". Phys. Rep. 12 (2): 75–199. Bibcode:1974PhR....12...75W. doi:10.1016/0370-1573(74)90023-4.\nKrieger, Martin H., Constitutions of matter : mathematically modelling the most everyday of physical phenomena, University of Chicago Press, 1996. Contains a detailed pedagogical discussion of Onsager\'s solution of the 2-D Ising Model.\nLandau, L.D. and Lifshitz, E.M., Statistical Physics Part 1, vol. 5 of Course of Theoretical Physics, Pergamon Press, 3rd Ed. (1994).\nMussardo G., "Statistical Field Theory. An Introduction to Exactly Solved Models of Statistical Physics", Oxford University Press, 2010.\nSchroeder, Manfred R., Fractals, chaos, power laws : minutes from an infinite paradise, New York: W. H. Freeman, 1991.  Very well-written book in "semi-popular" style—not a textbook—aimed at an audience with some training in mathematics and the physical sciences.  Explains what scaling in phase transitions is all about, among other things.\nH. E. Stanley, Introduction to Phase Transitions and Critical Phenomena (Oxford University Press, Oxford and New York 1971).\nYeomans J. M., Statistical Mechanics of Phase Transitions, Oxford University Press, 1992.\n== External links ==\nMedia related to Phase changes at Wikimedia Commons\nInteractive Phase Transitions on lattices with Java applets\nUniversality classes from Sklogwiki', 'is known as the Stefan–Boltzmann constant.\n=== Radiative transfer ===\nThe equation of radiative transfer describes the way in which radiation is affected as it travels through a material medium. For the special case in which the material medium is in thermodynamic equilibrium in the neighborhood of a point in the medium, Planck\'s law is of special importance.\nFor simplicity, we can consider the linear steady state, without scattering. The equation of radiative transfer states that for a beam of light going through a small distance ds, energy is conserved: The change in the (spectral) radiance of that beam (Iν) is equal to the amount removed by the material medium plus the amount gained from the material medium. If the radiation field is in equilibrium with the material medium, these two contributions will be equal. The material medium will have a certain emission coefficient and absorption coefficient.\nThe absorption coefficient α is the fractional change in the intensity of the light beam as it travels the distance ds, and has units of length−1. It is composed of two parts, the decrease due to absorption and the increase due to stimulated emission. Stimulated emission is emission by the material body which is caused by and is proportional to the incoming radiation. It is included in the absorption term because, like absorption, it is proportional to the intensity of the incoming radiation. Since the amount of absorption will generally vary linearly as the density ρ of the material, we may define a "mass absorption coefficient" κν = \u2060α/ρ\u2060 which is a property of the material itself. The change in intensity of a light beam due to absorption as it traverses a small distance ds will then be\n{\\displaystyle dI_{\\nu }=-\\kappa _{\\nu }\\rho I_{\\nu }\\,ds}\nThe "mass emission coefficient" jν is equal to the radiance per unit volume of a small volume element divided by its mass (since, as for the mass absorption coefficient, the emission is proportional to the emitting mass) and has units of power⋅solid angle−1⋅frequency−1⋅density−1. Like the mass absorption coefficient, it too is a property of the material itself. The change in a light beam as it traverses a small distance ds will then be\n{\\displaystyle dI_{\\nu }=j_{\\nu }\\rho \\,ds}\nThe equation of radiative transfer will then be the sum of these two contributions:\n{\\displaystyle {\\frac {dI_{\\nu }}{ds}}=j_{\\nu }\\rho -\\kappa _{\\nu }\\rho I_{\\nu }.}\nIf the radiation field is in equilibrium with the material medium, then the radiation will be homogeneous (independent of position) so that dIν = 0 and:\n{\\displaystyle \\kappa _{\\nu }B_{\\nu }=j_{\\nu }}\nwhich is another statement of Kirchhoff\'s law, relating two material properties of the medium, and which yields the radiative transfer equation at a point around which the medium is in thermodynamic equilibrium:\n{\\displaystyle {\\frac {dI_{\\nu }}{ds}}=\\kappa _{\\nu }\\rho (B_{\\nu }-I_{\\nu }).}\n=== Einstein coefficients ===\nThe principle of detailed balance states that, at thermodynamic equilibrium, each elementary process is in equilibrium with its reverse process.\nIn 1916, Albert Einstein applied this principle on an atomic level to the case of an atom radiating and absorbing radiation due to transitions between two particular energy levels, giving a deeper insight into the equation of radiative transfer and Kirchhoff\'s law for this type of radiation. If level 1 is the lower energy level with energy E1, and level 2 is the upper energy level with energy E2, then the frequency ν of the radiation radiated or absorbed will be determined by Bohr\'s frequency condition:\n{\\displaystyle E_{2}-E_{1}=h\\nu .}\nIf n1 and n2 are the number densities of the atom in states 1 and 2 respectively, then the rate of change of these densities in time will be due to three processes:\nSpontaneous emission\n21\n{\\displaystyle \\left({\\frac {dn_{1}}{dt}}\\right)_{\\mathrm {spon} }=A_{21}n_{2}}\nStimulated emission\n21\n{\\displaystyle \\left({\\frac {dn_{1}}{dt}}\\right)_{\\mathrm {stim} }=B_{21}n_{2}u_{\\nu }}\nPhoto-absorption\n12\n{\\displaystyle \\left({\\frac {dn_{2}}{dt}}\\right)_{\\mathrm {abs} }=B_{12}n_{1}u_{\\nu }}\nwhere uν is the spectral energy density of the radiation field. The three parameters A21, B21 and B12, known as the Einstein coefficients, are associated with the photon frequency ν produced by the transition between two energy levels (states). As a result, each line in a spectrum has its own set of associated coefficients. When the atoms and the radiation field are in equilibrium, the radiance will be given by Planck\'s law and, by the principle of detailed balance, the sum of these rates must be zero:\n21\n21\n12\n{\\displaystyle 0=A_{21}n_{2}+B_{21}n_{2}{\\frac {4\\pi }{c}}B_{\\nu }(T)-B_{12}n_{1}{\\frac {4\\pi }{c}}B_{\\nu }(T)}\nSince the atoms are also in equilibrium, the populations of the two levels are related by the Boltzmann factor:\n{\\displaystyle {\\frac {n_{2}}{n_{1}}}={\\frac {g_{2}}{g_{1}}}e^{-h\\nu /k_{\\mathrm {B} }T}}\nwhere g1 and g2 are the multiplicities of the respective energy levels. Combining the above two equations with the requirement that they be valid at any temperature yields two relationships between the Einstein coefficients:\n21\n21\n{\\displaystyle {\\frac {A_{21}}{B_{21}}}={\\frac {8\\pi h\\nu ^{3}}{c^{3}}}}\n21\n12\n{\\displaystyle {\\frac {B_{21}}{B_{12}}}={\\frac {g_{1}}{g_{2}}}}\nso that knowledge of one coefficient will yield the other two.\nFor the case of isotropic absorption and emission, the emission coefficient (jν) and absorption coefficient (κν) defined in the radiative transfer section above, can be expressed in terms of the Einstein coefficients. The relationships between the Einstein coefficients will yield the expression of Kirchhoff\'s law expressed in the Radiative transfer section above, namely that\n{\\displaystyle j_{\\nu }=\\kappa _{\\nu }B_{\\nu }.}\nThese coefficients apply to both atoms and molecules.\n== Properties ==\n=== Peaks ===\nThe distributions Bν, Bω, Bν̃ and Bk peak at a photon energy of\n2.821\n{\\displaystyle E=\\left[3+W\\left(-3e^{-3}\\right)\\right]k_{\\mathrm {B} }T\\approx 2.821\\ k_{\\mathrm {B} }T,}\nwhere W is the Lambert W function and e is Euler\'s number.\nHowever, the distribution Bλ peaks at a different energy\n4.965\n{\\displaystyle E=\\left[5+W\\left(-5e^{-5}\\right)\\right]k_{\\mathrm {B} }T\\approx 4.965\\ k_{\\mathrm {B} }T,}\nThe reason for this is that, as mentioned above, one cannot go from (for example) Bν to Bλ simply by substituting ν by λ. In addition, one must also multiply by\n{\\textstyle \\left|{d\\nu }/{d\\lambda }\\right|=c/{\\lambda ^{2}}}\n, which shifts the peak of the distribution to higher energies. These peaks are the mode energy of a photon, when binned using equal-size bins of frequency or wavelength, respectively. Dividing hc (14387.770 μm·K) by these energy expression gives the wavelength of the peak.\nThe spectral radiance at these peaks is given by:\nmax\n1.896\n10\n19\n{\\displaystyle {\\begin{aligned}B_{\\nu ,{\\text{max}}}(T)&={\\frac {2k_{\\mathrm {B} }^{3}T^{3}x^{3}}{h^{2}c^{2}}}{\\frac {1}{e^{x}-1}}\\\\&\\approx 1.896\\times 10^{-19}{\\frac {\\mathrm {W} }{\\mathrm {m^{2}\\cdot Hz\\cdot sr} }}\\times (T/\\mathrm {K} )^{3}\\\\\\end{aligned}}}\nwith\n{\\displaystyle x=3+W(-3e^{-3}),}\nand\nmax\n4.096\n10\nsr\n{\\displaystyle {\\begin{aligned}B_{\\lambda ,{\\text{max}}}(T)&={\\frac {2k_{\\mathrm {B} }^{5}T^{5}x^{5}}{h^{4}c^{3}}}{\\frac {1}{e^{x}-1}}\\\\&\\approx 4.096\\times 10^{-6}{\\frac {\\text{W}}{{\\text{m}}^{2}\\cdot {\\text{sr}}}}\\times ~(T/{\\text{K}})^{5}\\end{aligned}}}\nwith\n{\\displaystyle x=5+W(-5e^{-5}).}\nMeanwhile, the average energy of a photon from a blackbody is\n30\n2.701\n{\\displaystyle E=\\left[{\\frac {\\pi ^{4}}{30\\ \\zeta (3)}}\\right]k_{\\mathrm {B} }T\\approx 2.701\\ k_{\\mathrm {B} }T,}\nwhere\n{\\displaystyle \\zeta }\nis the Riemann zeta function.\n=== Approximations ===\nIn the limit of low frequencies (i.e. long wavelengths), Planck\'s law becomes the Rayleigh–Jeans law\n{\\displaystyle B_{\\nu }(T)\\approx {\\frac {2\\nu ^{2}}{c^{2}}}k_{\\mathrm {B} }T}\nor\n{\\displaystyle B_{\\lambda }(T)\\approx {\\frac {2c}{\\lambda ^{4}}}k_{\\mathrm {B} }T}\nThe radiance increases as the square of the frequency, illustrating the ultraviolet catastrophe. In the limit of high frequencies (i.e. small wavelengths) Planck\'s law tends to the Wien approximation:\n{\\displaystyle B_{\\nu }(T)\\approx {\\frac {2h\\nu ^{3}}{c^{2}}}e^{-{\\frac {h\\nu }{k_{\\mathrm {B} }T}}}}\nor\n{\\displaystyle B_{\\lambda }(T)\\approx {\\frac {2hc^{2}}{\\lambda ^{5}}}e^{-{\\frac {hc}{\\lambda k_{\\mathrm {B} }T}}}.}\n=== Percentiles ===\nWien\'s displacement law in its stronger form states that the shape of Planck\'s law is independent of temperature. It is therefore possible to list the percentile points of the total radiation as well as the peaks for wavelength and frequency, in a form which gives the wavelength λ when divided by temperature T. The second column of the following table lists the corresponding values of λT, that is, those values of x for which the wavelength λ is \u2060x/T\u2060 micrometers at the radiance percentile point given by the corresponding entry in the first column.\nThat is, 0.01% of the radiation is at a wavelength below \u2060910/T\u2060 μm, 20% below \u20602676/T\u2060 μm, etc. The wavelength and frequency peaks are in bold and occur at 25.0% and 64.6% respectively. The 41.8% point is the wavelength-frequency-neutral peak (i.e. the peak in power per unit change in logarithm of wavelength or frequency). These are the points at which the respective Planck-law functions \u20601/λ5\u2060, ν3 and \u2060ν2/λ2\u2060, respectively, divided by exp(\u2060hν/kBT\u2060) − 1 attain their maxima. The much smaller gap in ratio of wavelengths between 0.1% and 0.01% (1110 is 22% more than 910) than between 99.9% and 99.99% (113374 is 120% more than 51613) reflects the exponential decay of energy at short wavelengths (left end) and polynomial decay at long.', 'In response to the so-called "naturalness crisis" in the Minimal Supersymmetric Standard Model, some researchers have abandoned naturalness and the original motivation to solve the hierarchy problem naturally with supersymmetry, while other researchers have moved on to other supersymmetric models such as split supersymmetry. Still others have moved to string theory as a result of the naturalness crisis. Former enthusiastic supporter Mikhail Shifman went as far as urging the theoretical community to search for new ideas and accept that supersymmetry was a failed theory in particle physics. However, some researchers suggested that this "naturalness" crisis was premature because various calculations were too optimistic about the limits of masses which would allow a supersymmetric extension of the Standard Model as a solution.\n== General supersymmetry ==\nSupersymmetry appears in many related contexts of theoretical physics. It is possible to have multiple supersymmetries and also have supersymmetric extra dimensions.\n=== Extended supersymmetry ===\nIt is possible to have more than one kind of supersymmetry transformation. Theories with more than one supersymmetry transformation are known as extended supersymmetric theories. The more supersymmetry a theory has, the more constrained are the field content and interactions. Typically the number of copies of a supersymmetry is a power of 2 (1, 2, 4, 8...). In four dimensions, a spinor has four degrees of freedom and thus the minimal number of supersymmetry generators is four in four dimensions and having eight copies of supersymmetry means that there are 32 supersymmetry generators.\nThe maximal number of supersymmetry generators possible is 32. Theories with more than 32 supersymmetry generators automatically have massless fields with spin greater than 2. It is not known how to make massless fields with spin greater than two interact, so the maximal number of supersymmetry generators considered is 32. This is due to the Weinberg–Witten theorem. This corresponds to an N = 8 supersymmetry theory. Theories with 32 supersymmetries automatically have a graviton.\nFor four dimensions there are the following theories, with the corresponding multiplets (CPT adds a copy, whenever they are not invariant under such symmetry):\n=== Supersymmetry in alternate numbers of dimensions ===\nIt is possible to have supersymmetry in dimensions other than four. Because the properties of spinors change drastically between different dimensions, each dimension has its characteristic. In d dimensions, the size of spinors is approximately 2d/2 or 2(d − 1)/2. Since the maximum number of supersymmetries is 32, the greatest number of dimensions in which a supersymmetric theory can exist is eleven.\n=== Fractional supersymmetry ===\nFractional supersymmetry is a generalization of the notion of supersymmetry in which the minimal positive amount of spin does not have to be \u20601/2\u2060 but can be an arbitrary \u20601/N\u2060 for integer value of N. Such a generalization is possible in two or fewer spacetime dimensions.\n== See also ==\n== References ==\n== Further reading ==\n=== Theoretical introductions, free and online ===\n=== Monographs ===\n=== On experiments ===\n== External links ==\nSupersymmetry – European Organization for Nuclear Research (CERN)\nThe status of supersymmetry – Symmetry Magazine (Fermilab/SLAC), January 12, 2021\nAs Supersymmetry Fails Tests, Physicists Seek New Ideas – Quanta Magazine, November 20, 2012\nWhat is Supersymmetry? – Fermilab, May 21, 2013\nWhy Supersymmetry? – Fermilab, May 31, 2013\nThe Standard Model and Supersymmetry – World Science Festival, March 4, 2015\nSUSY running out of hiding places – BBC, December 11, 2012', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', '{\\displaystyle L_{z}=m_{\\ell }\\hbar }\nThe values of mℓ range from −ℓ to ℓ, with integer intervals.\nThe s subshell (ℓ = 0) contains only one orbital, and therefore the mℓ of an electron in an s orbital will always be 0. The p subshell (ℓ = 1) contains three orbitals, so the mℓ of an electron in a p orbital will be −1, 0, or 1. The d subshell (ℓ = 2) contains five orbitals, with mℓ values of −2, −1, 0, 1, and 2.\n=== Spin magnetic quantum number ===\nThe spin magnetic quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis:\n{\\displaystyle S_{z}=m_{s}\\hbar }\nIn general, the values of ms range from −s to s, where s is the spin quantum number, associated with the magnitude of particle\'s intrinsic spin angular momentum:\n{\\displaystyle m_{s}=-s,-s+1,-s+2,\\cdots ,s-2,s-1,s}\nAn electron state has spin number s = \u20601/2\u2060, consequently ms will be +\u20601/2\u2060 ("spin up") or −\u20601/2\u2060 "spin down" states. Since electron are fermions they obey the Pauli exclusion principle: each electron state must have different quantum numbers.  Therefore, every orbital will be occupied with at most two electrons, one for each spin state.\n=== The Aufbau principle and Hund\'s Rules ===\nA multi-electron atom can be modeled qualitatively as a hydrogen like atom with higher nuclear charge and correspondingly more electrons. The occupation of the electron states in such an atom can be predicted by the Aufbau principle and Hund\'s empirical rules for the quantum numbers.  The Aufbau principle fills orbitals based on their principal and azimuthal quantum numbers (lowest n + l first, with lowest n breaking ties; Hund\'s rule favors unpaired electrons in the outermost orbital). These rules are empirical but they can be related to electron physics.:\u200a10\u200a:\u200a260\n== Spin-orbit coupled systems ==\nWhen one takes the spin–orbit interaction into consideration, the L and S operators no longer commute with the Hamiltonian, and the eigenstates of the system no longer have well-defined orbital angular momentum and spin. Thus another set of quantum numbers should be used. This set includes\nThe total angular momentum quantum number:\n{\\displaystyle j=|\\ell \\pm s|,}\nwhich gives the total angular momentum through the relation\n{\\displaystyle J^{2}=\\hbar ^{2}j(j+1).}\nThe projection of the total angular momentum along a specified axis:\n{\\displaystyle m_{j}=-j,-j+1,-j+2,\\cdots ,j-2,j-1,j}\nanalogous to the above and satisfies both\n{\\displaystyle m_{j}=m_{\\ell }+m_{s},}\nand\n{\\displaystyle |m_{\\ell }+m_{s}|\\leq j.}\nParityThis is the eigenvalue under reflection: positive (+1) for states which came from even ℓ and negative (−1) for states which came from odd ℓ. The former is also known as even parity and the latter as odd parity, and is given by\n{\\displaystyle P=(-1)^{\\ell }.}\nFor example, consider the following 8 states, defined by their quantum numbers:\nThe quantum states in the system can be described as linear combination of these 8 states. However, in the presence of spin–orbit interaction, if one wants to describe the same system by 8 states that are eigenvectors of the Hamiltonian (i.e. each represents a state that does not mix with others over time), we should consider the following 8 states:\n== Atomic nuclei ==\nIn nuclei, the entire assembly of protons and neutrons (nucleons) has a resultant angular momentum due to the angular momenta of each nucleon, usually denoted I. If the total angular momentum of a neutron is jn = ℓ + s and for a proton is jp = ℓ + s (where s for protons and neutrons happens to be \u20601/2\u2060 again (see note)), then the nuclear angular momentum quantum numbers I are given by:\n{\\displaystyle I=|j_{n}-j_{p}|,|j_{n}-j_{p}|+1,|j_{n}-j_{p}|+2,\\cdots ,(j_{n}+j_{p})-2,(j_{n}+j_{p})-1,(j_{n}+j_{p})}\nNote: The orbital angular momenta of the nuclear (and atomic) states are all integer multiples of ħ while the intrinsic angular momentum of the neutron and  proton are half-integer multiples.  It should be immediately apparent that the combination of the intrinsic spins of the nucleons with their orbital motion will always give half-integer values for the total spin, I, of any odd-A nucleus and integer values for any even-A nucleus.\nParity with the number I is used to label nuclear angular momentum states, examples for some isotopes of hydrogen (H), carbon (C), and sodium (Na) are;\nThe reason for the unusual fluctuations in I, even by differences of just one nucleon, are due to the odd and even numbers of protons and neutrons – pairs of nucleons have a total angular momentum of zero (just like electrons in orbitals), leaving an odd or even number of unpaired nucleons. The property of nuclear spin is an important factor for the operation of NMR spectroscopy in organic chemistry, and MRI in nuclear medicine, due to the nuclear magnetic moment interacting with an external magnetic field.\n== Elementary particles ==\nElementary particles contain many quantum numbers which are usually said to be intrinsic to them. However, it should be understood that the elementary particles are quantum states of the standard model of particle physics, and hence the quantum numbers of these particles bear the same relation to the Hamiltonian of this model as the quantum numbers of the Bohr atom does to its Hamiltonian. In other words, each quantum number denotes a symmetry of the problem. It is more useful in quantum field theory to distinguish between spacetime and internal symmetries.\nTypical quantum numbers related to spacetime symmetries are spin (related to rotational symmetry), the parity, C-parity and T-parity (related to the Poincaré symmetry of spacetime). Typical internal symmetries are lepton number and baryon number or the electric charge. (For a full list of quantum numbers of this kind see the article on flavour.)\n== Multiplicative quantum numbers ==\nMost conserved quantum numbers are additive, so in an elementary particle reaction, the sum of the quantum numbers should be the same before and after the reaction. However, some, usually called a parity, are multiplicative; i.e., their product is conserved. All multiplicative quantum numbers belong to a symmetry (like parity) in which applying the symmetry transformation twice is equivalent to doing nothing (involution).\n== See also ==\nElectron configuration\n== References ==\n== Further reading ==\nDirac, Paul A. M. (1982). Principles of Quantum Mechanics. Oxford University Press. ISBN 0-19-852011-5.\nGriffiths, David J. (2004). Introduction to Quantum Mechanics (2nd ed.). Prentice Hall. ISBN 0-13-805326-X.\nHalzen, Francis & Martin, Alan D. (1984). Quarks and Leptons: An Introductory Course in Modern Particle Physics. John Wiley & Sons. ISBN 0-471-88741-2.\nEisberg, Robert Martin; Resnick, Robert (1985). Quantum Physics of Atoms, Molecules, Solids, Nuclei and Particles (2nd ed.). John Wiley & Sons. ISBN 978-0-471-87373-0 – via Internet Archive.']

Question: What is a regular polytope?

Choices:
Choice A) A regular polytope is a geometric shape whose symmetry group is transitive on its diagonals.
Choice B) A regular polytope is a geometric shape whose symmetry group is transitive on its vertices.
Choice C) A regular polytope is a geometric shape whose symmetry group is transitive on its flags.
Choice D) A regular polytope is a geometric shape whose symmetry group is transitive on its edges.
Choice E) A regular polytope is a geometric shape whose symmetry group is transitive on its faces.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Galaxy', "A supernova explosion blows away the star's outer layers, leaving a remnant such as the Crab Nebula. The core is compressed into a neutron star, which sometimes manifests itself as a pulsar or X-ray burster. In the case of the largest stars, the remnant is a black hole greater than 4 M☉. In a neutron star the matter is in a state known as neutron-degenerate matter, with a more exotic form of degenerate matter, QCD matter, possibly present in the core.\nThe blown-off outer layers of dying stars include heavy elements, which may be recycled during the formation of new stars. These heavy elements allow the formation of rocky planets. The outflow from supernovae and the stellar wind of large stars play an important part in shaping the interstellar medium.\n==== Binary stars ====\nBinary stars' evolution may significantly differ from that of single stars of the same mass. For example, when any star expands to become a red giant, it may overflow its Roche lobe, the surrounding region where material is gravitationally bound to it; if stars in a binary system are close enough, some of that material may overflow to the other star, yielding phenomena including contact binaries, common-envelope binaries, cataclysmic variables, blue stragglers, and type Ia supernovae. Mass transfer leads to cases such as the Algol paradox, where the most-evolved star in a system is the least massive.\nThe evolution of binary star and higher-order star systems is intensely researched since so many stars have been found to be members of binary systems. Around half of Sun-like stars, and an even higher proportion of more massive stars, form in multiple systems, and this may greatly influence such phenomena as novae and supernovae, the formation of certain types of star, and the enrichment of space with nucleosynthesis products.\nThe influence of binary star evolution on the formation of evolved massive stars such as luminous blue variables, Wolf–Rayet stars, and the progenitors of certain classes of core collapse supernova is still disputed. Single massive stars may be unable to expel their outer layers fast enough to form the types and numbers of evolved stars that are observed, or to produce progenitors that would explode as the supernovae that are observed. Mass transfer through gravitational stripping in binary systems is seen by some astronomers as the solution to that problem.\n== Distribution ==\nStars are not spread uniformly across the universe but are normally grouped into galaxies along with interstellar gas and dust. A typical large galaxy like the Milky Way contains hundreds of billions of stars. There are more than 2 trillion (1012) galaxies, though most are less than 10% the mass of the Milky Way. Overall, there are likely to be between 1022 and 1024 stars, which are more stars than all the grains of sand on planet Earth. Most stars are within galaxies, but between 10 and 50% of the starlight in large galaxy clusters may come from stars outside of any galaxy.\nA multi-star system consists of two or more gravitationally bound stars that orbit each other. The simplest and most common multi-star system is a binary star, but systems of three or more stars exist. For reasons of orbital stability, such multi-star systems are often organized into hierarchical sets of binary stars. Larger groups are called star clusters. These range from loose stellar associations with only a few stars to open clusters with dozens to thousands of stars, up to enormous globular clusters with hundreds of thousands of stars. Such systems orbit their host galaxy. The stars in an open or globular cluster all formed from the same giant molecular cloud, so all members normally have similar ages and compositions.\nMany stars are observed, and most or all may have originally formed in gravitationally bound, multiple-star systems. This is particularly true for very massive O and B class stars, 80% of which are believed to be part of multiple-star systems. The proportion of single star systems increases with decreasing star mass, so that only 25% of red dwarfs are known to have stellar companions. As 85% of all stars are red dwarfs, more than two thirds of stars in the Milky Way are likely single red dwarfs. In a 2017 study of the Perseus molecular cloud, astronomers found that most of the newly formed stars are in binary systems. In the model that best explained the data, all stars initially formed as binaries, though some binaries later split up and leave single stars behind.\nThe nearest star to the Earth, apart from the Sun, is Proxima Centauri, 4.2465 light-years (40.175 trillion kilometres) away. Travelling at the orbital speed of the Space Shuttle, 8 kilometres per second (29,000 kilometres per hour), it would take about 150,000 years to arrive. This is typical of stellar separations in galactic discs. Stars can be much closer to each other in the centres of galaxies and in globular clusters, or much farther apart in galactic halos.\nDue to the relatively vast distances between stars outside the galactic nucleus, collisions between stars are thought to be rare. In denser regions such as the core of globular clusters or the galactic center, collisions can be more common. Such collisions can produce what are known as blue stragglers. These abnormal stars have a higher surface temperature and thus are bluer than stars at the main sequence turnoff in the cluster to which they belong; in standard stellar evolution, blue stragglers would already have evolved off the main sequence and thus would not be seen in the cluster.\n== Characteristics ==\nAlmost everything about a star is determined by its initial mass, including such characteristics as luminosity, size, evolution, lifespan, and its eventual fate.\n=== Age ===\nMost stars are between 1 billion and 10 billion years old. Some stars may even be close to 13.8 billion years old—the observed age of the universe. The oldest star yet discovered, HD 140283, nicknamed Methuselah star, is an estimated 14.46 ± 0.8 billion years old. (Due to the uncertainty in the value, this age for the star does not conflict with the age of the universe, determined by the Planck satellite as 13.799 ± 0.021).\nThe more massive the star, the shorter its lifespan, primarily because massive stars have greater pressure on their cores, causing them to burn hydrogen more rapidly. The most massive stars last an average of a few million years, while stars of minimum mass (red dwarfs) burn their fuel very slowly and can last tens to hundreds of billions of years.\n=== Chemical composition ===\nWhen stars form in the present Milky Way galaxy, they are composed of about 71% hydrogen and 27% helium, as measured by mass, with a small fraction of heavier elements. Typically the portion of heavy elements is measured in terms of the iron content of the stellar atmosphere, as iron is a common element and its absorption lines are relatively easy to measure. The portion of heavier elements may be an indicator of the likelihood that the star has a planetary system.\nAs of 2005 the star with the lowest iron content ever measured is the dwarf HE1327-2326, with only 1/200,000th the iron content of the Sun. By contrast, the super-metal-rich star μ Leonis has nearly double the abundance of iron as the Sun, while the planet-bearing star 14 Herculis has nearly triple the iron. Chemically peculiar stars show unusual abundances of certain elements in their spectrum; especially chromium and rare earth elements. Stars with cooler outer atmospheres, including the Sun, can form various diatomic and polyatomic molecules.\n=== Diameter ===\nDue to their great distance from the Earth, all stars except the Sun appear to the unaided eye as shining points in the night sky that twinkle because of the effect of the Earth's atmosphere. The Sun is close enough to the Earth to appear as a disk instead, and to provide daylight. Other than the Sun, the star with the largest apparent size is R Doradus, with an angular diameter of only 0.057 arcseconds.\nThe disks of most stars are much too small in angular size to be observed with current ground-based optical telescopes, so interferometer telescopes are required to produce images of these objects. Another technique for measuring the angular size of stars is through occultation. By precisely measuring the drop in brightness of a star as it is occulted by the Moon (or the rise in brightness when it reappears), the star's angular diameter can be computed.\nStars range in size from neutron stars, which vary anywhere from 20 to 40 km (25 mi) in diameter, to supergiants like Betelgeuse in the Orion constellation, which has a diameter about 640 times that of the Sun with a much lower density.\n=== Kinematics ===\nThe motion of a star relative to the Sun can provide useful information about the origin and age of a star, as well as the structure and evolution of the surrounding galaxy. The components of motion of a star consist of the radial velocity toward or away from the Sun, and the traverse angular movement, which is called its proper motion.\nRadial velocity is measured by the doppler shift of the star's spectral lines and is given in units of km/s. The proper motion of a star, its parallax, is determined by precise astrometric measurements in units of milli-arc seconds (mas) per year. With knowledge of the star's parallax and its distance, the proper motion velocity can be calculated. Together with the radial velocity, the total velocity can be calculated. Stars with high rates of proper motion are likely to be relatively close to the Sun, making them good candidates for parallax measurements.", '=== Spirals ===\nSpiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept.\nSpiral galaxies consist of a rotating disk of stars and interstellar medium, along with a central bulge of generally older stars. Extending outward from the bulge are relatively bright arms. In the Hubble classification scheme, spiral galaxies are listed as type S, followed by a letter (a, b, or c) which indicates the degree of tightness of the spiral arms and the size of the central bulge. An Sa galaxy has tightly wound, poorly defined arms and possesses a relatively large core region. At the other extreme, an Sc galaxy has open, well-defined arms and a small core region. A galaxy with poorly defined arms is sometimes referred to as a flocculent spiral galaxy; in contrast to the grand design spiral galaxy that has prominent and well-defined spiral arms. The speed in which a galaxy rotates is thought to correlate with the flatness of the disc as some spiral galaxies have thick bulges, while others are thin and dense.\nIn spiral galaxies, the spiral arms do have the shape of approximate logarithmic spirals, a pattern that can be theoretically shown to result from a disturbance in a uniformly rotating mass of stars. Like the stars, the spiral arms rotate around the center, but they do so with constant angular velocity. The spiral arms are thought to be areas of high-density matter, or "density waves". As stars move through an arm, the space velocity of each stellar system is modified by the gravitational force of the higher density. (The velocity returns to normal after the stars depart on the other side of the arm.) This effect is akin to a "wave" of slowdowns moving along a highway full of moving cars. The arms are visible because the high density facilitates star formation, and therefore they harbor many bright and young stars.\n==== Barred spiral galaxy ====\nA majority of spiral galaxies, including the Milky Way galaxy, have a linear, bar-shaped band of stars that extends outward to either side of the core, then merges into the spiral arm structure. In the Hubble classification scheme, these are designated by an SB, followed by a lower-case letter (a, b or c) which indicates the form of the spiral arms (in the same manner as the categorization of normal spiral galaxies). Bars are thought to be temporary structures that can occur as a result of a density wave radiating outward from the core, or else due to a tidal interaction with another galaxy. Many barred spiral galaxies are active, possibly as a result of gas being channeled into the core along the arms.\nOur own galaxy, the Milky Way, is a large disk-shaped barred-spiral galaxy about 30 kiloparsecs in diameter and a kiloparsec thick. It contains about two hundred billion (2×1011) stars and has a total mass of about six hundred billion (6×1011) times the mass of the Sun.\n==== Super-luminous spiral ====\nRecently, researchers described galaxies called super-luminous spirals. They are very large with an upward diameter of 437,000 light-years (compared to the Milky Way\'s 87,400 light-year diameter). With a mass of 340 billion solar masses, they generate a significant amount of ultraviolet and mid-infrared light. They are thought to have an increased star formation rate around 30 times faster than the Milky Way.\n=== Other morphologies ===\nPeculiar galaxies are galactic formations that develop unusual properties due to tidal interactions with other galaxies.\nA ring galaxy has a ring-like structure of stars and interstellar medium surrounding a bare core. A ring galaxy is thought to occur when a smaller galaxy passes through the core of a spiral galaxy. Such an event may have affected the Andromeda Galaxy, as it displays a multi-ring-like structure when viewed in infrared radiation.\nA lenticular galaxy is an intermediate form that has properties of both elliptical and spiral galaxies. These are categorized as Hubble type S0, and they possess ill-defined spiral arms with an elliptical halo of stars (barred lenticular galaxies receive Hubble classification SB0).\nIrregular galaxies are galaxies that can not be readily classified into an elliptical or spiral morphology.\nAn Irr-I galaxy has some structure but does not align cleanly with the Hubble classification scheme.\nIrr-II galaxies do not possess any structure that resembles a Hubble classification, and may have been disrupted. Nearby examples of (dwarf) irregular galaxies include the Magellanic Clouds.\nA dark or "ultra diffuse" galaxy is an extremely-low-luminosity galaxy. It may be the same size as the Milky Way, but have a visible star count only one percent of the Milky Way\'s. Multiple mechanisms for producing this type of galaxy have been proposed, and it is possible that different dark galaxies formed by different means. One candidate explanation for the low luminosity is that the galaxy lost its star-forming gas at an early stage, resulting in old stellar populations.\n=== Dwarfs ===\nDespite the prominence of large elliptical and spiral galaxies, most galaxies are dwarf galaxies. They are relatively small when compared with other galactic formations, being about one hundredth the size of the Milky Way, with only a few billion stars. Blue compact dwarf galaxies contains large clusters of young, hot, massive stars. Ultra-compact dwarf galaxies have been discovered that are only 100 parsecs across.\nMany dwarf galaxies may orbit a single larger galaxy; the Milky Way has at least a dozen such satellites, with an estimated 300–500 yet to be discovered.\nMost of the information we have about dwarf galaxies come from observations of the local group, containing two spiral galaxies, the Milky Way and Andromeda, and many dwarf galaxies. These dwarf galaxies are classified as either irregular or dwarf elliptical/dwarf spheroidal galaxies.\nA study of 27 Milky Way neighbors found that in all dwarf galaxies, the central mass is approximately 10 million solar masses, regardless of whether it has thousands or millions of stars. This suggests that galaxies are largely formed by dark matter, and that the minimum size may indicate a form of warm dark matter incapable of gravitational coalescence on a smaller scale.\n== Variants ==\n=== Interacting ===\nInteractions between galaxies are relatively frequent, and they can play an important role in galactic evolution. Near misses between galaxies result in warping distortions due to tidal interactions, and may cause some exchange of gas and dust.\nCollisions occur when two galaxies pass directly through each other and have sufficient relative momentum not to merge. The stars of interacting galaxies usually do not collide, but the gas and dust within the two forms interacts, sometimes triggering star formation. A collision can severely distort the galaxies\' shapes, forming bars, rings or tail-like structures.\nAt the extreme of interactions are galactic mergers, where the galaxies\' relative momentums are insufficient to allow them to pass through each other. Instead, they gradually merge to form a single, larger galaxy. Mergers can result in significant changes to the galaxies\' original morphology. If one of the galaxies is much more massive than the other, the result is known as cannibalism, where the more massive larger galaxy remains relatively undisturbed, and the smaller one is torn apart. The Milky Way galaxy is currently in the process of cannibalizing the Sagittarius Dwarf Elliptical Galaxy and the Canis Major Dwarf Galaxy.\n=== Starburst ===\nStars are created within galaxies from a reserve of cold gas that forms giant molecular clouds. Some galaxies have been observed to form stars at an exceptional rate, which is known as a starburst. If they continue to do so, they would consume their reserve of gas in a time span less than the galaxy\'s lifespan. Hence starburst activity usually lasts only about ten million years, a relatively brief period in a galaxy\'s history. Starburst galaxies were more common during the universe\'s early history, but still contribute an estimated 15% to total star production.\nStarburst galaxies are characterized by dusty concentrations of gas and the appearance of newly formed stars, including massive stars that ionize the surrounding clouds to create H II regions. These stars produce supernova explosions, creating expanding remnants that interact powerfully with the surrounding gas. These outbursts trigger a chain reaction of star-building that spreads throughout the gaseous region. Only when the available gas is nearly consumed or dispersed does the activity end.\nStarbursts are often associated with merging or interacting galaxies. The prototype example of such a starburst-forming interaction is M82, which experienced a close encounter with the larger M81. Irregular galaxies often exhibit spaced knots of starburst activity.\n=== Radio galaxy ===\nA radio galaxy is a galaxy with giant regions of radio emission extending well beyond its visible structure. These energetic radio lobes are powered by jets from its active galactic nucleus. Radio galaxies are classified according to their Fanaroff–Riley classification. The FR I class have lower radio luminosity and exhibit structures which are more elongated; the FR II class are higher radio luminosity. The correlation of radio luminosity and structure suggests that the sources in these two types of galaxies may differ.', "When both rates of movement are known, the space velocity of the star relative to the Sun or the galaxy can be computed. Among nearby stars, it has been found that younger population I stars have generally lower velocities than older, population II stars. The latter have elliptical orbits that are inclined to the plane of the galaxy. A comparison of the kinematics of nearby stars has allowed astronomers to trace their origin to common points in giant molecular clouds; such groups with common points of origin are referred to as stellar associations.\n=== Magnetic field ===\nThe magnetic field of a star is generated within regions of the interior where convective circulation occurs. This movement of conductive plasma functions like a dynamo, wherein the movement of electrical charges induce magnetic fields, as does a mechanical dynamo. Those magnetic fields have a great range that extend throughout and beyond the star. The strength of the magnetic field varies with the mass and composition of the star, and the amount of magnetic surface activity depends upon the star's rate of rotation. This surface activity produces starspots, which are regions of strong magnetic fields and lower than normal surface temperatures. Coronal loops are arching magnetic field flux lines that rise from a star's surface into the star's outer atmosphere, its corona. The coronal loops can be seen due to the plasma they conduct along their length. Stellar flares are bursts of high-energy particles that are emitted due to the same magnetic activity.\nYoung, rapidly rotating stars tend to have high levels of surface activity because of their magnetic field. The magnetic field can act upon a star's stellar wind, functioning as a brake to gradually slow the rate of rotation with time. Thus, older stars such as the Sun have a much slower rate of rotation and a lower level of surface activity. The activity levels of slowly rotating stars tend to vary in a cyclical manner and can shut down altogether for periods of time. During the Maunder Minimum, for example, the Sun underwent a 70-year period with almost no sunspot activity.\n=== Mass ===\nStars have masses ranging from less than half the solar mass to over 200 solar masses (see List of most massive stars). One of the most massive stars known is Eta Carinae, which, with 100–150 times as much mass as the Sun, will have a lifespan of only several million years. Studies of the most massive open clusters suggests 150 M☉ as a rough upper limit for stars in the current era of the universe. This represents an empirical value for the theoretical limit on the mass of forming stars due to increasing radiation pressure on the accreting gas cloud. Several stars in the R136 cluster in the Large Magellanic Cloud have been measured with larger masses, but it has been determined that they could have been created through the collision and merger of massive stars in close binary systems, sidestepping the 150 M☉ limit on massive star formation.\nThe first stars to form after the Big Bang may have been larger, up to 300 M☉, due to the complete absence of elements heavier than lithium in their composition. This generation of supermassive population III stars is likely to have existed in the very early universe (i.e., they are observed to have a high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life. In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at z = 6.60.\nWith a mass only 80 times that of Jupiter (MJ), 2MASS J0523-1403 is the smallest known star undergoing nuclear fusion in its core. For stars with metallicity similar to the Sun, the theoretical minimum mass the star can have and still undergo fusion at the core, is estimated to be about 75 MJ. When the metallicity is very low, the minimum star size seems to be about 8.3% of the solar mass, or about 87 MJ. Smaller bodies called brown dwarfs, occupy a poorly defined grey area between stars and gas giants.\nThe combination of the radius and the mass of a star determines its surface gravity. Giant stars have a much lower surface gravity than do main sequence stars, while the opposite is the case for degenerate, compact stars such as white dwarfs. The surface gravity can influence the appearance of a star's spectrum, with higher gravity causing a broadening of the absorption lines.\n=== Rotation ===\nThe rotation rate of stars can be determined through spectroscopic measurement, or more exactly determined by tracking their starspots. Young stars can have a rotation greater than 100 km/s at the equator. The B-class star Achernar, for example, has an equatorial velocity of about 225 km/s or greater, causing its equator to bulge outward and giving it an equatorial diameter that is more than 50% greater than between the poles. This rate of rotation is just below the critical velocity of 300 km/s at which speed the star would break apart. By contrast, the Sun rotates once every 25–35 days depending on latitude, with an equatorial velocity of 1.93 km/s. A main sequence star's magnetic field and the stellar wind serve to slow its rotation by a significant amount as it evolves on the main sequence.\nDegenerate stars have contracted into a compact mass, resulting in a rapid rate of rotation. However they have relatively low rates of rotation compared to what would be expected by conservation of angular momentum—the tendency of a rotating body to compensate for a contraction in size by increasing its rate of spin. A large portion of the star's angular momentum is dissipated as a result of mass loss through the stellar wind. In spite of this, the rate of rotation for a pulsar can be very rapid. The pulsar at the heart of the Crab nebula, for example, rotates 30 times per second. The rotation rate of the pulsar will gradually slow due to the emission of radiation.\n=== Temperature ===\nThe surface temperature of a main sequence star is determined by the rate of energy production of its core and by its radius, and is often estimated from the star's color index. The temperature is normally given in terms of an effective temperature, which is the temperature of an idealized black body that radiates its energy at the same luminosity per surface area as the star. The effective temperature is only representative of the surface, as the temperature increases toward the core. The temperature in the core region of a star is several million kelvins.\nThe stellar temperature will determine the rate of ionization of various elements, resulting in characteristic absorption lines in the spectrum. The surface temperature of a star, along with its visual absolute magnitude and absorption features, is used to classify a star (see classification below).\nMassive main sequence stars can have surface temperatures of 50,000 K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600 K; but they have a high luminosity due to their large exterior surface area.\n== Radiation ==\nThe energy produced by stars, a product of nuclear fusion, radiates to space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind, which streams from the outer layers as electrically charged protons and alpha and beta particles. A steady stream of almost massless neutrinos emanate directly from the star's core.\nThe production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers.\nThe color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the star's outer layers, including its photosphere. Besides visible light, stars emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics.\nUsing the stellar spectrum, astronomers can determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is found, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.) With these parameters, astronomers can estimate the age of the star.\n=== Luminosity ===\nThe luminosity of a star is the amount of light and other forms of radiant energy it radiates per unit of time. It has units of power. The luminosity of a star is determined by its radius and surface temperature. Many stars do not radiate uniformly across their entire surface. The rapidly rotating star Vega, for example, has a higher energy flux (power per unit area) at its poles than along its equator.", '=== Evolution ===\nOnce stars begin to form, emit radiation, and in some cases explode, the process of galaxy formation becomes very complex, involving interactions between the forces of gravity, radiation, and thermal energy. Many details are still poorly understood.\nWithin a billion years of a galaxy\'s formation, key structures begin to appear. Globular clusters, the central supermassive black hole, and a galactic bulge of metal-poor Population II stars form. The creation of a supermassive black hole appears to play a key role in actively regulating the growth of galaxies by limiting the total amount of additional matter added. During this early epoch, galaxies undergo a major burst of star formation.\nDuring the following two billion years, the accumulated matter settles into a galactic disc. A galaxy will continue to absorb infalling material from high-velocity clouds and dwarf galaxies throughout its life. This matter is mostly hydrogen and helium. The cycle of stellar birth and death slowly increases the abundance of heavy elements, eventually allowing the formation of planets.\nStar formation rates in galaxies depend upon their local environment. Isolated \'void\' galaxies have highest rate per stellar mass, with \'field\' galaxies associated with spiral galaxies having lower rates and galaxies in dense cluster having the lowest rates.\nThe evolution of galaxies can be significantly affected by interactions and collisions. Mergers of galaxies were common during the early epoch, and the majority of galaxies were peculiar in morphology. Given the distances between the stars, the great majority of stellar systems in colliding galaxies will be unaffected. However, gravitational stripping of the interstellar gas and dust that makes up the spiral arms produces a long train of stars known as tidal tails. Examples of these formations can be seen in NGC 4676 or the Antennae Galaxies.\nThe Milky Way galaxy and the nearby Andromeda Galaxy are moving toward each other at about 130 km/s, and—depending upon the lateral movements—the two might collide in about five to six billion years. Although the Milky Way has never collided with a galaxy as large as Andromeda before, it has collided and merged with other galaxies in the past. Cosmological simulations indicate that, 11 billion years ago, it merged with a particularly large galaxy that has been labeled the Kraken.\nSuch large-scale interactions are rare. As time passes, mergers of two systems of equal size become less common. Most bright galaxies have remained fundamentally unchanged for the last few billion years, and the net rate of star formation probably also peaked about ten billion years ago.\n=== Future trends ===\nSpiral galaxies, like the Milky Way, produce new generations of stars as long as they have dense molecular clouds of interstellar hydrogen in their spiral arms. Elliptical galaxies are largely devoid of this gas, and so form few new stars. The supply of star-forming material is finite; once stars have converted the available supply of hydrogen into heavier elements, new star formation will come to an end.\nThe current era of star formation is expected to continue for up to one hundred billion years, and then the "stellar age" will wind down after about ten trillion to one hundred trillion years (1013–1014 years), as the smallest, longest-lived stars in the visible universe, tiny red dwarfs, begin to fade. At the end of the stellar age, galaxies will be composed of compact objects: brown dwarfs, white dwarfs that are cooling or cold ("black dwarfs"), neutron stars, and black holes. Eventually, as a result of gravitational relaxation, all stars will either fall into central supermassive black holes or be flung into intergalactic space as a result of collisions.\n== Gallery ==\n== See also ==\n== Notes ==\n== References ==\n== Bibliography ==\n== External links ==\nNASA/IPAC Extragalactic Database (NED)\nNED Redshift-Independent Distances\nGalaxies on In Our Time at the BBC\nAn Atlas of The Universe\nGalaxies – Information and amateur observations\nGalaxy Zoo – citizen science galaxy classification project\n"A Flight Through the Universe, by the Sloan Digital Sky Survey" – animated video from Berkeley Lab', 'A star is a luminous spheroid of plasma held together by self-gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye at night; their immense distances from Earth make them appear as fixed points of light. The most prominent stars have been categorised into constellations and asterisms, and many of the brightest stars have proper names. Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations. The observable universe contains an estimated 1022 to 1024 stars. Only about 4,000 of these stars are visible to the naked eye—all within the Milky Way galaxy.\nA star\'s life begins with the gravitational collapse of a gaseous nebula of material largely comprising hydrogen, helium, and trace heavier elements. Its total mass mainly determines its evolution and eventual fate. A star shines for most of its active life due to the thermonuclear fusion of hydrogen into helium in its core. This process releases energy that traverses the star\'s interior and radiates into outer space. At the end of a star\'s lifetime, fusion ceases and its core becomes a stellar remnant: a white dwarf, a neutron star, or—if it is sufficiently massive—a black hole.\nStellar nucleosynthesis in stars or their remnants creates almost all naturally occurring chemical elements heavier than lithium. Stellar mass loss or supernova explosions return chemically enriched material to the interstellar medium. These elements are then recycled into new stars. Astronomers can determine stellar properties—including mass, age, metallicity (chemical composition), variability, distance, and motion through space—by carrying out observations of a star\'s apparent brightness, spectrum, and changes in its position in the sky over time.\nStars can form orbital systems with other astronomical objects, as in planetary systems and star systems with two or more stars. When two such stars orbit closely, their gravitational interaction can significantly impact their evolution. Stars can form part of a much larger gravitationally bound structure, such as a star cluster or a galaxy.\n== Etymology ==\nThe word "star" ultimately derives from the Proto-Indo-European root "h₂stḗr" also meaning star, but further analyzable as h₂eh₁s- ("to burn", also the source of the word "ash") + -tēr (agentive suffix). Compare Latin stella, Greek aster, German Stern. Some scholars believe the word is a borrowing from Akkadian "istar" (Venus). "Star" is cognate (shares the same root) with the following words: asterisk, asteroid, astral, constellation, Esther.\n== Observation history ==\nHistorically, stars have been important to civilizations throughout the world. They have been part of religious practices, divination rituals, mythology, used for celestial navigation and orientation, to mark the passage of seasons, and to define calendars.\nEarly astronomers recognized a difference between "fixed stars", whose position on the celestial sphere does not change, and "wandering stars" (planets), which move noticeably relative to the fixed stars over days or weeks. Many ancient astronomers believed that the stars were permanently affixed to a heavenly sphere and that they were immutable. By convention, astronomers grouped prominent stars into asterisms and constellations and used them to track the motions of the planets and the inferred position of the Sun. The motion of the Sun against the background stars (and the horizon) was used to create calendars, which could be used to regulate agricultural practices. The Gregorian calendar, currently used nearly everywhere in the world, is a solar calendar based on the angle of the Earth\'s rotational axis relative to its local star, the Sun.\nThe oldest accurately dated star chart was the result of ancient Egyptian astronomy in 1534 BC. The earliest known star catalogues were compiled by the ancient Babylonian astronomers of Mesopotamia in the late 2nd millennium BC, during the Kassite Period (c.\u20091531 BC – c.\u20091155 BC).\nThe first star catalogue in Greek astronomy was created by Aristillus in approximately 300 BC, with the help of Timocharis. The star catalog of Hipparchus (2nd century BC) included 1,020 stars, and was used to assemble Ptolemy\'s star catalogue. Hipparchus is known for the discovery of the first recorded nova (new star). Many of the constellations and star names in use today derive from Greek astronomy.\nDespite the apparent immutability of the heavens, Chinese astronomers were aware that new stars could appear. In 185 AD, they were the first to observe and write about a supernova, now known as SN 185. The brightest stellar event in recorded history was the SN 1006 supernova, which was observed in 1006 and written about by the Egyptian astronomer Ali ibn Ridwan and several Chinese astronomers. The SN 1054 supernova, which gave birth to the Crab Nebula, was also observed by Chinese and Islamic astronomers.\nMedieval Islamic astronomers gave Arabic names to many stars that are still used today and they invented numerous astronomical instruments that could compute the positions of the stars. They built the first large observatory research institutes, mainly to produce Zij star catalogues. Among these, the Book of Fixed Stars (964) was written by the Persian astronomer Abd al-Rahman al-Sufi, who observed a number of stars, star clusters (including the Omicron Velorum and Brocchi\'s Clusters) and galaxies (including the Andromeda Galaxy). According to A. Zahoor, in the 11th century, the Persian polymath scholar Abu Rayhan Biruni described the Milky Way galaxy as a multitude of fragments having the properties of nebulous stars, and gave the latitudes of various stars during a lunar eclipse in 1019.\nAccording to Josep Puig, the Andalusian astronomer Ibn Bajjah proposed that the Milky Way was made up of many stars that almost touched one another and appeared to be a continuous image due to the effect of refraction from sublunary material, citing his observation of the conjunction of Jupiter and Mars on 500 AH (1106/1107 AD) as evidence.\nEarly European astronomers such as Tycho Brahe identified new stars in the night sky (later termed novae), suggesting that the heavens were not immutable. In 1584, Giordano Bruno suggested that the stars were like the Sun, and may have other planets, possibly even Earth-like, in orbit around them, an idea that had been suggested earlier by the ancient Greek philosophers, Democritus and Epicurus, and by medieval Islamic cosmologists such as Fakhr al-Din al-Razi. By the following century, the idea of the stars being the same as the Sun was reaching a consensus among astronomers. To explain why these stars exerted no net gravitational pull on the Solar System, Isaac Newton suggested that the stars were equally distributed in every direction, an idea prompted by the theologian Richard Bentley.\nThe Italian astronomer Geminiano Montanari recorded observing variations in luminosity of the star Algol in 1667. Edmond Halley published the first measurements of the proper motion of a pair of nearby "fixed" stars, demonstrating that they had changed positions since the time of the ancient Greek astronomers Ptolemy and Hipparchus.\nWilliam Herschel was the first astronomer to attempt to determine the distribution of stars in the sky. During the 1780s, he established a series of gauges in 600 directions and counted the stars observed along each line of sight. From this, he deduced that the number of stars steadily increased toward one side of the sky, in the direction of the Milky Way core. His son John Herschel repeated this study in the southern hemisphere and found a corresponding increase in the same direction. In addition to his other accomplishments, William Herschel is noted for his discovery that some stars do not merely lie along the same line of sight, but are physical companions that form binary star systems.\nThe science of stellar spectroscopy was pioneered by Joseph von Fraunhofer and Angelo Secchi. By comparing the spectra of stars such as Sirius to the Sun, they found differences in the strength and number of their absorption lines—the dark lines in stellar spectra caused by the atmosphere\'s absorption of specific frequencies. In 1865, Secchi began classifying stars into spectral types. The modern version of the stellar classification scheme was developed by Annie J. Cannon during the early 1900s.\nThe first direct measurement of the distance to a star (61 Cygni at 11.4 light-years) was made in 1838 by Friedrich Bessel using the parallax technique. Parallax measurements demonstrated the vast separation of the stars in the heavens. Observation of double stars gained increasing importance during the 19th century. In 1834, Friedrich Bessel observed changes in the proper motion of the star Sirius and inferred a hidden companion. Edward Pickering discovered the first spectroscopic binary in 1899 when he observed the periodic splitting of the spectral lines of the star Mizar in a 104-day period. Detailed observations of many binary star systems were collected by astronomers such as Friedrich Georg Wilhelm von Struve and S. W. Burnham, allowing the masses of stars to be determined from computation of orbital elements. The first solution to the problem of deriving an orbit of binary stars from telescope observations was made by Felix Savary in 1827.\nThe twentieth century saw increasingly rapid advances in the scientific study of stars. The photograph became a valuable astronomical tool. Karl Schwarzschild discovered that the color of a star and, hence, its temperature, could be determined by comparing the visual magnitude against the photographic magnitude. The development of the photoelectric photometer allowed precise measurements of magnitude at multiple wavelength intervals. In 1921 Albert A. Michelson made the first measurements of a stellar diameter using an interferometer on the Hooker telescope at Mount Wilson Observatory.', 'Technosignatures can be divided into three broad categories: astroengineering projects, signals of planetary origin, and spacecraft within and outside the Solar System.\nAn astroengineering installation such as a Dyson sphere, designed to convert all of the incident radiation of its host star into energy, could be detected through the observation of an infrared excess from a solar analog star, or by the star\'s apparent disappearance in the visible spectrum over several years. After examining some 100,000 nearby large galaxies, a team of researchers has concluded that none of them display any obvious signs of highly advanced technological civilizations.\nAnother hypothetical form of astroengineering, the Shkadov thruster, moves its host star by reflecting some of the star\'s light back on itself, and would be detected by observing if its transits across the star abruptly end with the thruster in front. Asteroid mining within the Solar System is also a detectable technosignature of the first kind.\nIndividual extrasolar planets can be analyzed for signs of technology. Avi Loeb of the Center for Astrophysics | Harvard & Smithsonian has proposed that persistent light signals on the night side of an exoplanet can be an indication of the presence of cities and an advanced civilization. In addition, the excess infrared radiation and chemicals produced by various industrial processes or terraforming efforts may point to intelligence.\nLight and heat detected from planets need to be distinguished from natural sources to conclusively prove the existence of civilization on a planet. However, as argued by the Colossus team, a civilization heat signature should be within a "comfortable" temperature range, like terrestrial urban heat islands, i.e., only a few degrees warmer than the planet itself. In contrast, such natural sources as wild fires, volcanoes, etc. are significantly hotter, so they will be well distinguished by their maximum flux at a different wavelength.\nOther than astroengineering, technosignatures such as artificial satellites around exoplanets, particularly such in geostationary orbit, might be detectable even with today\'s technology and data, and would allow, similar to fossils on Earth, to find traces of extrasolar life from long ago.\nExtraterrestrial craft are another target in the search for technosignatures. Magnetic sail interstellar spacecraft should be detectable over thousands of light-years of distance through the synchrotron radiation they would produce through interaction with the interstellar medium; other interstellar spacecraft designs may be detectable at more modest distances. In addition, robotic probes within the Solar System are also being sought with optical and radio searches.\nFor a sufficiently advanced civilization, hyper energetic neutrinos from Planck scale accelerators should be detectable at a distance of many Mpc.\n=== Advances for Bio and Technosignature Detection ===\nA notable advancement in technosignature detection is the development of an algorithm for signal reconstruction in zero-knowledge one-way communication channels. This algorithm decodes signals from unknown sources without prior knowledge of the encoding scheme, using principles from Algorithmic Information Theory to identify the geometric and topological dimensions of the encoding space. It successfully reconstructed the Arecibo message despite significant noise. The work establishes a connection between syntax and semantics in SETI and technosignature detection, enhancing fields like cryptography and Information Theory.\nBased on fractal theory and the Weierstrass function, a known fractal, another method authored by the same group called fractal messaging offers a framework for space-time scale-free communication. This method leverages properties of self-similarity and scale invariance, enabling spatio-temporal scale-independent and parallel infinite-frequency communication. It also embodies the concept of sending a self-encoding/self-decoding signal as a mathematical formula, equivalent to self-executable computer code that unfolds to read a message at all possible time scales and in all possible channels simultaneously.\n== Fermi paradox ==\nItalian physicist Enrico Fermi suggested in the 1950s that if technologically advanced civilizations are common in the universe, then they should be detectable in one way or another. According to those who were there, Fermi either asked "Where are they?" or "Where is everybody?"\nThe Fermi paradox is commonly understood as asking why extraterrestrials have not visited Earth, but the same reasoning applies to the question of why signals from extraterrestrials have not been heard. The SETI version of the question is sometimes referred to as "the Great Silence".\nThe Fermi paradox can be stated more completely as follows:\nThe size and age of the universe incline us to believe that many technologically advanced civilizations must exist. However, this belief seems logically inconsistent with our lack of observational evidence to support it. Either (1) the initial assumption is incorrect and technologically advanced intelligent life is much rarer than we believe, or (2) our current observations are incomplete, and we simply have not detected them yet, or (3) our search methodologies are flawed and we are not searching for the correct indicators, or (4) it is the nature of intelligent life to destroy itself.\nThere are multiple explanations proposed for the Fermi paradox, ranging from analyses suggesting that intelligent life is rare (the "Rare Earth hypothesis"), to analyses suggesting that although extraterrestrial civilizations may be common, they would not communicate with us, would communicate in a way we have not discovered yet, could not travel across interstellar distances, or destroy themselves before they master the technology of either interstellar travel or communication.\nThe German astrophysicist and radio astronomer Sebastian von Hoerner suggested that the average duration of civilization was 6,500 years. After this time, according to him, it disappears for external reasons (the destruction of life on the planet, the destruction of only rational beings) or internal causes (mental or physical degeneration). According to his calculations, on a habitable planet (one in three million stars) there is a sequence of technological species over a time distance of hundreds of millions of years, and each of them "produces" an average of four technological species. With these assumptions, the average distance between civilizations in the Milky Way is 1,000 light years.\nScience writer Timothy Ferris has posited that since galactic societies are most likely only transitory, an obvious solution is an interstellar communications network, or a type of library consisting mostly of automated systems. They would store the cumulative knowledge of vanished civilizations and communicate that knowledge through the galaxy. Ferris calls this the "Interstellar Internet", with the various automated systems acting as network "servers". If such an Interstellar Internet exists, the hypothesis states, communications between servers are mostly through narrow-band, highly directional radio or laser links. Intercepting such signals is, as discussed earlier, very difficult. However, the network could maintain some broadcast nodes in hopes of making contact with new civilizations.\nAlthough somewhat dated in terms of "information culture" arguments, not to mention the obvious technological problems of a system that could work effectively for billions of years and requires multiple lifeforms agreeing on certain basics of communications technologies, this hypothesis is actually testable (see below).\n=== Difficulty of detection ===\nA significant problem is the vastness of space. Despite piggybacking on the world\'s most sensitive radio telescope, astronomer and initiator of SERENDIP Charles Stuart Bowyer noted the then world\'s largest instrument could not detect random radio noise emanating from a civilization like ours, which has been leaking radio and TV signals for less than 100 years. For SERENDIP and most other SETI projects to detect a signal from an extraterrestrial civilization, the civilization would have to be beaming a powerful signal directly at us. It also means that Earth civilization will only be detectable within a distance of 100 light-years.\n== Post-detection disclosure protocol ==\nThe International Academy of Astronautics (IAA) has a long-standing SETI Permanent Study Group (SPSG, formerly called the IAA SETI Committee), which addresses matters of SETI science, technology, and international policy. The SPSG meets in conjunction with the International Astronautical Congress (IAC), held annually at different locations around the world, and sponsors two SETI Symposia at each IAC. In 2005, the IAA established the SETI: Post-Detection Science and Technology Taskgroup (chairman, Professor Paul Davies) "to act as a Standing Committee to be available to be called on at any time to advise and consult on questions stemming from the discovery of a putative signal of extraterrestrial intelligent (ETI) origin."\nHowever, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".', 'Ultraviolet radiation, also known as simply UV, is electromagnetic radiation of wavelengths of 10–400 nanometers, shorter than that of visible light, but longer than X-rays. UV radiation is present in sunlight and constitutes about 10% of the total electromagnetic radiation output from the Sun. It is also produced by electric arcs, Cherenkov radiation, and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights.\nThe photons of ultraviolet have greater energy than those of visible light, from about 3.1 to 12 electron volts, around the minimum energy required to ionize atoms.:\u200a25–26\u200a  Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack sufficient energy, it can induce chemical reactions and cause many substances to glow or fluoresce. Many practical applications, including chemical and biological effects, are derived from the way that UV radiation can interact with organic molecules. These interactions can involve exciting orbital electrons to higher energy states in molecules potentially breaking chemical bonds. In contrast, the main effect of longer wavelength radiation is to excite vibrational or rotational states of these molecules, increasing their temperature.:\u200a28\u200a  Short-wave ultraviolet light is ionizing radiation. Consequently, short-wave UV damages DNA and sterilizes surfaces with which it comes into contact.\nFor humans, suntan and sunburn are familiar effects of exposure of the skin to UV, along with an increased risk of skin cancer. The amount of UV radiation produced by the Sun means that the Earth would not be able to sustain life on dry land if most of that light were not filtered out by the atmosphere. More energetic, shorter-wavelength "extreme" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. However, UV (specifically, UVB) is also responsible for the formation of vitamin D in most land vertebrates, including humans. The UV spectrum, thus, has effects both beneficial and detrimental to life.\nThe lower wavelength limit of the visible spectrum is conventionally taken as 400 nm.  Although ultraviolet rays are not generally visible to humans, 400 nm is not a sharp cutoff, with shorter and shorter wavelengths becoming less and less visible in this range.  Insects, birds, and some mammals can see near-UV (NUV), i.e., somewhat shorter wavelengths than what humans can see.\n== Visibility ==\nUltraviolet rays are not usable for normal human vision.\nThe lens of the human eye and surgically implanted lens produced since 1986 blocks most radiation in the near UV wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Humans also lack color receptor adaptations for ultraviolet rays. The photoreceptors of the retina are sensitive to near-UV but the lens does not focus this light, causing UV light bulbs to look fuzzy.\nPeople lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet.  Near-UV radiation is visible to insects, some mammals, and some birds. Birds have a fourth color receptor for ultraviolet rays; this, coupled with eye structures that transmit more UV gives smaller birds "true" UV vision.\n== History and discovery ==\n"Ultraviolet" means "beyond violet" (from Latin ultra, "beyond"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency (thus a shorter wavelength) than violet light.\nUV radiation was discovered in February 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He announced the discovery in a very brief letter to the Annalen der Physik and later called them "(de-)oxidizing rays" (German: de-oxidierende Strahlen) to emphasize chemical reactivity and to distinguish them from "heat rays", discovered the previous year at the other end of the visible spectrum. The simpler term "chemical rays" was adopted soon afterwards, and remained popular throughout the 19th century, although some said that this radiation was entirely different from light (notably John William Draper, who named them "tithonic rays"). The terms "chemical rays" and "heat rays" were eventually dropped in favor of ultraviolet and infrared radiation, respectively. In 1878, the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903, the most effective wavelengths were known to be around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established.\nThe discovery of the ultraviolet radiation with wavelengths below 200 nm, named "vacuum ultraviolet" because it is strongly absorbed by the oxygen in air, was made in 1893 by German physicist Victor Schumann. The division of UV into UVA, UVB, and UVC was decided "unanimously" by a committee of the Second International Congress on Light on August 17th, 1932, at the Castle of Christiansborg in Copenhagen.\n== Subtypes ==\nThe electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO 21348:\nSeveral solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive UV photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum.\nVacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths around 150–200 nm can propagate through nitrogen. Scientific instruments can, therefore, use this spectral range by operating in an oxygen-free atmosphere (pure nitrogen, or argon for shorter wavelengths), without the need for costly vacuum chambers. Significant examples include 193-nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers.\nTechnology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of solar-blind devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes.\nExtreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but synthesizing multilayer optics that reflect up to about 50% of EUV radiation at normal incidence is possible. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer  satellite.\nSome sources use the distinction of "hard UV" and "soft UV". For instance, in the case of astrophysics, the boundary may be at the Lyman limit (wavelength 91.2 nm, the energy needed to ionise a hydrogen atom from its ground state), with "hard UV" being more energetic; the same terms may also be used in other fields, such as cosmetology, optoelectronic, etc. The numerical values of the boundary between hard/soft, even within similar scientific fields, do not necessarily coincide; for example, one applied-physics publication used a boundary of 190 nm between hard and soft UV regions.\n== Solar ultraviolet ==\nVery hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars (such as O- and B-type) emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth\'s atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum.\nThe atmosphere blocks about 77% of the Sun\'s UV, when the Sun is highest in the sky (at zenith), with absorption increasing at shorter UV wavelengths. At ground level with the sun at zenith, sunlight is 44% visible light, 3% ultraviolet, and the remainder infrared. Of the ultraviolet radiation that reaches the Earth\'s surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. Almost no UVC reaches the Earth\'s surface. The fraction of UVA and UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. On "partly cloudy" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UVB also plays a major role in plant development, as it affects most of the plant hormones. During total overcast, the amount of absorption due to clouds is heavily dependent on the thickness of the clouds and latitude, with no clear measurements correlating specific thickness and absorption of UVA and UVB.', 'In 1912, Vesto M. Slipher made spectrographic studies of the brightest spiral nebulae to determine their composition. Slipher discovered that the spiral nebulae have high Doppler shifts, indicating that they are moving at a rate exceeding the velocity of the stars he had measured. He found that the majority of these nebulae are moving away from us.\nIn 1917, Heber Doust Curtis observed nova S Andromedae within the "Great Andromeda Nebula", as the Andromeda Galaxy, Messier object M31, was then known. Searching the photographic record, he found 11 more novae. Curtis noticed that these novae were, on average, 10 magnitudes fainter than those that occurred within this galaxy. As a result, he was able to come up with a distance estimate of 150,000 parsecs. He became a proponent of the so-called "island universes" hypothesis, which holds that spiral nebulae are actually independent galaxies.\nIn 1920 a debate took place between Harlow Shapley and Heber Curtis, the Great Debate, concerning the nature of the Milky Way, spiral nebulae, and the dimensions of the universe. To support his claim that the Great Andromeda Nebula is an external galaxy, Curtis noted the appearance of dark lanes resembling the dust clouds in the Milky Way, as well as the significant Doppler shift.\nIn 1922, the Estonian astronomer Ernst Öpik gave a distance determination that supported the theory that the Andromeda Nebula is indeed a distant extra-galactic object. Using the new 100-inch Mt. Wilson telescope, Edwin Hubble was able to resolve the outer parts of some spiral nebulae as collections of individual stars and identified some Cepheid variables, thus allowing him to estimate the distance to the nebulae: they were far too distant to be part of the Milky Way. In 1926 Hubble produced a classification of galactic morphology that is used to this day.\n=== Multi-wavelength observation ===\nAdvances in astronomy have always been driven by technology. After centuries of success in optical astronomy, recent decades have seen major progress in other regions of the electromagnetic spectrum.\nThe dust present in the interstellar medium is opaque to visual light. It is more transparent to far-infrared, which can be used to observe the interior regions of giant molecular clouds and galactic cores in great detail. Infrared is also used to observe distant, red-shifted galaxies that were formed much earlier. Water vapor and carbon dioxide absorb a number of useful portions of the infrared spectrum, so high-altitude or space-based telescopes are used for infrared astronomy.\nThe first non-visual study of galaxies, particularly active galaxies, was made using radio frequencies. The Earth\'s atmosphere is nearly transparent to radio between 5 MHz and 30 GHz. The ionosphere blocks signals below this range. Large radio interferometers have been used to map the active jets emitted from active nuclei.\nUltraviolet and X-ray telescopes can observe highly energetic galactic phenomena. Ultraviolet flares are sometimes observed when a star in a distant galaxy is torn apart from the tidal forces of a nearby black hole. The distribution of hot gas in galactic clusters can be mapped by X-rays. The existence of supermassive black holes at the cores of galaxies was confirmed through X-ray astronomy.\n=== Modern research ===\nIn 1944, Hendrik van de Hulst predicted that microwave radiation with wavelength of 21 cm would be detectable from interstellar atomic hydrogen gas; and in 1951 it was observed. This radiation is not affected by dust absorption, and so its Doppler shift can be used to map the motion of the gas in this galaxy. These observations led to the hypothesis of a rotating bar structure in the center of this galaxy. With improved radio telescopes, hydrogen gas could also be traced in other galaxies.\nIn the 1970s, Vera Rubin uncovered a discrepancy between observed galactic rotation speed and that predicted by the visible mass of stars and gas. Today, the galaxy rotation problem is thought to be explained by the presence of large quantities of unseen dark matter.\nBeginning in the 1990s, the Hubble Space Telescope yielded improved observations. Among other things, its data helped establish that the missing dark matter in this galaxy could not consist solely of inherently faint and small stars. The Hubble Deep Field, an extremely long exposure of a relatively empty part of the sky, provided evidence that there are about 125 billion (1.25×1011) galaxies in the observable universe. Improved technology in detecting the spectra invisible to humans (radio telescopes, infrared cameras, and x-ray telescopes) allows detection of other galaxies that are not detected by Hubble. Particularly, surveys in the Zone of Avoidance (the region of sky blocked at visible-light wavelengths by the Milky Way) have revealed a number of new galaxies.\nA 2016 study published in The Astrophysical Journal, led by Christopher Conselice of the University of Nottingham, analyzed many sources of data to estimate that the observable universe (up to z=8) contained at least two trillion (2×1012) galaxies, a factor of 10 more than are directly observed in Hubble images.:\u200a12\u200a However, later observations with the New Horizons space probe from outside the zodiacal light observed less cosmic optical light than Conselice while still suggesting that direct observations are missing galaxies.\n== Types and morphology ==\nGalaxies come in three main types: ellipticals, spirals, and irregulars. A slightly more extensive description of galaxy types based on their appearance is given by the Hubble sequence. Since the Hubble sequence is entirely based upon visual morphological type (shape), it may miss certain important characteristics of galaxies such as star formation rate in starburst galaxies and activity in the cores of active galaxies.\nMany galaxies are thought to contain a supermassive black hole at their center. This includes the Milky Way, whose core region is called the Galactic Center.\n=== Ellipticals ===\nThe Hubble classification system rates elliptical galaxies on the basis of their ellipticity, ranging from E0, being nearly spherical, up to E7, which is highly elongated. These galaxies have an ellipsoidal profile, giving them an elliptical appearance regardless of the viewing angle. Their appearance shows little structure and they typically have relatively little interstellar matter. Consequently, these galaxies also have a low portion of open clusters and a reduced rate of new star formation. Instead, they are dominated by generally older, more evolved stars that are orbiting the common center of gravity in random directions. The stars contain low abundances of heavy elements because star formation ceases after the initial burst. In this sense they have some similarity to the much smaller globular clusters.\n==== Type-cD galaxies ====\nThe largest galaxies are the type-cD galaxies.\nFirst described in 1964 by a paper by Thomas A. Matthews and others, they are a subtype of the more general class of D galaxies, which are giant elliptical galaxies, except that they are much larger. They are popularly known as the supergiant elliptical galaxies and constitute the largest and most luminous galaxies known. These galaxies feature a central elliptical nucleus with an extensive, faint halo of stars extending to megaparsec scales. The profile of their surface brightnesses as a function of their radius (or distance from their cores) falls off more slowly than their smaller counterparts.\nThe formation of these cD galaxies remains an active area of research, but the leading model is that they are the result of the mergers of smaller galaxies in the environments of dense clusters, or even those outside of clusters with random overdensities. These processes are the mechanisms that drive the formation of fossil groups or fossil clusters, where a large, relatively isolated, supergiant elliptical resides in the middle of the cluster and are surrounded by an extensive cloud of X-rays as the residue of these galactic collisions. Another older model posits the phenomenon of cooling flow, where the heated gases in clusters collapses towards their centers as they cool, forming stars in the process, a phenomenon observed in clusters such as Perseus, and more recently in the Phoenix Cluster.\n==== Shell galaxy ====\nA shell galaxy is a type of elliptical galaxy where the stars in its halo are arranged in concentric shells. About one-tenth of elliptical galaxies have a shell-like structure, which has never been observed in spiral galaxies. These structures are thought to develop when a larger galaxy absorbs a smaller companion galaxy—that as the two galaxy centers approach, they start to oscillate around a center point, and the oscillation creates gravitational ripples forming the shells of stars, similar to ripples spreading on water. For example, galaxy NGC 3923 has over 20 shells.\n=== Spirals ===\nSpiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept.', 'Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.']

Question: What is the isophotal diameter used for in measuring a galaxy's size?

Choices:
Choice A) The isophotal diameter is a way of measuring a galaxy's distance from Earth.
Choice B) The isophotal diameter is a measure of a galaxy's age.
Choice C) The isophotal diameter is a measure of a galaxy's mass.
Choice D) The isophotal diameter is a measure of a galaxy's temperature.
Choice E) The isophotal diameter is a conventional way of measuring a galaxy's size based on its apparent surface brightness.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word κρύσταλλος (krústallos; "clear ice, rock-crystal"), and γράφειν (gráphein; "to write"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well new developments in these fields.\n== History and timeline ==\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the Davisson–Germer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n== Methodology ==\nCrystallographic methods depend mainly on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. Crystallographers often explicitly state the type of beam used, as in the terms X-ray diffraction, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.\nX-rays interact with the spatial distribution of electrons in the sample.\nNeutrons are scattered by the atomic nuclei through the strong nuclear forces, but in addition the magnetic moment of neutrons is non-zero, so they are also scattered by magnetic fields. When neutrons are scattered from hydrogen-containing materials, they produce diffraction patterns with high noise levels, which can sometimes be resolved by substituting deuterium for hydrogen.\nElectrons are charged particles and therefore interact with the total charge distribution of both the atomic nuclei and the electrons of the sample.:\u200aChpt 4\nIt is hard to focus x-rays or neutrons, but since electrons are charged they can be focused and are used in electron microscope to produce magnified images. There are many ways that transmission electron microscopy and related techniques such as scanning transmission electron microscopy, high-resolution electron microscopy can be used to obtain images with in many cases atomic resolution from which crystallographic information can be obtained. There are also other methods such as low-energy electron diffraction, low-energy electron microscopy and reflection high-energy electron diffraction which can be used to obtain crystallographic information about surfaces.\n== Applications in various areas ==\n=== Materials science ===\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Most materials do not occur as a single crystal, but are poly-crystalline in nature (they exist as an aggregate of small crystals with different orientations). As such, powder diffraction techniques, which take diffraction patterns of samples with a large number of crystals, play an important role in structural determination.\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements. Crystallographic studies help elucidate the relationship between a material\'s structure and its properties, aiding in developing new materials with tailored characteristics. This understanding is crucial in various fields, including metallurgy, geology, and materials science. Advancements in crystallographic techniques, such as electron diffraction and X-ray crystallography, continue to expand our understanding of material behavior at the atomic level.\nIn another example, iron transforms from a body-centered cubic (bcc) structure called ferrite to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material\'s properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which structures are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory.\n=== Biology ===\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. The double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol, Pymol or VMD can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron diffraction has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n== Notation ==\nCoordinates in square brackets such as [100] denote a direction vector (in real space).\nCoordinates in angle brackets or chevrons such as <100> denote a family of directions which are related by symmetry operations. In the cubic crystal system for example, <100> would mean [100], [010], [001] or the negative of any of those directions.\nMiller indices in parentheses such as (100) denote a plane of the crystal structure, and regular repetitions of that plane with a particular spacing. In the cubic system, the normal to the (hkl) plane is the direction [hkl], but in lower-symmetry cases, the normal to (hkl) is not parallel to [hkl].\nIndices in curly brackets or braces such as {100} denote a family of planes and their normals. In cubic materials the symmetry makes them equivalent, just as the way angle brackets denote a family of directions. In non-cubic materials, <hkl> is not necessarily perpendicular to {hkl}.\n== Reference literature ==\nThe International Tables for Crystallography is an eight-book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure through x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and do not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Crystallography', "Planck's law", 'Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==']

Question: What is an order parameter?

Choices:
Choice A) An order parameter is a measure of the temperature of a physical system.
Choice B) An order parameter is a measure of the gravitational force in a physical system.
Choice C) An order parameter is a measure of the magnetic field strength in a physical system.
Choice D) An order parameter is a measure of the degree of symmetry breaking in a physical system.
Choice E) An order parameter is a measure of the rotational symmetry in a physical system.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Supernova', 'The light curves for type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. Their optical energy output is driven by radioactive decay of ejected nickel-56 (half-life 6 days), which then decays to radioactive cobalt-56 (half-life 77 days). These radioisotopes excite the surrounding material to incandescence. Modern studies of cosmology rely on 56Ni radioactivity providing the energy for the optical brightness of supernovae of type Ia, which are the "standard candles" of cosmology but whose diagnostic 847 keV and 1,238 keV gamma rays were first detected only in 2014. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half-life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission from the remaining cobalt-56 becomes dominant, although this portion of the light curve has been little-studied.\nType Ib and Ic light curves are similar to type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of the created nickel-56. The peak luminosity varies considerably and there are even occasional type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increased peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.\nThe light curves for type II supernovae are characterised by a much slower decline than type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial destruction this hydrogen becomes heated and ionised. The majority of type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in type I supernovae, due to the efficiency of conversion into light by all the hydrogen.\nIn type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a type I supernova and the hydrogen even disappears from the spectrum after several weeks.\nType IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a superluminous supernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.\nLarge numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.\nNotes:\n=== Asymmetry ===\nA long-standing puzzle surrounding type II supernovae is why the remaining compact object receives a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high peculiar velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500 km/s or greater. This indicates an expansion asymmetry, but the mechanism by which momentum is transferred to the compact object remains a puzzle. Proposed explanations for this kick include convection in the collapsing star, asymmetric ejection of matter during neutron star formation, and asymmetrical neutrino emissions.\nOne possible explanation for this asymmetry is large-scale convection above the core. The convection can create radial variations in density giving rise to variations in the amount of energy absorbed from neutrino outflow. However analysis of this mechanism predicts only modest momentum transfer. Another possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova. (A similar model is used for explaining long gamma-ray bursts.) The dominant mechanism may depend upon the mass of the progenitor star.\nInitial asymmetries have also been confirmed in type Ia supernovae through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the expansion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarisation of the emitted light.\n=== Energy output ===\nAlthough supernovae are primarily known as luminous events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.\nThere is a fundamental difference between the balance of energy production in the different types of supernova. In type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.\nStandard type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is 56Ni generated from silicon burning. 56Ni is radioactive and decays into 56Co by beta plus decay (with a half life of six days) and gamma rays. 56Co itself decays by the beta plus (positron) path with a half life of 77 days into stable 56Fe. These two processes are responsible for the electromagnetic radiation from type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.\nCore collapse supernovae are on average visually fainter than type Ia supernovae, but the total energy released is far higher, as outlined in the following table.\nIn some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high-luminosity supernovae and is thought to be the cause of type Ic hypernovae and long-duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low-luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.\nWhen a supernova occurs inside a small dense cloud of circumstellar material, it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause type IIn hypernovae.\nAlthough pair-instability supernovae are core collapse supernovae with spectra and light curves similar to type II-P, the nature after core collapse is more like that of a giant type Ia with runaway fusion of carbon, oxygen and silicon. The total energy released by the highest-mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.\n=== Progenitor ===', '=== Progenitor ===\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic type Ic-BL events that are associated with long-duration gamma-ray bursts.\n== External impact ==\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\n=== Source of heavy elements ===\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\n=== Role in stellar evolution ===\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n=== Cosmic rays ===\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\n=== Gravitational waves ===', "==== Red-giant-branch phase ====\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star.  For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower 12C/13C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\nThe helium core continues to grow on the red-giant branch.  It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase.  The star increases in luminosity towards the tip of the red-giant branch.  Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n==== Horizontal branch ====\nIn the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 108 times the luminosity of the Sun for a few days and 1011 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning.  These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n==== Asymptotic-giant-branch phase ====\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time).  Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star.  Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically.  This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface.  This is known as the second dredge up, and in some stars there may even be a third dredge up.  In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra.  A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\nAnother well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths.  These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity.  These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n==== Post-AGB ====\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n=== Massive stars ===\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants.  Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.\n==== Supergiant evolution ====\nExtremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\nThe core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core.  In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process.  At the end of helium fusion, the core of a star consists primarily of carbon and oxygen.  In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium.  Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.\nThe exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉.  After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse.  Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning.  For a range of stars of approximately 8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova.", '==== Type II ====\nStars with initial masses less than about 8 M☉ never develop a core large enough to collapse and they eventually lose their atmospheres to become white dwarfs. Stars with at least 9 M☉ (possibly as much as 12 M☉) evolve in a complex fashion, progressively burning heavier elements at hotter temperatures in their cores. The star becomes layered like an onion, with the burning of more easily fused elements occurring in larger shells. Although popularly described as an onion with an iron core, the least massive supernova progenitors only have oxygen-neon(-magnesium) cores. These super-AGB stars may form the majority of core collapse supernovae, although less luminous and so less commonly observed than those from more massive progenitors.\nIf core collapse occurs during a supergiant phase when the star still has a hydrogen envelope, the result is a type II supernova. The rate of mass loss for luminous stars depends on the metallicity and luminosity. Extremely luminous stars at near solar metallicity will lose all their hydrogen before they reach core collapse and so will not form a supernova of type II. At low metallicity, all stars will reach core collapse with a hydrogen envelope but sufficiently massive stars collapse directly to a black hole without producing a visible supernova.\nStars with an initial mass up to about 90 times the Sun, or a little less at high metallicity, result in a type II-P supernova, which is the most commonly observed type. At moderate to high metallicity, stars near the upper end of that mass range will have lost most of their hydrogen when core collapse occurs and the result will be a type II-L supernova. At very low metallicity, stars of around 140–250 M☉ will reach core collapse by pair instability while they still have a hydrogen atmosphere and an oxygen core and the result will be a supernova with type II characteristics but a very large mass of ejected 56Ni and high luminosity.\n==== Type Ib and Ic ====\nThese supernovae, like those of type II, are massive stars that undergo core collapse. Unlike the progenitors of type II supernovae, the stars which become types Ib and Ic supernovae have lost most of their outer (hydrogen) envelopes due to strong stellar winds or else from interaction with a companion. These stars are known as Wolf–Rayet stars, and they occur at moderate to high metallicity where continuum driven winds cause sufficiently high mass-loss rates. Observations of type Ib/c supernova do not match the observed or expected occurrence of Wolf–Rayet stars. Alternate explanations for this type of core collapse supernova involve stars stripped of their hydrogen by binary interactions. Binary models provide a better match for the observed supernovae, with the proviso that no suitable binary helium stars have ever been observed.\nType Ib supernovae are the more common and result from Wolf–Rayet stars of type WC which still have helium in their atmospheres. For a narrow range of masses, stars evolve further before reaching core collapse to become WO stars with very little helium remaining, and these are the progenitors of type Ic supernovae.\nA few percent of the type Ic supernovae are associated with gamma-ray bursts (GRB), though it is also believed that any hydrogen-stripped type Ib or Ic supernova could produce a GRB, depending on the circumstances of the geometry. The mechanism for producing this type of GRB is the jets produced by the magnetic field of the rapidly spinning magnetar formed at the collapsing core of the star. The jets would also transfer energy into the expanding outer shell, producing a super-luminous supernova.\nUltra-stripped supernovae occur when the exploding star has been stripped (almost) all the way to the metal core, via mass transfer in a close binary. As a result, very little material is ejected from the exploding star (c. 0.1 M☉). In the most extreme cases, ultra-stripped supernovae can occur in naked metal cores, barely above the Chandrasekhar mass limit. SN 2005ek might be the first observational example of an ultra-stripped supernova, giving rise to a relatively dim and fast decaying light curve. The nature of ultra-stripped supernovae can be both iron core-collapse and electron capture supernovae, depending on the mass of the collapsing core. Ultra-stripped supernovae are believed to be associated with the second supernova explosion in a binary system, producing for example a tight double neutron star system.\nIn 2022 a team of astronomers led by researchers from the Weizmann Institute of Science reported the first supernova explosion showing direct evidence for a Wolf-Rayet progenitor star. SN 2019hgp was a type Icn supernova and is also the first in which the element neon has been detected.\n==== Electron-capture supernovae ====\nIn 1980, a "third type" of supernova was predicted by Ken\'ichi Nomoto of the University of Tokyo, called an electron-capture supernova. It would arise when a star "in the transitional range (~8 to 10 solar masses) between white dwarf formation and iron core-collapse supernovae", and with a degenerate O+Ne+Mg core, imploded after its core ran out of nuclear fuel, causing gravity to compress the electrons in the star\'s core into their atomic nuclei, leading to a supernova explosion and leaving behind a neutron star. In June 2021, a paper in the journal Nature Astronomy reported that the 2018 supernova SN 2018zd (in the galaxy NGC 2146, about 31 million light-years from Earth) appeared to be the first observation of an electron-capture supernova. The 1054 supernova explosion that created the Crab Nebula in our galaxy had been thought to be the best candidate for an electron-capture supernova, and the 2021 paper makes it more likely that this was correct.\n=== Failed supernovae ===\nThe core collapse of some massive stars may not result in a visible supernova. This happens if the initial core collapse cannot be reversed by the mechanism that produces an explosion, usually because the core is too massive. These events are difficult to detect, but large surveys have detected possible candidates. The red supergiant N6946-BH1 in NGC 6946 underwent a modest outburst in March 2009, before fading from view. Only a faint infrared source remains at the star\'s location.\n=== Light curves ===\nThe ejecta gases would dim quickly without some energy input to keep them hot. The source of this energy—which can maintain the optical supernova glow for months—was, at first, a puzzle. Some considered rotational energy from the central pulsar as a source. Although the energy that initially powers each type of supernovae is delivered promptly, the light curves are dominated by subsequent radioactive heating of the rapidly expanding ejecta. The intensely radioactive nature of the ejecta gases was first calculated on sound nucleosynthesis grounds in the late 1960s, and this has since been demonstrated as correct for most supernovae. It was not until SN 1987A that direct observation of gamma-ray lines unambiguously identified the major radioactive nuclei.\nIt is now known by direct observation that much of the light curve (the graph of luminosity as a function of time) after the occurrence of a type II Supernova, such as SN 1987A, is explained by those predicted radioactive decays. Although the luminous emission consists of optical photons, it is the radioactive power absorbed by the ejected gases that keeps the remnant hot enough to radiate light. The radioactive decay of 56Ni through its daughters 56Co to 56Fe produces gamma-ray photons, primarily with energies of 847 keV and 1,238 keV, that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of 56Ni to 56Co (half-life 6 days) while energy for the later light curve in particular fit very closely with the 77.3-day half-life of 56Co decaying to 56Fe. Later measurements by space gamma-ray telescopes of the small fraction of the 56Co and 57Co gamma rays that escaped the SN 1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power sources.\nThe late-time decay phase of visual light curves for different supernova types all depend on radioactive heating, but they vary in shape and amplitude because of the underlying mechanisms, the way that visible radiation is produced, the epoch of its observation, and the transparency of the ejected material. The light curves can be significantly different at other wavelengths. For example, at ultraviolet wavelengths there is an early extremely luminous peak lasting only a few hours corresponding to the breakout of the shock launched by the initial event, but that breakout is hardly detectable optically.', '=== Spirals ===\nSpiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept.\nSpiral galaxies consist of a rotating disk of stars and interstellar medium, along with a central bulge of generally older stars. Extending outward from the bulge are relatively bright arms. In the Hubble classification scheme, spiral galaxies are listed as type S, followed by a letter (a, b, or c) which indicates the degree of tightness of the spiral arms and the size of the central bulge. An Sa galaxy has tightly wound, poorly defined arms and possesses a relatively large core region. At the other extreme, an Sc galaxy has open, well-defined arms and a small core region. A galaxy with poorly defined arms is sometimes referred to as a flocculent spiral galaxy; in contrast to the grand design spiral galaxy that has prominent and well-defined spiral arms. The speed in which a galaxy rotates is thought to correlate with the flatness of the disc as some spiral galaxies have thick bulges, while others are thin and dense.\nIn spiral galaxies, the spiral arms do have the shape of approximate logarithmic spirals, a pattern that can be theoretically shown to result from a disturbance in a uniformly rotating mass of stars. Like the stars, the spiral arms rotate around the center, but they do so with constant angular velocity. The spiral arms are thought to be areas of high-density matter, or "density waves". As stars move through an arm, the space velocity of each stellar system is modified by the gravitational force of the higher density. (The velocity returns to normal after the stars depart on the other side of the arm.) This effect is akin to a "wave" of slowdowns moving along a highway full of moving cars. The arms are visible because the high density facilitates star formation, and therefore they harbor many bright and young stars.\n==== Barred spiral galaxy ====\nA majority of spiral galaxies, including the Milky Way galaxy, have a linear, bar-shaped band of stars that extends outward to either side of the core, then merges into the spiral arm structure. In the Hubble classification scheme, these are designated by an SB, followed by a lower-case letter (a, b or c) which indicates the form of the spiral arms (in the same manner as the categorization of normal spiral galaxies). Bars are thought to be temporary structures that can occur as a result of a density wave radiating outward from the core, or else due to a tidal interaction with another galaxy. Many barred spiral galaxies are active, possibly as a result of gas being channeled into the core along the arms.\nOur own galaxy, the Milky Way, is a large disk-shaped barred-spiral galaxy about 30 kiloparsecs in diameter and a kiloparsec thick. It contains about two hundred billion (2×1011) stars and has a total mass of about six hundred billion (6×1011) times the mass of the Sun.\n==== Super-luminous spiral ====\nRecently, researchers described galaxies called super-luminous spirals. They are very large with an upward diameter of 437,000 light-years (compared to the Milky Way\'s 87,400 light-year diameter). With a mass of 340 billion solar masses, they generate a significant amount of ultraviolet and mid-infrared light. They are thought to have an increased star formation rate around 30 times faster than the Milky Way.\n=== Other morphologies ===\nPeculiar galaxies are galactic formations that develop unusual properties due to tidal interactions with other galaxies.\nA ring galaxy has a ring-like structure of stars and interstellar medium surrounding a bare core. A ring galaxy is thought to occur when a smaller galaxy passes through the core of a spiral galaxy. Such an event may have affected the Andromeda Galaxy, as it displays a multi-ring-like structure when viewed in infrared radiation.\nA lenticular galaxy is an intermediate form that has properties of both elliptical and spiral galaxies. These are categorized as Hubble type S0, and they possess ill-defined spiral arms with an elliptical halo of stars (barred lenticular galaxies receive Hubble classification SB0).\nIrregular galaxies are galaxies that can not be readily classified into an elliptical or spiral morphology.\nAn Irr-I galaxy has some structure but does not align cleanly with the Hubble classification scheme.\nIrr-II galaxies do not possess any structure that resembles a Hubble classification, and may have been disrupted. Nearby examples of (dwarf) irregular galaxies include the Magellanic Clouds.\nA dark or "ultra diffuse" galaxy is an extremely-low-luminosity galaxy. It may be the same size as the Milky Way, but have a visible star count only one percent of the Milky Way\'s. Multiple mechanisms for producing this type of galaxy have been proposed, and it is possible that different dark galaxies formed by different means. One candidate explanation for the low luminosity is that the galaxy lost its star-forming gas at an early stage, resulting in old stellar populations.\n=== Dwarfs ===\nDespite the prominence of large elliptical and spiral galaxies, most galaxies are dwarf galaxies. They are relatively small when compared with other galactic formations, being about one hundredth the size of the Milky Way, with only a few billion stars. Blue compact dwarf galaxies contains large clusters of young, hot, massive stars. Ultra-compact dwarf galaxies have been discovered that are only 100 parsecs across.\nMany dwarf galaxies may orbit a single larger galaxy; the Milky Way has at least a dozen such satellites, with an estimated 300–500 yet to be discovered.\nMost of the information we have about dwarf galaxies come from observations of the local group, containing two spiral galaxies, the Milky Way and Andromeda, and many dwarf galaxies. These dwarf galaxies are classified as either irregular or dwarf elliptical/dwarf spheroidal galaxies.\nA study of 27 Milky Way neighbors found that in all dwarf galaxies, the central mass is approximately 10 million solar masses, regardless of whether it has thousands or millions of stars. This suggests that galaxies are largely formed by dark matter, and that the minimum size may indicate a form of warm dark matter incapable of gravitational coalescence on a smaller scale.\n== Variants ==\n=== Interacting ===\nInteractions between galaxies are relatively frequent, and they can play an important role in galactic evolution. Near misses between galaxies result in warping distortions due to tidal interactions, and may cause some exchange of gas and dust.\nCollisions occur when two galaxies pass directly through each other and have sufficient relative momentum not to merge. The stars of interacting galaxies usually do not collide, but the gas and dust within the two forms interacts, sometimes triggering star formation. A collision can severely distort the galaxies\' shapes, forming bars, rings or tail-like structures.\nAt the extreme of interactions are galactic mergers, where the galaxies\' relative momentums are insufficient to allow them to pass through each other. Instead, they gradually merge to form a single, larger galaxy. Mergers can result in significant changes to the galaxies\' original morphology. If one of the galaxies is much more massive than the other, the result is known as cannibalism, where the more massive larger galaxy remains relatively undisturbed, and the smaller one is torn apart. The Milky Way galaxy is currently in the process of cannibalizing the Sagittarius Dwarf Elliptical Galaxy and the Canis Major Dwarf Galaxy.\n=== Starburst ===\nStars are created within galaxies from a reserve of cold gas that forms giant molecular clouds. Some galaxies have been observed to form stars at an exceptional rate, which is known as a starburst. If they continue to do so, they would consume their reserve of gas in a time span less than the galaxy\'s lifespan. Hence starburst activity usually lasts only about ten million years, a relatively brief period in a galaxy\'s history. Starburst galaxies were more common during the universe\'s early history, but still contribute an estimated 15% to total star production.\nStarburst galaxies are characterized by dusty concentrations of gas and the appearance of newly formed stars, including massive stars that ionize the surrounding clouds to create H II regions. These stars produce supernova explosions, creating expanding remnants that interact powerfully with the surrounding gas. These outbursts trigger a chain reaction of star-building that spreads throughout the gaseous region. Only when the available gas is nearly consumed or dispersed does the activity end.\nStarbursts are often associated with merging or interacting galaxies. The prototype example of such a starburst-forming interaction is M82, which experienced a close encounter with the larger M81. Irregular galaxies often exhibit spaced knots of starburst activity.\n=== Radio galaxy ===\nA radio galaxy is a galaxy with giant regions of radio emission extending well beyond its visible structure. These energetic radio lobes are powered by jets from its active galactic nucleus. Radio galaxies are classified according to their Fanaroff–Riley classification. The FR I class have lower radio luminosity and exhibit structures which are more elongated; the FR II class are higher radio luminosity. The correlation of radio luminosity and structure suggests that the sources in these two types of galaxies may differ.', "In more massive stars, the fusion of neon proceeds without a runaway deflagration.  This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements.  Surrounding the core are shells of lighter elements still undergoing fusion.  The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged.  The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope.  The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars.  Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself.  The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.\n==== Supernova ====\nWhen the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole.  Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\n== Stellar remnants ==\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n=== White and black dwarfs ===\nFor a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years.\nThe chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n=== Neutron stars ===\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n=== Black holes ===\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉.", 'Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.', "When both rates of movement are known, the space velocity of the star relative to the Sun or the galaxy can be computed. Among nearby stars, it has been found that younger population I stars have generally lower velocities than older, population II stars. The latter have elliptical orbits that are inclined to the plane of the galaxy. A comparison of the kinematics of nearby stars has allowed astronomers to trace their origin to common points in giant molecular clouds; such groups with common points of origin are referred to as stellar associations.\n=== Magnetic field ===\nThe magnetic field of a star is generated within regions of the interior where convective circulation occurs. This movement of conductive plasma functions like a dynamo, wherein the movement of electrical charges induce magnetic fields, as does a mechanical dynamo. Those magnetic fields have a great range that extend throughout and beyond the star. The strength of the magnetic field varies with the mass and composition of the star, and the amount of magnetic surface activity depends upon the star's rate of rotation. This surface activity produces starspots, which are regions of strong magnetic fields and lower than normal surface temperatures. Coronal loops are arching magnetic field flux lines that rise from a star's surface into the star's outer atmosphere, its corona. The coronal loops can be seen due to the plasma they conduct along their length. Stellar flares are bursts of high-energy particles that are emitted due to the same magnetic activity.\nYoung, rapidly rotating stars tend to have high levels of surface activity because of their magnetic field. The magnetic field can act upon a star's stellar wind, functioning as a brake to gradually slow the rate of rotation with time. Thus, older stars such as the Sun have a much slower rate of rotation and a lower level of surface activity. The activity levels of slowly rotating stars tend to vary in a cyclical manner and can shut down altogether for periods of time. During the Maunder Minimum, for example, the Sun underwent a 70-year period with almost no sunspot activity.\n=== Mass ===\nStars have masses ranging from less than half the solar mass to over 200 solar masses (see List of most massive stars). One of the most massive stars known is Eta Carinae, which, with 100–150 times as much mass as the Sun, will have a lifespan of only several million years. Studies of the most massive open clusters suggests 150 M☉ as a rough upper limit for stars in the current era of the universe. This represents an empirical value for the theoretical limit on the mass of forming stars due to increasing radiation pressure on the accreting gas cloud. Several stars in the R136 cluster in the Large Magellanic Cloud have been measured with larger masses, but it has been determined that they could have been created through the collision and merger of massive stars in close binary systems, sidestepping the 150 M☉ limit on massive star formation.\nThe first stars to form after the Big Bang may have been larger, up to 300 M☉, due to the complete absence of elements heavier than lithium in their composition. This generation of supermassive population III stars is likely to have existed in the very early universe (i.e., they are observed to have a high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life. In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at z = 6.60.\nWith a mass only 80 times that of Jupiter (MJ), 2MASS J0523-1403 is the smallest known star undergoing nuclear fusion in its core. For stars with metallicity similar to the Sun, the theoretical minimum mass the star can have and still undergo fusion at the core, is estimated to be about 75 MJ. When the metallicity is very low, the minimum star size seems to be about 8.3% of the solar mass, or about 87 MJ. Smaller bodies called brown dwarfs, occupy a poorly defined grey area between stars and gas giants.\nThe combination of the radius and the mass of a star determines its surface gravity. Giant stars have a much lower surface gravity than do main sequence stars, while the opposite is the case for degenerate, compact stars such as white dwarfs. The surface gravity can influence the appearance of a star's spectrum, with higher gravity causing a broadening of the absorption lines.\n=== Rotation ===\nThe rotation rate of stars can be determined through spectroscopic measurement, or more exactly determined by tracking their starspots. Young stars can have a rotation greater than 100 km/s at the equator. The B-class star Achernar, for example, has an equatorial velocity of about 225 km/s or greater, causing its equator to bulge outward and giving it an equatorial diameter that is more than 50% greater than between the poles. This rate of rotation is just below the critical velocity of 300 km/s at which speed the star would break apart. By contrast, the Sun rotates once every 25–35 days depending on latitude, with an equatorial velocity of 1.93 km/s. A main sequence star's magnetic field and the stellar wind serve to slow its rotation by a significant amount as it evolves on the main sequence.\nDegenerate stars have contracted into a compact mass, resulting in a rapid rate of rotation. However they have relatively low rates of rotation compared to what would be expected by conservation of angular momentum—the tendency of a rotating body to compensate for a contraction in size by increasing its rate of spin. A large portion of the star's angular momentum is dissipated as a result of mass loss through the stellar wind. In spite of this, the rate of rotation for a pulsar can be very rapid. The pulsar at the heart of the Crab nebula, for example, rotates 30 times per second. The rotation rate of the pulsar will gradually slow due to the emission of radiation.\n=== Temperature ===\nThe surface temperature of a main sequence star is determined by the rate of energy production of its core and by its radius, and is often estimated from the star's color index. The temperature is normally given in terms of an effective temperature, which is the temperature of an idealized black body that radiates its energy at the same luminosity per surface area as the star. The effective temperature is only representative of the surface, as the temperature increases toward the core. The temperature in the core region of a star is several million kelvins.\nThe stellar temperature will determine the rate of ionization of various elements, resulting in characteristic absorption lines in the spectrum. The surface temperature of a star, along with its visual absolute magnitude and absorption features, is used to classify a star (see classification below).\nMassive main sequence stars can have surface temperatures of 50,000 K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600 K; but they have a high luminosity due to their large exterior surface area.\n== Radiation ==\nThe energy produced by stars, a product of nuclear fusion, radiates to space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind, which streams from the outer layers as electrically charged protons and alpha and beta particles. A steady stream of almost massless neutrinos emanate directly from the star's core.\nThe production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers.\nThe color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the star's outer layers, including its photosphere. Besides visible light, stars emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics.\nUsing the stellar spectrum, astronomers can determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is found, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.) With these parameters, astronomers can estimate the age of the star.\n=== Luminosity ===\nThe luminosity of a star is the amount of light and other forms of radiant energy it radiates per unit of time. It has units of power. The luminosity of a star is determined by its radius and surface temperature. Many stars do not radiate uniformly across their entire surface. The rapidly rotating star Vega, for example, has a higher energy flux (power per unit area) at its poles than along its equator.", 'Accretion of material onto the protostar continues partially from the newly formed circumstellar disc. When the density and temperature are high enough, deuterium fusion begins, and the outward pressure of the resultant radiation slows (but does not stop) the collapse. Material comprising the cloud continues to "rain" onto the protostar. In this stage bipolar jets are produced called Herbig–Haro objects. This is probably the means by which excess angular momentum of the infalling material is expelled, allowing the star to continue to form.\nWhen the surrounding gas and dust envelope disperses and accretion process stops, the star is considered a pre-main-sequence star (PMS star). The energy source of these objects is (gravitational contraction)Kelvin–Helmholtz mechanism, as opposed to hydrogen burning in main sequence stars. The PMS star follows a Hayashi track on the Hertzsprung–Russell (H–R) diagram. The contraction will proceed until the Hayashi limit is reached, and thereafter contraction will continue on a Kelvin–Helmholtz timescale with the temperature remaining stable. Stars with less than 0.5 M☉ thereafter join the main sequence. For more massive PMS stars, at the end of the Hayashi track they will slowly collapse in near hydrostatic equilibrium, following the Henyey track.\nFinally, hydrogen begins to fuse in the core of the star, and the rest of the enveloping material is cleared away. This ends the protostellar phase and begins the star\'s main sequence phase on the H–R diagram.\nThe stages of the process are well defined in stars with masses around 1 M☉ or less. In high mass stars, the length of the star formation process is comparable to the other timescales of their evolution, much shorter, and the process is not so well defined. The later evolution of stars is studied in stellar evolution.\n== Observations ==\nKey elements of star formation are only available by observing in wavelengths other than the optical. The protostellar stage of stellar existence is almost invariably hidden away deep inside dense clouds of gas and dust left over from the GMC. Often, these star-forming cocoons known as Bok globules, can be seen in silhouette against bright emission from surrounding gas. Early stages of a star\'s life can be seen in infrared light, which penetrates the dust more easily than visible light.\nObservations from the Wide-field Infrared Survey Explorer (WISE) have thus been especially important for unveiling numerous galactic protostars and their parent star clusters.  Examples of such embedded star clusters are FSR 1184, FSR 1190, Camargo 14, Camargo 74, Majaess 64, and Majaess 98.\nThe structure of the molecular cloud and the effects of the protostar can be observed in near-IR extinction maps (where the number of stars are counted per unit area and compared to a nearby zero extinction area of sky), continuum dust emission and rotational transitions of CO and other molecules; these last two are observed in the millimeter and submillimeter range. The radiation from the protostar and early star has to be observed in infrared astronomy wavelengths, as the extinction caused by the rest of the cloud in which the star is forming is usually too big to allow us to observe it in the visual part of the spectrum. This presents considerable difficulties as the Earth\'s atmosphere is almost entirely opaque from 20μm to 850μm, with narrow windows at 200μm and 450μm. Even outside this range, atmospheric subtraction techniques must be used.\nX-ray observations have proven useful for studying young stars, since X-ray emission from these objects is about 100–100,000 times stronger than X-ray emission from main-sequence stars. The earliest detections of X-rays from T Tauri stars were made by the Einstein X-ray Observatory. For low-mass stars X-rays are generated by the heating of the stellar corona through magnetic reconnection, while for high-mass O and early B-type stars X-rays are generated through supersonic shocks in the stellar winds. Photons in the soft X-ray energy range covered by the Chandra X-ray Observatory and XMM-Newton may penetrate the interstellar medium with only moderate absorption due to gas, making the X-ray a useful wavelength for seeing the stellar populations within molecular clouds. X-ray emission as evidence of stellar youth makes this band particularly useful for performing censuses of stars in star-forming regions, given that not all young stars have infrared excesses. X-ray observations have provided near-complete censuses of all stellar-mass objects in the Orion Nebula Cluster and Taurus Molecular Cloud.\nThe formation of individual stars can only be directly observed in the Milky Way Galaxy, but in distant galaxies star formation has been detected through its unique spectral signature.\nInitial research indicates star-forming clumps start as giant, dense areas in turbulent gas-rich matter in young galaxies, live about 500 million years, and may migrate to the center of a galaxy, creating the central bulge of a galaxy.\nOn February 21, 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\nIn February 2018, astronomers reported, for the first time, a signal of the reionization epoch, an indirect detection of light from the earliest stars formed - about 180 million years after the Big Bang.\nAn article published on October 22, 2019, reported on the detection of 3MM-1, a massive star-forming galaxy about 12.5 billion light-years away that is obscured by clouds of dust. At a mass of about 1010.8 solar masses, it showed a star formation rate about 100 times as high as in the Milky Way.\n=== Notable pathfinder objects ===\nMWC 349 was first discovered in 1978, and is estimated to be only 1,000 years old.\nVLA 1623 – The first exemplar Class 0 protostar, a type of embedded protostar that has yet to accrete the majority of its mass. Found in 1993, is possibly younger than 10,000 years.\nL1014 – An extremely faint embedded object representative of a new class of sources that are only now being detected with the newest telescopes. Their status is still undetermined, they could be the youngest low-mass Class 0 protostars yet seen or even very low-mass evolved objects (like brown dwarfs or even rogue planets).\nGCIRS 8* – The youngest known main sequence star in the Galactic Center region, discovered in August 2006. It is estimated to be 3.5 million years old.\n== Low mass and high mass star formation ==\nStars of different masses are thought to form by slightly different mechanisms.  The theory of low-mass star formation, which is well-supported by observation, suggests that low-mass stars form by the gravitational collapse of rotating density enhancements within molecular clouds.  As described above, the collapse of a rotating cloud of gas and dust leads to the formation of an accretion disk through which matter is channeled onto a central protostar.  For stars with masses higher than about 8 M☉, however, the mechanism of star formation is not well understood.\nMassive stars emit copious quantities of radiation which pushes against infalling material.  In the past, it was thought that this radiation pressure might be substantial enough to halt accretion onto the massive protostar and prevent the formation of stars with masses more than a few tens of solar masses. Recent theoretical work has shown that the production of a jet and outflow clears a cavity through which much of the radiation from a massive protostar can escape without hindering accretion through the disk and onto the protostar. Present thinking is that massive stars may therefore be able to form by a mechanism similar to that by which low mass stars form.\nThere is mounting evidence that at least some massive protostars are indeed surrounded by accretion disks.  Disk accretion in high-mass protostars, similar to their low-mass counterparts, is expected to exhibit bursts of episodic accretion as a result of a gravitationally instability leading to clumpy and in-continuous accretion rates. Recent evidence of accretion bursts in high-mass protostars has indeed been confirmed observationally. Several other theories of massive star formation remain to be tested observationally.  Of these, perhaps the most prominent is the theory of competitive accretion, which suggests that massive protostars are "seeded" by low-mass protostars which compete with other protostars to draw in matter from the entire parent molecular cloud, instead of simply from a small local region.\nAnother theory of massive star formation suggests that massive stars may form by the coalescence of two or more stars of lower mass.\n== Filamentary nature of star formation ==\nRecent studies have emphasized the role of filamentary structures in molecular clouds as the initial conditions for star formation. Findings from the Herschel Space Observatory highlight the ubiquitous nature of these filaments in the cold interstellar medium (ISM). The spatial relationship between cores and filaments indicates that the majority of prestellar cores are located within 0.1 pc of supercritical filaments. This supports the hypothesis that filamentary structures act as pathways for the accumulation of gas and dust, leading to core formation.']

Question: What is the main factor that determines the occurrence of each type of supernova?

Choices:
Choice A) The star's distance from Earth
Choice B) The star's age
Choice C) The star's temperature
Choice D) The star's luminosity
Choice E) The progenitor star's metallicity

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Axiom', 'Almost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought that, in principle, every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.\nNon-logical axioms are often simply referred to as axioms in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For instance, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom, we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.\n==== Examples ====\nThis section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.\nBasic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo–Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann–Bernays–Gödel set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse–Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe is used, but in fact, most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.\nThe study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of abstract algebra brought with itself group theory, rings, fields, and Galois theory.\nThis list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.\n===== Arithmetic =====\nThe Peano axioms are the most widely used axiomatization of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed Gödel to establish his famous second incompleteness theorem.\nWe have a language\n{\\displaystyle {\\mathfrak {L}}_{NT}=\\{0,S\\}}\nwhere\n{\\displaystyle 0}\nis a constant symbol and\n{\\displaystyle S}\nis a unary function and the following axioms:\n{\\displaystyle \\forall x.\\lnot (Sx=0)}\n{\\displaystyle \\forall x.\\forall y.(Sx=Sy\\to x=y)}\n{\\displaystyle (\\phi (0)\\land \\forall x.\\,(\\phi (x)\\to \\phi (Sx)))\\to \\forall x.\\phi (x)}\nfor any\n{\\displaystyle {\\mathfrak {L}}_{NT}}\nformula\n{\\displaystyle \\phi }\nwith one free variable.\nThe standard structure is\n{\\displaystyle {\\mathfrak {N}}=\\langle \\mathbb {N} ,0,S\\rangle }\nwhere\n{\\displaystyle \\mathbb {N} }\nis the set of natural numbers,\n{\\displaystyle S}\nis the successor function and\n{\\displaystyle 0}\nis naturally interpreted as the number 0.\n===== Euclidean geometry =====\nProbably the oldest, and most famous, list of axioms are the 4 + 1 Euclid\'s postulates of plane geometry. The axioms are referred to as "4 + 1" because for nearly two millennia the fifth (parallel) postulate ("through a point outside a line there is exactly one parallel") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. One can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate ("a line can be extended indefinitely") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.\n===== Real analysis =====\nThe objectives of the study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a Dedekind complete ordered field, meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires the use of second-order logic. The Löwenheim–Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.\n=== Role in mathematical logic ===\n==== Deductive systems and completeness ====\nA deductive system consists of a set\n{\\displaystyle \\Lambda }\nof logical axioms, a set\n{\\displaystyle \\Sigma }\nof non-logical axioms, and a set\n{\\displaystyle \\{(\\Gamma ,\\phi )\\}}\nof rules of inference.  A desirable property of a deductive system is that it be complete.  A system is said to be complete if, for all formulas\n{\\displaystyle \\phi }\nthat is, for any statement that is a logical consequence of\n{\\displaystyle \\Sigma }\nthere actually exists a deduction of the statement from\n{\\displaystyle \\Sigma }\n.  This is sometimes expressed as "everything that is true is provable", but it must be understood that "true" here means "made true by the set of axioms", and not, for example, "true in the intended interpretation". Gödel\'s completeness theorem establishes the completeness of a certain commonly used type of deductive system.\nNote that "completeness" has a different meaning here than it does in the context of Gödel\'s first incompleteness theorem, which states that no recursive, consistent set of non-logical axioms\n{\\displaystyle \\Sigma }\nof the Theory of Arithmetic is complete, in the sense that there will always exist an arithmetic statement\n{\\displaystyle \\phi }\nsuch that neither\n{\\displaystyle \\phi }\nnor\n{\\displaystyle \\lnot \\phi }\ncan be proved from the given set of axioms.\nThere is thus, on the one hand, the notion of completeness of a deductive system and on the other hand that of completeness of a set of non-logical axioms.  The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.\n=== Further discussion ===\nEarly mathematicians regarded axiomatic geometry as a model of physical space, implying, there could ultimately only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details, and modern algebra was born.  In the modern view, axioms may be any set of formulas, as long as they are not known to be inconsistent.\n== See also ==\nAxiomatic system\nDogma\nFirst principle, axiom in science and philosophy\nList of axioms\nModel theory\nRegulæ Juris\nTheorem\nPresupposition\nPrinciple\n== Notes ==\n== References ==\n== Further reading ==\nMendelson, Elliot (1987). Introduction to mathematical logic. Belmont, California: Wadsworth & Brooks. ISBN 0-534-06624-0\nJohn Cook Wilson (1889), On an Evolutionist Theory of Axioms: inaugural lecture delivered October 15, 1889 (1st ed.), Oxford, Wikidata Q26720682{{citation}}:  CS1 maint: location missing publisher (link)\n== External links ==\nAxiom at PhilPapers\nAxiom at PlanetMath.\nMetamath axioms page', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', '{\\displaystyle L_{z}=m_{\\ell }\\hbar }\nThe values of mℓ range from −ℓ to ℓ, with integer intervals.\nThe s subshell (ℓ = 0) contains only one orbital, and therefore the mℓ of an electron in an s orbital will always be 0. The p subshell (ℓ = 1) contains three orbitals, so the mℓ of an electron in a p orbital will be −1, 0, or 1. The d subshell (ℓ = 2) contains five orbitals, with mℓ values of −2, −1, 0, 1, and 2.\n=== Spin magnetic quantum number ===\nThe spin magnetic quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis:\n{\\displaystyle S_{z}=m_{s}\\hbar }\nIn general, the values of ms range from −s to s, where s is the spin quantum number, associated with the magnitude of particle\'s intrinsic spin angular momentum:\n{\\displaystyle m_{s}=-s,-s+1,-s+2,\\cdots ,s-2,s-1,s}\nAn electron state has spin number s = \u20601/2\u2060, consequently ms will be +\u20601/2\u2060 ("spin up") or −\u20601/2\u2060 "spin down" states. Since electron are fermions they obey the Pauli exclusion principle: each electron state must have different quantum numbers.  Therefore, every orbital will be occupied with at most two electrons, one for each spin state.\n=== The Aufbau principle and Hund\'s Rules ===\nA multi-electron atom can be modeled qualitatively as a hydrogen like atom with higher nuclear charge and correspondingly more electrons. The occupation of the electron states in such an atom can be predicted by the Aufbau principle and Hund\'s empirical rules for the quantum numbers.  The Aufbau principle fills orbitals based on their principal and azimuthal quantum numbers (lowest n + l first, with lowest n breaking ties; Hund\'s rule favors unpaired electrons in the outermost orbital). These rules are empirical but they can be related to electron physics.:\u200a10\u200a:\u200a260\n== Spin-orbit coupled systems ==\nWhen one takes the spin–orbit interaction into consideration, the L and S operators no longer commute with the Hamiltonian, and the eigenstates of the system no longer have well-defined orbital angular momentum and spin. Thus another set of quantum numbers should be used. This set includes\nThe total angular momentum quantum number:\n{\\displaystyle j=|\\ell \\pm s|,}\nwhich gives the total angular momentum through the relation\n{\\displaystyle J^{2}=\\hbar ^{2}j(j+1).}\nThe projection of the total angular momentum along a specified axis:\n{\\displaystyle m_{j}=-j,-j+1,-j+2,\\cdots ,j-2,j-1,j}\nanalogous to the above and satisfies both\n{\\displaystyle m_{j}=m_{\\ell }+m_{s},}\nand\n{\\displaystyle |m_{\\ell }+m_{s}|\\leq j.}\nParityThis is the eigenvalue under reflection: positive (+1) for states which came from even ℓ and negative (−1) for states which came from odd ℓ. The former is also known as even parity and the latter as odd parity, and is given by\n{\\displaystyle P=(-1)^{\\ell }.}\nFor example, consider the following 8 states, defined by their quantum numbers:\nThe quantum states in the system can be described as linear combination of these 8 states. However, in the presence of spin–orbit interaction, if one wants to describe the same system by 8 states that are eigenvectors of the Hamiltonian (i.e. each represents a state that does not mix with others over time), we should consider the following 8 states:\n== Atomic nuclei ==\nIn nuclei, the entire assembly of protons and neutrons (nucleons) has a resultant angular momentum due to the angular momenta of each nucleon, usually denoted I. If the total angular momentum of a neutron is jn = ℓ + s and for a proton is jp = ℓ + s (where s for protons and neutrons happens to be \u20601/2\u2060 again (see note)), then the nuclear angular momentum quantum numbers I are given by:\n{\\displaystyle I=|j_{n}-j_{p}|,|j_{n}-j_{p}|+1,|j_{n}-j_{p}|+2,\\cdots ,(j_{n}+j_{p})-2,(j_{n}+j_{p})-1,(j_{n}+j_{p})}\nNote: The orbital angular momenta of the nuclear (and atomic) states are all integer multiples of ħ while the intrinsic angular momentum of the neutron and  proton are half-integer multiples.  It should be immediately apparent that the combination of the intrinsic spins of the nucleons with their orbital motion will always give half-integer values for the total spin, I, of any odd-A nucleus and integer values for any even-A nucleus.\nParity with the number I is used to label nuclear angular momentum states, examples for some isotopes of hydrogen (H), carbon (C), and sodium (Na) are;\nThe reason for the unusual fluctuations in I, even by differences of just one nucleon, are due to the odd and even numbers of protons and neutrons – pairs of nucleons have a total angular momentum of zero (just like electrons in orbitals), leaving an odd or even number of unpaired nucleons. The property of nuclear spin is an important factor for the operation of NMR spectroscopy in organic chemistry, and MRI in nuclear medicine, due to the nuclear magnetic moment interacting with an external magnetic field.\n== Elementary particles ==\nElementary particles contain many quantum numbers which are usually said to be intrinsic to them. However, it should be understood that the elementary particles are quantum states of the standard model of particle physics, and hence the quantum numbers of these particles bear the same relation to the Hamiltonian of this model as the quantum numbers of the Bohr atom does to its Hamiltonian. In other words, each quantum number denotes a symmetry of the problem. It is more useful in quantum field theory to distinguish between spacetime and internal symmetries.\nTypical quantum numbers related to spacetime symmetries are spin (related to rotational symmetry), the parity, C-parity and T-parity (related to the Poincaré symmetry of spacetime). Typical internal symmetries are lepton number and baryon number or the electric charge. (For a full list of quantum numbers of this kind see the article on flavour.)\n== Multiplicative quantum numbers ==\nMost conserved quantum numbers are additive, so in an elementary particle reaction, the sum of the quantum numbers should be the same before and after the reaction. However, some, usually called a parity, are multiplicative; i.e., their product is conserved. All multiplicative quantum numbers belong to a symmetry (like parity) in which applying the symmetry transformation twice is equivalent to doing nothing (involution).\n== See also ==\nElectron configuration\n== References ==\n== Further reading ==\nDirac, Paul A. M. (1982). Principles of Quantum Mechanics. Oxford University Press. ISBN 0-19-852011-5.\nGriffiths, David J. (2004). Introduction to Quantum Mechanics (2nd ed.). Prentice Hall. ISBN 0-13-805326-X.\nHalzen, Francis & Martin, Alan D. (1984). Quarks and Leptons: An Introductory Course in Modern Particle Physics. John Wiley & Sons. ISBN 0-471-88741-2.\nEisberg, Robert Martin; Resnick, Robert (1985). Quantum Physics of Atoms, Molecules, Solids, Nuclei and Particles (2nd ed.). John Wiley & Sons. ISBN 978-0-471-87373-0 – via Internet Archive.', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', '{\\displaystyle H=\\bigoplus _{\\lambda \\in \\sigma (T)}H_{\\lambda }\\,.}\nMoreover, if Eλ denotes the orthogonal projection onto the eigenspace Hλ, then\n{\\displaystyle T=\\sum _{\\lambda \\in \\sigma (T)}\\lambda E_{\\lambda }\\,,}\nwhere the sum converges with respect to the norm on B(H).\nThis theorem plays a fundamental role in the theory of integral equations, as many integral operators are compact, in particular those that arise from Hilbert–Schmidt operators.\nThe general spectral theorem for self-adjoint operators involves a kind of operator-valued Riemann–Stieltjes integral, rather than an infinite summation. The spectral family associated to T associates to each real number λ an operator Eλ, which is the projection onto the nullspace of the operator (T − λ)+, where the positive part of a self-adjoint operator is defined by\n{\\displaystyle A^{+}={\\tfrac {1}{2}}{\\Bigl (}{\\sqrt {A^{2}}}+A{\\Bigr )}\\,.}\nThe operators Eλ are monotone increasing relative to the partial order defined on self-adjoint operators; the eigenvalues correspond precisely to the jump discontinuities. One has the spectral theorem, which asserts\n{\\displaystyle T=\\int _{\\mathbb {R} }\\lambda \\,\\mathrm {d} E_{\\lambda }\\,.}\nThe integral is understood as a Riemann–Stieltjes integral, convergent with respect to the norm on B(H). In particular, one has the ordinary scalar-valued integral representation\n{\\displaystyle \\langle Tx,y\\rangle =\\int _{\\mathbb {R} }\\lambda \\,\\mathrm {d} \\langle E_{\\lambda }x,y\\rangle \\,.}\nA somewhat similar spectral decomposition holds for normal operators, although because the spectrum may now contain non-real complex numbers, the operator-valued Stieltjes measure dEλ must instead be replaced by a resolution of the identity.\nA major application of spectral methods is the spectral mapping theorem, which allows one to apply to a self-adjoint operator T any continuous complex function f defined on the spectrum of T by forming the integral\n{\\displaystyle f(T)=\\int _{\\sigma (T)}f(\\lambda )\\,\\mathrm {d} E_{\\lambda }\\,.}\nThe resulting continuous functional calculus has applications in particular to pseudodifferential operators.\nThe spectral theory of unbounded self-adjoint operators is only marginally more difficult than for bounded operators. The spectrum of an unbounded operator is defined in precisely the same way as for bounded operators: λ is a spectral value if the resolvent operator\n{\\displaystyle R_{\\lambda }=(T-\\lambda )^{-1}}\nfails to be a well-defined continuous operator. The self-adjointness of T still guarantees that the spectrum is real. Thus the essential idea of working with unbounded operators is to look instead at the resolvent Rλ where λ is nonreal. This is a bounded normal operator, which admits a spectral representation that can then be transferred to a spectral representation of T itself. A similar strategy is used, for instance, to study the spectrum of the Laplace operator: rather than address the operator directly, one instead looks as an associated resolvent such as a Riesz potential or Bessel potential.\nA precise version of the spectral theorem in this case is:\nThere is also a version of the spectral theorem that applies to unbounded normal operators.\n== In popular culture ==\nIn Gravity\'s Rainbow (1973), a novel by Thomas Pynchon, one of the characters is called "Sammy Hilbert-Spaess", a pun on "Hilbert Space". The novel refers also to Gödel\'s incompleteness theorems.\n== See also ==\n== Remarks ==\n== Notes ==\n== References ==\n== External links ==\n"Hilbert space", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nHilbert space at Mathworld\n245B, notes 5: Hilbert spaces by Terence Tao', 'for convex polyhedra to higher-dimensional polytopes:\n{\\displaystyle \\sum \\varphi =(-1)^{d-1}}\n== Generalisations of a polytope ==\n=== Infinite polytopes ===\nNot all manifolds are finite. Where a polytope is understood as a tiling or decomposition of a manifold,  this idea may be extended to infinite manifolds. plane tilings, space-filling (honeycombs) and hyperbolic tilings are in this sense polytopes, and are sometimes called apeirotopes because they have infinitely many cells.\nAmong these, there are regular forms including the regular skew polyhedra and the infinite series of tilings represented by the regular apeirogon, square tiling, cubic honeycomb, and so on.\n=== Abstract polytopes ===\nThe theory of abstract polytopes attempts to detach polytopes from the space containing them, considering their purely combinatorial properties. This allows the definition of the term to be extended to include objects for which it is difficult to define an intuitive underlying space, such as the 11-cell.\nAn abstract polytope is a partially ordered set of elements or members, which obeys certain rules. It is a purely algebraic structure, and the theory was developed in order to avoid some of the issues which make it difficult to reconcile the various geometric classes within a consistent mathematical framework. A geometric polytope is said to be a realization in some real space of the associated abstract polytope.\n=== Complex polytopes ===\nStructures analogous to polytopes exist in complex Hilbert spaces\n{\\displaystyle \\mathbb {C} ^{n}}\nwhere n real dimensions are accompanied by n imaginary ones. Regular complex polytopes are more appropriately treated as configurations.\n== Duality ==\nEvery n-polytope has a dual structure, obtained by interchanging its vertices for facets, edges for ridges, and so on generally interchanging its (j − 1)-dimensional elements for (n − j)-dimensional elements (for j = 1 to n − 1), while retaining the connectivity or incidence between elements.\nFor an abstract polytope, this simply reverses the ordering of the set. This reversal is seen in the Schläfli symbols for regular polytopes, where the symbol for the dual polytope is simply the reverse of the original. For example, {4, 3, 3} is dual to {3, 3, 4}.\nIn the case of a geometric polytope, some geometric rule for dualising is necessary, see for example the rules described for dual polyhedra. Depending on circumstance, the dual figure may or may not be another geometric polytope.\nIf the dual is reversed, then the original polytope is recovered. Thus, polytopes exist in dual pairs.\n=== Self-dual polytopes ===\nIf a polytope has the same number of vertices as facets, of edges as ridges, and so forth, and the same connectivities, then the dual figure will be similar to the original and the polytope is self-dual.\nSome common self-dual polytopes include:\nEvery regular n-simplex, in any number of dimensions, with Schläfli symbol {3n}. These include the equilateral triangle {3}, regular tetrahedron {3,3}, and 5-cell  {3,3,3}.\nEvery hypercubic honeycomb, in any number of dimensions. These include the apeirogon {∞}, square tiling {4,4} and cubic honeycomb {4,3,4}.\nNumerous compact, paracompact and noncompact hyperbolic tilings, such as the icosahedral honeycomb {3,5,3}, and order-5 pentagonal tiling {5,5}.\nIn 2 dimensions, all regular polygons (regular 2-polytopes)\nIn 3 dimensions, the canonical polygonal pyramids and elongated pyramids, and tetrahedrally diminished dodecahedron.\nIn 4 dimensions, the 24-cell, with Schläfli symbol {3,4,3}. Also the great 120-cell {5,5/2,5} and grand stellated 120-cell {5/2,5,5/2}.\n== History ==\nPolygons and polyhedra have been known since ancient times.\nAn early hint of higher dimensions came in 1827 when August Ferdinand Möbius discovered that two mirror-image solids can be superimposed by rotating one of them through a fourth mathematical dimension. By the 1850s, a handful of other mathematicians such as Arthur Cayley and Hermann Grassmann had also considered higher dimensions.\nLudwig Schläfli was the first to consider analogues of polygons and polyhedra in these higher spaces. He described the six convex regular 4-polytopes in 1852 but his work was not published until 1901, six years after his death. By 1854, Bernhard Riemann\'s Habilitationsschrift had firmly established the geometry of higher dimensions, and thus the concept of n-dimensional polytopes was made acceptable. Schläfli\'s polytopes were rediscovered many times in the following decades, even during his lifetime.\nIn 1882 Reinhold Hoppe, writing in German, coined the word polytop to refer to this more general concept of polygons and polyhedra. In due course Alicia Boole Stott, daughter of logician George Boole, introduced the anglicised polytope into the English language.:\u200avi\nIn 1895, Thorold Gosset not only rediscovered Schläfli\'s regular polytopes but also investigated the ideas of semiregular polytopes and space-filling tessellations in higher dimensions. Polytopes also began to be studied in non-Euclidean spaces such as hyperbolic space.\nAn important milestone was reached in 1948 with H. S. M. Coxeter\'s book Regular Polytopes, summarizing work to date and adding new findings of his own.\nMeanwhile, the French mathematician Henri Poincaré had developed the topological idea of a polytope as the piecewise decomposition (e.g. CW-complex) of a manifold. Branko Grünbaum published his influential work on Convex Polytopes in 1967.\nIn 1952 Geoffrey Colin Shephard generalised the idea as complex polytopes in complex space, where each real dimension has an imaginary one associated with it. Coxeter developed the theory further.\nThe conceptual issues raised by complex polytopes, non-convexity, duality and other phenomena led Grünbaum and others to the more general study of abstract combinatorial properties relating vertices, edges, faces and so on. A related idea was that of incidence complexes, which studied the incidence or connection of the various elements with one another. These developments led eventually to the theory of abstract polytopes as partially ordered sets, or posets, of such elements. Peter McMullen and Egon Schulte published their book Abstract Regular Polytopes in 2002.\nEnumerating the uniform polytopes, convex and nonconvex, in four or more dimensions remains an outstanding problem. The convex uniform 4-polytopes were fully enumerated by John Conway and Michael Guy using a computer in 1965; in higher dimensions this problem was still open as of 1997. The full enumeration for nonconvex uniform polytopes is not known in dimensions four and higher as of 2008.\nIn modern times, polytopes and related concepts have found many important applications in fields as diverse as computer graphics, optimization, search engines, cosmology, quantum mechanics and numerous other fields. In 2013 the amplituhedron was discovered as a simplifying construct in certain calculations of theoretical physics.\n== Applications ==\nIn the field of optimization, linear programming studies the maxima and minima of linear functions; these maxima and minima occur on the boundary of an n-dimensional polytope. In linear programming, polytopes occur in the use of generalized barycentric coordinates and slack variables.\nIn  twistor theory, a branch of theoretical physics, a polytope called the amplituhedron is used in to calculate the scattering amplitudes of subatomic particles when they collide. The construct is purely theoretical with no known physical manifestation, but is said to greatly simplify certain calculations.\n== See also ==\n== References ==\n=== Citations ===\n=== Bibliography ===\n== External links ==\nWeisstein, Eric W. "Polytope". MathWorld.\n"Math will rock your world" – application of polytopes to a database of articles used to support custom news feeds via the Internet – (Business Week Online)\nRegular and semi-regular convex polytopes a short historical overview:', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  ħωk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, θk†, θk, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the θk, and the Fock space is built by applying all products of creation operators θk† to |0⟩.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at φ = 0, so that the vacuum minimizing the Hamiltonian satisfies ⟨φ⟩ = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  φ = v .  This occurs for example, if V(φ) = gφ4 − 2m2φ2 with g > 0 and m2 > 0, for which the minimum energy is found at v = ±m/√g. The value of v in one of these vacua may be considered as condensate of the field φ. Canonical quantization then can be carried out for the shifted field  φ(x,t) − v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator  [A, B]  expressed in the phase space formulation is iħ{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  ħ-deformations are highly nonunique—quantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  ħ → 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an ħ-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional Schrödinger Hilbert space. If the chosen variables are complex, we get something like the Segal–Bargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', 'Classical mechanics', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups']

Question: What is the role of axioms in a formal theory?

Choices:
Choice A) Basis statements called axioms form the foundation of a formal theory and, together with the deducing rules, help in deriving a set of statements called theorems using proof theory.
Choice B) Axioms are supplementary statements added to a formal theory that break down otherwise complex statements into more simple ones.
Choice C) Axioms are redundant statements that can be derived from other statements in a formal theory, providing additional perspective to theorems derived from the theory.
Choice D) The axioms in a theory are used for experimental validation of the theorems derived from the statements in the theory.
Choice E) The axioms in a formal theory are added to prove that the statements derived from the theory are true, irrespective of their validity in the real world.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Leidenfrost effect', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', 'An atmosphere (from Ancient Greek  ἀτμός (atmós) \'vapour, steam\' and  σφαῖρα (sphaîra) \'sphere\') is a layer of gases that envelop an astronomical object, held in place by the gravity of the object. A planet retains an atmosphere when the gravity is great and the temperature of the atmosphere is low. A stellar atmosphere is the outer region of a star, which includes the layers above the opaque photosphere; stars of low temperature might have outer atmospheres containing compound molecules.\nThe atmosphere of Earth is composed of nitrogen (78%), oxygen (21%), argon (0.9%), carbon dioxide (0.04%) and trace gases. Most organisms use oxygen for respiration; lightning and bacteria perform nitrogen fixation which produces ammonia that is used to make nucleotides and amino acids; plants, algae, and cyanobacteria use carbon dioxide for photosynthesis. The layered composition of the atmosphere minimises the harmful effects of sunlight, ultraviolet radiation, solar wind, and cosmic rays and thus protects the organisms from genetic damage. The current composition of the atmosphere of the Earth is the product of billions of years of biochemical modification of the paleoatmosphere by living organisms.\n== Occurrence and compositions ==\n=== Origins ===\nAtmospheres are clouds of gas bound to and engulfing an astronomical focal point of sufficiently dominating mass, adding to its mass, possibly escaping from it or collapsing into it.\nBecause of the latter, such planetary nucleus can develop from interstellar molecular clouds or protoplanetary disks into rocky astronomical objects with varyingly thick atmospheres, gas giants or fusors.\nComposition and thickness is originally determined by the stellar nebula\'s chemistry and temperature, but can also by a product processes within the astronomical body outgasing a different atmosphere.\n=== Compositions ===\nThe atmospheres of the planets Venus and Mars are principally composed of carbon dioxide and nitrogen, argon and oxygen.\nThe composition of Earth\'s atmosphere is determined by the by-products of the life that it sustains. Dry air (mixture of gases) from Earth\'s atmosphere contains 78.08% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and traces of hydrogen, helium, and other "noble" gases (by volume), but generally a variable amount of water vapor is also present, on average about 1% at sea level.\nThe low temperatures and higher gravity of the Solar System\'s giant planets—Jupiter, Saturn, Uranus and Neptune—allow them more readily to retain gases with low molecular masses. These planets have hydrogen–helium atmospheres, with trace amounts of more complex compounds.\nTwo satellites of the outer planets possess significant atmospheres. Titan, a moon of Saturn, and Triton, a moon of Neptune, have atmospheres mainly of nitrogen. When in the part of its orbit closest to the Sun, Pluto has an atmosphere of nitrogen and methane similar to Triton\'s, but these gases are frozen when it is farther from the Sun.\nOther bodies within the Solar System have extremely thin atmospheres not in equilibrium. These include the Moon (sodium gas), Mercury (sodium gas), Europa (oxygen), Io (sulfur), and Enceladus (water vapor).\nThe first exoplanet whose atmospheric composition was determined is HD 209458b, a gas giant with a close orbit around a star in the constellation Pegasus. Its atmosphere is heated to temperatures over 1,000 K, and is steadily escaping into space. Hydrogen, oxygen, carbon and sulfur have been detected in the planet\'s inflated atmosphere.\n=== Atmospheres in the Solar System ===\nAtmosphere of the Sun\nAtmosphere of Mercury\nAtmosphere of Venus\nAtmosphere of Earth\nAtmosphere of the Moon\nAtmosphere of Mars\nAtmosphere of Ceres\nAtmosphere of Jupiter\nAtmosphere of Io\nAtmosphere of Callisto\nAtmosphere of Europa\nAtmosphere of Ganymede\nAtmosphere of Saturn\nAtmosphere of Titan\nAtmosphere of Enceladus\nAtmosphere of Uranus\nAtmosphere of Titania\nAtmosphere of Neptune\nAtmosphere of Triton\nAtmosphere of Pluto\n== Structure of atmosphere ==\n=== Earth ===\nThe atmosphere of Earth is composed of layers with different properties, such as specific gaseous composition, temperature, and pressure.\nThe troposphere is the lowest layer of the atmosphere. This extends from the planetary surface to the bottom of the stratosphere. The troposphere contains 75–80% of the mass of the atmosphere, and is the atmospheric layer wherein the weather occurs; the height of the troposphere varies between 17 km at the equator and 7.0 km at the poles.\nThe stratosphere extends from the top of the troposphere to the bottom of the mesosphere, and contains the ozone layer, at an altitude between 15 km and 35 km. It is the atmospheric layer that absorbs most of the ultraviolet radiation that Earth receives from the Sun.\nThe mesosphere ranges from 50 km to 85 km and is the layer wherein most meteors are incinerated before reaching the surface.\nThe thermosphere extends from an altitude of 85 km to the base of the exosphere at 690 km and contains the ionosphere, where solar radiation ionizes the atmosphere. The density of the ionosphere is greater at short distances from the planetary surface in the daytime and decreases as the ionosphere rises at night-time, thereby allowing a greater range of radio frequencies to travel greater distances.\nThe exosphere begins at 690 to 1,000 km from the surface, and extends to roughly 10,000 km, where it interacts with the magnetosphere of Earth.\n== Pressure ==\nAtmospheric pressure is the force (per unit-area) perpendicular to a unit-area of planetary surface, as determined by the weight of the vertical column of atmospheric gases. In said atmospheric model, the atmospheric pressure, the weight of the mass of the gas, decreases at high altitude because of the diminishing mass of the gas above the point of barometric measurement. The units of air pressure are based upon the standard atmosphere (atm), which is 101,325 Pa (equivalent to 760 Torr or 14.696 psi). The height at which the atmospheric pressure declines by a factor of e (an irrational number equal to 2.71828) is called the scale height (H). For an atmosphere of uniform temperature, the scale height is proportional to the atmospheric temperature and is inversely proportional to the product of the mean molecular mass of dry air, and the local acceleration of gravity at the point of barometric measurement.\n== Escape ==\nSurface gravity differs significantly among the planets. For example, the large gravitational force of the giant planet Jupiter retains light gases such as hydrogen and helium that escape from objects with lower gravity. Secondly, the distance from the Sun determines the energy available to heat atmospheric gas to the point where some fraction of its molecules\' thermal motion exceed the planet\'s escape velocity, allowing those to escape a planet\'s gravitational grasp. Thus, distant and cold Titan, Triton, and Pluto are able to retain their atmospheres despite their relatively low gravities.\nSince a collection of gas molecules may be moving at a wide range of velocities, there will always be some fast enough to produce a slow leakage of gas into space. Lighter molecules move faster than heavier ones with the same thermal kinetic energy, and so gases of low molecular weight are lost more rapidly than those of high molecular weight. It is thought that Venus and Mars may have lost much of their water when, after being photodissociated into hydrogen and oxygen by solar ultraviolet radiation, the hydrogen escaped. Earth\'s magnetic field helps to prevent this, as, normally, the solar wind would greatly enhance the escape of hydrogen. However, over the past 3 billion years Earth may have lost gases through the magnetic polar regions due to auroral activity, including a net 2% of its atmospheric oxygen. The net effect, taking the most important escape processes into account, is that an intrinsic magnetic field does not protect a planet from atmospheric escape and that for some magnetizations the presence of a magnetic field works to increase the escape rate.\nOther mechanisms that can cause atmosphere depletion are solar wind-induced sputtering, impact erosion, weathering, and sequestration—sometimes referred to as "freezing out"—into the regolith and polar caps.\n== Terrain ==\nAtmospheres have dramatic effects on the surfaces of rocky bodies. Objects that have no atmosphere, or that have only an exosphere, have terrain that is covered in craters. Without an atmosphere, the planet has no protection from meteoroids, and all of them collide with the surface as meteorites and create craters.\nFor planets with a significant atmosphere, most meteoroids burn up as meteors before hitting a planet\'s surface. When meteoroids do impact, the effects are often erased by the action of wind.\nWind erosion is a significant factor in shaping the terrain of rocky planets with atmospheres, and over time can erase the effects of both craters and volcanoes. In addition, since liquids cannot exist without pressure, an atmosphere allows liquid to be present at the surface, resulting in lakes, rivers and oceans. Earth and Titan are known to have liquids at their surface and terrain on the planet suggests that Mars had liquid on its surface in the past.\n=== Outside the Solar System ===\nAtmosphere of HD 209458 b\n== Circulation ==\nThe circulation of the atmosphere occurs due to thermal differences when convection becomes a more efficient transporter of heat than thermal radiation. On planets where the primary heat source is solar radiation, excess heat in the tropics is transported to higher latitudes. When a planet generates a significant amount of heat internally, such as is the case for Jupiter, convection in the atmosphere can transport thermal energy from the higher temperature interior up to the surface.\n== Importance ==', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', "== Importance ==\nFrom the perspective of a planetary geologist, the atmosphere acts to shape a planetary surface. Wind picks up dust and other particles which, when they collide with the terrain, erode the relief and leave deposits (eolian processes). Frost and precipitations, which depend on the atmospheric composition, also influence the relief. Climate changes can influence a planet's geological history. Conversely, studying the surface of the Earth leads to an understanding of the atmosphere and climate of other planets.\nFor a meteorologist, the composition of the Earth's atmosphere is a factor affecting the climate and its variations.\nFor a biologist or paleontologist, the Earth's atmospheric composition is closely dependent on the appearance of life and its evolution.\n== See also ==\nAtmometer (evaporimeter)\nAtmospheric pressure\nInternational Standard Atmosphere\nKármán line\nSky\n== References ==\n== Further reading ==\nSanchez-Lavega, Agustin (2010). An Introduction to Planetary Atmospheres. Taylor & Francis. ISBN 978-1420067323.\n== External links ==\nProperties of atmospheric strata – The flight environment of the atmosphere\nAtmosphere – Everything you need to know", 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'Atmosphere']

Question: What is the 'reactive Leidenfrost effect' observed in non-volatile materials?

Choices:
Choice A) The 'reactive Leidenfrost effect' is a phenomenon where solid particles float above hot surfaces and move erratically, observed in non-volatile materials.
Choice B) The 'reactive Leidenfrost effect' is a phenomenon where solid particles float above hot surfaces and move erratically, observed in volatile materials.
Choice C) The 'reactive Leidenfrost effect' is a phenomenon where solid particles sink into hot surfaces and move slowly, observed in non-volatile materials.
Choice D) The 'reactive Leidenfrost effect' is a phenomenon where solid particles float above cold surfaces and move erratically, observed in non-volatile materials.
Choice E) The 'reactive Leidenfrost effect' is a phenomenon where solid particles sink into cold surfaces and move slowly, observed in non-volatile materials.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Time', '=== Arrow of time ===\nUnlike space, where an object can travel in the opposite directions (and in 3 dimensions), time appears to have only one dimension and only one direction—the past lies behind, fixed and immutable, while the future lies ahead and is not necessarily fixed. Yet most laws of physics allow any process to proceed both forward and in reverse. There are only a few physical phenomena that violate the reversibility of time. This time directionality is known as the arrow of time. Acknowledged examples of the arrow of time are:\nRadiative arrow of time, manifested in waves (e.g., light and sound) travelling only expanding (rather than focusing) in time (see light cone);\nEntropic arrow of time: according to the second law of thermodynamics an isolated system evolves toward a larger disorder rather than orders spontaneously;\nQuantum arrow time, which is related to irreversibility of measurement in quantum mechanics according to the Copenhagen interpretation of quantum mechanics;\nWeak arrow of time: preference for a certain time direction of weak force in particle physics (see violation of CP symmetry);\nCosmological arrow of time, which follows the accelerated expansion of the Universe after the Big Bang.\nThe relationships between these different arrows of time is a hotly debated topic in theoretical physics.\nThe second law of thermodynamics states that entropy must increase over time. Brian Greene theorizes that, according to the equations, the change in entropy occurs symmetrically whether going forward or backward in time. So entropy tends to increase in either direction, and our current low-entropy universe is a statistical aberration, in a similar manner as tossing a coin often enough that eventually heads will result ten times in a row. However, this theory is not supported empirically in local experiment.\n=== Classical mechanics ===\nIn non-relativistic classical mechanics, Newton\'s concept of "relative, apparent, and common time" can be used in the formulation of a prescription for the synchronization of clocks. Events seen by two different observers in motion relative to each other produce a mathematical concept of time that works sufficiently well for describing the everyday phenomena of most people\'s experience. In the late nineteenth century, physicists encountered problems with the classical understanding of time, in connection with the behavior of electricity and magnetism. The 1860s Maxwell\'s equations described that light always travels at a constant speed (in a vacuum). However, classical mechanics assumed that motion was measured relative to a fixed reference frame. The Michelson–Morley experiment contradicted the assumption. Einstein later proposed a method of synchronizing clocks using the constant, finite speed of light as the maximum signal velocity. This led directly to the conclusion that observers in motion relative to one another measure different elapsed times for the same event.\n=== Spacetime ===\nTime has historically been closely related with space, the two together merging into spacetime in Einstein\'s special relativity and general relativity. According to these theories, the concept of time depends on the spatial reference frame of the observer, and the human perception, as well as the measurement by instruments such as clocks, are different for observers in relative motion. For example, if a spaceship carrying a clock flies through space at (very nearly) the speed of light, its crew does not notice a change in the speed of time on board their vessel because everything traveling at the same speed slows down at the same rate (including the clock, the crew\'s thought processes, and the functions of their bodies). However, to a stationary observer watching the spaceship fly by, the spaceship appears flattened in the direction it is traveling and the clock on board the spaceship appears to move very slowly.\nOn the other hand, the crew on board the spaceship also perceives the observer as slowed down and flattened along the spaceship\'s direction of travel, because both are moving at very nearly the speed of light relative to each other. Because the outside universe appears flattened to the spaceship, the crew perceives themselves as quickly traveling between regions of space that (to the stationary observer) are many light years apart. This is reconciled by the fact that the crew\'s perception of time is different from the stationary observer\'s; what seems like seconds to the crew might be hundreds of years to the stationary observer. In either case, however, causality remains unchanged: the past is the set of events that can send light signals to an entity and the future is the set of events to which an entity can send light signals.\n=== Dilation ===\nEinstein showed in his thought experiments that people travelling at different speeds, while agreeing on cause and effect, measure different time separations between events, and can even observe different chronological orderings between non-causally related events. Though these effects are typically minute in the human experience, the effect becomes much more pronounced for objects moving at speeds approaching the speed of light. Subatomic particles exist for a well-known average fraction of a second in a lab relatively at rest, but when travelling close to the speed of light they are measured to travel farther and exist for much longer than when at rest.\nAccording to the special theory of relativity, in the high-speed particle\'s frame of reference, it exists, on the average, for a standard amount of time known as its mean lifetime, and the distance it travels in that time is zero, because its velocity is zero. Relative to a frame of reference at rest, time seems to "slow down" for the particle. Relative to the high-speed particle, distances seem to shorten. Einstein showed how both temporal and spatial dimensions can be altered (or "warped") by high-speed motion.\nEinstein (The Meaning of Relativity): "Two events taking place at the points A and B of a system K are simultaneous if they appear at the same instant when observed from the middle point, M, of the interval AB. Time is then defined as the ensemble of the indications of similar clocks, at rest relative to K, which register the same simultaneously." Einstein wrote in his book, Relativity, that simultaneity is also relative, i.e., two events that appear simultaneous to an observer in a particular inertial reference frame need not be judged as simultaneous by a second observer in a different inertial frame of reference.\nAccording to general relativity, time also runs slower in stronger gravitational fields; this is gravitational time dilation. The effect of the dilation becomes more noticeable in a mass-dense object. A famous example of time dilation is a thought experiment of a subject approaching the event horizon of a black hole. As a consequence of how gravitational fields warp spacetime, the subject will experience gravitational time dilation. From the perspective of the subject itself, they will experience time normally. Meanwhile, an observer from the outside will see the subject move closer to the black hole until the extreme, in which the subject appears \'frozen\' in time and eventually fade to nothingness due to the diminishing amount of light returning.\n=== Relativistic versus Newtonian ===\nThe animations visualise the different treatments of time in the Newtonian and the relativistic descriptions. At the heart of these differences are the Galilean and Lorentz transformations applicable in the Newtonian and relativistic theories, respectively. In the figures, the vertical direction indicates time. The horizontal direction indicates distance (only one spatial dimension is taken into account), and the thick dashed curve is the spacetime trajectory ("world line") of the observer. The small dots indicate specific (past and future) events in spacetime. The slope of the world line (deviation from being vertical) gives the relative velocity to the observer.\nIn the Newtonian description these changes are such that time is absolute: the movements of the observer do not influence whether an event occurs in the \'now\' (i.e., whether an event passes the horizontal line through the observer). However, in the relativistic description the observability of events is absolute: the movements of the observer do not influence whether an event passes the "light cone" of the observer. Notice that with the change from a Newtonian to a relativistic description, the concept of absolute time is no longer applicable: events move up and down in the figure depending on the acceleration of the observer.\n=== Quantization ===\nTime quantization refers to the theory that time has the smallest possible unit. Time quantization is a hypothetical concept. In the modern established physical theories like the Standard Model of particle physics and general relativity time is not quantized. Planck time (~ 5.4 × 10−44 seconds) is the unit of time in the system of natural units known as Planck units. Current established physical theories are believed to fail at this time scale, and many physicists expect that the Planck time might be the smallest unit of time that could ever be measured, even in principle. Though tentative physical theories that attempt to describe phenomena at this scale exist; an example is loop quantum gravity. Loop quantum gravity suggests that time is quantized; if gravity is quantized, spacetime is also quantized.\n== Travel ==', '== Travel ==\nTime travel is the concept of moving backwards or forwards to different points in time, in a manner analogous to moving through space, and different from the normal "flow" of time to an earthbound observer. In this view, all points in time (including future times) "persist" in some way. Time travel has been a plot device in fiction since the 19th century. Travelling backwards or forwards in time has never been verified as a process, and doing so presents many theoretical problems and contradictory logic which to date have not been overcome. Any technological device, whether fictional or hypothetical, that is used to achieve time travel is known as a time machine.\nA central problem with time travel to the past is the violation of causality; should an effect precede its cause, it would give rise to the possibility of a temporal paradox. Some interpretations of time travel resolve this by accepting the possibility of travel between branch points, parallel realities, or universes. The many-worlds interpretation has been used as a way to solve causality paradoxes arising from time travel. Any quantum event creates another branching timeline, and all possible outcomes coexist without any wave function collapse. This interpretation was an alternative but is opposite from the Copenhagen interpretation, which suggests that wave functions do collapse. In science, hypothetical faster-than-light particles are known as tachyons; the mathematics of Einstein\'s relativity suggests that they would have an imaginary rest mass. Some interpretations suggest that it might move backward in time. General relativity permits the existence of closed timelike curves, which could allow an observer to travel back in time to the same space. Though for the Gödel metric, such an occurrence requires a globally rotating universe, which has been contradicted by observations of the redshifts of distant galaxies and the cosmic background radiation. However, it has been suggested that a slowly rotating universe model may solve the Hubble tension, so it can not yet be ruled out.\nAnother solution to the problem of causality-based temporal paradoxes is that such paradoxes cannot arise simply because they have not arisen. As illustrated in numerous works of fiction, free will either ceases to exist in the past or the outcomes of such decisions are predetermined. A famous example is the grandfather paradox, in which a person is supposed to travel back in time to kill their own grandfather. This would not be possible to enact because it is a historical fact that one\'s grandfather was not killed before his child (one\'s parent) was conceived. This view does not simply hold that history is an unchangeable constant, but that any change made by a hypothetical future time traveller would already have happened in their past, resulting in the reality that the traveller moves from. The Novikov self-consistency principle asserts that due to causality constraints, time travel to the past is impossible.\n== Perception ==\nThe specious present refers to the time duration wherein one\'s perceptions are considered to be in the present. The experienced present is said to be \'specious\' in that, unlike the objective present, it is an interval and not a durationless instant. The term specious present was first introduced by the psychologist E. R. Clay, and later developed by William James.\n=== Biopsychology ===\nThe brain\'s judgment of time is known to be a highly distributed system, including at least the cerebral cortex, cerebellum, and basal ganglia as its components. One particular component, the suprachiasmatic nuclei, is responsible for the circadian (or daily) rhythm, while other cell clusters appear capable of shorter-range (ultradian) timekeeping. Mental chronometry is the use of response time in perceptual-motor tasks to infer the content, duration, and temporal sequencing of cognitive operations. Judgments of time can be altered by temporal illusions (like the kappa effect), age, psychoactive drugs, and hypnosis. The sense of time is impaired in some people with neurological diseases such as Parkinson\'s disease and attention deficit disorder.\nPsychoactive drugs can impair the judgment of time. Stimulants can lead both humans and rats to overestimate time intervals, while depressants can have the opposite effect. The level of activity in the brain of neurotransmitters such as dopamine and norepinephrine may be the reason for this. Such chemicals will either excite or inhibit the firing of neurons in the brain, with a greater firing rate allowing the brain to register the occurrence of more events within a given interval (speed up time) and a decreased firing rate reducing the brain\'s capacity to distinguish events occurring within a given interval (slow down time).\nPsychologists assert that time seems to go faster with age, but the literature on this age-related perception of time remains controversial. Those who support this notion argue that young people, having more excitatory neurotransmitters, are able to cope with faster external events. Some also argued that the perception of time is also influenced by memory and how much one have experienced; for example, as one get older, they will have spend less part of their total life waiting a month. Meanwhile children\'s expanding cognitive abilities allow them to understand time in a different way. Two- and three-year-olds\' understanding of time is mainly limited to "now and not now". Five- and six-year-olds can grasp the ideas of past, present, and future. Seven- to ten-year-olds can use clocks and calendars. Socioemotional selectivity theory proposed that when people perceive their time as open-ended and nebulous, they focus more on future-oriented goals.\n=== Spatial conceptualization ===\nAlthough time is regarded as an abstract concept, there is increasing evidence that time is conceptualized in the mind in terms of space. That is, instead of thinking about time in a general, abstract way, humans think about time in a spatial way and mentally organize it as such. Using space to think about time allows humans to mentally organize temporal events in a specific way. This spatial representation of time is often represented in the mind as a mental timeline (MTL). These origins are shaped by many environmental factors. Literacy appears to play a large role in the different types of MTLs, as reading/writing direction provides an everyday temporal orientation that differs from culture to culture. In Western cultures, the MTL may unfold rightward (with the past on the left and the future on the right) since people mostly read and write from left to right. Western calendars also continue this trend by placing the past on the left with the future progressing toward the right. Conversely, speakers of Arabic, Farsi, Urdu, and Hebrew read from right to left, and their MTLs unfold leftward (past on the right with future on the left); evidence suggests these speakers organize time events in their minds like this as well.\nThere is also evidence that some cultures use an allocentric spatialization, often based on environmental features. A study of the indigenous Yupno people of Papua New Guinea found that they may use an allocentric MTL, in which time flows uphill; when speaking of the past, individuals gestured downhill, where the river of the valley flowed into the ocean. When speaking of the future, they gestured uphill, toward the source of the river. This was common regardless of which direction the person faced. A similar study of the Pormpuraawans, an aboriginal group in Australia, reported that when they were asked to organize photos of a man aging "in order," individuals consistently placed the youngest photos to the east and the oldest photos to the west, regardless of which direction they faced. This directly clashed with an American group that consistently organized the photos from left to right. Therefore, this group also appears to have an allocentric MTL, but based on the cardinal directions instead of geographical features. The wide array of distinctions in the way different groups think about time leads to the broader question that different groups may also think about other abstract concepts in different ways as well, such as causality and number.\n== Use ==\nIn sociology and anthropology, time discipline is the general name given to social and economic rules, conventions, customs, and expectations governing the measurement of time, the social currency and awareness of time measurements, and people\'s expectations concerning the observance of these customs by others. Arlie Russell Hochschild and Norbert Elias have written on the use of time from a sociological perspective.\nThe use of time is an important issue in understanding human behavior, education, and travel behavior. Time-use research is a developing field of study. The question concerns how time is allocated across a number of activities (such as time spent at home, at work, shopping, etc.). Time use changes with technology, as the television or the Internet created new opportunities to use time in different ways. However, some aspects of time use are relatively stable over long periods of time, such as the amount of time spent traveling to work, which despite major changes in transport, has been observed to be about 20–30 minutes one-way for a large number of cities over a long period.\nTime management is the organization of tasks or events by first estimating how much time a task requires and when it must be completed, and adjusting events that would interfere with its completion so it is done in the appropriate amount of time. Calendars and day planners are common examples of time management tools.\n== Sequence of events ==', 'Time is the continuous progression of existence that occurs in an apparently irreversible succession from the past, through the present, and into the future. It is a component quantity of various measurements used to sequence events, to compare the duration of events (or the intervals between them), and to quantify rates of change of quantities in material reality or in the conscious experience. Time is often referred to as a fourth dimension, along with three spatial dimensions.\nTime is one of the seven fundamental physical quantities in both the International System of Units (SI) and International System of Quantities. The SI base unit of time is the second, which is defined by measuring the electronic transition frequency of caesium atoms. General relativity is the primary framework for understanding how spacetime works. Through advances in both theoretical and experimental investigations of spacetime, it has been shown that time can be distorted and dilated, particularly at the edges of black holes.\nThroughout history, time has been an important subject of study in religion, philosophy, and science. Temporal measurement has occupied scientists and technologists, and has been a prime motivation in navigation and astronomy. Time is also of significant social importance, having economic value ("time is money") as well as personal value, due to an awareness of the limited time in each day ("carpe diem") and in human life spans.\n== Definition ==\nThe concept of time can be complex. Multiple notions exist, and defining time in a manner applicable to all fields without circularity has consistently eluded scholars. Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems. Traditional definitions of time involved the observation of periodic motion such as the apparent motion of the sun across the sky, the phases of the moon, and the passage of a free-swinging pendulum. More modern systems include the Global Positioning System, other satellite systems, Coordinated Universal Time and mean solar time. Although these systems differ from one another, with careful measurements they can be synchronized.\nIn physics, time is a fundamental concept to define other quantities, such as velocity. To avoid a circular definition, time in physics is operationally defined as "what a clock reads", specifically a count of repeating events such as the SI second. Although this aids in practical measurements, it does not address the essence of time. Physicists developed the concept of the spacetime continuum, where events are assigned four coordinates: three for space and one for time. Events like particle collisions, supernovas, or rocket launches have coordinates that may vary for different observers, making concepts like "now" and "here" relative. In general relativity, these coordinates do not directly correspond to the causal structure of events. Instead, the spacetime interval is calculated and classified as either space-like or time-like, depending on whether an observer exists that would say the events are separated by space or by time. Since the time required for light to travel a specific distance is the same for all observers—a fact first publicly demonstrated by the Michelson–Morley experiment—all observers will consistently agree on this definition of time as a causal relation.\nGeneral relativity does not address the nature of time for extremely small intervals where quantum mechanics holds. In quantum mechanics, time is treated as a universal and absolute parameter, differing from general relativity\'s notion of independent clocks. The problem of time consists of reconciling these two theories. As of 2025, there is no generally accepted theory of quantum general relativity.\n== Measurement ==\nMethods of temporal measurement, or chronometry, generally take two forms. The first is a calendar, a mathematical tool for organising intervals of time on Earth, consulted for periods longer than a day. The second is a clock, a physical mechanism that indicates the passage of time, consulted for periods less than a day. The combined measurement marks a specific moment in time from a reference point, or epoch.\n=== History of the calendar ===\nArtifacts from the Paleolithic suggest that the moon was used to reckon time as early as 6,000 years ago. Lunar calendars were among the first to appear, with years of either 12 or 13 lunar months (either 354 or 384 days). Without intercalation to add days or months to some years, seasons quickly drift in a calendar based solely on twelve lunar months. Lunisolar calendars have a thirteenth month added to some years to make up for the difference between a full year (now known to be about 365.24 days) and a year of just twelve lunar months. The numbers twelve and thirteen came to feature prominently in many cultures, at least partly due to this relationship of months to years.\nOther early forms of calendars originated in Mesoamerica, particularly in ancient Mayan civilization, in which they developed the Maya calendar, consisting of multiple interrelated calendars. These calendars were religiously and astronomically based; the Haab\' calendar has 18 months in a year and 20 days in a month, plus five epagomenal days at the end of the year. In conjunction, the Maya also used a 260-day sacred calendar called the Tzolk\'in.\nThe reforms of Julius Caesar in 45 BC put the Roman world on a solar calendar. This Julian calendar was faulty in that its intercalation still allowed the astronomical solstices and equinoxes to advance against it by about 11 minutes per year. Pope Gregory XIII introduced a correction in 1582; the Gregorian calendar was only slowly adopted by different nations over a period of centuries, but it is now by far the most commonly used calendar around the world.\nDuring the French Revolution, a new clock and calendar were invented as part of the dechristianization of France and to create a more rational system in order to replace the Gregorian calendar. The French Republican Calendar\'s days consisted of ten hours of a hundred minutes of a hundred seconds, which marked a deviation from the base 12 (duodecimal) system used in many other devices by many cultures. The system was abolished in 1806.\n=== History of other devices ===\nA large variety of devices have been invented to measure time. The study of these devices is called horology. They can be driven by a variety of means, including gravity, springs, and various forms of electrical power, and regulated by a variety of means.\nA sundial is any device that uses the direction of sunlight to cast shadows from a gnomon onto a set of markings calibrated to indicate the local time, usually to the hour. The idea to separate the day into smaller parts is credited to Egyptians because of their sundials, which operated on a duodecimal system. The importance of the number 12 is due to the number of lunar cycles in a year and the number of stars used to count the passage of night. Obelisks made as a gnomon were built as early as c.\u20093500 BC. An Egyptian device that dates to c.\u20091500 BC, similar in shape to a bent T-square, also measured the passage of time from the shadow cast by its crossbar on a nonlinear rule. The T was oriented eastward in the mornings. At noon, the device was turned around so that it could cast its shadow in the evening direction.\nAlarm clocks reportedly first appeared in ancient Greece c.\u2009250 BC with a water clock made by Plato that would set off a whistle. The hydraulic alarm worked by gradually filling a series of vessels with water. After some time, the water emptied out of a siphon. Inventor Ctesibius revised Plato\'s design; the water clock uses a float as the power drive system and uses a sundial to correct the water flow rate.\nIn medieval philosophical writings, the atom was a unit of time referred to as the smallest possible division of time. The earliest known occurrence in English is in Byrhtferth\'s Enchiridion (a science text) of 1010–1012, where it was defined as 1/564 of a momentum (11⁄2 minutes), and thus equal to 15/94 of a second. It was used in the computus, the process of calculating the date of Easter. The most precise timekeeping device of the ancient world was the water clock, or clepsydra, one of which was found in the tomb of Egyptian pharaoh Amenhotep I. They could be used to measure the hours even at night but required manual upkeep to replenish the flow of water. The ancient Greeks and the people from Chaldea (southeastern Mesopotamia) regularly maintained timekeeping records as an essential part of their astronomical observations. Arab inventors and engineers, in particular, made improvements on the use of water clocks up to the Middle Ages. In the 11th century, Chinese inventors and engineers invented the first mechanical clocks driven by an escapement mechanism.\nIncense sticks and candles were, and are, commonly used to measure time in temples and churches across the globe. Water clocks, and, later, mechanical clocks, were used to mark the events of the abbeys and monasteries of the Middle Ages. The passage of the hours at sea can also be marked by bell. The hours were marked by bells in abbeys as well as at sea. Richard of Wallingford (1292–1336), abbot of St. Alban\'s abbey, famously built a mechanical clock as an astronomical orrery about 1330. The hourglass uses the flow of sand to measure the flow of time. They were also used in navigation. Ferdinand Magellan used 18 glasses on each ship for his circumnavigation of the globe (1522). The English word clock probably comes from the Middle Dutch word klocke which, in turn, derives from the medieval Latin word clocca, which ultimately derives from Celtic and is cognate with French, Latin, and German words that mean bell.', 'Time management is the organization of tasks or events by first estimating how much time a task requires and when it must be completed, and adjusting events that would interfere with its completion so it is done in the appropriate amount of time. Calendars and day planners are common examples of time management tools.\n== Sequence of events ==\nA sequence of events, or series of events, is a sequence of items, facts, events, actions, changes, or procedural steps, arranged in time order (chronological order), often with causality relationships among the items. Because of causality, cause precedes effect, or cause and effect may appear together in a single item, but effect never precedes cause. A sequence of events can be presented in text, tables, charts, or timelines. The description of the items or events may include a timestamp. A sequence of events that includes the time along with place or location information to describe a sequential path may be referred to as a world line.\nUses of a sequence of events include stories, historical events (chronology), directions and steps in procedures, and timetables for scheduling activities. A sequence of events may also be used to help describe processes in science, technology, and medicine. A sequence of events may be focused on past events (e.g., stories, history, chronology), on future events that must be in a predetermined order (e.g., plans, schedules, procedures, timetables), or focused on the observation of past events with the expectation that the events will occur in the future (e.g., processes, projections). The use of a sequence of events occurs in fields as diverse as machines (cam timer), documentaries (Seconds From Disaster), law (choice of law), finance (directional-change intrinsic time), computer simulation (discrete event simulation), and electric power transmission (sequence of events recorder). A specific example of a sequence of events is the timeline of the Fukushima Daiichi nuclear disaster.\n== See also ==\nList of UTC timing centers\nLoschmidt\'s paradox\nTime metrology\n=== Organizations ===\nAntiquarian Horological Society – AHS (United Kingdom)\nChronometrophilia (Switzerland)\nDeutsche Gesellschaft für Chronometrie – DGC (Germany)\nNational Association of Watch and Clock Collectors – NAWCC (United States)\n== References ==\n== Further reading ==\n== External links ==\nDifferent systems of measuring time (archived 16 October 2015).\nTime on In Our Time at the BBC.\n"Time". Merriam-Webster.com Dictionary. Merriam-Webster.\nTime in the Internet Encyclopedia of Philosophy, by Bradley Dowden.\nLe Poidevin, Robin (Winter 2004). "The Experience and Perception of Time". In Edward N. Zalta (ed.). The Stanford Encyclopedia of Philosophy. Retrieved 9 April 2011.\nTime Expansion Experiences: Time may just be a creation of our minds by Steve Taylor Ph.D. January 31, 2025, Psychology Today.', 'Great advances in accurate time-keeping were made by Galileo Galilei and especially Christiaan Huygens with the invention of pendulum-driven clocks along with the invention of the minute hand by Jost Burgi. There is also a clock that was designed to keep time for 10,000 years called the Clock of the Long Now. Alarm clock devices were later mechanized. Levi Hutchins\'s alarm clock has been credited as the first American alarm clock, though it can only ring at 4 a.m. Antoine Redier was also credited as the first person to patent an adjustable mechanical alarm clock in 1847. Digital forms of alarm clocks became more accessible through digitization and integration with other technologies, such as smartphones.\nThe most accurate timekeeping devices are atomic clocks, which are accurate to seconds in many millions of years, and are used to calibrate other clocks and timekeeping instruments. Atomic clocks use the frequency of electronic transitions in certain atoms to measure the second. One of the atoms used is caesium; most modern atomic clocks probe caesium with microwaves to determine the frequency of these electron vibrations. Since 1967, the International System of Measurements bases its unit of time, the second, on the properties of caesium atoms. SI defines the second as 9,192,631,770 cycles of the radiation that corresponds to the transition between two electron spin energy levels of the ground state of the 133Cs atom. A portable timekeeper that meets certain precision standards is called a chronometer. Initially, the term was used to refer to the marine chronometer, a timepiece used to determine longitude by means of celestial navigation, a precision first achieved by John Harrison. More recently, the term has also been applied to the chronometer watch, a watch that meets precision standards set by the Swiss agency COSC.\nIn modern times, the Global Positioning System in coordination with the Network Time Protocol can be used to synchronize timekeeping systems across the globe. As of May 2010, the smallest time interval uncertainty in direct measurements is on the order of 12 attoseconds (1.2 × 10−17 seconds), about 3.7 × 1026 Planck times. The time measured was the delay caused by out-of-sync electron waves\' interference patterns.\n=== Units ===\nThe second (s) is the SI base unit. A minute (min) is 60 seconds in length (or, rarely, 59 or 61 seconds when leap seconds are employed), and an hour is 60 minutes or 3600 seconds in length. A day is usually 24 hours or 86,400 seconds in length; however, the duration of a calendar day can vary due to daylight saving time and leap seconds.\n== Time standards ==\nA time standard is a specification for measuring time: assigning a number or calendar date to an instant (point in time), quantifying the duration of a time interval, and establishing a chronology (ordering of events). In modern times, several time specifications have been officially recognized as standards, where formerly they were matters of custom and practice. The invention in 1955 of the caesium atomic clock has led to the replacement of older and purely astronomical time standards such as sidereal time and ephemeris time, for most practical purposes, by newer time standards based wholly or partly on atomic time using the SI second.\nInternational Atomic Time (TAI) is the primary international time standard from which other time standards are calculated. Universal Time (UT1) is mean solar time at 0° longitude, computed from astronomical observations. It varies from TAI because of the irregularities in Earth\'s rotation. Coordinated Universal Time (UTC) is an atomic time scale designed to approximate Universal Time. UTC differs from TAI by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the leap second. The Global Positioning System broadcasts a very precise time signal based on UTC time.\nThe surface of the Earth is split into a number of time zones. Standard time or civil time in a time zone deviates a fixed, round amount, usually a whole number of hours, from some form of Universal Time, usually UTC. Most time zones are exactly one hour apart, and by convention compute their local time as an offset from UTC. For example, time zones at sea are based on UTC. In many locations (but not at sea) these offsets vary twice yearly due to daylight saving time transitions.\nSome other time standards are used mainly for scientific work. Terrestrial Time is a theoretical ideal scale realized by TAI. Geocentric Coordinate Time and Barycentric Coordinate Time are scales defined as coordinate times in the context of the general theory of relativity, with TCG applying to Earth\'s center and TCB to the solar system\'s barycenter. Barycentric Dynamical Time is an older relativistic scale related to TCB that is still in use.\n== Philosophy ==\n=== Religion ===\n==== Cyclical views of time ====\nMany ancient cultures, particularly in the East, had a cyclical view of time. In these traditions, time was often seen as a recurring pattern of ages or cycles, where events and phenomena repeated themselves in a predictable manner. One of the most famous examples of this concept is found in Hindu philosophy, where time is depicted as a wheel called the "Kalachakra" or "Wheel of Time." According to this belief, the universe undergoes endless cycles of creation, preservation, and destruction.\nSimilarly, in other ancient cultures such as those of the Mayans, Aztecs, and Chinese, there were also beliefs in cyclical time, often associated with astronomical observations and calendars. These cultures developed complex systems to track time, seasons, and celestial movements, reflecting their understanding of cyclical patterns in nature and the universe.\nThe cyclical view of time contrasts with the linear concept of time more common in Western thought, where time is seen as progressing in a straight line from past to future without repetition.\n==== Time in Abrahamic religions ====\nIn general, the Islamic and Judeo-Christian world-view regards time as linear and directional, beginning with the act of creation by God. The traditional Christian view sees time ending, teleologically, with the eschatological end of the present order of things, the "end time". Though some Christian theologians (such as Augustine of Hippo and Aquinas) believe that God is outside of time, seeing all events simultaneously, that time did not exist before God, and that God created time.\nIn the Old Testament book Ecclesiastes, traditionally ascribed to Solomon (970–928 BC), time is depicted as cyclical and beyond human control. The book wrote that there is an appropriate season or time for every activity.\n==== Time in Greek mythology ====\nThe Greek language denotes two distinct principles, Chronos and Kairos. The former refers to numeric, or chronological, time. The latter, literally "the right or opportune moment", relates specifically to metaphysical or Divine time. In theology, Kairos is qualitative, as opposed to quantitative.\nIn Greek mythology, Chronos (ancient Greek: Χρόνος) is identified as the personification of time. His name in Greek means "time" and is alternatively spelled Chronus (Latin spelling) or Khronos. Chronos is usually portrayed as an old, wise man with a long, gray beard, such as "Father Time". Some English words whose etymological root is khronos/chronos include chronology, chronometer, chronic, anachronism, synchronise, and chronicle.\n==== Time in Kabbalah & Rabbinical thought ====\nRabbis sometimes saw time like "an accordion that was expanded and collapsed at will." According to Kabbalists, "time" is a paradox and an illusion.\n==== Time in Advaita Vedanta ====\nAccording to Advaita Vedanta, time is integral to the phenomenal world, which lacks independent reality. Time and the phenomenal world are products of maya, influenced by our senses, concepts, and imaginations. The phenomenal world, including time, is seen as impermanent and characterized by plurality, suffering, conflict, and division. Since phenomenal existence is dominated by temporality (kala), everything within time is subject to change and decay. Overcoming pain and death requires knowledge that transcends temporal existence and reveals its eternal foundation.\n=== In Western philosophy ===\nTwo contrasting viewpoints on time divide prominent philosophers. One view is that time is part of the fundamental structure of the universe—a dimension independent of events, in which events occur in sequence. Isaac Newton subscribed to this realist view, and hence it is sometimes referred to as Newtonian time.\nThe opposing view is that time does not refer to any kind of "container" that events and objects "move through", nor to any entity that "flows", but that it is instead part of a fundamental intellectual structure (together with space and number) within which humans sequence and compare events. This second view, in the tradition of Gottfried Leibniz and Immanuel Kant, holds that time is neither an event nor a thing, and thus is not itself measurable nor can it be travelled. Furthermore, it may be that there is a subjective component to time, but whether or not time itself is "felt", as a sensation, or is a judgment, is a matter of debate.', 'Time zone', 'An early use of the piezoelectricity of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. The quartz clock is a familiar device using the mineral. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.\nAlmost all the industrial demand for quartz crystal (used primarily in electronics) is met with synthetic quartz produced by the hydrothermal process. However, synthetic crystals are less prized for use as gemstones. The popularity of crystal healing has increased the demand for natural quartz crystals, which are now often mined in developing countries using primitive mining methods, sometimes involving child labor.\n== See also ==\nFused quartz\nList of minerals\nQuartz fiber\nQuartz reef mining\nQuartzolite\nShocked quartz\n== References ==\n== External links ==\nQuartz varieties, properties, crystal morphology. Photos and illustrations\nGilbert Hart, "Nomenclature of Silica", American Mineralogist, Volume 12, pp. 383–395. 1927\n"The Quartz Watch – Inventors". The Lemelson Center, National Museum of American History, Smithsonian Institution. Archived from the original on 7 January 2009.\nTerminology used to describe the characteristics of quartz crystals when used as oscillators\nQuartz use as prehistoric stone tool raw material', '==== Microsoft Windows ====\nWindows-based computer systems prior to Windows 95 and Windows NT used local time, but Windows 95 and later, and Windows NT, base system time on UTC. They allow a program to fetch the system time as UTC, represented as a year, month, day, hour, minute, second, and millisecond; Windows 95 and later, and Windows NT 3.5 and later, also allow the system time to be fetched as a count of 100 ns units since 1601-01-01 00:00:00 UTC. The system registry contains time zone information that includes the offset from UTC and rules that indicate the start and end dates for daylight saving in each zone. Interaction with the user normally uses local time, and application software is able to calculate the time in various zones. Terminal Servers allow remote computers to redirect their time zone settings to the Terminal Server so that users see the correct time for their time zone in their desktop/application sessions. Terminal Services uses the server base time on the Terminal Server and the client time zone information to calculate the time in the session.\n=== Programming languages ===\n==== Java ====\nWhile most application software will use the underlying operating system for time zone and daylight saving time rule information, the Java Platform, from version 1.3.1, has maintained its own database of time zone and daylight saving time rule information. This database is updated whenever time zone or daylight saving time rules change. Oracle provides an updater tool for this purpose.\nAs an alternative to the information bundled with the Java Platform, programmers may choose to use the Joda-Time library. This library includes its own data based on the IANA time zone database.\nAs of Java 8 there is a new date and time API that can help with converting times.\n==== JavaScript ====\nTraditionally, there was very little in the way of time zone support for JavaScript. Essentially the programmer had to extract the UTC offset by instantiating a time object, getting a GMT time from it, and differencing the two. This does not provide a solution for more complex daylight saving variations, such as divergent DST directions between northern and southern hemispheres.\nECMA-402, the standard on Internationalization API for JavaScript, provides ways of formatting Time Zones. However, due to size constraint, some implementations or distributions do not include it.\n==== Perl ====\nThe DateTime object in Perl supports all entries in the IANA time zone database and includes the ability to get, set and convert between time zones.\n==== PHP ====\nThe DateTime objects and related functions have been compiled into the PHP core since 5.2. This includes the ability to get and set the default script time zone, and DateTime is aware of its own time zone internally. PHP.net provides extensive documentation on this. As noted there, the most current time zone database can be implemented via the PECL timezonedb.\n==== Python ====\nThe standard module datetime included with Python stores and operates on the time zone information class tzinfo. The third party pytz module provides access to the full IANA time zone database. Negated time zone offset in seconds is stored time.timezone and time.altzone attributes. From Python 3.9, the zoneinfo module introduces timezone management without need for third party module.\n==== Smalltalk ====\nEach Smalltalk dialect comes with its own built-in classes for dates, times and timestamps, only a few of which implement the DateAndTime and Duration classes as specified by the ANSI Smalltalk Standard. VisualWorks provides a TimeZone class that supports up to two annually recurring offset transitions, which are assumed to apply to all years (same behavior as Windows time zones). Squeak provides a Timezone class that does not support any offset transitions. Dolphin Smalltalk does not support time zones at all.\nFor full support of the tz database (zoneinfo) in a Smalltalk application (including support for any number of annually recurring offset transitions, and support for different intra-year offset transition rules in different years) the third-party, open-source, ANSI-Smalltalk-compliant Chronos Date/Time Library is available for use with any of the following Smalltalk dialects: VisualWorks, Squeak, Gemstone, or Dolphin.\n== Time in outer space ==\nOrbiting spacecraft may experience many sunrises and sunsets, or none, in a 24-hour period. Therefore, it is not possible to calibrate the time with respect to the Sun and still respect a 24-hour sleep/wake cycle. A common practice for space exploration is to use the Earth-based time of the launch site or mission control, synchronizing the sleeping cycles of the crew and controllers. The International Space Station normally uses Greenwich Mean Time (GMT).\nTimekeeping on Mars can be more complex, since the planet has a solar day of approximately 24 hours and 40 minutes, known as a sol. Earth controllers for some Mars missions have synchronized their sleep/wake cycles with the Martian day, when specifically solar-powered rover activity occurs.\n== See also ==\nJet lag\nLists of time zones\nMetric time\nTime by country\nTime in Europe\nAbolition of time zones – Replacing time zones with UTC\nWorld clock – Clock that displays the times in various locations around the globe\nInternational Date Line – Imaginary line that demarcates the change of one calendar day to the next\n== Notes ==\n== References ==\n== Sources ==\nAsimov, Isaac (1964). "Abbe, Cleveland". Asimov\'s Biographical Encyclopedia of Science and Technology: The Living Stories of More than 1000 Great Scientists from the Age of Greece to the Space Age. Garden City, NY: Doubleday & Company, Inc. pp. 343–344. LCCN 64016199.\nDebus, Allen G., ed. (1968). "Abbe, Cleveland". World Who\'s Who in Science: A Biographical Dictionary of Notable Scientists from Antiquity to the Present (1st ed.). Chicago, IL: A. N. Marquis Company. ISBN 0-8379-1001-3. LCCN 68056149.\n== Further reading ==\nBiswas, Soutik (February 12, 2019). "How India\'s single time zone is hurting its people". BBC News. Retrieved February 12, 2019.\nMaulik Jagnani, economist at Cornell University (January 15, 2019). "Poor Sleep: Sunset Time and Human Capital Production" (Job Market Paper). Retrieved April 28, 2025.\n"Time Bandits: The countries rebelling against GMT" (Video). BBC News. August 14, 2015. Retrieved February 12, 2019.\n"How time zones confused the world". BBC News. August 7, 2015. Retrieved February 12, 2019.\nLane, Megan (May 10, 2011). "How does a country change its time zone?". BBC News. Retrieved February 12, 2019.\n"A brief history of time zones" (Video). BBC News. March 24, 2011. Retrieved February 12, 2019.\nThe Time Zone Information Format (TZif). doi:10.17487/RFC8536. RFC 8536.\n== External links ==\nMedia related to Time zones at Wikimedia Commons', 'Technosignatures can be divided into three broad categories: astroengineering projects, signals of planetary origin, and spacecraft within and outside the Solar System.\nAn astroengineering installation such as a Dyson sphere, designed to convert all of the incident radiation of its host star into energy, could be detected through the observation of an infrared excess from a solar analog star, or by the star\'s apparent disappearance in the visible spectrum over several years. After examining some 100,000 nearby large galaxies, a team of researchers has concluded that none of them display any obvious signs of highly advanced technological civilizations.\nAnother hypothetical form of astroengineering, the Shkadov thruster, moves its host star by reflecting some of the star\'s light back on itself, and would be detected by observing if its transits across the star abruptly end with the thruster in front. Asteroid mining within the Solar System is also a detectable technosignature of the first kind.\nIndividual extrasolar planets can be analyzed for signs of technology. Avi Loeb of the Center for Astrophysics | Harvard & Smithsonian has proposed that persistent light signals on the night side of an exoplanet can be an indication of the presence of cities and an advanced civilization. In addition, the excess infrared radiation and chemicals produced by various industrial processes or terraforming efforts may point to intelligence.\nLight and heat detected from planets need to be distinguished from natural sources to conclusively prove the existence of civilization on a planet. However, as argued by the Colossus team, a civilization heat signature should be within a "comfortable" temperature range, like terrestrial urban heat islands, i.e., only a few degrees warmer than the planet itself. In contrast, such natural sources as wild fires, volcanoes, etc. are significantly hotter, so they will be well distinguished by their maximum flux at a different wavelength.\nOther than astroengineering, technosignatures such as artificial satellites around exoplanets, particularly such in geostationary orbit, might be detectable even with today\'s technology and data, and would allow, similar to fossils on Earth, to find traces of extrasolar life from long ago.\nExtraterrestrial craft are another target in the search for technosignatures. Magnetic sail interstellar spacecraft should be detectable over thousands of light-years of distance through the synchrotron radiation they would produce through interaction with the interstellar medium; other interstellar spacecraft designs may be detectable at more modest distances. In addition, robotic probes within the Solar System are also being sought with optical and radio searches.\nFor a sufficiently advanced civilization, hyper energetic neutrinos from Planck scale accelerators should be detectable at a distance of many Mpc.\n=== Advances for Bio and Technosignature Detection ===\nA notable advancement in technosignature detection is the development of an algorithm for signal reconstruction in zero-knowledge one-way communication channels. This algorithm decodes signals from unknown sources without prior knowledge of the encoding scheme, using principles from Algorithmic Information Theory to identify the geometric and topological dimensions of the encoding space. It successfully reconstructed the Arecibo message despite significant noise. The work establishes a connection between syntax and semantics in SETI and technosignature detection, enhancing fields like cryptography and Information Theory.\nBased on fractal theory and the Weierstrass function, a known fractal, another method authored by the same group called fractal messaging offers a framework for space-time scale-free communication. This method leverages properties of self-similarity and scale invariance, enabling spatio-temporal scale-independent and parallel infinite-frequency communication. It also embodies the concept of sending a self-encoding/self-decoding signal as a mathematical formula, equivalent to self-executable computer code that unfolds to read a message at all possible time scales and in all possible channels simultaneously.\n== Fermi paradox ==\nItalian physicist Enrico Fermi suggested in the 1950s that if technologically advanced civilizations are common in the universe, then they should be detectable in one way or another. According to those who were there, Fermi either asked "Where are they?" or "Where is everybody?"\nThe Fermi paradox is commonly understood as asking why extraterrestrials have not visited Earth, but the same reasoning applies to the question of why signals from extraterrestrials have not been heard. The SETI version of the question is sometimes referred to as "the Great Silence".\nThe Fermi paradox can be stated more completely as follows:\nThe size and age of the universe incline us to believe that many technologically advanced civilizations must exist. However, this belief seems logically inconsistent with our lack of observational evidence to support it. Either (1) the initial assumption is incorrect and technologically advanced intelligent life is much rarer than we believe, or (2) our current observations are incomplete, and we simply have not detected them yet, or (3) our search methodologies are flawed and we are not searching for the correct indicators, or (4) it is the nature of intelligent life to destroy itself.\nThere are multiple explanations proposed for the Fermi paradox, ranging from analyses suggesting that intelligent life is rare (the "Rare Earth hypothesis"), to analyses suggesting that although extraterrestrial civilizations may be common, they would not communicate with us, would communicate in a way we have not discovered yet, could not travel across interstellar distances, or destroy themselves before they master the technology of either interstellar travel or communication.\nThe German astrophysicist and radio astronomer Sebastian von Hoerner suggested that the average duration of civilization was 6,500 years. After this time, according to him, it disappears for external reasons (the destruction of life on the planet, the destruction of only rational beings) or internal causes (mental or physical degeneration). According to his calculations, on a habitable planet (one in three million stars) there is a sequence of technological species over a time distance of hundreds of millions of years, and each of them "produces" an average of four technological species. With these assumptions, the average distance between civilizations in the Milky Way is 1,000 light years.\nScience writer Timothy Ferris has posited that since galactic societies are most likely only transitory, an obvious solution is an interstellar communications network, or a type of library consisting mostly of automated systems. They would store the cumulative knowledge of vanished civilizations and communicate that knowledge through the galaxy. Ferris calls this the "Interstellar Internet", with the various automated systems acting as network "servers". If such an Interstellar Internet exists, the hypothesis states, communications between servers are mostly through narrow-band, highly directional radio or laser links. Intercepting such signals is, as discussed earlier, very difficult. However, the network could maintain some broadcast nodes in hopes of making contact with new civilizations.\nAlthough somewhat dated in terms of "information culture" arguments, not to mention the obvious technological problems of a system that could work effectively for billions of years and requires multiple lifeforms agreeing on certain basics of communications technologies, this hypothesis is actually testable (see below).\n=== Difficulty of detection ===\nA significant problem is the vastness of space. Despite piggybacking on the world\'s most sensitive radio telescope, astronomer and initiator of SERENDIP Charles Stuart Bowyer noted the then world\'s largest instrument could not detect random radio noise emanating from a civilization like ours, which has been leaking radio and TV signals for less than 100 years. For SERENDIP and most other SETI projects to detect a signal from an extraterrestrial civilization, the civilization would have to be beaming a powerful signal directly at us. It also means that Earth civilization will only be detectable within a distance of 100 light-years.\n== Post-detection disclosure protocol ==\nThe International Academy of Astronautics (IAA) has a long-standing SETI Permanent Study Group (SPSG, formerly called the IAA SETI Committee), which addresses matters of SETI science, technology, and international policy. The SPSG meets in conjunction with the International Astronautical Congress (IAC), held annually at different locations around the world, and sponsors two SETI Symposia at each IAC. In 2005, the IAA established the SETI: Post-Detection Science and Technology Taskgroup (chairman, Professor Paul Davies) "to act as a Standing Committee to be available to be called on at any time to advise and consult on questions stemming from the discovery of a putative signal of extraterrestrial intelligent (ETI) origin."\nHowever, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".']

Question: What is the SI base unit of time and how is it defined?

Choices:
Choice A) The SI base unit of time is the week, which is defined by measuring the electronic transition frequency of caesium atoms.
Choice B) The SI base unit of time is the second, which is defined by measuring the electronic transition frequency of caesium atoms.
Choice C) The SI base unit of time is the hour, which is defined by measuring the electronic transition frequency of caesium atoms.
Choice D) The SI base unit of time is the day, which is defined by measuring the electronic transition frequency of caesium atoms.
Choice E) The SI base unit of time is the minute, which is defined by measuring the electronic transition frequency of caesium atoms.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['An early use of the piezoelectricity of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. The quartz clock is a familiar device using the mineral. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.\nAlmost all the industrial demand for quartz crystal (used primarily in electronics) is met with synthetic quartz produced by the hydrothermal process. However, synthetic crystals are less prized for use as gemstones. The popularity of crystal healing has increased the demand for natural quartz crystals, which are now often mined in developing countries using primitive mining methods, sometimes involving child labor.\n== See also ==\nFused quartz\nList of minerals\nQuartz fiber\nQuartz reef mining\nQuartzolite\nShocked quartz\n== References ==\n== External links ==\nQuartz varieties, properties, crystal morphology. Photos and illustrations\nGilbert Hart, "Nomenclature of Silica", American Mineralogist, Volume 12, pp. 383–395. 1927\n"The Quartz Watch – Inventors". The Lemelson Center, National Museum of American History, Smithsonian Institution. Archived from the original on 7 January 2009.\nTerminology used to describe the characteristics of quartz crystals when used as oscillators\nQuartz use as prehistoric stone tool raw material', 'Quartz', 'Crystallography', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', "Planck's law", 'Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word κρύσταλλος (krústallos; "clear ice, rock-crystal"), and γράφειν (gráphein; "to write"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well new developments in these fields.\n== History and timeline ==\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the Davisson–Germer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n== Methodology ==\nCrystallographic methods depend mainly on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. Crystallographers often explicitly state the type of beam used, as in the terms X-ray diffraction, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.\nX-rays interact with the spatial distribution of electrons in the sample.\nNeutrons are scattered by the atomic nuclei through the strong nuclear forces, but in addition the magnetic moment of neutrons is non-zero, so they are also scattered by magnetic fields. When neutrons are scattered from hydrogen-containing materials, they produce diffraction patterns with high noise levels, which can sometimes be resolved by substituting deuterium for hydrogen.\nElectrons are charged particles and therefore interact with the total charge distribution of both the atomic nuclei and the electrons of the sample.:\u200aChpt 4\nIt is hard to focus x-rays or neutrons, but since electrons are charged they can be focused and are used in electron microscope to produce magnified images. There are many ways that transmission electron microscopy and related techniques such as scanning transmission electron microscopy, high-resolution electron microscopy can be used to obtain images with in many cases atomic resolution from which crystallographic information can be obtained. There are also other methods such as low-energy electron diffraction, low-energy electron microscopy and reflection high-energy electron diffraction which can be used to obtain crystallographic information about surfaces.\n== Applications in various areas ==\n=== Materials science ===\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Most materials do not occur as a single crystal, but are poly-crystalline in nature (they exist as an aggregate of small crystals with different orientations). As such, powder diffraction techniques, which take diffraction patterns of samples with a large number of crystals, play an important role in structural determination.\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements. Crystallographic studies help elucidate the relationship between a material\'s structure and its properties, aiding in developing new materials with tailored characteristics. This understanding is crucial in various fields, including metallurgy, geology, and materials science. Advancements in crystallographic techniques, such as electron diffraction and X-ray crystallography, continue to expand our understanding of material behavior at the atomic level.\nIn another example, iron transforms from a body-centered cubic (bcc) structure called ferrite to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material\'s properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which structures are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory.\n=== Biology ===\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. The double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol, Pymol or VMD can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron diffraction has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n== Notation ==\nCoordinates in square brackets such as [100] denote a direction vector (in real space).\nCoordinates in angle brackets or chevrons such as <100> denote a family of directions which are related by symmetry operations. In the cubic crystal system for example, <100> would mean [100], [010], [001] or the negative of any of those directions.\nMiller indices in parentheses such as (100) denote a plane of the crystal structure, and regular repetitions of that plane with a particular spacing. In the cubic system, the normal to the (hkl) plane is the direction [hkl], but in lower-symmetry cases, the normal to (hkl) is not parallel to [hkl].\nIndices in curly brackets or braces such as {100} denote a family of planes and their normals. In cubic materials the symmetry makes them equivalent, just as the way angle brackets denote a family of directions. In non-cubic materials, <hkl> is not necessarily perpendicular to {hkl}.\n== Reference literature ==\nThe International Tables for Crystallography is an eight-book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure through x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and do not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.']

Question: What is the piezoelectric strain coefficient for AT-cut quartz crystals?

Choices:
Choice A) d = 1.9·10‑12 m/V
Choice B) d = 3.1·10‑12 m/V
Choice C) d = 4.2·10‑12 m/V
Choice D) d = 2.5·10‑12 m/V
Choice E) d = 5.8·10‑12 m/V

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Brown dwarf', "In more massive stars, the fusion of neon proceeds without a runaway deflagration.  This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements.  Surrounding the core are shells of lighter elements still undergoing fusion.  The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged.  The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope.  The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars.  Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself.  The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.\n==== Supernova ====\nWhen the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole.  Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\n== Stellar remnants ==\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n=== White and black dwarfs ===\nFor a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years.\nThe chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n=== Neutron stars ===\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n=== Black holes ===\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉.", 'Stellar evolution is the process by which a star changes over the course of time. Depending on the mass of the star, its lifetime can range from a few million years for the most massive to trillions of years for the least massive, which is considerably longer than the current age of the universe. The table shows the lifetimes of stars as a function of their masses. All stars are formed from collapsing clouds of gas and dust, often called nebulae or molecular clouds.  Over the course of millions of years, these protostars settle down into a state of equilibrium, becoming what is known as a main-sequence star.\nNuclear fusion powers a star for most of its existence. Initially the energy is generated by the fusion of hydrogen atoms at the core of the main-sequence star. Later, as the preponderance of atoms at the core becomes helium, stars like the Sun begin to fuse hydrogen along a spherical shell surrounding the core. This process causes the star to gradually grow in size, passing through the subgiant stage until it reaches the red-giant phase. Stars with at least half the mass of the Sun can also begin to generate energy through the fusion of helium at their core, whereas more-massive stars can fuse heavier elements along a series of concentric shells. Once a star like the Sun has exhausted its nuclear fuel, its core collapses into a dense white dwarf and the outer layers are expelled as a planetary nebula. Stars with around ten or more times the mass of the Sun can explode in a supernova as their inert iron cores collapse into an extremely dense neutron star or black hole. Although the universe is not old enough for any of the smallest red dwarfs to have reached the end of their existence, stellar models suggest they will slowly become brighter and hotter before running out of hydrogen fuel and becoming low-mass white dwarfs.\nStellar evolution is not studied by observing the life of a single star, as most stellar changes occur too slowly to be detected, even over many centuries. Instead, astrophysicists come to understand how stars evolve by observing numerous stars at various points in their lifetime, and by simulating stellar structure using computer models.\n== Star formation ==\n=== Protostar ===\nStellar evolution starts with the gravitational collapse of a giant molecular cloud. Typical giant molecular clouds are roughly 100 light-years (9.5×1014 km) across and contain up to 6,000,000 solar masses (1.2×1037 kg). As it collapses, a giant molecular cloud breaks into smaller and smaller pieces. In each of these fragments, the collapsing gas releases gravitational potential energy as heat. As its temperature and pressure increase, a fragment condenses into a rotating ball of superhot gas known as a protostar.  Filamentary structures are truly ubiquitous in the molecular cloud. Dense molecular filaments will fragment into gravitationally bound cores, which are the precursors of stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed fragmentation manner of the filaments. In supercritical filaments, observations have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded two protostars with gas outflows.\nA protostar continues to grow by accretion of gas and dust from the molecular cloud, becoming a pre-main-sequence star as it reaches its final mass. Further development is determined by its mass. Mass is typically compared to the mass of the Sun: 1.0 M☉ (2.0×1030 kg) means 1 solar mass.\nProtostars are encompassed in dust, and are thus more readily visible at infrared wavelengths.\nObservations from the Wide-field Infrared Survey Explorer (WISE) have been especially important for unveiling numerous galactic protostars and their parent star clusters.\n=== Brown dwarfs and sub-stellar objects ===\nProtostars with masses less than roughly 0.08 M☉ (1.6×1029 kg) never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (MJ), 2.5 × 1028 kg, or 0.0125 M☉). Objects smaller than 13 MJ are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets). Both types, deuterium-burning and not, shine dimly and fade away slowly, cooling gradually over hundreds of millions of years.\n=== Main sequence stellar mass objects ===\nFor a more-massive protostar, the core temperature will eventually reach 10 million kelvin, initiating the proton–proton chain reaction and allowing hydrogen to fuse, first to deuterium and then to helium. In stars of slightly over 1 M☉ (2.0×1030 kg), the carbon–nitrogen–oxygen fusion reaction (CNO cycle) contributes a large portion of the energy generation. The onset of nuclear fusion leads relatively quickly to a hydrostatic equilibrium in which energy released by the core maintains a high gas pressure, balancing the weight of the star\'s matter and preventing further gravitational collapse. The star thus evolves rapidly to a stable state, beginning the main-sequence phase of its evolution.\nA new star will sit at a specific point on the main sequence of the Hertzsprung–Russell diagram, with the main-sequence spectral type depending upon the mass of the star. Small, relatively cold, low-mass red dwarfs fuse hydrogen slowly and will remain on the main sequence for hundreds of billions of years or longer, whereas massive, hot O-type stars will leave the main sequence after just a few million years. A mid-sized yellow dwarf star, like the Sun, will remain on the main sequence for about 10 billion years. The Sun is thought to be in the middle of its main sequence lifespan.\n=== Planetary system ===\nA star may gain a protoplanetary disk, which furthermore can develop into a planetary system.\n== Mature stars ==\nEventually the star\'s core exhausts its supply of hydrogen and the star begins to evolve off the main sequence. Without the outward radiation pressure generated by the fusion of hydrogen to counteract the force of gravity, the core contracts until either electron degeneracy pressure becomes sufficient to oppose gravity or the core becomes hot enough (around 100 MK) for helium fusion to begin. Which of these happens first depends upon the star\'s mass.\n=== Low-mass stars ===\nWhat happens after a low-mass star ceases to produce energy through fusion has not been directly observed; the universe is around 13.8 billion years old, which is less time (by several orders of magnitude, in some cases) than it takes for fusion to cease in such stars.\nRecent astrophysical models suggest that red dwarfs of 0.1 M☉ may stay on the main sequence for some six to twelve trillion years, gradually increasing in both temperature and luminosity, and take several hundred billion years more to collapse, slowly, into a white dwarf.  Such stars will not become red giants as the whole star is a convection zone and it will not develop a degenerate helium core with a shell burning hydrogen.  Instead, hydrogen fusion will proceed until almost the whole star is helium.\nSlightly more massive stars do expand into red giants, but their helium cores are not massive enough to reach the temperatures required for helium fusion so they never reach the tip of the red-giant branch.  When hydrogen shell burning finishes, these stars move directly off the red-giant branch like a post-asymptotic-giant-branch (AGB) star, but at lower luminosity, to become a white dwarf.  A star with an initial mass about 0.6 M☉ will be able to reach temperatures high enough to fuse helium, and these "mid-sized" stars go on to further stages of evolution beyond the red-giant branch.\n=== Mid-sized stars ===\nStars of roughly 0.6–10 M☉ become red giants, which are large non-main-sequence stars of stellar classification K or M. Red giants lie along the right edge of the Hertzsprung–Russell diagram due to their red color and large luminosity. Examples include Aldebaran in the constellation Taurus and Arcturus in the constellation of Boötes.\nMid-sized stars are red giants during two different phases of their post-main-sequence evolution: red-giant-branch stars, with inert cores made of helium and hydrogen-burning shells, and asymptotic-giant-branch stars, with inert cores made of carbon and helium-burning shells inside the hydrogen-burning shells.  Between these two phases, stars spend a period on the horizontal branch with a helium-fusing core.  Many of these helium-fusing stars cluster towards the cool end of the horizontal branch as K-type giants and are referred to as red clump giants.\n==== Subgiant phase ====\nWhen a star exhausts the hydrogen in its core, it leaves the main sequence and begins to fuse hydrogen in a shell outside the core.  The core increases in mass as the shell produces more helium.  Depending on the mass of the helium core, this continues for several million to one or two billion years, with the star expanding and cooling at a similar or slightly lower luminosity to its main sequence state.  Eventually either the core becomes degenerate, in stars around the mass of the sun, or the outer layers cool sufficiently to become opaque, in more massive stars.  Either of these changes cause the hydrogen shell to increase in temperature and the luminosity of the star to increase, at which point the star expands onto the red-giant branch.\n==== Red-giant-branch phase ====', "==== Red-giant-branch phase ====\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star.  For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower 12C/13C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\nThe helium core continues to grow on the red-giant branch.  It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase.  The star increases in luminosity towards the tip of the red-giant branch.  Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n==== Horizontal branch ====\nIn the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 108 times the luminosity of the Sun for a few days and 1011 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning.  These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n==== Asymptotic-giant-branch phase ====\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time).  Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star.  Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically.  This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface.  This is known as the second dredge up, and in some stars there may even be a third dredge up.  In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra.  A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\nAnother well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths.  These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity.  These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n==== Post-AGB ====\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n=== Massive stars ===\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants.  Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.\n==== Supergiant evolution ====\nExtremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\nThe core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core.  In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process.  At the end of helium fusion, the core of a star consists primarily of carbon and oxygen.  In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium.  Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.\nThe exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉.  After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse.  Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning.  For a range of stars of approximately 8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova.", 'Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.', "When both rates of movement are known, the space velocity of the star relative to the Sun or the galaxy can be computed. Among nearby stars, it has been found that younger population I stars have generally lower velocities than older, population II stars. The latter have elliptical orbits that are inclined to the plane of the galaxy. A comparison of the kinematics of nearby stars has allowed astronomers to trace their origin to common points in giant molecular clouds; such groups with common points of origin are referred to as stellar associations.\n=== Magnetic field ===\nThe magnetic field of a star is generated within regions of the interior where convective circulation occurs. This movement of conductive plasma functions like a dynamo, wherein the movement of electrical charges induce magnetic fields, as does a mechanical dynamo. Those magnetic fields have a great range that extend throughout and beyond the star. The strength of the magnetic field varies with the mass and composition of the star, and the amount of magnetic surface activity depends upon the star's rate of rotation. This surface activity produces starspots, which are regions of strong magnetic fields and lower than normal surface temperatures. Coronal loops are arching magnetic field flux lines that rise from a star's surface into the star's outer atmosphere, its corona. The coronal loops can be seen due to the plasma they conduct along their length. Stellar flares are bursts of high-energy particles that are emitted due to the same magnetic activity.\nYoung, rapidly rotating stars tend to have high levels of surface activity because of their magnetic field. The magnetic field can act upon a star's stellar wind, functioning as a brake to gradually slow the rate of rotation with time. Thus, older stars such as the Sun have a much slower rate of rotation and a lower level of surface activity. The activity levels of slowly rotating stars tend to vary in a cyclical manner and can shut down altogether for periods of time. During the Maunder Minimum, for example, the Sun underwent a 70-year period with almost no sunspot activity.\n=== Mass ===\nStars have masses ranging from less than half the solar mass to over 200 solar masses (see List of most massive stars). One of the most massive stars known is Eta Carinae, which, with 100–150 times as much mass as the Sun, will have a lifespan of only several million years. Studies of the most massive open clusters suggests 150 M☉ as a rough upper limit for stars in the current era of the universe. This represents an empirical value for the theoretical limit on the mass of forming stars due to increasing radiation pressure on the accreting gas cloud. Several stars in the R136 cluster in the Large Magellanic Cloud have been measured with larger masses, but it has been determined that they could have been created through the collision and merger of massive stars in close binary systems, sidestepping the 150 M☉ limit on massive star formation.\nThe first stars to form after the Big Bang may have been larger, up to 300 M☉, due to the complete absence of elements heavier than lithium in their composition. This generation of supermassive population III stars is likely to have existed in the very early universe (i.e., they are observed to have a high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life. In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at z = 6.60.\nWith a mass only 80 times that of Jupiter (MJ), 2MASS J0523-1403 is the smallest known star undergoing nuclear fusion in its core. For stars with metallicity similar to the Sun, the theoretical minimum mass the star can have and still undergo fusion at the core, is estimated to be about 75 MJ. When the metallicity is very low, the minimum star size seems to be about 8.3% of the solar mass, or about 87 MJ. Smaller bodies called brown dwarfs, occupy a poorly defined grey area between stars and gas giants.\nThe combination of the radius and the mass of a star determines its surface gravity. Giant stars have a much lower surface gravity than do main sequence stars, while the opposite is the case for degenerate, compact stars such as white dwarfs. The surface gravity can influence the appearance of a star's spectrum, with higher gravity causing a broadening of the absorption lines.\n=== Rotation ===\nThe rotation rate of stars can be determined through spectroscopic measurement, or more exactly determined by tracking their starspots. Young stars can have a rotation greater than 100 km/s at the equator. The B-class star Achernar, for example, has an equatorial velocity of about 225 km/s or greater, causing its equator to bulge outward and giving it an equatorial diameter that is more than 50% greater than between the poles. This rate of rotation is just below the critical velocity of 300 km/s at which speed the star would break apart. By contrast, the Sun rotates once every 25–35 days depending on latitude, with an equatorial velocity of 1.93 km/s. A main sequence star's magnetic field and the stellar wind serve to slow its rotation by a significant amount as it evolves on the main sequence.\nDegenerate stars have contracted into a compact mass, resulting in a rapid rate of rotation. However they have relatively low rates of rotation compared to what would be expected by conservation of angular momentum—the tendency of a rotating body to compensate for a contraction in size by increasing its rate of spin. A large portion of the star's angular momentum is dissipated as a result of mass loss through the stellar wind. In spite of this, the rate of rotation for a pulsar can be very rapid. The pulsar at the heart of the Crab nebula, for example, rotates 30 times per second. The rotation rate of the pulsar will gradually slow due to the emission of radiation.\n=== Temperature ===\nThe surface temperature of a main sequence star is determined by the rate of energy production of its core and by its radius, and is often estimated from the star's color index. The temperature is normally given in terms of an effective temperature, which is the temperature of an idealized black body that radiates its energy at the same luminosity per surface area as the star. The effective temperature is only representative of the surface, as the temperature increases toward the core. The temperature in the core region of a star is several million kelvins.\nThe stellar temperature will determine the rate of ionization of various elements, resulting in characteristic absorption lines in the spectrum. The surface temperature of a star, along with its visual absolute magnitude and absorption features, is used to classify a star (see classification below).\nMassive main sequence stars can have surface temperatures of 50,000 K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600 K; but they have a high luminosity due to their large exterior surface area.\n== Radiation ==\nThe energy produced by stars, a product of nuclear fusion, radiates to space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind, which streams from the outer layers as electrically charged protons and alpha and beta particles. A steady stream of almost massless neutrinos emanate directly from the star's core.\nThe production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers.\nThe color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the star's outer layers, including its photosphere. Besides visible light, stars emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics.\nUsing the stellar spectrum, astronomers can determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is found, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.) With these parameters, astronomers can estimate the age of the star.\n=== Luminosity ===\nThe luminosity of a star is the amount of light and other forms of radiant energy it radiates per unit of time. It has units of power. The luminosity of a star is determined by its radius and surface temperature. Many stars do not radiate uniformly across their entire surface. The rapidly rotating star Vega, for example, has a higher energy flux (power per unit area) at its poles than along its equator.", "A supernova explosion blows away the star's outer layers, leaving a remnant such as the Crab Nebula. The core is compressed into a neutron star, which sometimes manifests itself as a pulsar or X-ray burster. In the case of the largest stars, the remnant is a black hole greater than 4 M☉. In a neutron star the matter is in a state known as neutron-degenerate matter, with a more exotic form of degenerate matter, QCD matter, possibly present in the core.\nThe blown-off outer layers of dying stars include heavy elements, which may be recycled during the formation of new stars. These heavy elements allow the formation of rocky planets. The outflow from supernovae and the stellar wind of large stars play an important part in shaping the interstellar medium.\n==== Binary stars ====\nBinary stars' evolution may significantly differ from that of single stars of the same mass. For example, when any star expands to become a red giant, it may overflow its Roche lobe, the surrounding region where material is gravitationally bound to it; if stars in a binary system are close enough, some of that material may overflow to the other star, yielding phenomena including contact binaries, common-envelope binaries, cataclysmic variables, blue stragglers, and type Ia supernovae. Mass transfer leads to cases such as the Algol paradox, where the most-evolved star in a system is the least massive.\nThe evolution of binary star and higher-order star systems is intensely researched since so many stars have been found to be members of binary systems. Around half of Sun-like stars, and an even higher proportion of more massive stars, form in multiple systems, and this may greatly influence such phenomena as novae and supernovae, the formation of certain types of star, and the enrichment of space with nucleosynthesis products.\nThe influence of binary star evolution on the formation of evolved massive stars such as luminous blue variables, Wolf–Rayet stars, and the progenitors of certain classes of core collapse supernova is still disputed. Single massive stars may be unable to expel their outer layers fast enough to form the types and numbers of evolved stars that are observed, or to produce progenitors that would explode as the supernovae that are observed. Mass transfer through gravitational stripping in binary systems is seen by some astronomers as the solution to that problem.\n== Distribution ==\nStars are not spread uniformly across the universe but are normally grouped into galaxies along with interstellar gas and dust. A typical large galaxy like the Milky Way contains hundreds of billions of stars. There are more than 2 trillion (1012) galaxies, though most are less than 10% the mass of the Milky Way. Overall, there are likely to be between 1022 and 1024 stars, which are more stars than all the grains of sand on planet Earth. Most stars are within galaxies, but between 10 and 50% of the starlight in large galaxy clusters may come from stars outside of any galaxy.\nA multi-star system consists of two or more gravitationally bound stars that orbit each other. The simplest and most common multi-star system is a binary star, but systems of three or more stars exist. For reasons of orbital stability, such multi-star systems are often organized into hierarchical sets of binary stars. Larger groups are called star clusters. These range from loose stellar associations with only a few stars to open clusters with dozens to thousands of stars, up to enormous globular clusters with hundreds of thousands of stars. Such systems orbit their host galaxy. The stars in an open or globular cluster all formed from the same giant molecular cloud, so all members normally have similar ages and compositions.\nMany stars are observed, and most or all may have originally formed in gravitationally bound, multiple-star systems. This is particularly true for very massive O and B class stars, 80% of which are believed to be part of multiple-star systems. The proportion of single star systems increases with decreasing star mass, so that only 25% of red dwarfs are known to have stellar companions. As 85% of all stars are red dwarfs, more than two thirds of stars in the Milky Way are likely single red dwarfs. In a 2017 study of the Perseus molecular cloud, astronomers found that most of the newly formed stars are in binary systems. In the model that best explained the data, all stars initially formed as binaries, though some binaries later split up and leave single stars behind.\nThe nearest star to the Earth, apart from the Sun, is Proxima Centauri, 4.2465 light-years (40.175 trillion kilometres) away. Travelling at the orbital speed of the Space Shuttle, 8 kilometres per second (29,000 kilometres per hour), it would take about 150,000 years to arrive. This is typical of stellar separations in galactic discs. Stars can be much closer to each other in the centres of galaxies and in globular clusters, or much farther apart in galactic halos.\nDue to the relatively vast distances between stars outside the galactic nucleus, collisions between stars are thought to be rare. In denser regions such as the core of globular clusters or the galactic center, collisions can be more common. Such collisions can produce what are known as blue stragglers. These abnormal stars have a higher surface temperature and thus are bluer than stars at the main sequence turnoff in the cluster to which they belong; in standard stellar evolution, blue stragglers would already have evolved off the main sequence and thus would not be seen in the cluster.\n== Characteristics ==\nAlmost everything about a star is determined by its initial mass, including such characteristics as luminosity, size, evolution, lifespan, and its eventual fate.\n=== Age ===\nMost stars are between 1 billion and 10 billion years old. Some stars may even be close to 13.8 billion years old—the observed age of the universe. The oldest star yet discovered, HD 140283, nicknamed Methuselah star, is an estimated 14.46 ± 0.8 billion years old. (Due to the uncertainty in the value, this age for the star does not conflict with the age of the universe, determined by the Planck satellite as 13.799 ± 0.021).\nThe more massive the star, the shorter its lifespan, primarily because massive stars have greater pressure on their cores, causing them to burn hydrogen more rapidly. The most massive stars last an average of a few million years, while stars of minimum mass (red dwarfs) burn their fuel very slowly and can last tens to hundreds of billions of years.\n=== Chemical composition ===\nWhen stars form in the present Milky Way galaxy, they are composed of about 71% hydrogen and 27% helium, as measured by mass, with a small fraction of heavier elements. Typically the portion of heavy elements is measured in terms of the iron content of the stellar atmosphere, as iron is a common element and its absorption lines are relatively easy to measure. The portion of heavier elements may be an indicator of the likelihood that the star has a planetary system.\nAs of 2005 the star with the lowest iron content ever measured is the dwarf HE1327-2326, with only 1/200,000th the iron content of the Sun. By contrast, the super-metal-rich star μ Leonis has nearly double the abundance of iron as the Sun, while the planet-bearing star 14 Herculis has nearly triple the iron. Chemically peculiar stars show unusual abundances of certain elements in their spectrum; especially chromium and rare earth elements. Stars with cooler outer atmospheres, including the Sun, can form various diatomic and polyatomic molecules.\n=== Diameter ===\nDue to their great distance from the Earth, all stars except the Sun appear to the unaided eye as shining points in the night sky that twinkle because of the effect of the Earth's atmosphere. The Sun is close enough to the Earth to appear as a disk instead, and to provide daylight. Other than the Sun, the star with the largest apparent size is R Doradus, with an angular diameter of only 0.057 arcseconds.\nThe disks of most stars are much too small in angular size to be observed with current ground-based optical telescopes, so interferometer telescopes are required to produce images of these objects. Another technique for measuring the angular size of stars is through occultation. By precisely measuring the drop in brightness of a star as it is occulted by the Moon (or the rise in brightness when it reappears), the star's angular diameter can be computed.\nStars range in size from neutron stars, which vary anywhere from 20 to 40 km (25 mi) in diameter, to supergiants like Betelgeuse in the Orion constellation, which has a diameter about 640 times that of the Sun with a much lower density.\n=== Kinematics ===\nThe motion of a star relative to the Sun can provide useful information about the origin and age of a star, as well as the structure and evolution of the surrounding galaxy. The components of motion of a star consist of the radial velocity toward or away from the Sun, and the traverse angular movement, which is called its proper motion.\nRadial velocity is measured by the doppler shift of the star's spectral lines and is given in units of km/s. The proper motion of a star, its parallax, is determined by precise astrometric measurements in units of milli-arc seconds (mas) per year. With knowledge of the star's parallax and its distance, the proper motion velocity can be calculated. Together with the radial velocity, the total velocity can be calculated. Stars with high rates of proper motion are likely to be relatively close to the Sun, making them good candidates for parallax measurements.", '=== Progenitor ===\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic type Ic-BL events that are associated with long-duration gamma-ray bursts.\n== External impact ==\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\n=== Source of heavy elements ===\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\n=== Role in stellar evolution ===\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n=== Cosmic rays ===\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\n=== Gravitational waves ===', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', '=== Spirals ===\nSpiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept.\nSpiral galaxies consist of a rotating disk of stars and interstellar medium, along with a central bulge of generally older stars. Extending outward from the bulge are relatively bright arms. In the Hubble classification scheme, spiral galaxies are listed as type S, followed by a letter (a, b, or c) which indicates the degree of tightness of the spiral arms and the size of the central bulge. An Sa galaxy has tightly wound, poorly defined arms and possesses a relatively large core region. At the other extreme, an Sc galaxy has open, well-defined arms and a small core region. A galaxy with poorly defined arms is sometimes referred to as a flocculent spiral galaxy; in contrast to the grand design spiral galaxy that has prominent and well-defined spiral arms. The speed in which a galaxy rotates is thought to correlate with the flatness of the disc as some spiral galaxies have thick bulges, while others are thin and dense.\nIn spiral galaxies, the spiral arms do have the shape of approximate logarithmic spirals, a pattern that can be theoretically shown to result from a disturbance in a uniformly rotating mass of stars. Like the stars, the spiral arms rotate around the center, but they do so with constant angular velocity. The spiral arms are thought to be areas of high-density matter, or "density waves". As stars move through an arm, the space velocity of each stellar system is modified by the gravitational force of the higher density. (The velocity returns to normal after the stars depart on the other side of the arm.) This effect is akin to a "wave" of slowdowns moving along a highway full of moving cars. The arms are visible because the high density facilitates star formation, and therefore they harbor many bright and young stars.\n==== Barred spiral galaxy ====\nA majority of spiral galaxies, including the Milky Way galaxy, have a linear, bar-shaped band of stars that extends outward to either side of the core, then merges into the spiral arm structure. In the Hubble classification scheme, these are designated by an SB, followed by a lower-case letter (a, b or c) which indicates the form of the spiral arms (in the same manner as the categorization of normal spiral galaxies). Bars are thought to be temporary structures that can occur as a result of a density wave radiating outward from the core, or else due to a tidal interaction with another galaxy. Many barred spiral galaxies are active, possibly as a result of gas being channeled into the core along the arms.\nOur own galaxy, the Milky Way, is a large disk-shaped barred-spiral galaxy about 30 kiloparsecs in diameter and a kiloparsec thick. It contains about two hundred billion (2×1011) stars and has a total mass of about six hundred billion (6×1011) times the mass of the Sun.\n==== Super-luminous spiral ====\nRecently, researchers described galaxies called super-luminous spirals. They are very large with an upward diameter of 437,000 light-years (compared to the Milky Way\'s 87,400 light-year diameter). With a mass of 340 billion solar masses, they generate a significant amount of ultraviolet and mid-infrared light. They are thought to have an increased star formation rate around 30 times faster than the Milky Way.\n=== Other morphologies ===\nPeculiar galaxies are galactic formations that develop unusual properties due to tidal interactions with other galaxies.\nA ring galaxy has a ring-like structure of stars and interstellar medium surrounding a bare core. A ring galaxy is thought to occur when a smaller galaxy passes through the core of a spiral galaxy. Such an event may have affected the Andromeda Galaxy, as it displays a multi-ring-like structure when viewed in infrared radiation.\nA lenticular galaxy is an intermediate form that has properties of both elliptical and spiral galaxies. These are categorized as Hubble type S0, and they possess ill-defined spiral arms with an elliptical halo of stars (barred lenticular galaxies receive Hubble classification SB0).\nIrregular galaxies are galaxies that can not be readily classified into an elliptical or spiral morphology.\nAn Irr-I galaxy has some structure but does not align cleanly with the Hubble classification scheme.\nIrr-II galaxies do not possess any structure that resembles a Hubble classification, and may have been disrupted. Nearby examples of (dwarf) irregular galaxies include the Magellanic Clouds.\nA dark or "ultra diffuse" galaxy is an extremely-low-luminosity galaxy. It may be the same size as the Milky Way, but have a visible star count only one percent of the Milky Way\'s. Multiple mechanisms for producing this type of galaxy have been proposed, and it is possible that different dark galaxies formed by different means. One candidate explanation for the low luminosity is that the galaxy lost its star-forming gas at an early stage, resulting in old stellar populations.\n=== Dwarfs ===\nDespite the prominence of large elliptical and spiral galaxies, most galaxies are dwarf galaxies. They are relatively small when compared with other galactic formations, being about one hundredth the size of the Milky Way, with only a few billion stars. Blue compact dwarf galaxies contains large clusters of young, hot, massive stars. Ultra-compact dwarf galaxies have been discovered that are only 100 parsecs across.\nMany dwarf galaxies may orbit a single larger galaxy; the Milky Way has at least a dozen such satellites, with an estimated 300–500 yet to be discovered.\nMost of the information we have about dwarf galaxies come from observations of the local group, containing two spiral galaxies, the Milky Way and Andromeda, and many dwarf galaxies. These dwarf galaxies are classified as either irregular or dwarf elliptical/dwarf spheroidal galaxies.\nA study of 27 Milky Way neighbors found that in all dwarf galaxies, the central mass is approximately 10 million solar masses, regardless of whether it has thousands or millions of stars. This suggests that galaxies are largely formed by dark matter, and that the minimum size may indicate a form of warm dark matter incapable of gravitational coalescence on a smaller scale.\n== Variants ==\n=== Interacting ===\nInteractions between galaxies are relatively frequent, and they can play an important role in galactic evolution. Near misses between galaxies result in warping distortions due to tidal interactions, and may cause some exchange of gas and dust.\nCollisions occur when two galaxies pass directly through each other and have sufficient relative momentum not to merge. The stars of interacting galaxies usually do not collide, but the gas and dust within the two forms interacts, sometimes triggering star formation. A collision can severely distort the galaxies\' shapes, forming bars, rings or tail-like structures.\nAt the extreme of interactions are galactic mergers, where the galaxies\' relative momentums are insufficient to allow them to pass through each other. Instead, they gradually merge to form a single, larger galaxy. Mergers can result in significant changes to the galaxies\' original morphology. If one of the galaxies is much more massive than the other, the result is known as cannibalism, where the more massive larger galaxy remains relatively undisturbed, and the smaller one is torn apart. The Milky Way galaxy is currently in the process of cannibalizing the Sagittarius Dwarf Elliptical Galaxy and the Canis Major Dwarf Galaxy.\n=== Starburst ===\nStars are created within galaxies from a reserve of cold gas that forms giant molecular clouds. Some galaxies have been observed to form stars at an exceptional rate, which is known as a starburst. If they continue to do so, they would consume their reserve of gas in a time span less than the galaxy\'s lifespan. Hence starburst activity usually lasts only about ten million years, a relatively brief period in a galaxy\'s history. Starburst galaxies were more common during the universe\'s early history, but still contribute an estimated 15% to total star production.\nStarburst galaxies are characterized by dusty concentrations of gas and the appearance of newly formed stars, including massive stars that ionize the surrounding clouds to create H II regions. These stars produce supernova explosions, creating expanding remnants that interact powerfully with the surrounding gas. These outbursts trigger a chain reaction of star-building that spreads throughout the gaseous region. Only when the available gas is nearly consumed or dispersed does the activity end.\nStarbursts are often associated with merging or interacting galaxies. The prototype example of such a starburst-forming interaction is M82, which experienced a close encounter with the larger M81. Irregular galaxies often exhibit spaced knots of starburst activity.\n=== Radio galaxy ===\nA radio galaxy is a galaxy with giant regions of radio emission extending well beyond its visible structure. These energetic radio lobes are powered by jets from its active galactic nucleus. Radio galaxies are classified according to their Fanaroff–Riley classification. The FR I class have lower radio luminosity and exhibit structures which are more elongated; the FR II class are higher radio luminosity. The correlation of radio luminosity and structure suggests that the sources in these two types of galaxies may differ.']

Question: What is the reason behind the designation of Class L dwarfs, and what is their color and composition?

Choices:
Choice A) Class L dwarfs are hotter than M stars and are designated L because L is the remaining letter alphabetically closest to M. They are bright blue in color and are brightest in ultraviolet. Their atmosphere is hot enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.
Choice B) Class L dwarfs are cooler than M stars and are designated L because L is the remaining letter alphabetically closest to M. They are dark red in color and are brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.
Choice C) Class L dwarfs are hotter than M stars and are designated L because L is the next letter alphabetically after M. They are dark red in color and are brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.
Choice D) Class L dwarfs are cooler than M stars and are designated L because L is the next letter alphabetically after M. They are bright yellow in color and are brightest in visible light. Their atmosphere is hot enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.
Choice E) Class L dwarfs are cooler than M stars and are designated L because L is the remaining letter alphabetically closest to M. They are bright green in color and are brightest in visible light. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses small enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'While supersymmetry has not been discovered at high energy, see Section Supersymmetry in particle physics, supersymmetry was found to be effectively realized at the intermediate energy of hadronic physics where baryons and mesons are superpartners. An exception is the pion that appears as a zero mode in the mass spectrum and thus protected by the supersymmetry: It has no baryonic partner. The realization of this effective supersymmetry is readily explained in quark–diquark models: Because two different color charges close together (e.g., blue and red) appear under coarse resolution as the corresponding anti-color (e.g. anti-green), a diquark cluster viewed with coarse resolution (i.e., at the energy-momentum scale used to study hadron structure) effectively appears as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves likes a meson.\n=== Supersymmetry in condensed matter physics ===\nSUSY concepts have provided useful extensions to the WKB approximation. Additionally, SUSY has been applied to disorder averaged systems both quantum and non-quantum (through statistical mechanics), the Fokker–Planck equation being an example of a non-quantum theory. The \'supersymmetry\' in all these systems arises from the fact that one is modelling one particle and as such the \'statistics\' do not matter. The use of the supersymmetry method provides a mathematical rigorous alternative to the replica trick, but only in non-interacting systems, which attempts to address the so-called \'problem of the denominator\' under disorder averaging. For more on the applications of supersymmetry in condensed matter physics see Efetov (1997).\nIn 2021, a group of researchers showed that, in theory,\n{\\displaystyle N=(0,1)}\nSUSY could be realised at the edge of a Moore–Read quantum Hall state. However, to date, no experiments have been done yet to realise it at an edge of a Moore–Read state. In 2022, a different group of researchers created a computer simulation of atoms in 1 dimensions that had supersymmetric topological quasiparticles.\n=== Supersymmetry in optics ===\nIn 2013, integrated optics was found to provide a fertile ground on which certain ramifications of SUSY can be explored in readily-accessible laboratory settings. Making use of the analogous mathematical structure of the quantum-mechanical Schrödinger equation and the wave equation governing the evolution of light in one-dimensional settings, one may interpret the refractive index distribution of a structure as a potential landscape in which optical wave packets propagate. In this manner, a new class of functional optical structures with possible applications in phase matching, mode conversion and space-division multiplexing becomes possible. SUSY transformations have been also proposed as a way to address inverse scattering problems in optics and as a one-dimensional transformation optics.\n=== Supersymmetry in dynamical systems ===\nAll stochastic (partial) differential equations, the models for all types of continuous time dynamical systems, possess topological supersymmetry. In the operator representation of stochastic evolution, the topological supersymmetry is the exterior derivative which is commutative with the stochastic evolution operator defined as the stochastically averaged pullback induced on differential forms by SDE-defined diffeomorphisms of the phase space. The topological sector of the so-emerging supersymmetric theory of stochastic dynamics can be recognized as the Witten-type topological field theory.\nThe meaning of the topological supersymmetry in dynamical systems is the preservation of the phase space continuity—infinitely close points will remain close during continuous time evolution even in the presence of noise. When the topological supersymmetry is broken spontaneously, this property is violated in the limit of the infinitely long temporal evolution and the model can be said to exhibit (the stochastic generalization of) the butterfly effect. From a more general perspective, spontaneous breakdown of the topological supersymmetry is the theoretical essence of the ubiquitous dynamical phenomenon variously known as chaos, turbulence, self-organized criticality etc. The Goldstone theorem explains the associated emergence of the long-range dynamical behavior that manifests itself as \u20601/f\u2060 noise, butterfly effect, and the scale-free statistics of sudden (instantonic) processes, such as earthquakes, neuroavalanches, and solar flares, known as the Zipf\'s law and the Richter scale.\n=== Supersymmetry in mathematics ===\nSUSY is also sometimes studied mathematically for its intrinsic properties. This is because it describes complex fields satisfying a property known as holomorphy, which allows holomorphic quantities to be exactly computed. This makes supersymmetric models useful "toy models" of more realistic theories. A prime example of this has been the demonstration of S-duality in four-dimensional gauge theories that interchanges particles and monopoles.\nThe proof of the Atiyah–Singer index theorem is much simplified by the use of supersymmetric quantum mechanics.\n=== Supersymmetry in string theory ===\nSupersymmetry is an integral part of string theory, a possible theory of everything. There are two types of string theory, supersymmetric string theory or superstring theory, and non-supersymmetric string theory. By definition of superstring theory, supersymmetry is required in superstring theory at some level. However, even in non-supersymmetric string theory, a type of supersymmetry called misaligned supersymmetry is still required in the theory in order to ensure no physical tachyons appear. Any string theories without some kind of supersymmetry, such as bosonic string theory and the\n{\\displaystyle E_{7}\\times E_{7}}\n16\n{\\displaystyle SU(16)}\n, and\n{\\displaystyle E_{8}}\nheterotic string theories, will have a tachyon and therefore the spacetime vacuum itself would be unstable and would decay into some tachyon-free string theory usually in a lower spacetime dimension. There is no experimental evidence that either supersymmetry or misaligned supersymmetry holds in our universe, and many physicists have moved on from supersymmetry and string theory entirely due to the non-detection of supersymmetry at the LHC.\nDespite the null results for supersymmetry at the LHC so far, some particle physicists have nevertheless moved to string theory in order to resolve the naturalness crisis for certain supersymmetric extensions of the Standard Model. According to the particle physicists, there exists a concept of "stringy naturalness" in string theory, where the string theory landscape could have a power law statistical pull on soft SUSY breaking terms to large values (depending on the number of hidden sector SUSY breaking fields contributing to the soft terms). If this is coupled with an anthropic requirement that contributions to the weak scale not exceed a factor between 2 and 5 from its measured value (as argued by Agrawal et al.), then the Higgs mass is pulled up to the vicinity of 125 GeV while most sparticles are pulled to values beyond the current reach of LHC. (The Higgs was determined to have a mass of 125 GeV ±0.15 GeV in 2022.) An exception occurs for higgsinos which gain mass not from SUSY breaking but rather from whatever mechanism solves the SUSY mu problem. Light higgsino pair production in association with hard initial state jet radiation leads to a soft opposite-sign dilepton plus jet plus missing transverse energy signal.\n== Supersymmetry in particle physics ==\nIn particle physics, a supersymmetric extension of the Standard Model is a possible candidate for undiscovered particle physics, and seen by some physicists as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to be incomplete and where limitations of current theories are well established. In particular, one supersymmetric extension of the Standard Model, the Minimal Supersymmetric Standard Model (MSSM), became popular in theoretical particle physics, as the Minimal Supersymmetric Standard Model is the simplest supersymmetric extension of the Standard Model that could resolve major hierarchy problems within the Standard Model, by guaranteeing that quadratic divergences of all orders will cancel out in perturbation theory. If a supersymmetric extension of the Standard Model is correct, superpartners of the existing elementary particles would be new and undiscovered particles and supersymmetry is expected to be spontaneously broken.\nThere is no experimental evidence that a supersymmetric extension to the Standard Model is correct, or whether or not other extensions to current models might be more accurate. It is only since around 2010 that particle accelerators specifically designed to study physics beyond the Standard Model have become operational (i.e. the Large Hadron Collider (LHC)), and it is not known where exactly to look, nor the energies required for a successful search. However, the negative results from the LHC since 2010 have already ruled out some supersymmetric extensions to the Standard Model, and many physicists believe that the Minimal Supersymmetric Standard Model, while not ruled out, is no longer able to fully resolve the hierarchy problem.\n=== Supersymmetric extensions of the Standard Model ===', "Planck's law", 'Fischer–Tropsch process', 'In the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein\'s proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal. This has at times been called Planck\'s "second theory".\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his \'third theory\', that both emission and absorption of light were quantal.\nThe colourful term "ultraviolet catastrophe" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck\'s thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of "catastrophe". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called \'photons\', and by Rayleigh and by Jeans.\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν. In contrast to Planck\'s and Einstein\'s formulas, Bohr\'s formula referred explicitly and categorically to energy levels of atoms. Bohr\'s formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck\'s model, the frequency\n{\\displaystyle \\nu }\nhas no immediate relation to frequencies that might describe those quantum states themselves.\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck\'s law. The actual word \'photon\' was invented still later, by G.N. Lewis in 1926, who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word \'photon\' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein\'s 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\nUltimately, Planck\'s law of black-body radiation contributed to Einstein\'s concept of quanta of light carrying linear momentum, which became the fundamental basis for the development of quantum mechanics.\nThe above-mentioned linearity of Planck\'s mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg\'s original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg\'s theory accounted for Bohr\'s above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states. At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg\'s paper and recognized the matrix character of Heisenberg\'s theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg\'s original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics. Heisenberg\'s explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck\'s oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = \u2060h/2π\u2060, and ω = 2πν denotes angular frequency, and less often the equivalent formula E = hν. This statement about a really existing and propagating light quantum, based on Einstein\'s, has a physical meaning different from that of Planck\'s above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\nAn article by Helge Kragh published in Physics World gives an account of this history.\n== See also ==\nEmissivity\nRadiance\nSakuma–Hattori equation\n== References ==\n=== Bibliography ===\n== External links ==\nSummary of Radiation\nRadiation of a Blackbody – interactive simulation to play with Planck\'s law\nScienceworld entry on Planck\'s Law', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', '== See also ==\n== References ==\n== Further reading ==\nAnderson, P.W., Basic Notions of Condensed Matter Physics, Perseus Publishing (1997).\nFaghri, A., and Zhang, Y., Fundamentals of Multiphase Heat Transfer and Flow, Springer Nature Switzerland AG, 2020.\nFisher, M.E. (1974). "The renormalization group in the theory of critical behavior". Rev. Mod. Phys. 46 (4): 597–616. Bibcode:1974RvMP...46..597F. doi:10.1103/revmodphys.46.597.\nGoldenfeld, N., Lectures on Phase Transitions and the Renormalization Group, Perseus Publishing (1992).\nIvancevic, Vladimir G; Ivancevic, Tijana T (2008), Chaos, Phase Transitions, Topology Change and Path Integrals, Berlin: Springer, ISBN 978-3-540-79356-4, retrieved 14 March 2013\nM.R. Khoshbin-e-Khoshnazar, Ice Phase Transition as a sample of finite system phase transition, (Physics Education (India) Volume 32. No. 2, Apr - Jun 2016)\nKleinert, H., Gauge Fields in Condensed Matter, Vol. I, "Superfluidity and Vortex lines; Disorder Fields, Phase Transitions", pp. 1–742, World Scientific (Singapore, 1989); Paperback ISBN 9971-5-0210-0 (physik.fu-berlin.de readable online)\nKleinert, Hagen; Verena Schulte-Frohlinde (2001). Critical Properties of φ4-Theories. World Scientific. ISBN 981-02-4659-5. Archived from the original on 26 February 2008. (readable online).\nKogut, J.; Wilson, K (1974). "The Renormalization Group and the epsilon-Expansion". Phys. Rep. 12 (2): 75–199. Bibcode:1974PhR....12...75W. doi:10.1016/0370-1573(74)90023-4.\nKrieger, Martin H., Constitutions of matter : mathematically modelling the most everyday of physical phenomena, University of Chicago Press, 1996. Contains a detailed pedagogical discussion of Onsager\'s solution of the 2-D Ising Model.\nLandau, L.D. and Lifshitz, E.M., Statistical Physics Part 1, vol. 5 of Course of Theoretical Physics, Pergamon Press, 3rd Ed. (1994).\nMussardo G., "Statistical Field Theory. An Introduction to Exactly Solved Models of Statistical Physics", Oxford University Press, 2010.\nSchroeder, Manfred R., Fractals, chaos, power laws : minutes from an infinite paradise, New York: W. H. Freeman, 1991.  Very well-written book in "semi-popular" style—not a textbook—aimed at an audience with some training in mathematics and the physical sciences.  Explains what scaling in phase transitions is all about, among other things.\nH. E. Stanley, Introduction to Phase Transitions and Critical Phenomena (Oxford University Press, Oxford and New York 1971).\nYeomans J. M., Statistical Mechanics of Phase Transitions, Oxford University Press, 1992.\n== External links ==\nMedia related to Phase changes at Wikimedia Commons\nInteractive Phase Transitions on lattices with Java applets\nUniversality classes from Sklogwiki', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'Thus Kirchhoff\'s law of thermal radiation can be stated: For any material at all, radiating and absorbing in thermodynamic equilibrium at any given temperature T, for every wavelength λ, the ratio of emissive power to absorptive ratio has one universal value, which is characteristic of a perfect black body, and is an emissive power which we here represent by Bλ (λ, T). (For our notation Bλ (λ, T), Kirchhoff\'s original notation was simply e.)\nKirchhoff announced that the determination of the function Bλ (λ, T) was a problem of the highest importance, though he recognized that there would be experimental difficulties to be overcome. He supposed that like other functions that do not depend on the properties of individual bodies, it would be a simple function. That function Bλ (λ, T) has occasionally been called \'Kirchhoff\'s (emission, universal) function\', though its precise mathematical form would not be known for another forty years, till it was discovered by Planck in 1900. The theoretical proof for Kirchhoff\'s universality principle was worked on and debated by various physicists over the same time, and later. Kirchhoff stated later in 1860 that his theoretical proof was better than Balfour Stewart\'s, and in some respects it was so. Kirchhoff\'s 1860 paper did not mention the second law of thermodynamics, and of course did not mention the concept of entropy which had not at that time been established. In a more considered account in a book in 1862, Kirchhoff mentioned the connection of his law with "Carnot\'s principle", which is a form of the second law.\nAccording to Helge Kragh, "Quantum theory owes its origin to the study of thermal radiation, in particular to the "blackbody" radiation that Robert Kirchhoff had first defined in 1859–1860."\n=== Empirical and theoretical ingredients for the scientific induction of Planck\'s law ===\nIn 1860, Kirchhoff predicted experimental difficulties for the empirical determination of the function that described the dependence of the black-body spectrum as a function only of temperature and wavelength. And so it turned out. It took some forty years of development of improved methods of measurement of electromagnetic radiation to get a reliable result.\nIn 1865, John Tyndall described radiation from electrically heated filaments and from carbon arcs as visible and invisible. Tyndall spectrally decomposed the radiation by use of a rock salt prism, which passed heat as well as visible rays, and measured the radiation intensity by means of a thermopile.\nIn 1880, André-Prosper-Paul Crova published a diagram of the three-dimensional appearance of the graph of the strength of thermal radiation as a function of wavelength and temperature. He determined the spectral variable by use of prisms. He analyzed the surface through what he called "isothermal" curves, sections for a single temperature, with a spectral variable on the abscissa and a power variable on the ordinate. He put smooth curves through his experimental data points. They had one peak at a spectral value characteristic for the temperature, and fell either side of it towards the horizontal axis. Such spectral sections are widely shown even today.\nIn a series of papers from 1881 to 1886, Langley reported measurements of the spectrum of heat radiation, using diffraction gratings and prisms, and the most sensitive detectors that he could make. He reported that there was a peak intensity that increased with temperature, that the shape of the spectrum was not symmetrical about the peak, that there was a strong fall-off of intensity when the wavelength was shorter than an approximate cut-off value for each temperature, that the approximate cut-off wavelength decreased with increasing temperature, and that the wavelength of the peak intensity decreased with temperature, so that the intensity increased strongly with temperature for short wavelengths that were longer than the approximate cut-off for the temperature.\nHaving read Langley, in 1888, Russian physicist V.A. Michelson published a consideration of the idea that the unknown Kirchhoff radiation function could be explained physically and stated mathematically in terms of "complete irregularity of the vibrations of ... atoms". At this time, Planck was not studying radiation closely, and believed in neither atoms nor statistical physics. Michelson produced a formula for the spectrum for temperature:\nexp\n{\\displaystyle I_{\\lambda }=B_{1}\\theta ^{\\frac {3}{2}}\\exp \\left(-{\\frac {c}{\\lambda ^{2}\\theta }}\\right)\\lambda ^{-6},}\nwhere Iλ denotes specific radiative intensity at wavelength λ and temperature θ, and where B1 and c are empirical constants.\nIn 1898, Otto Lummer and Ferdinand Kurlbaum published an account of their cavity radiation source. Their design has been used largely unchanged for radiation measurements to the present day. It was a platinum box, divided by diaphragms, with its interior blackened with iron oxide. It was an important ingredient for the progressively improved measurements that led to the discovery of Planck\'s law. A version described in 1901 had its interior blackened with a mixture of chromium, nickel, and cobalt oxides.\nThe importance of the Lummer and Kurlbaum cavity radiation source was that it was an experimentally accessible source of black-body radiation, as distinct from radiation from a simply exposed incandescent solid body, which had been the nearest available experimental approximation to black-body radiation over a suitable range of temperatures. The simply exposed incandescent solid bodies, that had been used before, emitted radiation with departures from the black-body spectrum that made it impossible to find the true black-body spectrum from experiments.\n=== Planck\'s views before the empirical facts led him to find his eventual law ===\nPlanck first turned his attention to the problem of black-body radiation in 1897.\nTheoretical and empirical progress enabled Lummer and Pringsheim to write in 1899 that available experimental evidence was approximately consistent with the specific intensity law Cλ−5e−c⁄λT where C and c denote empirically measurable constants, and where λ and T denote wavelength and temperature respectively. For theoretical reasons, Planck at that time accepted this formulation, which has an effective cut-off of short wavelengths.\nGustav Kirchhoff was Max Planck\'s teacher and surmised that there was a universal law for blackbody radiation and this was called "Kirchhoff\'s challenge". Planck, a theorist, believed that Wilhelm Wien had discovered this law and Planck expanded on Wien\'s work presenting it in 1899 to the meeting of the German Physical Society. Experimentalists Otto Lummer, Ferdinand Kurlbaum, Ernst Pringsheim Sr., and Heinrich Rubens did experiments that appeared to support Wien\'s law especially at higher frequency short wavelengths which Planck so wholly endorsed at the German Physical Society that it began to be called the Wien-Planck Law. However, by September 1900, the experimentalists had proven beyond a doubt that the Wien-Planck law failed at the longer wavelengths. They would present their data on October 19.  Planck was informed by his friend Rubens and quickly created a formula within a few days. In June of that same year, Lord Rayleigh had created a formula that would work for short lower frequency wavelengths based on the widely accepted theory of equipartition. So Planck submitted a formula combining both Rayleigh\'s Law (or a similar equipartition theory) and Wien\'s law which would be weighted to one or the other law depending on wavelength to match the experimental data. However, although this equation worked, Planck himself said unless he could explain the formula derived from a "lucky intuition" into one of "true meaning" in physics, it did not have true significance. Planck explained that thereafter followed the hardest work of his life. Planck did not believe in atoms, nor did he think the second law of thermodynamics should be statistical because probability does not provide an absolute answer, and Boltzmann\'s entropy law rested on the hypothesis of atoms and was statistical. But Planck was unable to find a way to reconcile his Blackbody equation with continuous laws such as Maxwell\'s wave equations. So in what Planck called "an act of desperation", he turned to Boltzmann\'s atomic law of entropy as it was the only one that made his equation work. Therefore, he used the Boltzmann constant k and his new constant h to explain the blackbody radiation law which became widely known through his published paper.\n=== Finding the empirical law ===\nMax Planck produced his law on 19 October 1900 as an improvement upon the Wien approximation, published in 1896 by Wilhelm Wien, which fit the experimental data at short wavelengths (high frequencies) but deviated from it at long wavelengths (low frequencies). In June 1900, based on heuristic theoretical considerations, Rayleigh had suggested a formula that he proposed might be checked experimentally. The suggestion was that the Stewart–Kirchhoff universal function might be of the form c1Tλ−4exp(–\u2060c2/λT\u2060) . This was not the celebrated Rayleigh–Jeans formula 8πkBTλ−4, which did not emerge until 1905, though it did reduce to the latter for long wavelengths, which are the relevant ones here. According to Klein, one may speculate that it is likely that Planck had seen this suggestion though he did not mention it in his papers of 1900 and 1901. Planck would have been aware of various other proposed formulas which had been offered. On 7 October 1900, Rubens told Planck that in the complementary domain (long wavelength, low frequency), and only there, Rayleigh\'s 1900 formula fitted the observed data well.']

Question: What is the Landau-Lifshitz-Gilbert equation used for in physics?

Choices:
Choice A) The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a liquid, and is commonly used in micromagnetics to model the effects of a magnetic field on ferromagnetic materials.
Choice B) The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a solid, and is commonly used in astrophysics to model the effects of a magnetic field on celestial bodies.
Choice C) The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a solid, and is commonly used in micromagnetics to model the effects of a magnetic field on ferromagnetic materials.
Choice D) The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a solid, and is commonly used in macro-magnetics to model the effects of a magnetic field on ferromagnetic materials.
Choice E) The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a liquid, and is commonly used in macro-magnetics to model the effects of a magnetic field on ferromagnetic materials.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Ultraviolet catastrophe\n\nThe ultraviolet catastrophe, also called the Rayleigh–Jeans catastrophe, was the prediction of late 19th century and early 20th century classical physics that an ideal black body at thermal equilibrium would emit an unbounded quantity of energy as wavelength decreased into the ultraviolet range.:\u200a6–7\u200a The term "ultraviolet catastrophe" was first used in 1911 by the Austrian physicist Paul Ehrenfest, but the concept originated with the 1900 statistical derivation of the Rayleigh–Jeans law.\nThe phrase refers to the fact that the empirically derived Rayleigh–Jeans law, which accurately predicted experimental results at large wavelengths, failed to do so for short wavelengths. (See the image for further elaboration.)  As the theory diverged from empirical observations when these frequencies reached the ultraviolet region of the electromagnetic spectrum, there was a problem. This problem was later found to be due to a property of quanta as proposed by Max Planck: There could be no fraction of a discrete energy package already carrying minimal energy.\nSince the first use of this term, it has also been used for other predictions of a similar nature, as in quantum electrodynamics and such cases as ultraviolet divergence.\n== Problem ==\nThe Rayleigh-Jeans law is an approximation to the spectral radiance of electromagnetic radiation as a function of wavelength from a black body at a given temperature through classical arguments. For wavelength\n{\\displaystyle \\lambda }\n, it is:\n{\\displaystyle B_{\\lambda }(T)={\\frac {2ck_{\\mathrm {B} }T}{\\lambda ^{4}}},}\nwhere\n{\\displaystyle B_{\\lambda }}\nis the spectral radiance, the power emitted per unit emitting area, per steradian, per unit wavelength;\n{\\displaystyle c}\nis the speed of light;\n{\\displaystyle k_{\\mathrm {B} }}\nis the Boltzmann constant; and\n{\\displaystyle T}\nis the temperature in kelvins.  For frequency\n{\\displaystyle \\nu }\n, the expression is instead\n{\\displaystyle B_{\\nu }(T)={\\frac {2\\nu ^{2}k_{\\mathrm {B} }T}{c^{2}}}.}\nThis formula is obtained from the equipartition theorem of classical statistical mechanics which states that all harmonic oscillator modes (degrees of freedom) of a system at equilibrium have an average energy of\n{\\displaystyle k_{\\rm {B}}T}\nThe "ultraviolet catastrophe" is the expression of the fact that the formula misbehaves at higher frequencies; it predicts infinite energy emission because\n{\\displaystyle B_{\\nu }(T)\\to \\infty }\nas\n{\\displaystyle \\nu \\to \\infty }\nAn example, from Mason\'s A History of the Sciences, illustrates multi-mode vibration via a piece of string. As a natural vibrator, the string will oscillate with specific modes (the standing waves of a string in harmonic resonance), dependent on the length of the string. In classical physics, a radiator of energy will act as a natural vibrator. Since each mode will have the same energy, most of the energy in a natural vibrator will be in the smaller wavelengths and higher frequencies, where most of the modes are.\nAccording to classical electromagnetism, the number of electromagnetic modes in a 3-dimensional cavity, per unit frequency, is proportional to the square of the frequency. This implies that the radiated power per unit frequency should be proportional to frequency squared. Thus, both the power at a given frequency and the total radiated power is unlimited as higher and higher frequencies are considered:  this is unphysical, as the total radiated power of a cavity is not observed to be infinite, a point that was made independently by Einstein, Lord Rayleigh, and Sir James Jeans in 1905.\n== Solution ==\nIn 1900, Max Planck derived the correct form for the intensity spectral distribution function by making some assumptions that were strange for the time. In particular, Planck assumed that electromagnetic radiation can be emitted or absorbed only in discrete packets, called quanta, of energy:\nquanta\n{\\displaystyle E_{\\text{quanta}}=h\\nu =h{\\frac {c}{\\lambda }},}\nwhere:\nh is the Planck constant,\nν is the frequency of light,\nc is the speed of light,\nλ is the wavelength of light.\nBy applying this new energy to the partition function in statistical mechanics, Planck\'s assumptions led to the correct form of the spectral distribution functions:\nexp\n{\\displaystyle B_{\\lambda }(\\lambda ,T)={\\frac {2hc^{2}}{\\lambda ^{5}}}{\\frac {1}{\\exp \\left({\\frac {hc}{\\lambda k_{\\mathrm {B} }T}}\\right)-1}}}\nwhere:\nT is the absolute temperature of the body,\nkB is the Boltzmann constant,\nexp denotes the exponential function.\nIn 1905, Albert Einstein solved the problem physically by postulating that Planck\'s quanta were real physical particles – what we now call photons, not just a mathematical fiction. They modified statistical mechanics in the style of Boltzmann to an ensemble of photons. Einstein\'s photon had an energy proportional to its frequency and also explained an unpublished law of Stokes and the photoelectric effect.  This published postulate was specifically cited by the Nobel Prize in Physics committee in their decision to award the prize for 1921 to Einstein.\n== See also ==\nWien approximation\nVacuum catastrophe\nPlanckian locus\n== References ==\n=== Bibliography ===\n== Further reading ==\nKroemer, Herbert; Kittel, Charles (1980). "Chapter 4". Thermal Physics (2 ed.). W. H. Freeman Company. ISBN 0-7167-1088-9.\nCohen-Tannoudji, Claude; Diu, Bernard; Laloë; Franck (1977). Quantum Mechanics: Volume One. Hermann, Paris. pp. 624–626. ISBN 0-471-16433-X.', 'Time is the continuous progression of existence that occurs in an apparently irreversible succession from the past, through the present, and into the future. It is a component quantity of various measurements used to sequence events, to compare the duration of events (or the intervals between them), and to quantify rates of change of quantities in material reality or in the conscious experience. Time is often referred to as a fourth dimension, along with three spatial dimensions.\nTime is one of the seven fundamental physical quantities in both the International System of Units (SI) and International System of Quantities. The SI base unit of time is the second, which is defined by measuring the electronic transition frequency of caesium atoms. General relativity is the primary framework for understanding how spacetime works. Through advances in both theoretical and experimental investigations of spacetime, it has been shown that time can be distorted and dilated, particularly at the edges of black holes.\nThroughout history, time has been an important subject of study in religion, philosophy, and science. Temporal measurement has occupied scientists and technologists, and has been a prime motivation in navigation and astronomy. Time is also of significant social importance, having economic value ("time is money") as well as personal value, due to an awareness of the limited time in each day ("carpe diem") and in human life spans.\n== Definition ==\nThe concept of time can be complex. Multiple notions exist, and defining time in a manner applicable to all fields without circularity has consistently eluded scholars. Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems. Traditional definitions of time involved the observation of periodic motion such as the apparent motion of the sun across the sky, the phases of the moon, and the passage of a free-swinging pendulum. More modern systems include the Global Positioning System, other satellite systems, Coordinated Universal Time and mean solar time. Although these systems differ from one another, with careful measurements they can be synchronized.\nIn physics, time is a fundamental concept to define other quantities, such as velocity. To avoid a circular definition, time in physics is operationally defined as "what a clock reads", specifically a count of repeating events such as the SI second. Although this aids in practical measurements, it does not address the essence of time. Physicists developed the concept of the spacetime continuum, where events are assigned four coordinates: three for space and one for time. Events like particle collisions, supernovas, or rocket launches have coordinates that may vary for different observers, making concepts like "now" and "here" relative. In general relativity, these coordinates do not directly correspond to the causal structure of events. Instead, the spacetime interval is calculated and classified as either space-like or time-like, depending on whether an observer exists that would say the events are separated by space or by time. Since the time required for light to travel a specific distance is the same for all observers—a fact first publicly demonstrated by the Michelson–Morley experiment—all observers will consistently agree on this definition of time as a causal relation.\nGeneral relativity does not address the nature of time for extremely small intervals where quantum mechanics holds. In quantum mechanics, time is treated as a universal and absolute parameter, differing from general relativity\'s notion of independent clocks. The problem of time consists of reconciling these two theories. As of 2025, there is no generally accepted theory of quantum general relativity.\n== Measurement ==\nMethods of temporal measurement, or chronometry, generally take two forms. The first is a calendar, a mathematical tool for organising intervals of time on Earth, consulted for periods longer than a day. The second is a clock, a physical mechanism that indicates the passage of time, consulted for periods less than a day. The combined measurement marks a specific moment in time from a reference point, or epoch.\n=== History of the calendar ===\nArtifacts from the Paleolithic suggest that the moon was used to reckon time as early as 6,000 years ago. Lunar calendars were among the first to appear, with years of either 12 or 13 lunar months (either 354 or 384 days). Without intercalation to add days or months to some years, seasons quickly drift in a calendar based solely on twelve lunar months. Lunisolar calendars have a thirteenth month added to some years to make up for the difference between a full year (now known to be about 365.24 days) and a year of just twelve lunar months. The numbers twelve and thirteen came to feature prominently in many cultures, at least partly due to this relationship of months to years.\nOther early forms of calendars originated in Mesoamerica, particularly in ancient Mayan civilization, in which they developed the Maya calendar, consisting of multiple interrelated calendars. These calendars were religiously and astronomically based; the Haab\' calendar has 18 months in a year and 20 days in a month, plus five epagomenal days at the end of the year. In conjunction, the Maya also used a 260-day sacred calendar called the Tzolk\'in.\nThe reforms of Julius Caesar in 45 BC put the Roman world on a solar calendar. This Julian calendar was faulty in that its intercalation still allowed the astronomical solstices and equinoxes to advance against it by about 11 minutes per year. Pope Gregory XIII introduced a correction in 1582; the Gregorian calendar was only slowly adopted by different nations over a period of centuries, but it is now by far the most commonly used calendar around the world.\nDuring the French Revolution, a new clock and calendar were invented as part of the dechristianization of France and to create a more rational system in order to replace the Gregorian calendar. The French Republican Calendar\'s days consisted of ten hours of a hundred minutes of a hundred seconds, which marked a deviation from the base 12 (duodecimal) system used in many other devices by many cultures. The system was abolished in 1806.\n=== History of other devices ===\nA large variety of devices have been invented to measure time. The study of these devices is called horology. They can be driven by a variety of means, including gravity, springs, and various forms of electrical power, and regulated by a variety of means.\nA sundial is any device that uses the direction of sunlight to cast shadows from a gnomon onto a set of markings calibrated to indicate the local time, usually to the hour. The idea to separate the day into smaller parts is credited to Egyptians because of their sundials, which operated on a duodecimal system. The importance of the number 12 is due to the number of lunar cycles in a year and the number of stars used to count the passage of night. Obelisks made as a gnomon were built as early as c.\u20093500 BC. An Egyptian device that dates to c.\u20091500 BC, similar in shape to a bent T-square, also measured the passage of time from the shadow cast by its crossbar on a nonlinear rule. The T was oriented eastward in the mornings. At noon, the device was turned around so that it could cast its shadow in the evening direction.\nAlarm clocks reportedly first appeared in ancient Greece c.\u2009250 BC with a water clock made by Plato that would set off a whistle. The hydraulic alarm worked by gradually filling a series of vessels with water. After some time, the water emptied out of a siphon. Inventor Ctesibius revised Plato\'s design; the water clock uses a float as the power drive system and uses a sundial to correct the water flow rate.\nIn medieval philosophical writings, the atom was a unit of time referred to as the smallest possible division of time. The earliest known occurrence in English is in Byrhtferth\'s Enchiridion (a science text) of 1010–1012, where it was defined as 1/564 of a momentum (11⁄2 minutes), and thus equal to 15/94 of a second. It was used in the computus, the process of calculating the date of Easter. The most precise timekeeping device of the ancient world was the water clock, or clepsydra, one of which was found in the tomb of Egyptian pharaoh Amenhotep I. They could be used to measure the hours even at night but required manual upkeep to replenish the flow of water. The ancient Greeks and the people from Chaldea (southeastern Mesopotamia) regularly maintained timekeeping records as an essential part of their astronomical observations. Arab inventors and engineers, in particular, made improvements on the use of water clocks up to the Middle Ages. In the 11th century, Chinese inventors and engineers invented the first mechanical clocks driven by an escapement mechanism.\nIncense sticks and candles were, and are, commonly used to measure time in temples and churches across the globe. Water clocks, and, later, mechanical clocks, were used to mark the events of the abbeys and monasteries of the Middle Ages. The passage of the hours at sea can also be marked by bell. The hours were marked by bells in abbeys as well as at sea. Richard of Wallingford (1292–1336), abbot of St. Alban\'s abbey, famously built a mechanical clock as an astronomical orrery about 1330. The hourglass uses the flow of sand to measure the flow of time. They were also used in navigation. Ferdinand Magellan used 18 glasses on each ship for his circumnavigation of the globe (1522). The English word clock probably comes from the Middle Dutch word klocke which, in turn, derives from the medieval Latin word clocca, which ultimately derives from Celtic and is cognate with French, Latin, and German words that mean bell.', 'A star is a luminous spheroid of plasma held together by self-gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye at night; their immense distances from Earth make them appear as fixed points of light. The most prominent stars have been categorised into constellations and asterisms, and many of the brightest stars have proper names. Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations. The observable universe contains an estimated 1022 to 1024 stars. Only about 4,000 of these stars are visible to the naked eye—all within the Milky Way galaxy.\nA star\'s life begins with the gravitational collapse of a gaseous nebula of material largely comprising hydrogen, helium, and trace heavier elements. Its total mass mainly determines its evolution and eventual fate. A star shines for most of its active life due to the thermonuclear fusion of hydrogen into helium in its core. This process releases energy that traverses the star\'s interior and radiates into outer space. At the end of a star\'s lifetime, fusion ceases and its core becomes a stellar remnant: a white dwarf, a neutron star, or—if it is sufficiently massive—a black hole.\nStellar nucleosynthesis in stars or their remnants creates almost all naturally occurring chemical elements heavier than lithium. Stellar mass loss or supernova explosions return chemically enriched material to the interstellar medium. These elements are then recycled into new stars. Astronomers can determine stellar properties—including mass, age, metallicity (chemical composition), variability, distance, and motion through space—by carrying out observations of a star\'s apparent brightness, spectrum, and changes in its position in the sky over time.\nStars can form orbital systems with other astronomical objects, as in planetary systems and star systems with two or more stars. When two such stars orbit closely, their gravitational interaction can significantly impact their evolution. Stars can form part of a much larger gravitationally bound structure, such as a star cluster or a galaxy.\n== Etymology ==\nThe word "star" ultimately derives from the Proto-Indo-European root "h₂stḗr" also meaning star, but further analyzable as h₂eh₁s- ("to burn", also the source of the word "ash") + -tēr (agentive suffix). Compare Latin stella, Greek aster, German Stern. Some scholars believe the word is a borrowing from Akkadian "istar" (Venus). "Star" is cognate (shares the same root) with the following words: asterisk, asteroid, astral, constellation, Esther.\n== Observation history ==\nHistorically, stars have been important to civilizations throughout the world. They have been part of religious practices, divination rituals, mythology, used for celestial navigation and orientation, to mark the passage of seasons, and to define calendars.\nEarly astronomers recognized a difference between "fixed stars", whose position on the celestial sphere does not change, and "wandering stars" (planets), which move noticeably relative to the fixed stars over days or weeks. Many ancient astronomers believed that the stars were permanently affixed to a heavenly sphere and that they were immutable. By convention, astronomers grouped prominent stars into asterisms and constellations and used them to track the motions of the planets and the inferred position of the Sun. The motion of the Sun against the background stars (and the horizon) was used to create calendars, which could be used to regulate agricultural practices. The Gregorian calendar, currently used nearly everywhere in the world, is a solar calendar based on the angle of the Earth\'s rotational axis relative to its local star, the Sun.\nThe oldest accurately dated star chart was the result of ancient Egyptian astronomy in 1534 BC. The earliest known star catalogues were compiled by the ancient Babylonian astronomers of Mesopotamia in the late 2nd millennium BC, during the Kassite Period (c.\u20091531 BC – c.\u20091155 BC).\nThe first star catalogue in Greek astronomy was created by Aristillus in approximately 300 BC, with the help of Timocharis. The star catalog of Hipparchus (2nd century BC) included 1,020 stars, and was used to assemble Ptolemy\'s star catalogue. Hipparchus is known for the discovery of the first recorded nova (new star). Many of the constellations and star names in use today derive from Greek astronomy.\nDespite the apparent immutability of the heavens, Chinese astronomers were aware that new stars could appear. In 185 AD, they were the first to observe and write about a supernova, now known as SN 185. The brightest stellar event in recorded history was the SN 1006 supernova, which was observed in 1006 and written about by the Egyptian astronomer Ali ibn Ridwan and several Chinese astronomers. The SN 1054 supernova, which gave birth to the Crab Nebula, was also observed by Chinese and Islamic astronomers.\nMedieval Islamic astronomers gave Arabic names to many stars that are still used today and they invented numerous astronomical instruments that could compute the positions of the stars. They built the first large observatory research institutes, mainly to produce Zij star catalogues. Among these, the Book of Fixed Stars (964) was written by the Persian astronomer Abd al-Rahman al-Sufi, who observed a number of stars, star clusters (including the Omicron Velorum and Brocchi\'s Clusters) and galaxies (including the Andromeda Galaxy). According to A. Zahoor, in the 11th century, the Persian polymath scholar Abu Rayhan Biruni described the Milky Way galaxy as a multitude of fragments having the properties of nebulous stars, and gave the latitudes of various stars during a lunar eclipse in 1019.\nAccording to Josep Puig, the Andalusian astronomer Ibn Bajjah proposed that the Milky Way was made up of many stars that almost touched one another and appeared to be a continuous image due to the effect of refraction from sublunary material, citing his observation of the conjunction of Jupiter and Mars on 500 AH (1106/1107 AD) as evidence.\nEarly European astronomers such as Tycho Brahe identified new stars in the night sky (later termed novae), suggesting that the heavens were not immutable. In 1584, Giordano Bruno suggested that the stars were like the Sun, and may have other planets, possibly even Earth-like, in orbit around them, an idea that had been suggested earlier by the ancient Greek philosophers, Democritus and Epicurus, and by medieval Islamic cosmologists such as Fakhr al-Din al-Razi. By the following century, the idea of the stars being the same as the Sun was reaching a consensus among astronomers. To explain why these stars exerted no net gravitational pull on the Solar System, Isaac Newton suggested that the stars were equally distributed in every direction, an idea prompted by the theologian Richard Bentley.\nThe Italian astronomer Geminiano Montanari recorded observing variations in luminosity of the star Algol in 1667. Edmond Halley published the first measurements of the proper motion of a pair of nearby "fixed" stars, demonstrating that they had changed positions since the time of the ancient Greek astronomers Ptolemy and Hipparchus.\nWilliam Herschel was the first astronomer to attempt to determine the distribution of stars in the sky. During the 1780s, he established a series of gauges in 600 directions and counted the stars observed along each line of sight. From this, he deduced that the number of stars steadily increased toward one side of the sky, in the direction of the Milky Way core. His son John Herschel repeated this study in the southern hemisphere and found a corresponding increase in the same direction. In addition to his other accomplishments, William Herschel is noted for his discovery that some stars do not merely lie along the same line of sight, but are physical companions that form binary star systems.\nThe science of stellar spectroscopy was pioneered by Joseph von Fraunhofer and Angelo Secchi. By comparing the spectra of stars such as Sirius to the Sun, they found differences in the strength and number of their absorption lines—the dark lines in stellar spectra caused by the atmosphere\'s absorption of specific frequencies. In 1865, Secchi began classifying stars into spectral types. The modern version of the stellar classification scheme was developed by Annie J. Cannon during the early 1900s.\nThe first direct measurement of the distance to a star (61 Cygni at 11.4 light-years) was made in 1838 by Friedrich Bessel using the parallax technique. Parallax measurements demonstrated the vast separation of the stars in the heavens. Observation of double stars gained increasing importance during the 19th century. In 1834, Friedrich Bessel observed changes in the proper motion of the star Sirius and inferred a hidden companion. Edward Pickering discovered the first spectroscopic binary in 1899 when he observed the periodic splitting of the spectral lines of the star Mizar in a 104-day period. Detailed observations of many binary star systems were collected by astronomers such as Friedrich Georg Wilhelm von Struve and S. W. Burnham, allowing the masses of stars to be determined from computation of orbital elements. The first solution to the problem of deriving an orbit of binary stars from telescope observations was made by Felix Savary in 1827.\nThe twentieth century saw increasingly rapid advances in the scientific study of stars. The photograph became a valuable astronomical tool. Karl Schwarzschild discovered that the color of a star and, hence, its temperature, could be determined by comparing the visual magnitude against the photographic magnitude. The development of the photoelectric photometer allowed precise measurements of magnitude at multiple wavelength intervals. In 1921 Albert A. Michelson made the first measurements of a stellar diameter using an interferometer on the Hooker telescope at Mount Wilson Observatory.', "== Importance ==\nFrom the perspective of a planetary geologist, the atmosphere acts to shape a planetary surface. Wind picks up dust and other particles which, when they collide with the terrain, erode the relief and leave deposits (eolian processes). Frost and precipitations, which depend on the atmospheric composition, also influence the relief. Climate changes can influence a planet's geological history. Conversely, studying the surface of the Earth leads to an understanding of the atmosphere and climate of other planets.\nFor a meteorologist, the composition of the Earth's atmosphere is a factor affecting the climate and its variations.\nFor a biologist or paleontologist, the Earth's atmospheric composition is closely dependent on the appearance of life and its evolution.\n== See also ==\nAtmometer (evaporimeter)\nAtmospheric pressure\nInternational Standard Atmosphere\nKármán line\nSky\n== References ==\n== Further reading ==\nSanchez-Lavega, Agustin (2010). An Introduction to Planetary Atmospheres. Taylor & Francis. ISBN 978-1420067323.\n== External links ==\nProperties of atmospheric strata – The flight environment of the atmosphere\nAtmosphere – Everything you need to know", 'Thus Kirchhoff\'s law of thermal radiation can be stated: For any material at all, radiating and absorbing in thermodynamic equilibrium at any given temperature T, for every wavelength λ, the ratio of emissive power to absorptive ratio has one universal value, which is characteristic of a perfect black body, and is an emissive power which we here represent by Bλ (λ, T). (For our notation Bλ (λ, T), Kirchhoff\'s original notation was simply e.)\nKirchhoff announced that the determination of the function Bλ (λ, T) was a problem of the highest importance, though he recognized that there would be experimental difficulties to be overcome. He supposed that like other functions that do not depend on the properties of individual bodies, it would be a simple function. That function Bλ (λ, T) has occasionally been called \'Kirchhoff\'s (emission, universal) function\', though its precise mathematical form would not be known for another forty years, till it was discovered by Planck in 1900. The theoretical proof for Kirchhoff\'s universality principle was worked on and debated by various physicists over the same time, and later. Kirchhoff stated later in 1860 that his theoretical proof was better than Balfour Stewart\'s, and in some respects it was so. Kirchhoff\'s 1860 paper did not mention the second law of thermodynamics, and of course did not mention the concept of entropy which had not at that time been established. In a more considered account in a book in 1862, Kirchhoff mentioned the connection of his law with "Carnot\'s principle", which is a form of the second law.\nAccording to Helge Kragh, "Quantum theory owes its origin to the study of thermal radiation, in particular to the "blackbody" radiation that Robert Kirchhoff had first defined in 1859–1860."\n=== Empirical and theoretical ingredients for the scientific induction of Planck\'s law ===\nIn 1860, Kirchhoff predicted experimental difficulties for the empirical determination of the function that described the dependence of the black-body spectrum as a function only of temperature and wavelength. And so it turned out. It took some forty years of development of improved methods of measurement of electromagnetic radiation to get a reliable result.\nIn 1865, John Tyndall described radiation from electrically heated filaments and from carbon arcs as visible and invisible. Tyndall spectrally decomposed the radiation by use of a rock salt prism, which passed heat as well as visible rays, and measured the radiation intensity by means of a thermopile.\nIn 1880, André-Prosper-Paul Crova published a diagram of the three-dimensional appearance of the graph of the strength of thermal radiation as a function of wavelength and temperature. He determined the spectral variable by use of prisms. He analyzed the surface through what he called "isothermal" curves, sections for a single temperature, with a spectral variable on the abscissa and a power variable on the ordinate. He put smooth curves through his experimental data points. They had one peak at a spectral value characteristic for the temperature, and fell either side of it towards the horizontal axis. Such spectral sections are widely shown even today.\nIn a series of papers from 1881 to 1886, Langley reported measurements of the spectrum of heat radiation, using diffraction gratings and prisms, and the most sensitive detectors that he could make. He reported that there was a peak intensity that increased with temperature, that the shape of the spectrum was not symmetrical about the peak, that there was a strong fall-off of intensity when the wavelength was shorter than an approximate cut-off value for each temperature, that the approximate cut-off wavelength decreased with increasing temperature, and that the wavelength of the peak intensity decreased with temperature, so that the intensity increased strongly with temperature for short wavelengths that were longer than the approximate cut-off for the temperature.\nHaving read Langley, in 1888, Russian physicist V.A. Michelson published a consideration of the idea that the unknown Kirchhoff radiation function could be explained physically and stated mathematically in terms of "complete irregularity of the vibrations of ... atoms". At this time, Planck was not studying radiation closely, and believed in neither atoms nor statistical physics. Michelson produced a formula for the spectrum for temperature:\nexp\n{\\displaystyle I_{\\lambda }=B_{1}\\theta ^{\\frac {3}{2}}\\exp \\left(-{\\frac {c}{\\lambda ^{2}\\theta }}\\right)\\lambda ^{-6},}\nwhere Iλ denotes specific radiative intensity at wavelength λ and temperature θ, and where B1 and c are empirical constants.\nIn 1898, Otto Lummer and Ferdinand Kurlbaum published an account of their cavity radiation source. Their design has been used largely unchanged for radiation measurements to the present day. It was a platinum box, divided by diaphragms, with its interior blackened with iron oxide. It was an important ingredient for the progressively improved measurements that led to the discovery of Planck\'s law. A version described in 1901 had its interior blackened with a mixture of chromium, nickel, and cobalt oxides.\nThe importance of the Lummer and Kurlbaum cavity radiation source was that it was an experimentally accessible source of black-body radiation, as distinct from radiation from a simply exposed incandescent solid body, which had been the nearest available experimental approximation to black-body radiation over a suitable range of temperatures. The simply exposed incandescent solid bodies, that had been used before, emitted radiation with departures from the black-body spectrum that made it impossible to find the true black-body spectrum from experiments.\n=== Planck\'s views before the empirical facts led him to find his eventual law ===\nPlanck first turned his attention to the problem of black-body radiation in 1897.\nTheoretical and empirical progress enabled Lummer and Pringsheim to write in 1899 that available experimental evidence was approximately consistent with the specific intensity law Cλ−5e−c⁄λT where C and c denote empirically measurable constants, and where λ and T denote wavelength and temperature respectively. For theoretical reasons, Planck at that time accepted this formulation, which has an effective cut-off of short wavelengths.\nGustav Kirchhoff was Max Planck\'s teacher and surmised that there was a universal law for blackbody radiation and this was called "Kirchhoff\'s challenge". Planck, a theorist, believed that Wilhelm Wien had discovered this law and Planck expanded on Wien\'s work presenting it in 1899 to the meeting of the German Physical Society. Experimentalists Otto Lummer, Ferdinand Kurlbaum, Ernst Pringsheim Sr., and Heinrich Rubens did experiments that appeared to support Wien\'s law especially at higher frequency short wavelengths which Planck so wholly endorsed at the German Physical Society that it began to be called the Wien-Planck Law. However, by September 1900, the experimentalists had proven beyond a doubt that the Wien-Planck law failed at the longer wavelengths. They would present their data on October 19.  Planck was informed by his friend Rubens and quickly created a formula within a few days. In June of that same year, Lord Rayleigh had created a formula that would work for short lower frequency wavelengths based on the widely accepted theory of equipartition. So Planck submitted a formula combining both Rayleigh\'s Law (or a similar equipartition theory) and Wien\'s law which would be weighted to one or the other law depending on wavelength to match the experimental data. However, although this equation worked, Planck himself said unless he could explain the formula derived from a "lucky intuition" into one of "true meaning" in physics, it did not have true significance. Planck explained that thereafter followed the hardest work of his life. Planck did not believe in atoms, nor did he think the second law of thermodynamics should be statistical because probability does not provide an absolute answer, and Boltzmann\'s entropy law rested on the hypothesis of atoms and was statistical. But Planck was unable to find a way to reconcile his Blackbody equation with continuous laws such as Maxwell\'s wave equations. So in what Planck called "an act of desperation", he turned to Boltzmann\'s atomic law of entropy as it was the only one that made his equation work. Therefore, he used the Boltzmann constant k and his new constant h to explain the blackbody radiation law which became widely known through his published paper.\n=== Finding the empirical law ===\nMax Planck produced his law on 19 October 1900 as an improvement upon the Wien approximation, published in 1896 by Wilhelm Wien, which fit the experimental data at short wavelengths (high frequencies) but deviated from it at long wavelengths (low frequencies). In June 1900, based on heuristic theoretical considerations, Rayleigh had suggested a formula that he proposed might be checked experimentally. The suggestion was that the Stewart–Kirchhoff universal function might be of the form c1Tλ−4exp(–\u2060c2/λT\u2060) . This was not the celebrated Rayleigh–Jeans formula 8πkBTλ−4, which did not emerge until 1905, though it did reduce to the latter for long wavelengths, which are the relevant ones here. According to Klein, one may speculate that it is likely that Planck had seen this suggestion though he did not mention it in his papers of 1900 and 1901. Planck would have been aware of various other proposed formulas which had been offered. On 7 October 1900, Rubens told Planck that in the complementary domain (long wavelength, low frequency), and only there, Rayleigh\'s 1900 formula fitted the observed data well.', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'In the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein\'s proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal. This has at times been called Planck\'s "second theory".\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his \'third theory\', that both emission and absorption of light were quantal.\nThe colourful term "ultraviolet catastrophe" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck\'s thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of "catastrophe". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called \'photons\', and by Rayleigh and by Jeans.\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν. In contrast to Planck\'s and Einstein\'s formulas, Bohr\'s formula referred explicitly and categorically to energy levels of atoms. Bohr\'s formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck\'s model, the frequency\n{\\displaystyle \\nu }\nhas no immediate relation to frequencies that might describe those quantum states themselves.\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck\'s law. The actual word \'photon\' was invented still later, by G.N. Lewis in 1926, who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word \'photon\' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein\'s 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\nUltimately, Planck\'s law of black-body radiation contributed to Einstein\'s concept of quanta of light carrying linear momentum, which became the fundamental basis for the development of quantum mechanics.\nThe above-mentioned linearity of Planck\'s mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg\'s original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg\'s theory accounted for Bohr\'s above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states. At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg\'s paper and recognized the matrix character of Heisenberg\'s theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg\'s original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics. Heisenberg\'s explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck\'s oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = \u2060h/2π\u2060, and ω = 2πν denotes angular frequency, and less often the equivalent formula E = hν. This statement about a really existing and propagating light quantum, based on Einstein\'s, has a physical meaning different from that of Planck\'s above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\nAn article by Helge Kragh published in Physics World gives an account of this history.\n== See also ==\nEmissivity\nRadiance\nSakuma–Hattori equation\n== References ==\n=== Bibliography ===\n== External links ==\nSummary of Radiation\nRadiation of a Blackbody – interactive simulation to play with Planck\'s law\nScienceworld entry on Planck\'s Law', 'Petrosian magnitudes have the advantage of being redshift and distance independent, allowing the measurement of the galaxy\'s apparent size since the Petrosian radius is defined in terms of the galaxy\'s overall luminous flux.\nA critique of an earlier version of this method has been issued by the Infrared Processing and Analysis Center, with the method causing a magnitude of error (upwards to 10%) of the values than using isophotal diameter. The use of Petrosian magnitudes also have the disadvantage of missing most of the light outside the Petrosian aperture, which is defined relative to the galaxy\'s overall brightness profile, especially for elliptical galaxies, with higher signal-to-noise ratios on higher distances and redshifts. A correction for this method has been issued by Graham et al. in 2005, based on the assumption that galaxies follow Sérsic\'s law.\n=== Near-infrared method ===\nThis method has been used by 2MASS as an adaptation from the previously used methods of isophotal measurement. Since 2MASS operates in the near infrared, which has the advantage of being able to recognize dimmer, cooler, and older stars, it has a different form of approach compared to other methods that normally use B-filter. The detail of the method used by 2MASS has been described thoroughly in a document by Jarrett et al., with the survey measuring several parameters.\nThe standard aperture ellipse (area of detection) is defined by the infrared isophote at the Ks band (roughly 2.2 μm wavelength) of 20 mag/arcsec2. Gathering the overall luminous flux of the galaxy has been employed by at least four methods: the first being a circular aperture extending 7 arcseconds from the center, an isophote at 20 mag/arcsec2, a "total" aperture defined by the radial light distribution that covers the supposed extent of the galaxy, and the Kron aperture (defined as 2.5 times the first-moment radius, an integration of the flux of the "total" aperture).\n== Larger-scale structures ==\nDeep-sky surveys show that galaxies are often found in groups and clusters. Solitary galaxies that have not significantly interacted with other galaxies of comparable mass in the past few billion years are relatively scarce. Only about 5% of the galaxies surveyed are isolated in this sense. However, they may have interacted and even merged with other galaxies in the past, and may still be orbited by smaller satellite galaxies.\nOn the largest scale, the universe is continually expanding, resulting in an average increase in the separation between individual galaxies (see Hubble\'s law). Associations of galaxies can overcome this expansion on a local scale through their mutual gravitational attraction. These associations formed early, as clumps of dark matter pulled their respective galaxies together. Nearby groups later merged to form larger-scale clusters. This ongoing merging process, as well as an influx of infalling gas, heats the intergalactic gas in a cluster to very high temperatures of 30–100 megakelvins. About 70–80% of a cluster\'s mass is in the form of dark matter, with 10–30% consisting of this heated gas and the remaining few percent in the form of galaxies.\nMost galaxies are gravitationally bound to a number of other galaxies. These form a fractal-like hierarchical distribution of clustered structures, with the smallest such associations being termed groups. A group of galaxies is the most common type of galactic cluster; these formations contain the majority of galaxies (as well as most of the baryonic mass) in the universe. To remain gravitationally bound to such a group, each member galaxy must have a sufficiently low velocity to prevent it from escaping (see Virial theorem). If there is insufficient kinetic energy, however, the group may evolve into a smaller number of galaxies through mergers.\nClusters of galaxies consist of hundreds to thousands of galaxies bound together by gravity. Clusters of galaxies are often dominated by a single giant elliptical galaxy, known as the brightest cluster galaxy, which, over time, tidally destroys its satellite galaxies and adds their mass to its own.\nSuperclusters contain tens of thousands of galaxies, which are found in clusters, groups and sometimes individually. At the supercluster scale, galaxies are arranged into sheets and filaments surrounding vast empty voids. Above this scale, the universe appears to be the same in all directions (isotropic and homogeneous), though this notion has been challenged in recent years by numerous findings of large-scale structures that appear to be exceeding this scale. The Hercules–Corona Borealis Great Wall, currently the largest structure in the universe found so far, is 10 billion light-years (three gigaparsecs) in length.\nThe Milky Way galaxy is a member of an association named the Local Group, a relatively small group of galaxies that has a diameter of approximately one megaparsec. The Milky Way and the Andromeda Galaxy are the two brightest galaxies within the group; many of the other member galaxies are dwarf companions of these two. The Local Group itself is a part of a cloud-like structure within the Virgo Supercluster, a large, extended structure of groups and clusters of galaxies centered on the Virgo Cluster. In turn, the Virgo Supercluster is a portion of the Laniakea Supercluster.\n== Magnetic fields ==\nGalaxies have magnetic fields of their own. A galaxy\'s magnetic field influences its dynamics in multiple ways, including affecting the formation of spiral arms and transporting angular momentum in gas clouds. The latter effect is particularly important, as it is a necessary factor for the gravitational collapse of those clouds, and thus for star formation.\nThe typical average equipartition strength for spiral galaxies is about 10 μG (microgauss) or 1 nT (nanotesla). By comparison, the Earth\'s magnetic field has an average strength of about 0.3 G (Gauss) or 30 μT (microtesla). Radio-faint galaxies like M 31 and M33, the Milky Way\'s neighbors, have weaker fields (about 5 μG), while gas-rich galaxies with high star-formation rates, like M 51, M 83 and NGC 6946, have 15 μG on average. In prominent spiral arms, the field strength can be up to 25 μG, in regions where cold gas and dust are also concentrated. The strongest total equipartition fields (50–100 μG) were found in starburst galaxies—for example, in M 82 and the Antennae; and in nuclear starburst regions, such as the centers of NGC 1097 and other barred galaxies.\n== Formation and evolution ==\n=== Formation ===\nCurrent models of the formation of galaxies in the early universe are based on the ΛCDM model. About 300,000 years after the Big Bang, atoms of hydrogen and helium began to form, in an event called recombination. Nearly all the hydrogen was neutral (non-ionized) and readily absorbed light, and no stars had yet formed. As a result, this period has been called the "dark ages". It was from density fluctuations (or anisotropic irregularities) in this primordial matter that larger structures began to appear. As a result, masses of baryonic matter started to condense within cold dark matter halos. These primordial structures allowed gasses to condense in to protogalaxies, large scale gas clouds that were precursors to the first galaxies.:\u200a6\nAs gas falls in to the gravity of the dark matter halos, its pressure and temperature rise. To condense further, the gas must radiate energy. This process was slow in the early universe dominated by hydrogen atoms and molecules which are inefficient radiators compared to heavier elements. As clumps of gas aggregate forming rotating disks, temperatures and pressures continue to increase. Some places within the disk reach high enough density to form stars.\nOnce protogalaxies began to form and contract, the first halo stars, called Population III stars, appeared within them. These were composed of primordial gas, almost entirely of hydrogen and helium.\nEmission from the first stars heats the remaining gas helping to trigger additional star formation; the ultraviolet light emission from the first generation of stars re-ionized the surrounding neutral hydrogen in expanding spheres eventually reaching the entire universe, an event called reionization. The most massive stars collapse in violent supernova explosions releasing heavy elements ("metals") into the interstellar medium.:\u200a14\u200a This metal content is incorporated into population II stars.\nTheoretical models for early galaxy formation have been verified and informed by a large number and variety of sophisticated astronomical observations.:\u200a43\u200a The photometric observations generally need spectroscopic confirmation due the large number mechanisms that can introduce systematic errors. For example, a high redshift (z ~ 16) photometric observation by James Webb Space Telescope (JWST) was later corrected to be closer to z ~ 5.\nNevertheless, confirmed observations from the JWST and other observatories are accumulating, allowing systematic comparison of early galaxies to predictions of theory.\nEvidence for individual Population III stars in early galaxies is even more challenging. Even seemingly confirmed spectroscopic evidence may turn out to have other origins. For example, astronomers reported HeII emission evidence for Population III stars in the Cosmos Redshift 7 galaxy, with a redshift value of 6.60. Subsequent observations found metallic emission lines, OIII, inconsistent with an early-galaxy star.:\u200a108\n=== Evolution ===\nOnce stars begin to form, emit radiation, and in some cases explode, the process of galaxy formation becomes very complex, involving interactions between the forces of gravity, radiation, and thermal energy. Many details are still poorly understood.', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', 'UVC LEDs are relatively new to the commercial market and are gaining in popularity. Due to their monochromatic nature (±5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.\nDisinfection using UV radiation is commonly used in wastewater treatment applications and is finding an increased usage in municipal drinking water treatment. Many bottlers of spring water use UV disinfection equipment to sterilize their water. Solar water disinfection has been researched for cheaply treating contaminated water using natural sunlight. The UVA irradiation and increased water temperature kill organisms in the water.\nUltraviolet radiation is used in several food processes to kill unwanted microorganisms. UV can be used to pasteurize fruit juices by flowing the juice over a high-intensity ultraviolet source. The effectiveness of such a process depends on the UV absorbance of the juice.\nPulsed light (PL) is a technique of killing microorganisms on surfaces using pulses of an intense broad spectrum, rich in UVC between 200 and 280 nm. Pulsed light works with xenon flash lamps that can produce flashes several times per second. Disinfection robots use pulsed UV.\nThe antimicrobial effectiveness of filtered far-UVC (222\u2009nm) light on a range of pathogens, including bacteria and fungi showed inhibition of pathogen growth, and since it has lesser harmful effects, it provides essential insights for reliable disinfection in healthcare settings, such as hospitals and long-term care homes. UVC has also been shown to be effective at degrading SARS-CoV-2 virus.\n==== Biological ====\nSome animals, including birds, reptiles, and insects such as bees, can see near-ultraviolet wavelengths. Many fruits, flowers, and seeds stand out more strongly from the background in ultraviolet wavelengths as compared to human color vision. Scorpions glow or take on a yellow to green color under UV illumination, thus assisting in the control of these arachnids. Many birds have patterns in their plumage that are invisible at usual wavelengths but observable in ultraviolet, and the urine and other secretions of some animals, including dogs, cats, and human beings, are much easier to spot with ultraviolet. Urine trails of rodents can be detected by pest control technicians for proper treatment of infested dwellings.\nButterflies use ultraviolet as a communication system for sex recognition and mating behavior. For example, in the Colias eurytheme butterfly, males rely on visual cues to locate and identify females. Instead of using chemical stimuli to find mates, males are attracted to the ultraviolet-reflecting color of female hind wings. In Pieris napi butterflies it was shown that females in northern Finland with less UV-radiation present in the environment possessed stronger UV signals to attract their males than those occurring further south. This suggested that it was evolutionarily more difficult to increase the UV-sensitivity of the eyes of the males than to increase the UV-signals emitted by the females.\nMany insects use the ultraviolet wavelength emissions from celestial objects as references for flight navigation. A local ultraviolet emitter will normally disrupt the navigation process and will eventually attract the flying insect.\nThe green fluorescent protein (GFP) is often used in genetics as a marker. Many substances, such as proteins, have significant light absorption bands in the ultraviolet that are of interest in biochemistry and related fields. UV-capable spectrophotometers are common in such laboratories.\nUltraviolet traps called bug zappers are used to eliminate various small flying insects. They are attracted to the UV and are killed using an electric shock, or trapped once they come into contact with the device. Different designs of ultraviolet radiation traps are also used by entomologists for collecting nocturnal insects during faunistic survey studies.\n==== Therapy ====\nUltraviolet radiation is helpful in the treatment of skin conditions such as psoriasis and vitiligo. Exposure to UVA, while the skin is hyper-photosensitive, by taking psoralens is an effective treatment for psoriasis. Due to the potential of psoralens to cause damage to the liver, PUVA therapy may be used only a limited number of times over a patient\'s lifetime.\nUVB phototherapy does not require additional medications or topical preparations for the therapeutic benefit; only the exposure is needed. However, phototherapy can be effective when used in conjunction with certain topical treatments such as anthralin, coal tar, and vitamin A and D derivatives, or systemic treatments such as methotrexate and Soriatane.\n==== Herpetology ====\nReptiles need UVB for biosynthesis of vitamin D, and other metabolic processes. Specifically cholecalciferol (vitamin D3), which is needed for basic cellular / neural functioning as well as the utilization of calcium for bone and egg production. The UVA wavelength is also visible to many reptiles and might play a significant role in their ability survive in the wild as well as in visual communication between individuals. Therefore, in a typical reptile enclosure, a fluorescent UV a/b source (at the proper strength / spectrum for the species), must be available for many captive species to survive. Simple supplementation with cholecalciferol (Vitamin D3) will not be enough as there is a complete biosynthetic pathway that is "leapfrogged" (risks of possible overdoses), the intermediate molecules and metabolites also play important functions in the animals health. Natural sunlight in the right levels is always going to be superior to artificial sources, but this might not be possible for keepers in different parts of the world.\nIt is a known problem that high levels of output of the UVa part of the spectrum can both cause cellular and DNA damage to sensitive parts of their bodies – especially the eyes where blindness is the result of an improper UVa/b source use and placement photokeratitis. For many keepers there must also be a provision for an adequate heat source this has resulted in the marketing of heat and light "combination" products. Keepers should be careful of these "combination" light/ heat and UVa/b generators, they typically emit high levels of UVa with lower levels of UVb that are set and difficult to control so that animals can have their needs met. A better strategy is to use individual sources of these elements and so they can be placed and controlled by the keepers for the max benefit of the animals.\n== Evolutionary significance ==\nThe evolution of early reproductive proteins and enzymes is attributed in modern models of evolutionary theory to ultraviolet radiation. UVB causes thymine base pairs next to each other in genetic sequences to bond together into thymine dimers, a disruption in the strand that reproductive enzymes cannot copy. This leads to frameshifting during genetic replication and protein synthesis, usually killing the cell. Before formation of the UV-blocking ozone layer, when early prokaryotes approached the surface of the ocean, they almost invariably died out. The few that survived had developed enzymes that monitored the genetic material and removed thymine dimers by nucleotide excision repair enzymes. Many enzymes and proteins involved in modern mitosis and meiosis are similar to repair enzymes, and are believed to be evolved modifications of the enzymes originally used to overcome DNA damages caused by UV.\nElevated levels of ultraviolet radiation, in particular UV-B, have also been speculated as a cause of mass extinctions in the fossil record.\n== Photobiology ==\nPhotobiology is the scientific study of the beneficial and harmful interactions of non-ionizing radiation in living organisms, conventionally demarcated around 10 eV, the first ionization energy of oxygen. UV ranges roughly from 3 to 30 eV in energy. Hence photobiology entertains some, but not all, of the UV spectrum.\n== See also ==\n== References ==\n== Further reading ==\nAllen, Jeannie (6 September 2001). Ultraviolet Radiation: How it Affects Life on Earth. Earth Observatory. NASA, USA.\nHockberger, Philip E. (2002). "A History of Ultraviolet Photobiology for Humans, Animals and Microorganisms". Photochemistry and Photobiology. 76 (6): 561–569. doi:10.1562/0031-8655(2002)0760561AHOUPF2.0.CO2. PMID 12511035. S2CID 222100404.\nHu, S; Ma, F; Collado-Mesa, F; Kirsner, R. S. (July 2004). "UV radiation, latitude, and melanoma in US Hispanics and blacks". Arch. Dermatol. 140 (7): 819–824. doi:10.1001/archderm.140.7.819. PMID 15262692.\nStrauss, CEM; Funk, DJ (1991). "Broadly tunable difference-frequency generation of VUV using two-photon resonances in H2 and Kr". Optics Letters. 16 (15): 1192–4. Bibcode:1991OptL...16.1192S. doi:10.1364/ol.16.001192. PMID 19776917.\n== External links ==\nMedia related to Ultraviolet light at Wikimedia Commons\nThe dictionary definition of ultraviolet at Wiktionary']

Question: Who was the first person to describe the pulmonary circulation system?

Choices:
Choice A) Galen
Choice B) Avicenna
Choice C) Hippocrates
Choice D) Aristotle
Choice E) Ibn al-Nafis

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Qatar University', 'Qatar University (Arabic: جامعة قطر; transliterated: Jami\'at Qatar) is a public research university located on the northern outskirts of Doha, Qatar. It is the only public university in the country. The university hosts twelve colleges – Arts and Sciences, Business and Economics, Education, Engineering, Law, Sharia and Islamic Studies, Pharmacy, College of Health Science, College of Medicine, College of Dental Medicine, College of Pharmacy, College of nursing, and College of Sport Science.\nCourses are taught in Arabic and English. Students entering the University are sometimes placed in a “Foundation Program”, which ensures the acquirement of skills such as Math and English.\nMany of its academic departments have received or are currently under evaluation for accreditation from a number of organizations. In addition to undergraduate academics, QU has a research infrastructure including research labs, an ocean vessel, technical equipment and a library including a collection of rare manuscripts.\nThe university serves on behalf of the government and private industry to conduct regional research, particularly in areas of the environment and energy technologies. Qatar University has a student body of fifty-two nationalities, 65% of which are Qatari nationals. About 35% are children of expats. Women make up approximately 70% of the student population, and are provided their own set of facilities and classrooms. QU has an alumni body of over 30,000 graduates, and an active student body of over 20,000 students.\n== History ==\nThe institution was established as the College of Education by a decree from the Emir of Qatar in 1973. The college began with a total of 150 students (93 women and 57 men) and was later expanded to become the University of Qatar in 1977 with four new colleges : Education, Humanities & Social Sciences, Sharia & Law & Islamic Studies, and Science.\nThree years later, the College of Engineering was established. By then, the number of enrolled students was 2,600. This was followed by the establishment of the College of Business & Economics in 1985. The new colleges prompted a large expansion of the university campus, which was overseen Kamal El Kafrawi. By Fall Semester 2005 / 2006, the number of registrants for study at Qatar University had reached 7660 male and female students, equaling almost 1/6 of the eligible Qatari population.\nAs of 2011, there are seven colleges: College of Education, College of Arts and Sciences, College of Shariah and Islamic Studies, College of Engineering, College of Law, College of Business & Economics, and College of Pharmacy. The new College of Pharmacy was established in 2006, with its first intake of BSc (Pharm) students in 2007.\nBetween 2003 and 2015, the president of the university was Sheikha Abdulla Al-Misnad. She left in 2015. Her replacement was Hassan Rashid Al-Derham, who currently holds the position, and is the university\'s sixth president. Al-Derham received his doctoral degree from the University of South Wales (United Kingdom).\nIn December 2015, the university awarded its first-ever honorary doctorate degree to Turkish president Recep Tayyip Erdoğan.\nIn 2019, the College of Dental Medicine was founded, which is the tenth college for Qatar University. The college has a competitive 25 seats in its first class and is a six-year program leading to a Doctor of Dental Medicine.\n== Strategies ==\nThe Qatar University \'Reform Project (2003-2007)\' evaluated and restructured the university administration and direction to enhance the quality of instruction and place emphasis on research. The reform was initiated in 2003, led by Sheikh Tamim bin Hamad Al Thani, QU Former President Dr. Sheikha Al Misnad, and the newly established Office of Institutional Research and Planning (OIPD). It focused on three principles; “Autonomy”, “Decentralization”, and “Accountability”. While the university had previously operated as a government entity, the reformed institution would be an autonomous body governed by a board of regents who reported to the Emir. This change allowed the University to manage its own finances, stated objectives and vision, and personnel and decentralization within the university granted similar financial and personnel control to colleges, departments and programs.\nAcademically, the reform resulted in the establishment of offices such as the Student Learning Support Center (SLSC) and Student Counseling Center (SCC). Construction was undertaken to ensure accessibility of university facilities by handicapped persons. A newsletter, Tawasol, began publication in the university\nThe reform changed the title of the university from “University of Qatar” to “Qatar University” with a new slogan; “Qatar University, Changing for You” and a new university logo.\nA new strategy was put in place "from reform to transformation" covering the years 2018–2022. This new strategy takes into consideration the Qatar National Vision 2030.\n== Local significance ==\nThe university contributes to the process of “Qatarization”, which places an emphasis on the hiring and support of national citizens. While western nations may have trouble implementing such a system due to equality legislation, Qatari nationals only account for approximately 1/4 of the country\'s population, and this movement is deemed necessary to maintain cultural and national identity.\nOn 9 October 2021, Qatar University hosted 3MT (Three Minutes Thesis) competition with the participation of six local universities (Doha Institute for Graduate Studies, Hamad Bin Khalifa University, Al Rayyan International University College, Virginia Commonwealth University- Qatar, Texas A&M University at Qatar (Tamuq) and University of Calgary-Qatar). The 3MT Competition is an academic competition that challenges Masters and PhD students to describe their research within three minutes to a general audience.\n== World view and influence ==\nQatari leaders have recognized the vulnerability of oil and natural gases as a long-term economic model, especially for a smaller area such as Qatar.\nThe university has directed funding towards contributions to international projects. This has included taking part in global environmental studies through regional measurements, promotion of energy-awareness, and the recent contribution to CERN of data gathered through the university\'s new positron beam.\nSince graduate programs are not available in many fields, Qatar University often works closely with a network of international affiliate schools. Students who have shown exceptional potential or progress can often receive sponsorship from the university for graduate studies abroad, on the condition that they will return to work once finished.\n== Construction ==\nQatar University is situated on the northern edge of Doha in the district of Al Tarfa, approximately 16 kilometers from the city center. Due to the growth of the city, this area has recently become more valuable, and a popular development site for upscale residential and commercial buildings. QU has agreed to lease a portion of its property to the construction of new commercial zones to the north and east, as well as a substantial plot for the College of the North Atlantic to the south.\nA QAR 20 million Scientific and Applied Research Center is under construction.\n== Colleges and Departments ==\n=== College of Arts and Sciences ===\nThe College of Arts and Sciences was established in 2004 through the merging of two former colleges; the College of Humanities and Social Sciences, and the College of Science. It is the largest college by both number of programs and student population at Qatar University, with a total of 2,383 students; 1,933 Arts majors and 450 Science majors. This reflects approximately 37% of the student body. The college has 240 faculty members, including Dean Dr. Kassim Ali Shaaban.\nDepartments:\nDepartment of Arabic Language\nHistory\nDepartment of Biological & Environmental Sciences\nBiological Sciences\nEnvironmental Sciences\nDepartment of Chemistry & Earth Sciences\nChemistry Program accredited by the CSC\nDepartment of English Literature and Linguistics\nDepartment of Health Sciences\nBiomedical Program accredited by the NAACLS\nHuman Nutrition Program\nPublic health\nDepartment of Humanities\nDepartment of Mass Communication\nMass Communication Program\nDepartment of Mathematics, Statistics & Physics\nDepartment of Social Sciences\nSocial Work\nPsychology\nSociology\nInternational Affairs\nPolicy, Planning and Development\nStatistics\nSport Science\nPrograms:\nArabic for Non-Native Speakers Program\n=== College of Business & Economics ===\nFounded in 1985, it has begun work on a new QR 185 million facility to accommodate its student body and provide resources. Dr. Nitham M. Hindi was appointed as Dean in August 2010.\nDepartments:\nAccounting and Information Systems\nFinance and Economics\nManagement and Marketing\nMarketing\nThe College is accredited by the Association to Advance Collegiate Schools of Business (AACSB). An MBA degree program is available for graduate students, as well as a CPA testing program.\n=== College of Education ===\nThe College of Education was the primary academic body under which Qatar University was founded in 1973. The dean is Dr. Hissa Sadiq.\nDepartments:\nEducational Sciences\nPsychological Sciences\nArt Education\n=== College of Engineering ===\nThe College of Engineering was established in 1980, and has become one of the largest at Qatar University. The college offers both undergraduate and graduate courses.\nThe college\'s previous dean, Dr. Alfadala, was also the founder and former chairman of the university\'s Gas Processing Center (GPC) research facility. The current Dean is Dr. Khalifa Al-Khalifa.\n==== Programs ====', "Departments:\nEducational Sciences\nPsychological Sciences\nArt Education\n=== College of Engineering ===\nThe College of Engineering was established in 1980, and has become one of the largest at Qatar University. The college offers both undergraduate and graduate courses.\nThe college's previous dean, Dr. Alfadala, was also the founder and former chairman of the university's Gas Processing Center (GPC) research facility. The current Dean is Dr. Khalifa Al-Khalifa.\n==== Programs ====\nThe College of Engineering offers the following undergraduate programs: Bachelor of Architecture, Bachelor of Science in Chemical Engineering, Bachelor of Science in Civil Engineering, Bachelor of Science in Computer Engineering, Bachelor of Science in Computer Science, Bachelor of Science in Electrical Engineering, Bachelor of Science in Industrial and Systems Engineering, Bachelor of Science in Mechanical Engineering.\nThe College of Engineering offers the following graduate programs: Masters of Science in Computing, Masters of Urban Planning & Design, Masters of Science in Engineering Management, Masters of Science in Environmental Engineering, Master of Science in Civil Engineering, Master of Science in Mechanical Engineering, Master of Science in Electrical Engineering.\nThe College of Engineering offers the following Doctor of Philosophy (PhD) programs: Doctor of Philosophy in Architecture, Doctor of Philosophy in Urban Planning, Doctor of Philosophy Chemical Engineering, Doctor of Philosophy in Computer Science, Doctor of Philosophy in Computer Engineering, Doctor of Philosophy in Civil Engineering, Doctor of Philosophy in Electrical Engineering, Doctor of Philosophy in Industrial and Systems Engineering, Doctor of Philosophy in Mechanical Engineering, Doctor of Philosophy in Engineering Management, Doctor of Philosophy in Environmental Engineering, Doctor of Philosophy in Material Science and Engineering.\n==== Departments ====\nDepartments are:\nArchitectural Engineering\nChemical Engineering\n(ABET Substantial Equivalency accredited)\nCivil Engineering\n(ABET Substantial Equivalency accredited)\nComputer Engineering\n(ABET Substantial Equivalency accredited)\nComputer Science\n(ABET Substantial Equivalency accredited)\nElectrical Engineering (former chairman: Soliman Abdel-hady Soliman)\n(ABET Substantial Equivalency accredited)\nMechanical Engineering\n(ABET Substantial Equivalency accredited)\nIndustrial & System Engineering\n(ABET Substantial Equivalency accredited)\n==== Deans ====\nProf. Galal Shawky (1980-1990)\nProf. Ismael Abdulrahman Taj (1990-1999)\nDr. Mohammed Al-Hammadi (2000-2001)\nProf. Adnan Nayfah (2003-2005)\nDr. Nabil Al-Salem (2005-2006)\nDr. Hassan Al-Fadala (2006-2008)\nProf. Mazen Hasna (2008-2013)\nProf. Rashid Alammari (2013-2016)\nProf. Khalifa Nasser Al-Khalifa (2016-2018)\nProf. Abdelmagid Hamouda (2018–2019)\nDr. Khalid Kamal Naji (2019–Present)\n==== Notable alumni ====\nNotable alumni include:\nEng. Fawaz Al-Baker executive managing director Qatar Power Company\nNasser Al-Khelaifi\nIbrahim bin Yousuf Al-Fakhro\nSaad Ahmed Al Mohannadi\nIlham Al-Qaradawi\nDr. Yousef Alhorr\n==== Research Centers ====\nGas Processing Center\nThe center addresses the problems, challenges and opportunities facing the state of Qatar's gas processing industry. The center is focused on two main themes which are Asset Management/Process Optimization and Sustainable Development.\nThe services provided by the center have been designed to address the necessities and challenges of both Qatar University and the Qatari Industry. These services include: Applied Research Projects/Consulting, Professional Training and Seminars, Bi-annual Gas Processing Symposium, Information Management/Library.\nKindi Lab for Computing Research\nQatar Transportation and Traffic Safety Center\nRoad traffic accidents have major societal, health, environmental and economic impacts on Qatar's economy. Moreover, the accidents cause significant delays and traffic congestions. The expected increase in population and special events that occur in Qatar on regular basis as well as the above impacts have prompted the College of Engineering, Qatar University to establish Qatar Transportation and Traffic Safety Center (QTTSC) to conduct extensive studies and analysis of accidents data and information in order to significantly reduce such road accidents.\nThe studies will include studies related to patterns of accidents, factors that contribute to road accidents, drivers' attributes and others as well as recommendations for approaches that result in an improved road safety.\nThere are three major areas of concern: Road User Behavioral Change, Vehicle Safety and Biomechanics and Road Engineering and Environment. The center will be housed and managed by the College of Engineering and its funding will be obtained from different sources including Qatar University, companies and government agencies.\nChemE Car Competition\nThe ChemE competition's aim is to encourage creativity and innovation among students and also to reach out high school students to motivate them towards studies in chemical engineering.\nThe competition requires participants to design and race a small car that is operated through chemical processes and carrying a load of water (0-500g) for a distance of between 15 and 30 meters.\n=== College of Law ===\nIn 2004, Qatar University instated a College of Law by separating the law department of the existing College of Sharia. 10%-15% of its undergraduate program is instructed in English. In 2008, it asked the ABA to conduct a full-scale, on-site evaluation of all aspects of the school's objectives, programs, and administration. Many of the recommendations made by the ABA were subsequently implemented including the introduction of the first legal writing and research skills program (taught in English) established in any law school in the Middle East. The legal skills program was recognized at the 13th Global Legal Skills Conference held in Melbourne, Australia in December 2018.\nThe College of Law has the highest percentage of Qatari students of any college in Qatar University.\nThe College of Law is accredited by the High Council for the Evaluation of Research and Higher Education (HCERES) and the British Accreditation Council (BAC). The College of Law also partners with leading law schools, and welcomes visiting Fulbright fellows and exchange students.\nThe International Review of Law is an international legal periodical that is published by the College of Law (through Qatar University Press) biannually. It is an internationally peer reviewed multi-lingual law journal that seeks to articulate contemporary legal discourse across cultures and borders. The journal is open to doctrinal, context based, reformative or comparative work, in all fields of law. The journal accepts submissions in English, Arabic and French and provides abstract translations for all publications. The chief editor of the journal is currently Dr. Sonia Malak.\nDr. Mohamed Abdulaziz Al Khulaifi was appointed dean in 2014.\nIn November 2018, the College of Law hosted the Annual Conference of the International Association of Law Schools.\nIn April 2020, the College of Law will move into its new purpose build facility currently under construction.\nPrograms:\nUndergraduate Law\nLLM in Public Law\nLLM in Private Law\nGraduate Certificate in Legal Studies\n=== College of Pharmacy ===\nThe College of Pharmacy at Qatar University was founded as a college in 2008. It is the first pharmacy college to be established in Qatar. It began as a program in 2006, and saw its first student intake in 2007. The year 2008 also marked the college's accreditation by the CCAPP (Canada) and became the first international pharmacy program to receive accreditation by that organization. Peter Jewesson was the founding College Dean, and had also been the director of the previous Pharmacy program.\nThe College of Pharmacy offers three degrees:\n5-year program Bachelor of Science in Pharmacy – BSc (Pharm)\nAccredited by CCAPP\n6-year Doctor of Pharmacy - PharmD\nThe PharmD degree is targeted for select graduates pursuing advanced clinical training. It is offered as a full-time study plan for QU graduates, and a part-time study plan for BSc (Pharm) pharmacists practicing in Qatar\n2-year Master of Science in Pharmacy - MSc (Pharm)\nThe MSc (Pharm) degree is designed to build on the undergraduate degree experience.\n=== College of Sharia and Islamic Studies ===\nCollege of Sharia and Islamic Studies was among the first founded at Qatar University when it was established in 1977. In recent years, it added new major and minor programs in subjects such as “Da’wa and Media” and “Banking and Insurance”. Dr. Aisha Yousuf Al-Mannai is the current dean.\nDepartments:\nIslamic Jurisprudence\nIslamic Culture & Preaching\nFoundations of Islam\n=== College of Medicine ===\nIn December 2014, Qatar University announced the establishment of the College of Medicine which was planned to take the 1st batch in by Fall 2015 (September) and graduate them by 2021.\n=== College of Health Science ===\nThe College of Health Science (CHS) was founded in 2016 following 30 years as a department within various colleges at Qatar University. CHS is the national provider of health sciences education and research in Qatar and offers undergraduate (BSc) programs in biomedical sciences, human nutrition and public health, as well as graduate programs; (MSc) in biomedical science and master of public health (MPH).\n=== Sport Science Program ===\nThe Sport Science Program was opened to students in the fall 2009 semester. The program was constructed as a joint project sponsored by Aspire Academy.\nWhile QU's Sport Science program is not an independent college, it has been formed with autonomy from the other colleges, much as the College of Pharmacy began.\nThe Program offers a Bachelor of Science degree which allows for one of 3 concentrations:\nSport Management\nExercise and Fitness\nPhysical Education\n== Honors Program ==", 'The Sport Science Program was opened to students in the fall 2009 semester. The program was constructed as a joint project sponsored by Aspire Academy.\nWhile QU\'s Sport Science program is not an independent college, it has been formed with autonomy from the other colleges, much as the College of Pharmacy began.\nThe Program offers a Bachelor of Science degree which allows for one of 3 concentrations:\nSport Management\nExercise and Fitness\nPhysical Education\n== Honors Program ==\nQatar University\'s Honors Program was established in 2009. to provide academic opportunities for high-achieving students. Students in the program complete 24 credit hours of honors courses, as part of the 120 credit hours necessary for their undergraduate program. Students must graduate with an overall GPA of 3.5 or above, with a minimum score of 3.0 in all honors courses. Qatar University holds strict requirements for students wishing to apply:\nMinimum of 90% or higher in high school certificate or an equivalent certificate (current QU students must complete 12 to 18 post-foundation credit hours with a 3.5 GPA)\nMinimum score of 500 on the TOEFL exam or 5.5 on the ILETS\nMinimum score of 550 in the Math portion of SAT, 24 on the ACT\nTwo letters of recommendation from current or previous instructors, counselors or academic advisers\nCopy of transcript\nA written essay\nPass an interview\nIn addition to academic advisers, Honors students are assigned an adviser to assist with honors issues and other consultation. For courses which are not offered as Honors, students may propose an "Honors Contract" to specify honors-level objectives and goals to be monitored by a sponsoring professor.\n== Qatar University student clubs ==\nQatar University is the biggest and most popular university in Qatar, as stated by UniRank. It has 10 colleges that teach the courses in either Arabic or English. Qatar University Student clubs represent the non-academic aspect of student\'s life. These clubs are student meetings that fall under the supervision of the Student Activities Department. It aims to build the educational, life and social experience of students.\n=== Student Clubs ===\nStudent clubs are divided into three categories:\nDepartmental and College clubs such as the Statistics Club\nTalent and skill clubs such as the Voice Club and the Poetry Club\nClubs and public associations, such as the Book Club\n== Research centers ==\nResearch is conducted in and across colleges and is buoyed by an increased research budget, a multimillion-dollar Research Complex and partnerships.\n18 centers of research\nBiomedical Research Center (BRC)\nCenter for Advanced Materials (CAM)\nEnvironmental Science Center (ESC)\nSocial and Economic Survey Research Institute (SESRI)\nLaboratory Animal Research Center (LARC)\nQatar University Young Scientists Center (QUYSC)\nIbn Khaldon Center for Humanities and Social Sciences\nCentral Lab Unit (CLU)\nCenter for Entrepreneurship (CFE)\nCenter for Sustainable Development (CSD)\nCentre for Law and Development (CLD)\nEarly Childhood Center\nGas Processing Center (GPC)\nGulf Studies Center (GSC)\nKINDI Center for Computing Research (KINDI)\nNational Center for Educational Development (NCED)\nQatar Mobility Innovation Center (QMIC)\nQatar Transportation and Traffic Safety Center (QTTSC)\n== Notable alumni ==\nNasser Al-Khelaifi, businessman, president of Paris Saint-Germain\nMariam Al Maadeed, Qatari scientist, vice president for research and graduate studies at Qatar University\nAmal Al-Malki, academic\nNoor Al Mazroei, chef and activist\nSaad Al Mohannadi, Qatari president of Public Works Authority Ashgal\nMoza bint Nasser, consort of Hamad bin Khalifa Al Thani\nNourah Al Saad, writer\nFatma Al Sharshani, mural artist and calligrapher\nAbdulla bin Abdulaziz bin Turki Al Subaie, Qatari Minister of Municipality\nAbdulrahman bin Hamad bin Jassim bin Hamad Al Thani, Qatari Minister of Culture\nJawaher bint Hamad bin Suhaim Al Thani, wife of the Emir of Qatar\nMohammed bin Abdulrahman bin Jassim Al Thani, Qatari Prime Minister\n== See also ==\nList of Islamic educational institutions\nQatar University Library\nQatar University Stadium\nEducation in Qatar\nList of universities and colleges in Qatar\n== References ==', "== Importance ==\nFrom the perspective of a planetary geologist, the atmosphere acts to shape a planetary surface. Wind picks up dust and other particles which, when they collide with the terrain, erode the relief and leave deposits (eolian processes). Frost and precipitations, which depend on the atmospheric composition, also influence the relief. Climate changes can influence a planet's geological history. Conversely, studying the surface of the Earth leads to an understanding of the atmosphere and climate of other planets.\nFor a meteorologist, the composition of the Earth's atmosphere is a factor affecting the climate and its variations.\nFor a biologist or paleontologist, the Earth's atmospheric composition is closely dependent on the appearance of life and its evolution.\n== See also ==\nAtmometer (evaporimeter)\nAtmospheric pressure\nInternational Standard Atmosphere\nKármán line\nSky\n== References ==\n== Further reading ==\nSanchez-Lavega, Agustin (2010). An Introduction to Planetary Atmospheres. Taylor & Francis. ISBN 978-1420067323.\n== External links ==\nProperties of atmospheric strata – The flight environment of the atmosphere\nAtmosphere – Everything you need to know", 'UVC LEDs are relatively new to the commercial market and are gaining in popularity. Due to their monochromatic nature (±5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.\nDisinfection using UV radiation is commonly used in wastewater treatment applications and is finding an increased usage in municipal drinking water treatment. Many bottlers of spring water use UV disinfection equipment to sterilize their water. Solar water disinfection has been researched for cheaply treating contaminated water using natural sunlight. The UVA irradiation and increased water temperature kill organisms in the water.\nUltraviolet radiation is used in several food processes to kill unwanted microorganisms. UV can be used to pasteurize fruit juices by flowing the juice over a high-intensity ultraviolet source. The effectiveness of such a process depends on the UV absorbance of the juice.\nPulsed light (PL) is a technique of killing microorganisms on surfaces using pulses of an intense broad spectrum, rich in UVC between 200 and 280 nm. Pulsed light works with xenon flash lamps that can produce flashes several times per second. Disinfection robots use pulsed UV.\nThe antimicrobial effectiveness of filtered far-UVC (222\u2009nm) light on a range of pathogens, including bacteria and fungi showed inhibition of pathogen growth, and since it has lesser harmful effects, it provides essential insights for reliable disinfection in healthcare settings, such as hospitals and long-term care homes. UVC has also been shown to be effective at degrading SARS-CoV-2 virus.\n==== Biological ====\nSome animals, including birds, reptiles, and insects such as bees, can see near-ultraviolet wavelengths. Many fruits, flowers, and seeds stand out more strongly from the background in ultraviolet wavelengths as compared to human color vision. Scorpions glow or take on a yellow to green color under UV illumination, thus assisting in the control of these arachnids. Many birds have patterns in their plumage that are invisible at usual wavelengths but observable in ultraviolet, and the urine and other secretions of some animals, including dogs, cats, and human beings, are much easier to spot with ultraviolet. Urine trails of rodents can be detected by pest control technicians for proper treatment of infested dwellings.\nButterflies use ultraviolet as a communication system for sex recognition and mating behavior. For example, in the Colias eurytheme butterfly, males rely on visual cues to locate and identify females. Instead of using chemical stimuli to find mates, males are attracted to the ultraviolet-reflecting color of female hind wings. In Pieris napi butterflies it was shown that females in northern Finland with less UV-radiation present in the environment possessed stronger UV signals to attract their males than those occurring further south. This suggested that it was evolutionarily more difficult to increase the UV-sensitivity of the eyes of the males than to increase the UV-signals emitted by the females.\nMany insects use the ultraviolet wavelength emissions from celestial objects as references for flight navigation. A local ultraviolet emitter will normally disrupt the navigation process and will eventually attract the flying insect.\nThe green fluorescent protein (GFP) is often used in genetics as a marker. Many substances, such as proteins, have significant light absorption bands in the ultraviolet that are of interest in biochemistry and related fields. UV-capable spectrophotometers are common in such laboratories.\nUltraviolet traps called bug zappers are used to eliminate various small flying insects. They are attracted to the UV and are killed using an electric shock, or trapped once they come into contact with the device. Different designs of ultraviolet radiation traps are also used by entomologists for collecting nocturnal insects during faunistic survey studies.\n==== Therapy ====\nUltraviolet radiation is helpful in the treatment of skin conditions such as psoriasis and vitiligo. Exposure to UVA, while the skin is hyper-photosensitive, by taking psoralens is an effective treatment for psoriasis. Due to the potential of psoralens to cause damage to the liver, PUVA therapy may be used only a limited number of times over a patient\'s lifetime.\nUVB phototherapy does not require additional medications or topical preparations for the therapeutic benefit; only the exposure is needed. However, phototherapy can be effective when used in conjunction with certain topical treatments such as anthralin, coal tar, and vitamin A and D derivatives, or systemic treatments such as methotrexate and Soriatane.\n==== Herpetology ====\nReptiles need UVB for biosynthesis of vitamin D, and other metabolic processes. Specifically cholecalciferol (vitamin D3), which is needed for basic cellular / neural functioning as well as the utilization of calcium for bone and egg production. The UVA wavelength is also visible to many reptiles and might play a significant role in their ability survive in the wild as well as in visual communication between individuals. Therefore, in a typical reptile enclosure, a fluorescent UV a/b source (at the proper strength / spectrum for the species), must be available for many captive species to survive. Simple supplementation with cholecalciferol (Vitamin D3) will not be enough as there is a complete biosynthetic pathway that is "leapfrogged" (risks of possible overdoses), the intermediate molecules and metabolites also play important functions in the animals health. Natural sunlight in the right levels is always going to be superior to artificial sources, but this might not be possible for keepers in different parts of the world.\nIt is a known problem that high levels of output of the UVa part of the spectrum can both cause cellular and DNA damage to sensitive parts of their bodies – especially the eyes where blindness is the result of an improper UVa/b source use and placement photokeratitis. For many keepers there must also be a provision for an adequate heat source this has resulted in the marketing of heat and light "combination" products. Keepers should be careful of these "combination" light/ heat and UVa/b generators, they typically emit high levels of UVa with lower levels of UVb that are set and difficult to control so that animals can have their needs met. A better strategy is to use individual sources of these elements and so they can be placed and controlled by the keepers for the max benefit of the animals.\n== Evolutionary significance ==\nThe evolution of early reproductive proteins and enzymes is attributed in modern models of evolutionary theory to ultraviolet radiation. UVB causes thymine base pairs next to each other in genetic sequences to bond together into thymine dimers, a disruption in the strand that reproductive enzymes cannot copy. This leads to frameshifting during genetic replication and protein synthesis, usually killing the cell. Before formation of the UV-blocking ozone layer, when early prokaryotes approached the surface of the ocean, they almost invariably died out. The few that survived had developed enzymes that monitored the genetic material and removed thymine dimers by nucleotide excision repair enzymes. Many enzymes and proteins involved in modern mitosis and meiosis are similar to repair enzymes, and are believed to be evolved modifications of the enzymes originally used to overcome DNA damages caused by UV.\nElevated levels of ultraviolet radiation, in particular UV-B, have also been speculated as a cause of mass extinctions in the fossil record.\n== Photobiology ==\nPhotobiology is the scientific study of the beneficial and harmful interactions of non-ionizing radiation in living organisms, conventionally demarcated around 10 eV, the first ionization energy of oxygen. UV ranges roughly from 3 to 30 eV in energy. Hence photobiology entertains some, but not all, of the UV spectrum.\n== See also ==\n== References ==\n== Further reading ==\nAllen, Jeannie (6 September 2001). Ultraviolet Radiation: How it Affects Life on Earth. Earth Observatory. NASA, USA.\nHockberger, Philip E. (2002). "A History of Ultraviolet Photobiology for Humans, Animals and Microorganisms". Photochemistry and Photobiology. 76 (6): 561–569. doi:10.1562/0031-8655(2002)0760561AHOUPF2.0.CO2. PMID 12511035. S2CID 222100404.\nHu, S; Ma, F; Collado-Mesa, F; Kirsner, R. S. (July 2004). "UV radiation, latitude, and melanoma in US Hispanics and blacks". Arch. Dermatol. 140 (7): 819–824. doi:10.1001/archderm.140.7.819. PMID 15262692.\nStrauss, CEM; Funk, DJ (1991). "Broadly tunable difference-frequency generation of VUV using two-photon resonances in H2 and Kr". Optics Letters. 16 (15): 1192–4. Bibcode:1991OptL...16.1192S. doi:10.1364/ol.16.001192. PMID 19776917.\n== External links ==\nMedia related to Ultraviolet light at Wikimedia Commons\nThe dictionary definition of ultraviolet at Wiktionary', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', 'Atmosphere', 'Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.', 'In mountainous areas, heavy snowfall accumulates when air is forced to ascend the mountains and squeeze out precipitation along their windward slopes, which in cold conditions, falls in the form of snow. Because of the ruggedness of terrain, forecasting the location of heavy snowfall remains a significant challenge.\n=== Within the tropics ===\nThe wet, or rainy, season is the time of year, covering one or more months, when most of the average annual rainfall in a region falls.  The term green season is also sometimes used as a euphemism by tourist authorities.  Areas with wet seasons are dispersed across portions of the tropics and subtropics.  Savanna climates and areas with monsoon regimes have wet summers and dry winters. Tropical rainforests technically do not have dry or wet seasons, since their rainfall is equally distributed through the year.  Some areas with pronounced rainy seasons will see a break in rainfall mid-season when the Intertropical Convergence Zone or monsoon trough move poleward of their location during the middle of the warm season.  When the wet season occurs during the warm season, or summer, rain falls mainly during the late afternoon and early evening hours. The wet season is a time when air quality improves, freshwater quality improves, and vegetation grows significantly. Soil nutrients diminish and erosion increases.  Animals have adaptation and survival strategies for the wetter regime. The previous dry season leads to food shortages into the wet season, as the crops have yet to mature. Developing countries have noted that their populations show seasonal weight fluctuations due to food shortages seen before the first harvest, which occurs late in the wet season.\nTropical cyclones, a source of very heavy rainfall, consist of large air masses several hundred miles across with low pressure at the centre and with winds blowing inward towards the centre in either a clockwise direction (southern hemisphere) or counterclockwise (northern hemisphere).  Although cyclones can take an enormous toll in lives and personal property, they may be important factors in the precipitation regimes of places they impact, as they may bring much-needed precipitation to otherwise dry regions.  Areas in their path can receive a year\'s worth of rainfall from a tropical cyclone passage.\n=== Large-scale geographical distribution ===\nOn the large scale, the highest precipitation amounts outside topography fall in the tropics, closely tied to the Intertropical Convergence Zone, itself the ascending branch of the Hadley cell. Mountainous locales near the equator in Colombia are amongst the wettest places on Earth.  North and south of this are regions of descending air that form subtropical ridges where precipitation is low; the land surface underneath these ridges is usually arid, and these regions make up most of the Earth\'s deserts.  An exception to this rule is in Hawaii, where upslope flow due to the trade winds lead to one of the wettest locations on Earth.\n== Measurement ==\nThe standard way of measuring rainfall or snowfall is the standard rain gauge, which can be found in 10 cm (3.9 in) plastic and 20 cm (7.9 in) metal varieties. The inner cylinder is filled by 2.5 cm (0.98 in) of rain, with overflow flowing into the outer cylinder. Plastic gauges have markings on the inner cylinder down to 1⁄4 mm (0.0098 in) resolution, while metal gauges require use of a stick designed with the appropriate 1⁄4 mm (0.0098 in) markings. After the inner cylinder is filled, the amount inside is discarded, then filled with the remaining rainfall in the outer cylinder until all the fluid in the outer cylinder is gone, adding to the overall total until the outer cylinder is empty. These gauges are used in the winter by removing the funnel and inner cylinder and allowing snow and freezing rain to collect inside the outer cylinder. Some add anti-freeze to their gauge so they do not have to melt the snow or ice that falls into the gauge.  Once the snowfall/ice is finished accumulating, or as 30 cm (12 in) is approached, one can either bring it inside to melt, or use lukewarm water to fill the inner cylinder with in order to melt the frozen precipitation in the outer cylinder, keeping track of the warm fluid added, which is subsequently subtracted from the overall total once all the ice/snow is melted.\nWhen a precipitation measurement is made, various networks exist across the United States and elsewhere where rainfall measurements can be submitted through the Internet, such as CoCoRAHS or GLOBE.  If a network is not available in the area where one lives, the nearest local weather office will likely be interested in the measurement.\n=== Hydrometeor definition ===\nA concept used in precipitation measurement is the hydrometeor. Any particulates of liquid or solid water in the atmosphere are known as hydrometeors. Formations due to condensation, such as clouds, haze, fog, and mist, are composed of hydrometeors. All precipitation types are made up of hydrometeors by definition, including virga, which is precipitation which evaporates before reaching the ground. Particles blown from the Earth\'s surface by wind, such as blowing snow and blowing sea spray, are also hydrometeors, as are hail and snow.\n=== Satellite estimates ===\nAlthough surface precipitation gauges are considered the standard for measuring precipitation, there are many areas in which their use is not feasible. This includes the vast expanses of ocean and remote land areas. In other cases, social, technical or administrative issues prevent the dissemination of gauge observations. As a result, the modern global record of precipitation largely depends on satellite observations.\nSatellite sensors now in practical use for precipitation fall into two categories. Thermal infrared (IR) sensors record a channel around 11 micron wavelength and primarily give information about cloud tops. Due to the typical structure of the atmosphere, cloud-top temperatures are approximately inversely related to cloud-top heights, meaning colder clouds almost always occur at higher altitudes. Further, cloud tops with a lot of small-scale variation are likely to be more vigorous than smooth-topped clouds. Various mathematical schemes, or algorithms, use these and other properties to estimate precipitation from the IR data.\nAdditional sensor channels and products have been demonstrated to provide additional useful information including visible channels, additional IR channels, water vapor channels and atmospheric sounding retrievals. However, most precipitation data sets in current use do not employ these data sources.\n== Return period ==\nThe likelihood or probability of an event with a specified intensity and duration is called the return period or frequency. The intensity of a storm can be predicted for any return period and storm duration, from charts based on historical data for the location.  The term 1 in 10 year storm describes a rainfall event which is rare and is only likely to occur once every 10 years, so it has a 10 percent likelihood any given year. The rainfall will be greater and the flooding will be worse than the worst storm expected in any single year. The term 1 in 100 year storm describes a rainfall event which is extremely rare and which will occur with a likelihood of only once in a century, so has a 1 percent likelihood in any given year. The rainfall will be extreme and flooding to be worse than a 1 in 10 year event. As with all probability events, it is possible though unlikely to have two "1 in 100 Year Storms" in a single year.\n== Uneven pattern of precipitation ==\nA significant portion of the annual precipitation in any particular place (no weather station in Africa or South America were considered) falls on only a few days, typically about 50% during the 12 days with the most precipitation.\n== Role in Köppen climate classification ==\nRain forests are characterized by high rainfall, with definitions setting minimum normal annual rainfall between 1,750 and 2,000 mm (69 and 79 in).  A tropical savanna is a grassland biome located in semi-arid to semi-humid climate regions of subtropical and tropical latitudes, with rainfall between 750 and 1,270 mm (30 and 50 in) a year. They are widespread on Africa, and are also found in India, the northern parts of South America, Malaysia, and Australia.  The humid subtropical climate zone is where winter rainfall (and sometimes snowfall) is associated with large storms that the westerlies steer from west to east. Most summer rainfall occurs during thunderstorms and from occasional tropical cyclones.  Humid subtropical climates lie on the east side continents, roughly between latitudes 20° and 40° degrees from the equator.\nAn oceanic (or maritime) climate is typically found along the west coasts at the middle latitudes of all the world\'s continents, bordering cool oceans, as well as southeastern Australia, and is accompanied by plentiful precipitation year-round.  The Mediterranean climate regime resembles the climate of the lands in the Mediterranean Basin, parts of western North America, parts of western and southern Australia, in southwestern South Africa and in parts of central Chile. The climate is characterized by hot, dry summers and cool, wet winters.  A steppe is a dry grassland.  Subarctic climates are cold with continuous permafrost and little precipitation.\n== Effect on agriculture ==']

Question: What is the main focus of the Environmental Science Center at Qatar University?

Choices:
Choice A) Environmental studies, with a main focus on marine science, atmospheric and political sciences.
Choice B) Environmental studies, with a main focus on marine science, atmospheric and physical sciences.
Choice C) Environmental studies, with a main focus on marine science, atmospheric and social sciences.
Choice D) Environmental studies, with a main focus on marine science, atmospheric and biological sciences.
Choice E) Environmental studies, with a main focus on space science, atmospheric and biological sciences.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Modified Newtonian dynamics', '== See also ==\n== Notes ==\n== References ==\n== Further reading ==\nTechnical (books & book-length reviews):\nBanik, Indranil; Zhao, Hongsheng (2022-06-27). "From Galactic Bars to the Hubble Tension: Weighing Up the Astrophysical Evidence for Milgromian Gravity". Symmetry. 14 (7): 1331. arXiv:2110.06936. Bibcode:2022Symm...14.1331B. doi:10.3390/sym14071331. ISSN 2073-8994.\nMerritt, David (2020). A Philosophical Approach to MOND: Assessing the Milgromian Research Program in Cosmology (Cambridge: Cambridge University Press), 282 pp. ISBN 9781108492690\nFamaey, Benoît; McGaugh, Stacy S. (2012). "Modified Newtonian Dynamics (MOND): Observational Phenomenology and Relativistic Extensions". Living Reviews in Relativity. 15 (1): 10. arXiv:1112.3960. Bibcode:2012LRR....15...10F. doi:10.12942/lrr-2012-10. PMC 5255531. PMID 28163623.\nTechnical (review articles):\nMcGaugh, Stacy S. (2015). "A tale of two paradigms: The mutual incommensurability of ΛCDM and MOND". Canadian Journal of Physics. 93 (2): 250–259. arXiv:1404.7525. Bibcode:2015CaJPh..93..250M. doi:10.1139/cjp-2014-0203. S2CID 51822163.\nMilgrom, Mordehai (2015). "MOND theory". Canadian Journal of Physics. 93 (2): 107–118. arXiv:1404.7661. Bibcode:2015CaJPh..93..107M. doi:10.1139/cjp-2014-0211. S2CID 119183394.\nKroupa, Pavel (2015). "Galaxies as simple dynamical systems: Observational data disfavor dark matter and stochastic star formation". Canadian Journal of Physics. 93 (2): 169–202. arXiv:1406.4860. Bibcode:2015CaJPh..93..169K. doi:10.1139/cjp-2014-0179. S2CID 118479184.\nMilgrom, Mordehai (2014). "The MOND paradigm of modified dynamics". Scholarpedia. 9 (6): 31410. Bibcode:2014SchpJ...931410M. doi:10.4249/scholarpedia.31410.\nScarpa, Riccardo (2006). "Modified Newtonian Dynamics, an Introductory Review". AIP Conference Proceedings. Vol. 822. AIP. pp. 253–265. arXiv:astro-ph/0601478. doi:10.1063/1.2189141.\nPopular:\nA non-Standard model, David Merritt, Aeon Magazine, July 2021\nDark matter critics focus on details, ignore big picture, Lee, 14 Nov 2012\nMilgrom, Mordehai (2009). "MOND: Time for a change of mind?". arXiv:0908.3842 [astro-ph.CO].\n"Dark matter" doubters not silenced yet Archived 2016-05-20 at the Wayback Machine, World Science, 2 Aug 2007\nDoes Dark Matter Really Exist?, Milgrom, Scientific American, Aug 2002\n== External links ==\nMedia related to Modified Newtonian Dynamic at Wikimedia Commons\nMordehai Milgrom\'s website\nLarge collection of lectures and talks on Youtube', '=== Rotation curves ===\nIn addition to demonstrating that rotation curves in MOND are flat, equation 2 provides a concrete relation between a galaxy\'s total baryonic mass (the sum of its mass in stars and gas) and its asymptotic rotation velocity. This predicted relation was called the mass-asymptotic speed relation (MASSR) by Milgrom; its observational manifestation is known as the baryonic Tully–Fisher relation (BTFR), and is found to conform quite closely to the MOND prediction. This relation is derived from the Deep-MOND limit as follows:\nMilgrom\'s law fully specifies the rotation curve of a galaxy given only the distribution of its baryonic mass. In particular, MOND predicts a far stronger correlation between features in the baryonic mass distribution and features in the rotation curve than does the dark matter hypothesis (since dark matter dominates the galaxy\'s mass budget and is conventionally assumed not to closely track the distribution of baryons). Such a tight correlation is claimed to be observed in several spiral galaxies, a fact which has been referred to as "Renzo\'s rule".\nSince MOND modifies Newtonian dynamics in an acceleration-dependent way, it predicts a specific relationship between the acceleration of a star at any radius from the centre of a galaxy and the amount of unseen (dark matter) mass within that radius that would be inferred in a Newtonian analysis. This is known as the mass discrepancy-acceleration relation, and has been measured observationally. One aspect of the MOND prediction is that the mass of the inferred dark matter goes to zero when the stellar centripetal acceleration becomes greater than a0, where MOND reverts to Newtonian mechanics. In a dark matter hypothesis, it is a challenge to understand why this mass should correlate so closely with acceleration, and why there appears to be a critical acceleration above which dark matter is not required.\nParticularly massive galaxies are within the Newtonian regime (a > a0) out to radii enclosing the vast majority of their baryonic mass. At these radii, MOND predicts that the rotation curve should fall as 1/r, in accordance with Kepler\'s Laws. In contrast, from a dark matter perspective one would expect the halo to significantly boost the rotation velocity and cause it to asymptote to a constant value, as in less massive galaxies. Observations of high-mass ellipticals bear out the MOND prediction.\nIn 2020, a group of astronomers analyzing data from the Spitzer Photometry and Accurate Rotation Curves (SPARC) sample together with estimates of the large-scale external gravitational field from an all-sky galaxy catalog, concluded that there was highly statistically significant evidence of violations of the strong equivalence principle in weak gravitational fields in the vicinity of rotationally supported galaxies. They observed an effect consistent with the external field effect of modified Newtonian dynamics and inconsistent with tidal effects in the Lambda-CDM model paradigm commonly known as the Standard Model of Cosmology.\nIn 2023, a study claimed that cold dark matter cannot explain galactic rotation curves, while MOND can.\n=== Dwarf galaxies ===\nRecent work has shown that many of the dwarf galaxies around the Milky Way and Andromeda are located preferentially in a single plane and have correlated motions. This suggests that they may have formed during a close encounter with another galaxy and hence are tidal dwarf galaxies. If so, the presence of mass discrepancies in these systems constitutes evidence for MOND. In addition, it has been claimed that a gravitational force stronger than Newton\'s (such as Milgrom\'s) is required for these galaxies to retain their orbits over time. Centaurus A has a similar plane of dwarf galaxies around it which is challenging for LCDM which expects uniform halos of dwarf galaxies.\nIn MOND, all isolated gravitationally bound objects with a < a0 that are in equilibrium – regardless of their origin – should exhibit a mass discrepancy when analyzed using Newtonian mechanics, and should lie on the BTFR. Under the dark matter hypothesis, objects formed from baryonic material ejected during the merger or tidal interaction of two galaxies ("tidal dwarf galaxies") are expected to be devoid of dark matter and hence show no mass discrepancy. Three objects unambiguously identified as tidal dwarf galaxies appear to have mass discrepancies in agreement with the MOND prediction.\nIn a 2022 published survey of dwarf galaxies from the Fornax Deep Survey (FDS) catalogue, a group of astronomers and physicists conclude that \'observed deformations of dwarf galaxies in the Fornax Cluster and the lack of low surface brightness dwarfs towards its centre are incompatible with ΛCDM expectations but well consistent with MOND.\'\n=== Gravitational lensing ===\nWeak gravitational lensing around isolated spiral and elliptical galaxies confirms the gravitational field of such galaxies follows Milgrom\'s law. This corresponds to flat rotation curves out to distances of 1 Mpc.\nStrong gravitational lensing using Einstein rings also seems to confirm the MOND expectation for the mass discrepancy-acceleration relation.\n=== Other ===\nBoth MOND and dark matter halos stabilize disk galaxies, helping them retain their rotation-supported structure and preventing their transformation into elliptical galaxies. In MOND, this added stability is only available for regions of galaxies within the deep-MOND regime (i.e., with a < a0), suggesting that spirals with a > a0 in their central regions should be prone to instabilities and hence less likely to survive to the present day. This may explain the "Freeman limit" to the observed central surface mass density of spiral galaxies, which is roughly a0/G. This scale must be put in by hand in dark matter-based galaxy formation models.\nGalactic bars in barred galaxies are in tension with dark matter simulations as they are too pronounced and rotate too fast, yet do match MOND based calculations.\nIn 2022, Kroupa et al. published a study of open star clusters, arguing that asymmetry in the population of leading and trailing tidal tails, and the observed lifetime of these clusters, are inconsistent with Newtonian dynamics but consistent with MOND.\nIn 2023, a study measured the acceleration of 26,615 wide binaries within 200 parsecs. The study showed that those binaries with accelerations less than 1 nm/s2 systematically deviate from Newtonian dynamics, but conform to MOND predictions, specifically to AQUAL. The results are disputed, with some authors arguing that the detection is caused by poor quality controls, while the original authors claimed that the added quality controls do not significantly affect the results.\nIn 2024, a study claimed that the universe\'s earliest galaxies formed and grew too quickly for the Lambda-CDM model to explain, but such rapid growth is predicted in MOND.\n== Responses and criticism ==\n=== Dark matter explanation ===\nWhile acknowledging that Milgrom\'s law provides a succinct and accurate description of a range of galactic phenomena, many physicists reject the idea that classical dynamics itself needs to be modified and attempt instead to explain the law\'s success by reference to the behavior of dark matter. Some effort has gone towards establishing the presence of a characteristic acceleration scale as a natural consequence of the behavior of cold dark matter halos, although Milgrom has argued that such arguments explain only a small subset of MOND phenomena. An alternative proposal is to ad hoc modify the properties of dark matter (e.g., to make it interact strongly with itself or baryons) in order to induce the tight coupling between the baryonic and dark matter mass that the observations point to. Finally, some researchers suggest that explaining the empirical success of Milgrom\'s law requires a more radical break with conventional assumptions about the nature of dark matter. One idea (dubbed "dipolar dark matter") is to make dark matter gravitationally polarizable by ordinary matter and have this polarization enhance the gravitational attraction between baryons.\n=== Outstanding problems for MOND ===\nSome ultra diffuse galaxies, such as NGC 1052-DF2, originally appeared to be free of dark matter. Were this the case, it would have posed a problem for MOND because it cannot explain the rotation curves. However, further research showed that the galaxies were at a different distance than previously thought, leaving the galaxies with plenty of room for dark matter. The idea that a single value of a0 can fit all the different galaxies\' rotation curves has also been criticized, although this finding is disputed. It has also been claimed that MOND offers a poor fit to both the HI column density and size of Lyα absorbers. Modified inertia versions of MOND have long suffered from poor theoretical compatibility with cherished physical principles such as conservation laws. Researchers working on MOND generally do not interpret it as a modification of inertia, with only very limited work done on this area.\n==== Solar system ====', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Classical mechanics', 'Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  ħωk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, θk†, θk, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the θk, and the Fock space is built by applying all products of creation operators θk† to |0⟩.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at φ = 0, so that the vacuum minimizing the Hamiltonian satisfies ⟨φ⟩ = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  φ = v .  This occurs for example, if V(φ) = gφ4 − 2m2φ2 with g > 0 and m2 > 0, for which the minimum energy is found at v = ±m/√g. The value of v in one of these vacua may be considered as condensate of the field φ. Canonical quantization then can be carried out for the shifted field  φ(x,t) − v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator  [A, B]  expressed in the phase space formulation is iħ{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  ħ-deformations are highly nonunique—quantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  ħ → 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an ħ-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional Schrödinger Hilbert space. If the chosen variables are complex, we get something like the Segal–Bargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', "Planck's law"]

Question: What is Modified Newtonian Dynamics (MOND)?

Choices:
Choice A) MOND is a theory that explains the behavior of light in the presence of strong gravitational fields. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.
Choice B) MOND is a hypothesis that proposes a modification of Einstein's theory of general relativity to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.
Choice C) MOND is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.
Choice D) MOND is a hypothesis that proposes a modification of Coulomb's law to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.
Choice E) MOND is a theory that explains the behavior of subatomic particles in the presence of strong magnetic fields. It is an alternative to the hypothesis of dark energy in terms of explaining why subatomic particles do not appear to obey the currently understood laws of physics.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==', 'The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions, which groups by the nature of the outcome being considered (discrete, absolutely continuous, multivariate, etc.)\nAll of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution.\n=== Linear growth (e.g. errors, offsets) ===\nNormal distribution (Gaussian distribution), for a single such quantity; the most commonly used absolutely continuous distribution\n=== Exponential growth (e.g. prices, incomes, populations) ===\nLog-normal distribution, for a single such quantity whose log is normally distributed\nPareto distribution, for a single such quantity whose log is exponentially distributed; the prototypical power law distribution\n=== Uniformly distributed quantities ===\nDiscrete uniform distribution, for a finite set of values (e.g. the outcome of a fair dice)\nContinuous uniform distribution, for absolutely continuously distributed values\n=== Bernoulli trials (yes/no events, with a given probability) ===\nBasic distributions:\nBernoulli distribution, for the outcome of a single Bernoulli trial (e.g. success/failure, yes/no)\nBinomial distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed total number of independent occurrences\nNegative binomial distribution, for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs\nGeometric distribution, for binomial-type observations but where the quantity of interest is the number of failures before the first success; a special case of the negative binomial distribution\nRelated to sampling schemes over a finite population:\nHypergeometric distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, using sampling without replacement\nBeta-binomial distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, sampling using a Pólya urn model (in some sense, the "opposite" of sampling without replacement)\n=== Categorical outcomes (events with K possible outcomes) ===\nCategorical distribution, for a single categorical outcome (e.g. yes/no/maybe in a survey); a generalization of the Bernoulli distribution\nMultinomial distribution, for the number of each type of categorical outcome, given a fixed number of total outcomes; a generalization of the binomial distribution\nMultivariate hypergeometric distribution, similar to the multinomial distribution, but using sampling without replacement; a generalization of the hypergeometric distribution\n=== Poisson process (events that occur independently with a given rate) ===\nPoisson distribution, for the number of occurrences of a Poisson-type event in a given period of time\nExponential distribution, for the time before the next Poisson-type event occurs\nGamma distribution, for the time before the next k Poisson-type events occur\n=== Absolute values of vectors with normally distributed components ===\nRayleigh distribution, for the distribution of vector magnitudes with Gaussian distributed orthogonal components. Rayleigh distributions are found in RF signals with Gaussian real and imaginary components.\nRice distribution, a generalization of the Rayleigh distributions for where there is a stationary background signal component. Found in Rician fading of radio signals due to multipath propagation and in MR images with noise corruption on non-zero NMR signals.\n=== Normally distributed quantities operated with sum of squares ===\nChi-squared distribution, the distribution of a sum of squared standard normal variables; useful e.g. for inference regarding the sample variance of normally distributed samples (see chi-squared test)\nStudent\'s t distribution, the distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable; useful for inference regarding the mean of normally distributed samples with unknown variance (see Student\'s t-test)\nF-distribution, the distribution of the ratio of two scaled chi squared variables; useful e.g. for inferences that involve comparing variances or involving R-squared (the squared correlation coefficient)\n=== As conjugate prior distributions in Bayesian inference ===\nBeta distribution, for a single probability (real number between 0 and 1); conjugate to the Bernoulli distribution and binomial distribution\nGamma distribution, for a non-negative scaling parameter; conjugate to the rate parameter of a Poisson distribution or exponential distribution, the precision (inverse variance) of a normal distribution, etc.\nDirichlet distribution, for a vector of probabilities that must sum to 1; conjugate to the categorical distribution and multinomial distribution; generalization of the beta distribution\nWishart distribution, for a symmetric non-negative definite matrix; conjugate to the inverse of the covariance matrix of a multivariate normal distribution; generalization of the gamma distribution\n=== Some specialized applications of probability distributions ===\nThe cache language models and other statistical language models used in natural language processing to assign probabilities to the occurrence of particular words and word sequences do so by means of probability distributions.\nIn quantum mechanics, the probability density of finding the particle at a given point is proportional to the square of the magnitude of the particle\'s wavefunction at that point (see Born rule). Therefore, the probability distribution function of the position of a particle is described by\n{\\textstyle P_{a\\leq x\\leq b}(t)=\\int _{a}^{b}dx\\,|\\Psi (x,t)|^{2}}\n, probability that the particle\'s position x will be in the interval a ≤ x ≤ b in dimension one, and a similar triple integral in dimension three. This is a key principle of quantum mechanics.\nProbabilistic load flow in power-flow study explains the uncertainties of input variables as probability distribution and provides the power flow calculation also in term of probability distribution.\nPrediction of natural phenomena occurrences based on previous frequency distributions such as tropical cyclones, hail, time in between events, etc.\n== Fitting ==\n== See also ==\nConditional probability distribution\nEmpirical probability distribution\nHistogram\nJoint probability distribution\nProbability measure\nQuasiprobability distribution\nRiemann–Stieltjes integral application to probability theory\n=== Lists ===\nList of probability distributions\nList of statistical topics\n== References ==\n=== Citations ===\n=== Sources ===\n== External links ==\n"Probability distribution", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nField Guide to Continuous Probability Distributions, Gavin E. Crooks.\nDistinguishing probability measure, function and distribution, Math Stack Exchange', 'Probability distribution', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.  The main forms of precipitation include drizzle, rain, Rain and snow mixed ("sleet" in Commonwealth usage), snow, ice pellets, graupel and hail. Precipitation occurs when a portion of the atmosphere becomes saturated with water vapor (reaching 100% relative humidity), so that the water condenses and "precipitates" or falls. Thus, fog and mist are not precipitation; their water vapor does not condense sufficiently to precipitate, so fog and mist do not fall. (Such a non-precipitating combination is a colloid.) Two processes, possibly acting together, can lead to air becoming saturated with water vapor: cooling the air or adding water vapor to the air. Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called showers.\nMoisture that is lifted or otherwise forced to rise over a layer of sub-freezing air at the surface may be condensed by the low temperature into clouds and rain. This process is typically active when freezing rain occurs. A stationary front is often present near the area of freezing rain and serves as the focus for forcing moist air to rise. Provided there is necessary and sufficient atmospheric moisture content, the moisture within the rising air will condense into clouds, namely nimbostratus and cumulonimbus if significant precipitation is involved. Eventually, the cloud droplets will grow large enough to form raindrops and descend toward the Earth where they will freeze on contact with exposed objects. Where relatively warm water bodies are present, for example due to water evaporation from lakes, lake-effect snowfall becomes a concern downwind of the warm lakes within the cold cyclonic flow around the backside of extratropical cyclones. Lake-effect snowfall can be locally heavy.  Thundersnow is possible within a cyclone\'s comma head and within lake effect precipitation bands. In mountainous areas, heavy precipitation is possible where upslope flow is maximized within windward sides of the terrain at elevation. On the leeward side of mountains, desert climates can exist due to the dry air caused by compressional heating. Most precipitation occurs within the tropics and is caused by convection.\nPrecipitation is a major component of the water cycle, and is responsible for depositing most of the fresh water on the planet. Approximately 505,000 cubic kilometres (121,000 cu mi) of water falls as precipitation each year: 398,000 cubic kilometres (95,000 cu mi) over oceans and 107,000 cubic kilometres (26,000 cu mi) over land. Given the Earth\'s surface area, that means the globally averaged annual precipitation is 990 millimetres (39 in), but over land it is only 715 millimetres (28.1 in). Climate classification systems such as the Köppen climate classification system use average annual rainfall to help differentiate between differing climate regimes. Global warming is already causing changes to weather, increasing precipitation in some geographies, and reducing it in others, resulting in additional extreme weather.\nPrecipitation may occur on other celestial bodies. Saturn\'s largest satellite, Titan, hosts methane precipitation as a slow-falling drizzle, which has been observed as rain puddles at its equator and polar regions.\n== Types ==\nMechanisms of producing precipitation include convective, stratiform, and orographic rainfall.  Convective processes involve strong vertical motions that can cause the overturning of the atmosphere in that location within an hour and cause heavy precipitation, while stratiform processes involve weaker upward motions and less intense precipitation.  Precipitation can be divided into three categories, based on whether it falls as liquid water, liquid water that freezes on contact with the surface, or ice. Mixtures of different types of precipitation, including types in different categories, can fall simultaneously. Liquid forms of precipitation include rain and drizzle. Rain or drizzle that freezes on contact within a subfreezing air mass is called "freezing rain" or "freezing drizzle". Frozen forms of precipitation include snow, ice needles, ice pellets, hail, and graupel.\n=== Measurement ===\nLiquid precipitation\nRainfall (including drizzle and rain) is usually measured using a rain gauge and expressed in units of millimeters (mm) of height or depth. Equivalently, it can be expressed as a physical quantity with dimension of volume of water per collection area, in units of liters per square meter (L/m2); as 1L = 1dm3 = 1mm·m2, the units of area (m2) cancel out, resulting in simply "mm". This also corresponds to an area density expressed in kg/m2, if assuming that 1 liter of water has a mass of 1 kg (water density), which is acceptable for most practical purposes. The corresponding English unit used is usually inches. In Australia before metrication, rainfall was also measured in "points", each of which was defined as one-hundredth of an inch.\nSolid precipitation\nA snow gauge is usually used to measure the amount of solid precipitation. Snowfall is usually measured in centimeters by letting snow fall into a container and then measure the height. The snow can then optionally be melted to obtain a water equivalent measurement in millimeters like for liquid precipitation. The relationship between snow height and water equivalent depends on the water content of the snow; the water equivalent can thus only provide a rough estimate of snow depth. Other forms of solid precipitation, such as snow pellets and hail or even rain and snow mixed ("sleet" in Commonwealth usage), can also be melted and measured as their respective water equivalents, usually expressed in millimeters as for liquid precipitation.\n== Air becomes saturated ==\n=== Cooling air to its dew point ===\nThe dew point is the temperature to which a parcel of air must be cooled in order to become saturated, and (unless super-saturation occurs) condenses to water.  Water vapor normally begins to condense on condensation nuclei such as dust, ice, and salt in order to form clouds. The cloud condensation nuclei concentration will determine the cloud microphysics. An elevated portion of a frontal zone forces broad areas of lift, which form cloud decks such as altostratus or cirrostratus.  Stratus is a stable cloud deck which tends to form when a cool, stable air mass is trapped underneath a warm air mass. It can also form due to the lifting of advection fog during breezy conditions.\nThere are four main mechanisms for cooling the air to its dew point: adiabatic cooling, conductive cooling, radiational cooling, and evaporative cooling. Adiabatic cooling occurs when air rises and expands. The air can rise due to convection, large-scale atmospheric motions, or a physical barrier such as a mountain (orographic lift). Conductive cooling occurs when the air comes into contact with a colder surface, usually by being blown from one surface to another, for example from a liquid water surface to colder land. Radiational cooling occurs due to the emission of infrared radiation, either by the air or by the surface underneath.  Evaporative cooling occurs when moisture is added to the air through evaporation, which forces the air temperature to cool to its wet-bulb temperature, or until it reaches saturation.\n=== Adding moisture to the air ===\nThe main ways water vapor is added to the air are: wind convergence into areas of upward motion, precipitation or virga falling from above, daytime heating evaporating water from the surface of oceans, water bodies or wet land, transpiration from plants, cool or dry air moving over warmer water, and lifting air over mountains.\n== Forms of precipitation ==\n=== Raindrops ===\nCoalescence occurs when water droplets fuse to create larger water droplets, or when water droplets freeze onto an ice crystal, which is known as the Bergeron process. The fall rate of very small droplets is negligible, hence clouds do not fall out of the sky; precipitation will only occur when these coalesce into larger drops. droplets with different size will have different terminal velocity that cause droplets collision and producing larger droplets, Turbulence will enhance the collision process. As these larger water droplets descend, coalescence continues, so that drops become heavy enough to overcome air resistance and fall as rain.\nRaindrops have sizes ranging from 5.1 to 20 millimetres (0.20 to 0.79 in) mean diameter, above which they tend to break up. Smaller drops are called cloud droplets, and their shape is spherical. As a raindrop increases in size, its shape becomes more oblate, with its largest cross-section facing the oncoming airflow. Contrary to the cartoon pictures of raindrops, their shape does not resemble a teardrop. Intensity and duration of rainfall are usually inversely related, i.e., high intensity storms are likely to be of short duration and low intensity storms can have a long duration.  Rain drops associated with melting hail tend to be larger than other rain drops.  The METAR code for rain is RA, while the coding for rain showers is SHRA.\n=== Ice pellets ===\nIce pellets ("sleet" in US usage) are a form of precipitation consisting of small, translucent balls of ice. Ice pellets are usually (but not always) smaller than hailstones.  They often bounce when they hit the ground, and generally do not freeze into a solid mass unless mixed with freezing rain. The METAR code for ice pellets is PL.', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', 'Jordy, W. H. (1952). Henry Adams: Scientific Historian. New Haven. ISBN 978-0-685-26683-0. {{cite book}}: ISBN / Date incompatibility (help)\nKhan, Salman. "Maxwell\'s Demon". Archived from the original on 2010-03-17.\nMaroney, O. J. E. (2009) ""Information Processing and Thermodynamic Entropy" The Stanford Encyclopedia of Philosophy (Autumn 2009 Edition)\nMaxwell, J. C. (1871). Theory of Heat. London, New York [etc.] Longmans, Green., reprinted (2001) New York: Dover, ISBN 0-486-41735-2\nNorton, J. (2005). "Eaters of the lotus: Landauer\'s principle and the return of Maxwell\'s demon" (PDF). Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics. 36 (2): 375–411. Bibcode:2005SHPMP..36..375N. CiteSeerX 10.1.1.468.3017. doi:10.1016/j.shpsb.2004.12.002. S2CID 21104635. Archived (PDF) from the original on 2006-09-01.\nRaizen, Mark G. (2011) "Demons, Entropy, and the Quest for Absolute Zero", Scientific American, March, pp54-59\nReaney, Patricia. "Scientists build nanomachine", Reuters, February 1, 2007\nRubi, J Miguel, "Does Nature Break the Second Law of Thermodynamics?"; Scientific American, October 2008 :\nSplasho (2008) – Historical development of Maxwell\'s demon\nWeiss, Peter. "Breaking the Law – Can quantum mechanics + thermodynamics = perpetual motion?", Science News, October 7, 2000', 'Fusion powers stars and produces most elements lighter than cobalt in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 616 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.645% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and the result is fusion; this is an exothermic process.\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is 13.6 eV—less than one-millionth of the 17.6 MeV released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though individual fission reactions are generally much more energetic than individual fusion ones, which are themselves millions of times more energetic than chemical reactions. Via the mass–energy equivalence, fusion yields a 0.7% efficiency of reactant mass into energy. This can be only be exceeded by the extreme cases of the accretion process involving neutron stars or black holes, approaching 40% efficiency, and antimatter annihilation at 100% efficiency. (The complete conversion of one gram of matter would expel 9×1013 joules of energy.)\n== In astrophysics ==\nFusion is responsible for the astrophysical production of the majority of elements lighter than iron. This includes most types of Big Bang nucleosynthesis and stellar nucleosynthesis. Non-fusion processes that contribute include the s-process and r-process in neutron merger and supernova nucleosynthesis, responsible for elements heavier than iron.\n=== Stars ===\nAn important fusion process is the stellar nucleosynthesis that powers stars, including the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounts for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was unknown; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein\'s equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy had not yet been discovered, nor even that stars are largely composed of hydrogen (see metallicity). Eddington\'s paper reasoned that:\nThe leading theory of stellar energy, the contraction hypothesis, should cause the rotation of a star to visibly speed up due to conservation of angular momentum. But observations of Cepheid variable stars showed this was not happening.\nThe only other known plausible source of energy was conversion of matter to energy; Einstein had shown some years earlier that a small amount of matter was equivalent to a large amount of energy.\nFrancis Aston had also recently shown that the mass of a helium atom was about 0.8% less than the mass of the four hydrogen atoms which would, combined, form a helium atom (according to the then-prevailing theory of atomic structure which held atomic weight to be the distinguishing property between elements; work by Henry Moseley and Antonius van den Broek would later show that nucleic charge was the distinguishing property and that a helium nucleus, therefore, consisted of two hydrogen nuclei plus additional mass). This suggested that if such a combination could happen, it would release considerable energy as a byproduct.\nIf a star contained just 5% of fusible hydrogen, it would suffice to explain how stars got their energy. (It is now known that most \'ordinary\' stars are usually made of around 70% to 75% hydrogen)\nFurther elements might also be fused, and other scientists had speculated that stars were the "crucible" in which light elements combined to create heavy elements, but without more accurate measurements of their atomic masses nothing more could be said at the time.\nAll of these speculations were proven correct in the following decades.\nThe primary source of solar energy, and that of similar size stars, is the fusion of hydrogen to form helium (the proton–proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to fuse heavier elements. In massive cores, silicon-burning is the final fusion cycle, leading to a build-up of iron and nickel nuclei.\nNuclear binding energy makes the production of elements heavier than nickel via fusion energetically unfavorable. These elements are produced in non-fusion processes: the s-process, r-process, and the variety of processes that can produce p-nuclei. Such processes occur in giant star shells, or supernovae, or neutron star mergers.\n=== Brown dwarfs ===\nBrown dwarfs fuse deuterium and in very high mass cases also fuse lithium.\n=== White dwarfs ===\nCarbon-oxygen white dwarfs, which accrete matter either from an active stellar companion or white dwarf merger, approach the Chandrasekhar limit of 1.44 solar masses. Immediately prior, carbon burning fusion begins, destroying the Earth-sized dwarf within one second, in a Type Ia supernova.\nMuch more rarely, helium white dwarfs may merge, which does not cause an explosion but begins helium burning in an extreme type of helium star.\n=== Neutron stars ===\nSome neutron stars accrete hydrogen and helium from an active stellar companion. Periodically, the helium accretion reaches a critical level, and a thermonuclear burn wave propagates across the surface, on the timescale of one second.\n=== Black hole accretion disks ===\nSimilar to stellar fusion, extreme conditions within black hole accretion disks can allow fusion reactions. Calculations show the most energetic reactions occur around lower stellar mass black holes, below 10 solar masses, compared to those above 100. Beyond five Schwarzschild radii, carbon-burning and fusion of helium-3 dominates the reactions. Within this distance, around lower mass black holes, fusion of nitrogen, oxygen, neon, and magnesium can occur. In the extreme limit, the silicon-burning process can begin with the fusion of silicon and selenium nuclei.\n=== Big Bang ===\nFrom the period approximately 10 seconds to 20 minutes after the Big Bang, the universe cooled from over 100 keV to 1 keV. This allowed the combination of protons and neutrons in deuterium nuclei, and beginning a rapid fusion chain into tritium and helium-3 and ending in predominantly helium-4, with a minimal fraction of lithium, beryllium, and boron nuclei.\n== Requirements ==\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbors due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface-area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.', 'The neutron is a subatomic particle, symbol n or n0, that has no electric charge, and a mass slightly greater than that of a proton. The neutron was discovered by James Chadwick in 1932, leading to the discovery of nuclear fission in 1938, the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).\nNeutrons are found, together with a similar number of protons in the nuclei of atoms. Atoms of a chemical element that differ only in neutron number are called isotopes. Free neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes. Neutron stars, formed from massive collapsing stars, consist of neutrons at the density of atomic nuclei but a total mass more than the Sun.\nNeutron properties and interactions are described by nuclear physics.  Neutrons are not elementary particles; each is composed of three quarks. A free neutron spontaneously decays to a proton, an electron, and an antineutrino, with a mean lifetime of about 15 minutes.\nThe neutron is essential to the production of nuclear power.\nDedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments.  Free neutrons do not directly ionize atoms, but they do indirectly cause ionizing radiation, so they can be a biological hazard, depending on dose. A small natural "neutron background" flux of free neutrons exists on Earth, caused by cosmic rays, and by the natural radioactivity of spontaneously fissionable elements in the Earth\'s crust.\n== Discovery ==\nThe story of the discovery of the neutron and its properties is central to the extraordinary developments in atomic physics that occurred in the first half of the 20th century, leading ultimately to the atomic bomb in 1945. The name derives from the Latin root for neutralis (neuter) and the Greek suffix -on (a suffix used in the names of subatomic particles, e.g. electron and proton)\nand references to the word neutron can be found in the literature as early as 1899 in connection with discussion on the nature of the atom.\nIn the 1911 Rutherford model, the atom consisted of a small positively charged massive nucleus surrounded by a much larger cloud of negatively charged electrons. In 1920, Ernest Rutherford suggested that the nucleus consisted of positive protons and neutrally charged particles, suggested to be a proton and an electron bound in some way. Electrons were assumed to reside within the nucleus because it was known that beta radiation consisted of electrons emitted from the nucleus. About the time Rutherford suggested the neutral proton-electron composite, several other publications appeared making similar suggestions, and in 1921 the American chemist W. D. Harkins first named the hypothetical particle a "neutron".\nThroughout the 1920s, physicists assumed that the atomic nucleus was composed of protons and "nuclear electrons". Beginning in 1928, it became clear that this model was inconsistent with the then-new quantum theory. Confined to a volume the size of an nucleus, an electron consistent with the Heisenberg uncertainty relation of quantum mechanics would have an energy exceeding the binding energy of the nucleus. The energy was so large that according to the Klein paradox, discovered by Oskar Klein in 1928, an electron would escape the confinement of a nucleus. Furthermore, the observed properties of atoms and molecules were inconsistent with the nuclear spin expected from the proton–electron hypothesis. Protons and electrons both carry an intrinsic spin of \u20601/2\u2060ħ, and the isotopes of the same species were found to have either integer or fractional spin. By the hypothesis, isotopes would be composed of the same number of protons, but differing numbers of neutral bound proton+electron "particles". This physical picture was a contradiction, since there is no way to arrange the spins of an electron and a proton in a bound state to get a fractional spin.\nIn 1931, Walther Bothe and Herbert Becker found that if alpha particle radiation from polonium fell on beryllium, boron, or lithium, an unusually penetrating radiation was produced. The radiation was not influenced by an electric field, so Bothe and Becker assumed it was gamma radiation. The following year Irène Joliot-Curie and Frédéric Joliot-Curie in Paris showed that if this "gamma" radiation fell on paraffin, or any other hydrogen-containing compound, it ejected protons of very high energy. Neither Rutherford nor James Chadwick at the Cavendish Laboratory in Cambridge were convinced by the gamma ray interpretation. Chadwick quickly performed a series of experiments that showed that the new radiation consisted of uncharged particles with about the same mass as the proton. These properties matched Rutherford\'s hypothesized neutron. Chadwick won the 1935 Nobel Prize in Physics for this discovery.\nModels for an atomic nucleus consisting of protons and neutrons were quickly developed by Werner Heisenberg and others. The proton–neutron model explained the puzzle of nuclear spins. The origins of beta radiation were explained by Enrico Fermi in 1934 by the process of beta decay, in which the neutron decays to a proton by creating an electron and a then-undiscovered neutrino. In 1935, Chadwick and his doctoral student Maurice Goldhaber reported the first accurate measurement of the mass of the neutron.\nBy 1934, Fermi had bombarded heavier elements with neutrons to induce radioactivity in elements of high atomic number. In 1938, Fermi received the Nobel Prize in Physics "for his demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons". In December 1938 Otto Hahn, Lise Meitner, and Fritz Strassmann discovered nuclear fission, or the fractionation of uranium nuclei into lighter elements, induced by neutron bombardment. In 1945 Hahn received the 1944 Nobel Prize in Chemistry "for his discovery of the fission of heavy atomic nuclei".\nThe discovery of nuclear fission would lead to the development of nuclear power and the atomic bomb by the end of World War II. It was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, in a cascade known as a nuclear chain reaction.:\u200a460–461\u200a These events and findings led Fermi to construct the Chicago Pile-1 at the University of Chicago in 1942, the first self-sustaining nuclear reactor. Just three years later the Manhattan Project was able to test the first atomic bomb, the Trinity nuclear test in July 1945.\n== Occurrence ==\n=== Atomic nucleus ===\nAn atomic nucleus is formed by a number of protons, Z (the atomic number), and a number of neutrons, N (the neutron number), bound together by the nuclear force. Protons and neutrons each have a mass of approximately one dalton. The atomic number determines the chemical properties of the atom, and the neutron number determines the isotope or nuclide.:\u200a4\u200a The terms isotope and nuclide are often used synonymously, but they refer to chemical and nuclear properties, respectively.:\u200a4\u200a Isotopes are nuclides with the same atomic number, but different neutron number. Nuclides with the same neutron number, but different atomic number, are called isotones. The atomic mass number, A, is equal to the sum of atomic and neutron numbers. Nuclides with the same atomic mass number, but different atomic and neutron numbers, are called isobars.  The mass of a nucleus is always slightly less than the sum of its proton and neutron masses: the difference in mass represents the mass equivalent to nuclear binding energy, the energy which would need to be added to take the nucleus apart.:\u200a822\nThe nucleus of the most common isotope of the hydrogen atom (with the chemical symbol 1H) is a lone proton.:\u200a20\u200a The nuclei of the heavy hydrogen isotopes deuterium (D or 2H) and tritium (T or 3H) contain one proton bound to one and two neutrons, respectively.:\u200a20\u200a All other types of atomic nuclei are composed of two or more protons and various numbers of neutrons. The most common nuclide of the common chemical element lead, 208Pb, has 82 protons and 126 neutrons, for example. The table of nuclides comprises all the known nuclides. Even though it is not a chemical element, the neutron is included in this table.\nProtons and neutrons behave almost identically under the influence of the nuclear force within the nucleus. They are therefore both referred to collectively as nucleons.  The concept of isospin, in which the proton and neutron are viewed as two quantum states of the same particle, is used to model the interactions of nucleons by the nuclear or weak forces.:\u200a141\nNeutrons are a necessary constituent of any atomic nucleus that contains more than one proton. As a result of their positive charges, interacting protons have a mutual electromagnetic repulsion that is stronger than their attractive nuclear interaction, so proton-only nuclei are unstable (see diproton and neutron–proton ratio). Neutrons bind with protons and one another in the nucleus via the nuclear force, effectively moderating the repulsive forces between the protons and stabilizing the nucleus.:\u200a461\u200a Heavy nuclei carry a large positive charge, hence they require "extra" neutrons to be stable.:\u200a461']

Question: What is the difference between probability mass function (PMF) and probability density function (PDF)?

Choices:
Choice A) PMF is used only for continuous random variables, while PDF is used for both continuous and discrete random variables.
Choice B) PMF is used for both continuous and discrete random variables, while PDF is used only for continuous random variables.
Choice C) PMF is used for continuous random variables, while PDF is used for discrete random variables.
Choice D) PMF is used for discrete random variables, while PDF is used for continuous random variables.
Choice E) PMF and PDF are interchangeable terms used for the same concept in probability theory.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Hilbert space', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  ħωk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, θk†, θk, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the θk, and the Fock space is built by applying all products of creation operators θk† to |0⟩.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at φ = 0, so that the vacuum minimizing the Hamiltonian satisfies ⟨φ⟩ = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  φ = v .  This occurs for example, if V(φ) = gφ4 − 2m2φ2 with g > 0 and m2 > 0, for which the minimum energy is found at v = ±m/√g. The value of v in one of these vacua may be considered as condensate of the field φ. Canonical quantization then can be carried out for the shifted field  φ(x,t) − v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator  [A, B]  expressed in the phase space formulation is iħ{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  ħ-deformations are highly nonunique—quantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  ħ → 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an ħ-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional Schrödinger Hilbert space. If the chosen variables are complex, we get something like the Segal–Bargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', '{\\displaystyle H=\\bigoplus _{\\lambda \\in \\sigma (T)}H_{\\lambda }\\,.}\nMoreover, if Eλ denotes the orthogonal projection onto the eigenspace Hλ, then\n{\\displaystyle T=\\sum _{\\lambda \\in \\sigma (T)}\\lambda E_{\\lambda }\\,,}\nwhere the sum converges with respect to the norm on B(H).\nThis theorem plays a fundamental role in the theory of integral equations, as many integral operators are compact, in particular those that arise from Hilbert–Schmidt operators.\nThe general spectral theorem for self-adjoint operators involves a kind of operator-valued Riemann–Stieltjes integral, rather than an infinite summation. The spectral family associated to T associates to each real number λ an operator Eλ, which is the projection onto the nullspace of the operator (T − λ)+, where the positive part of a self-adjoint operator is defined by\n{\\displaystyle A^{+}={\\tfrac {1}{2}}{\\Bigl (}{\\sqrt {A^{2}}}+A{\\Bigr )}\\,.}\nThe operators Eλ are monotone increasing relative to the partial order defined on self-adjoint operators; the eigenvalues correspond precisely to the jump discontinuities. One has the spectral theorem, which asserts\n{\\displaystyle T=\\int _{\\mathbb {R} }\\lambda \\,\\mathrm {d} E_{\\lambda }\\,.}\nThe integral is understood as a Riemann–Stieltjes integral, convergent with respect to the norm on B(H). In particular, one has the ordinary scalar-valued integral representation\n{\\displaystyle \\langle Tx,y\\rangle =\\int _{\\mathbb {R} }\\lambda \\,\\mathrm {d} \\langle E_{\\lambda }x,y\\rangle \\,.}\nA somewhat similar spectral decomposition holds for normal operators, although because the spectrum may now contain non-real complex numbers, the operator-valued Stieltjes measure dEλ must instead be replaced by a resolution of the identity.\nA major application of spectral methods is the spectral mapping theorem, which allows one to apply to a self-adjoint operator T any continuous complex function f defined on the spectrum of T by forming the integral\n{\\displaystyle f(T)=\\int _{\\sigma (T)}f(\\lambda )\\,\\mathrm {d} E_{\\lambda }\\,.}\nThe resulting continuous functional calculus has applications in particular to pseudodifferential operators.\nThe spectral theory of unbounded self-adjoint operators is only marginally more difficult than for bounded operators. The spectrum of an unbounded operator is defined in precisely the same way as for bounded operators: λ is a spectral value if the resolvent operator\n{\\displaystyle R_{\\lambda }=(T-\\lambda )^{-1}}\nfails to be a well-defined continuous operator. The self-adjointness of T still guarantees that the spectrum is real. Thus the essential idea of working with unbounded operators is to look instead at the resolvent Rλ where λ is nonreal. This is a bounded normal operator, which admits a spectral representation that can then be transferred to a spectral representation of T itself. A similar strategy is used, for instance, to study the spectrum of the Laplace operator: rather than address the operator directly, one instead looks as an associated resolvent such as a Riesz potential or Bessel potential.\nA precise version of the spectral theorem in this case is:\nThere is also a version of the spectral theorem that applies to unbounded normal operators.\n== In popular culture ==\nIn Gravity\'s Rainbow (1973), a novel by Thomas Pynchon, one of the characters is called "Sammy Hilbert-Spaess", a pun on "Hilbert Space". The novel refers also to Gödel\'s incompleteness theorems.\n== See also ==\n== Remarks ==\n== Notes ==\n== References ==\n== External links ==\n"Hilbert space", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nHilbert space at Mathworld\n245B, notes 5: Hilbert spaces by Terence Tao', 'Classical mechanics', 'Quantum number', '{\\displaystyle L_{z}=m_{\\ell }\\hbar }\nThe values of mℓ range from −ℓ to ℓ, with integer intervals.\nThe s subshell (ℓ = 0) contains only one orbital, and therefore the mℓ of an electron in an s orbital will always be 0. The p subshell (ℓ = 1) contains three orbitals, so the mℓ of an electron in a p orbital will be −1, 0, or 1. The d subshell (ℓ = 2) contains five orbitals, with mℓ values of −2, −1, 0, 1, and 2.\n=== Spin magnetic quantum number ===\nThe spin magnetic quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis:\n{\\displaystyle S_{z}=m_{s}\\hbar }\nIn general, the values of ms range from −s to s, where s is the spin quantum number, associated with the magnitude of particle\'s intrinsic spin angular momentum:\n{\\displaystyle m_{s}=-s,-s+1,-s+2,\\cdots ,s-2,s-1,s}\nAn electron state has spin number s = \u20601/2\u2060, consequently ms will be +\u20601/2\u2060 ("spin up") or −\u20601/2\u2060 "spin down" states. Since electron are fermions they obey the Pauli exclusion principle: each electron state must have different quantum numbers.  Therefore, every orbital will be occupied with at most two electrons, one for each spin state.\n=== The Aufbau principle and Hund\'s Rules ===\nA multi-electron atom can be modeled qualitatively as a hydrogen like atom with higher nuclear charge and correspondingly more electrons. The occupation of the electron states in such an atom can be predicted by the Aufbau principle and Hund\'s empirical rules for the quantum numbers.  The Aufbau principle fills orbitals based on their principal and azimuthal quantum numbers (lowest n + l first, with lowest n breaking ties; Hund\'s rule favors unpaired electrons in the outermost orbital). These rules are empirical but they can be related to electron physics.:\u200a10\u200a:\u200a260\n== Spin-orbit coupled systems ==\nWhen one takes the spin–orbit interaction into consideration, the L and S operators no longer commute with the Hamiltonian, and the eigenstates of the system no longer have well-defined orbital angular momentum and spin. Thus another set of quantum numbers should be used. This set includes\nThe total angular momentum quantum number:\n{\\displaystyle j=|\\ell \\pm s|,}\nwhich gives the total angular momentum through the relation\n{\\displaystyle J^{2}=\\hbar ^{2}j(j+1).}\nThe projection of the total angular momentum along a specified axis:\n{\\displaystyle m_{j}=-j,-j+1,-j+2,\\cdots ,j-2,j-1,j}\nanalogous to the above and satisfies both\n{\\displaystyle m_{j}=m_{\\ell }+m_{s},}\nand\n{\\displaystyle |m_{\\ell }+m_{s}|\\leq j.}\nParityThis is the eigenvalue under reflection: positive (+1) for states which came from even ℓ and negative (−1) for states which came from odd ℓ. The former is also known as even parity and the latter as odd parity, and is given by\n{\\displaystyle P=(-1)^{\\ell }.}\nFor example, consider the following 8 states, defined by their quantum numbers:\nThe quantum states in the system can be described as linear combination of these 8 states. However, in the presence of spin–orbit interaction, if one wants to describe the same system by 8 states that are eigenvectors of the Hamiltonian (i.e. each represents a state that does not mix with others over time), we should consider the following 8 states:\n== Atomic nuclei ==\nIn nuclei, the entire assembly of protons and neutrons (nucleons) has a resultant angular momentum due to the angular momenta of each nucleon, usually denoted I. If the total angular momentum of a neutron is jn = ℓ + s and for a proton is jp = ℓ + s (where s for protons and neutrons happens to be \u20601/2\u2060 again (see note)), then the nuclear angular momentum quantum numbers I are given by:\n{\\displaystyle I=|j_{n}-j_{p}|,|j_{n}-j_{p}|+1,|j_{n}-j_{p}|+2,\\cdots ,(j_{n}+j_{p})-2,(j_{n}+j_{p})-1,(j_{n}+j_{p})}\nNote: The orbital angular momenta of the nuclear (and atomic) states are all integer multiples of ħ while the intrinsic angular momentum of the neutron and  proton are half-integer multiples.  It should be immediately apparent that the combination of the intrinsic spins of the nucleons with their orbital motion will always give half-integer values for the total spin, I, of any odd-A nucleus and integer values for any even-A nucleus.\nParity with the number I is used to label nuclear angular momentum states, examples for some isotopes of hydrogen (H), carbon (C), and sodium (Na) are;\nThe reason for the unusual fluctuations in I, even by differences of just one nucleon, are due to the odd and even numbers of protons and neutrons – pairs of nucleons have a total angular momentum of zero (just like electrons in orbitals), leaving an odd or even number of unpaired nucleons. The property of nuclear spin is an important factor for the operation of NMR spectroscopy in organic chemistry, and MRI in nuclear medicine, due to the nuclear magnetic moment interacting with an external magnetic field.\n== Elementary particles ==\nElementary particles contain many quantum numbers which are usually said to be intrinsic to them. However, it should be understood that the elementary particles are quantum states of the standard model of particle physics, and hence the quantum numbers of these particles bear the same relation to the Hamiltonian of this model as the quantum numbers of the Bohr atom does to its Hamiltonian. In other words, each quantum number denotes a symmetry of the problem. It is more useful in quantum field theory to distinguish between spacetime and internal symmetries.\nTypical quantum numbers related to spacetime symmetries are spin (related to rotational symmetry), the parity, C-parity and T-parity (related to the Poincaré symmetry of spacetime). Typical internal symmetries are lepton number and baryon number or the electric charge. (For a full list of quantum numbers of this kind see the article on flavour.)\n== Multiplicative quantum numbers ==\nMost conserved quantum numbers are additive, so in an elementary particle reaction, the sum of the quantum numbers should be the same before and after the reaction. However, some, usually called a parity, are multiplicative; i.e., their product is conserved. All multiplicative quantum numbers belong to a symmetry (like parity) in which applying the symmetry transformation twice is equivalent to doing nothing (involution).\n== See also ==\nElectron configuration\n== References ==\n== Further reading ==\nDirac, Paul A. M. (1982). Principles of Quantum Mechanics. Oxford University Press. ISBN 0-19-852011-5.\nGriffiths, David J. (2004). Introduction to Quantum Mechanics (2nd ed.). Prentice Hall. ISBN 0-13-805326-X.\nHalzen, Francis & Martin, Alan D. (1984). Quarks and Leptons: An Introductory Course in Modern Particle Physics. John Wiley & Sons. ISBN 0-471-88741-2.\nEisberg, Robert Martin; Resnick, Robert (1985). Quantum Physics of Atoms, Molecules, Solids, Nuclei and Particles (2nd ed.). John Wiley & Sons. ISBN 978-0-471-87373-0 – via Internet Archive.', 'The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions, which groups by the nature of the outcome being considered (discrete, absolutely continuous, multivariate, etc.)\nAll of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution.\n=== Linear growth (e.g. errors, offsets) ===\nNormal distribution (Gaussian distribution), for a single such quantity; the most commonly used absolutely continuous distribution\n=== Exponential growth (e.g. prices, incomes, populations) ===\nLog-normal distribution, for a single such quantity whose log is normally distributed\nPareto distribution, for a single such quantity whose log is exponentially distributed; the prototypical power law distribution\n=== Uniformly distributed quantities ===\nDiscrete uniform distribution, for a finite set of values (e.g. the outcome of a fair dice)\nContinuous uniform distribution, for absolutely continuously distributed values\n=== Bernoulli trials (yes/no events, with a given probability) ===\nBasic distributions:\nBernoulli distribution, for the outcome of a single Bernoulli trial (e.g. success/failure, yes/no)\nBinomial distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed total number of independent occurrences\nNegative binomial distribution, for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs\nGeometric distribution, for binomial-type observations but where the quantity of interest is the number of failures before the first success; a special case of the negative binomial distribution\nRelated to sampling schemes over a finite population:\nHypergeometric distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, using sampling without replacement\nBeta-binomial distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, sampling using a Pólya urn model (in some sense, the "opposite" of sampling without replacement)\n=== Categorical outcomes (events with K possible outcomes) ===\nCategorical distribution, for a single categorical outcome (e.g. yes/no/maybe in a survey); a generalization of the Bernoulli distribution\nMultinomial distribution, for the number of each type of categorical outcome, given a fixed number of total outcomes; a generalization of the binomial distribution\nMultivariate hypergeometric distribution, similar to the multinomial distribution, but using sampling without replacement; a generalization of the hypergeometric distribution\n=== Poisson process (events that occur independently with a given rate) ===\nPoisson distribution, for the number of occurrences of a Poisson-type event in a given period of time\nExponential distribution, for the time before the next Poisson-type event occurs\nGamma distribution, for the time before the next k Poisson-type events occur\n=== Absolute values of vectors with normally distributed components ===\nRayleigh distribution, for the distribution of vector magnitudes with Gaussian distributed orthogonal components. Rayleigh distributions are found in RF signals with Gaussian real and imaginary components.\nRice distribution, a generalization of the Rayleigh distributions for where there is a stationary background signal component. Found in Rician fading of radio signals due to multipath propagation and in MR images with noise corruption on non-zero NMR signals.\n=== Normally distributed quantities operated with sum of squares ===\nChi-squared distribution, the distribution of a sum of squared standard normal variables; useful e.g. for inference regarding the sample variance of normally distributed samples (see chi-squared test)\nStudent\'s t distribution, the distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable; useful for inference regarding the mean of normally distributed samples with unknown variance (see Student\'s t-test)\nF-distribution, the distribution of the ratio of two scaled chi squared variables; useful e.g. for inferences that involve comparing variances or involving R-squared (the squared correlation coefficient)\n=== As conjugate prior distributions in Bayesian inference ===\nBeta distribution, for a single probability (real number between 0 and 1); conjugate to the Bernoulli distribution and binomial distribution\nGamma distribution, for a non-negative scaling parameter; conjugate to the rate parameter of a Poisson distribution or exponential distribution, the precision (inverse variance) of a normal distribution, etc.\nDirichlet distribution, for a vector of probabilities that must sum to 1; conjugate to the categorical distribution and multinomial distribution; generalization of the beta distribution\nWishart distribution, for a symmetric non-negative definite matrix; conjugate to the inverse of the covariance matrix of a multivariate normal distribution; generalization of the gamma distribution\n=== Some specialized applications of probability distributions ===\nThe cache language models and other statistical language models used in natural language processing to assign probabilities to the occurrence of particular words and word sequences do so by means of probability distributions.\nIn quantum mechanics, the probability density of finding the particle at a given point is proportional to the square of the magnitude of the particle\'s wavefunction at that point (see Born rule). Therefore, the probability distribution function of the position of a particle is described by\n{\\textstyle P_{a\\leq x\\leq b}(t)=\\int _{a}^{b}dx\\,|\\Psi (x,t)|^{2}}\n, probability that the particle\'s position x will be in the interval a ≤ x ≤ b in dimension one, and a similar triple integral in dimension three. This is a key principle of quantum mechanics.\nProbabilistic load flow in power-flow study explains the uncertainties of input variables as probability distribution and provides the power flow calculation also in term of probability distribution.\nPrediction of natural phenomena occurrences based on previous frequency distributions such as tropical cyclones, hail, time in between events, etc.\n== Fitting ==\n== See also ==\nConditional probability distribution\nEmpirical probability distribution\nHistogram\nJoint probability distribution\nProbability measure\nQuasiprobability distribution\nRiemann–Stieltjes integral application to probability theory\n=== Lists ===\nList of probability distributions\nList of statistical topics\n== References ==\n=== Citations ===\n=== Sources ===\n== External links ==\n"Probability distribution", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nField Guide to Continuous Probability Distributions, Gavin E. Crooks.\nDistinguishing probability measure, function and distribution, Math Stack Exchange', 'Canonical quantization', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as']

Question: What is a Hilbert space in quantum mechanics?

Choices:
Choice A) A complex vector space where the state of a classical mechanical system is described by a vector |Ψ⟩.
Choice B) A physical space where the state of a classical mechanical system is described by a vector |Ψ⟩.
Choice C) A physical space where the state of a quantum mechanical system is described by a vector |Ψ⟩.
Choice D) A mathematical space where the state of a classical mechanical system is described by a vector |Ψ⟩.
Choice E) A complex vector space where the state of a quantum mechanical system is described by a vector |Ψ⟩.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', "==== Red-giant-branch phase ====\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star.  For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower 12C/13C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\nThe helium core continues to grow on the red-giant branch.  It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase.  The star increases in luminosity towards the tip of the red-giant branch.  Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n==== Horizontal branch ====\nIn the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 108 times the luminosity of the Sun for a few days and 1011 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning.  These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n==== Asymptotic-giant-branch phase ====\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time).  Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star.  Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically.  This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface.  This is known as the second dredge up, and in some stars there may even be a third dredge up.  In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra.  A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\nAnother well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths.  These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity.  These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n==== Post-AGB ====\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n=== Massive stars ===\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants.  Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.\n==== Supergiant evolution ====\nExtremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\nThe core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core.  In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process.  At the end of helium fusion, the core of a star consists primarily of carbon and oxygen.  In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium.  Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.\nThe exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉.  After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse.  Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning.  For a range of stars of approximately 8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova.", 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', '=== Progenitor ===\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic type Ic-BL events that are associated with long-duration gamma-ray bursts.\n== External impact ==\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\n=== Source of heavy elements ===\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\n=== Role in stellar evolution ===\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n=== Cosmic rays ===\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\n=== Gravitational waves ===', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', 'Fusion powers stars and produces most elements lighter than cobalt in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 616 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.645% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and the result is fusion; this is an exothermic process.\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is 13.6 eV—less than one-millionth of the 17.6 MeV released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though individual fission reactions are generally much more energetic than individual fusion ones, which are themselves millions of times more energetic than chemical reactions. Via the mass–energy equivalence, fusion yields a 0.7% efficiency of reactant mass into energy. This can be only be exceeded by the extreme cases of the accretion process involving neutron stars or black holes, approaching 40% efficiency, and antimatter annihilation at 100% efficiency. (The complete conversion of one gram of matter would expel 9×1013 joules of energy.)\n== In astrophysics ==\nFusion is responsible for the astrophysical production of the majority of elements lighter than iron. This includes most types of Big Bang nucleosynthesis and stellar nucleosynthesis. Non-fusion processes that contribute include the s-process and r-process in neutron merger and supernova nucleosynthesis, responsible for elements heavier than iron.\n=== Stars ===\nAn important fusion process is the stellar nucleosynthesis that powers stars, including the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounts for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was unknown; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein\'s equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy had not yet been discovered, nor even that stars are largely composed of hydrogen (see metallicity). Eddington\'s paper reasoned that:\nThe leading theory of stellar energy, the contraction hypothesis, should cause the rotation of a star to visibly speed up due to conservation of angular momentum. But observations of Cepheid variable stars showed this was not happening.\nThe only other known plausible source of energy was conversion of matter to energy; Einstein had shown some years earlier that a small amount of matter was equivalent to a large amount of energy.\nFrancis Aston had also recently shown that the mass of a helium atom was about 0.8% less than the mass of the four hydrogen atoms which would, combined, form a helium atom (according to the then-prevailing theory of atomic structure which held atomic weight to be the distinguishing property between elements; work by Henry Moseley and Antonius van den Broek would later show that nucleic charge was the distinguishing property and that a helium nucleus, therefore, consisted of two hydrogen nuclei plus additional mass). This suggested that if such a combination could happen, it would release considerable energy as a byproduct.\nIf a star contained just 5% of fusible hydrogen, it would suffice to explain how stars got their energy. (It is now known that most \'ordinary\' stars are usually made of around 70% to 75% hydrogen)\nFurther elements might also be fused, and other scientists had speculated that stars were the "crucible" in which light elements combined to create heavy elements, but without more accurate measurements of their atomic masses nothing more could be said at the time.\nAll of these speculations were proven correct in the following decades.\nThe primary source of solar energy, and that of similar size stars, is the fusion of hydrogen to form helium (the proton–proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to fuse heavier elements. In massive cores, silicon-burning is the final fusion cycle, leading to a build-up of iron and nickel nuclei.\nNuclear binding energy makes the production of elements heavier than nickel via fusion energetically unfavorable. These elements are produced in non-fusion processes: the s-process, r-process, and the variety of processes that can produce p-nuclei. Such processes occur in giant star shells, or supernovae, or neutron star mergers.\n=== Brown dwarfs ===\nBrown dwarfs fuse deuterium and in very high mass cases also fuse lithium.\n=== White dwarfs ===\nCarbon-oxygen white dwarfs, which accrete matter either from an active stellar companion or white dwarf merger, approach the Chandrasekhar limit of 1.44 solar masses. Immediately prior, carbon burning fusion begins, destroying the Earth-sized dwarf within one second, in a Type Ia supernova.\nMuch more rarely, helium white dwarfs may merge, which does not cause an explosion but begins helium burning in an extreme type of helium star.\n=== Neutron stars ===\nSome neutron stars accrete hydrogen and helium from an active stellar companion. Periodically, the helium accretion reaches a critical level, and a thermonuclear burn wave propagates across the surface, on the timescale of one second.\n=== Black hole accretion disks ===\nSimilar to stellar fusion, extreme conditions within black hole accretion disks can allow fusion reactions. Calculations show the most energetic reactions occur around lower stellar mass black holes, below 10 solar masses, compared to those above 100. Beyond five Schwarzschild radii, carbon-burning and fusion of helium-3 dominates the reactions. Within this distance, around lower mass black holes, fusion of nitrogen, oxygen, neon, and magnesium can occur. In the extreme limit, the silicon-burning process can begin with the fusion of silicon and selenium nuclei.\n=== Big Bang ===\nFrom the period approximately 10 seconds to 20 minutes after the Big Bang, the universe cooled from over 100 keV to 1 keV. This allowed the combination of protons and neutrons in deuterium nuclei, and beginning a rapid fusion chain into tritium and helium-3 and ending in predominantly helium-4, with a minimal fraction of lithium, beryllium, and boron nuclei.\n== Requirements ==\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbors due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface-area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.', "In more massive stars, the fusion of neon proceeds without a runaway deflagration.  This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements.  Surrounding the core are shells of lighter elements still undergoing fusion.  The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged.  The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope.  The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars.  Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself.  The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.\n==== Supernova ====\nWhen the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole.  Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\n== Stellar remnants ==\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n=== White and black dwarfs ===\nFor a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years.\nThe chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n=== Neutron stars ===\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n=== Black holes ===\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉.", 'Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.', 'An atmosphere (from Ancient Greek  ἀτμός (atmós) \'vapour, steam\' and  σφαῖρα (sphaîra) \'sphere\') is a layer of gases that envelop an astronomical object, held in place by the gravity of the object. A planet retains an atmosphere when the gravity is great and the temperature of the atmosphere is low. A stellar atmosphere is the outer region of a star, which includes the layers above the opaque photosphere; stars of low temperature might have outer atmospheres containing compound molecules.\nThe atmosphere of Earth is composed of nitrogen (78%), oxygen (21%), argon (0.9%), carbon dioxide (0.04%) and trace gases. Most organisms use oxygen for respiration; lightning and bacteria perform nitrogen fixation which produces ammonia that is used to make nucleotides and amino acids; plants, algae, and cyanobacteria use carbon dioxide for photosynthesis. The layered composition of the atmosphere minimises the harmful effects of sunlight, ultraviolet radiation, solar wind, and cosmic rays and thus protects the organisms from genetic damage. The current composition of the atmosphere of the Earth is the product of billions of years of biochemical modification of the paleoatmosphere by living organisms.\n== Occurrence and compositions ==\n=== Origins ===\nAtmospheres are clouds of gas bound to and engulfing an astronomical focal point of sufficiently dominating mass, adding to its mass, possibly escaping from it or collapsing into it.\nBecause of the latter, such planetary nucleus can develop from interstellar molecular clouds or protoplanetary disks into rocky astronomical objects with varyingly thick atmospheres, gas giants or fusors.\nComposition and thickness is originally determined by the stellar nebula\'s chemistry and temperature, but can also by a product processes within the astronomical body outgasing a different atmosphere.\n=== Compositions ===\nThe atmospheres of the planets Venus and Mars are principally composed of carbon dioxide and nitrogen, argon and oxygen.\nThe composition of Earth\'s atmosphere is determined by the by-products of the life that it sustains. Dry air (mixture of gases) from Earth\'s atmosphere contains 78.08% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and traces of hydrogen, helium, and other "noble" gases (by volume), but generally a variable amount of water vapor is also present, on average about 1% at sea level.\nThe low temperatures and higher gravity of the Solar System\'s giant planets—Jupiter, Saturn, Uranus and Neptune—allow them more readily to retain gases with low molecular masses. These planets have hydrogen–helium atmospheres, with trace amounts of more complex compounds.\nTwo satellites of the outer planets possess significant atmospheres. Titan, a moon of Saturn, and Triton, a moon of Neptune, have atmospheres mainly of nitrogen. When in the part of its orbit closest to the Sun, Pluto has an atmosphere of nitrogen and methane similar to Triton\'s, but these gases are frozen when it is farther from the Sun.\nOther bodies within the Solar System have extremely thin atmospheres not in equilibrium. These include the Moon (sodium gas), Mercury (sodium gas), Europa (oxygen), Io (sulfur), and Enceladus (water vapor).\nThe first exoplanet whose atmospheric composition was determined is HD 209458b, a gas giant with a close orbit around a star in the constellation Pegasus. Its atmosphere is heated to temperatures over 1,000 K, and is steadily escaping into space. Hydrogen, oxygen, carbon and sulfur have been detected in the planet\'s inflated atmosphere.\n=== Atmospheres in the Solar System ===\nAtmosphere of the Sun\nAtmosphere of Mercury\nAtmosphere of Venus\nAtmosphere of Earth\nAtmosphere of the Moon\nAtmosphere of Mars\nAtmosphere of Ceres\nAtmosphere of Jupiter\nAtmosphere of Io\nAtmosphere of Callisto\nAtmosphere of Europa\nAtmosphere of Ganymede\nAtmosphere of Saturn\nAtmosphere of Titan\nAtmosphere of Enceladus\nAtmosphere of Uranus\nAtmosphere of Titania\nAtmosphere of Neptune\nAtmosphere of Triton\nAtmosphere of Pluto\n== Structure of atmosphere ==\n=== Earth ===\nThe atmosphere of Earth is composed of layers with different properties, such as specific gaseous composition, temperature, and pressure.\nThe troposphere is the lowest layer of the atmosphere. This extends from the planetary surface to the bottom of the stratosphere. The troposphere contains 75–80% of the mass of the atmosphere, and is the atmospheric layer wherein the weather occurs; the height of the troposphere varies between 17 km at the equator and 7.0 km at the poles.\nThe stratosphere extends from the top of the troposphere to the bottom of the mesosphere, and contains the ozone layer, at an altitude between 15 km and 35 km. It is the atmospheric layer that absorbs most of the ultraviolet radiation that Earth receives from the Sun.\nThe mesosphere ranges from 50 km to 85 km and is the layer wherein most meteors are incinerated before reaching the surface.\nThe thermosphere extends from an altitude of 85 km to the base of the exosphere at 690 km and contains the ionosphere, where solar radiation ionizes the atmosphere. The density of the ionosphere is greater at short distances from the planetary surface in the daytime and decreases as the ionosphere rises at night-time, thereby allowing a greater range of radio frequencies to travel greater distances.\nThe exosphere begins at 690 to 1,000 km from the surface, and extends to roughly 10,000 km, where it interacts with the magnetosphere of Earth.\n== Pressure ==\nAtmospheric pressure is the force (per unit-area) perpendicular to a unit-area of planetary surface, as determined by the weight of the vertical column of atmospheric gases. In said atmospheric model, the atmospheric pressure, the weight of the mass of the gas, decreases at high altitude because of the diminishing mass of the gas above the point of barometric measurement. The units of air pressure are based upon the standard atmosphere (atm), which is 101,325 Pa (equivalent to 760 Torr or 14.696 psi). The height at which the atmospheric pressure declines by a factor of e (an irrational number equal to 2.71828) is called the scale height (H). For an atmosphere of uniform temperature, the scale height is proportional to the atmospheric temperature and is inversely proportional to the product of the mean molecular mass of dry air, and the local acceleration of gravity at the point of barometric measurement.\n== Escape ==\nSurface gravity differs significantly among the planets. For example, the large gravitational force of the giant planet Jupiter retains light gases such as hydrogen and helium that escape from objects with lower gravity. Secondly, the distance from the Sun determines the energy available to heat atmospheric gas to the point where some fraction of its molecules\' thermal motion exceed the planet\'s escape velocity, allowing those to escape a planet\'s gravitational grasp. Thus, distant and cold Titan, Triton, and Pluto are able to retain their atmospheres despite their relatively low gravities.\nSince a collection of gas molecules may be moving at a wide range of velocities, there will always be some fast enough to produce a slow leakage of gas into space. Lighter molecules move faster than heavier ones with the same thermal kinetic energy, and so gases of low molecular weight are lost more rapidly than those of high molecular weight. It is thought that Venus and Mars may have lost much of their water when, after being photodissociated into hydrogen and oxygen by solar ultraviolet radiation, the hydrogen escaped. Earth\'s magnetic field helps to prevent this, as, normally, the solar wind would greatly enhance the escape of hydrogen. However, over the past 3 billion years Earth may have lost gases through the magnetic polar regions due to auroral activity, including a net 2% of its atmospheric oxygen. The net effect, taking the most important escape processes into account, is that an intrinsic magnetic field does not protect a planet from atmospheric escape and that for some magnetizations the presence of a magnetic field works to increase the escape rate.\nOther mechanisms that can cause atmosphere depletion are solar wind-induced sputtering, impact erosion, weathering, and sequestration—sometimes referred to as "freezing out"—into the regolith and polar caps.\n== Terrain ==\nAtmospheres have dramatic effects on the surfaces of rocky bodies. Objects that have no atmosphere, or that have only an exosphere, have terrain that is covered in craters. Without an atmosphere, the planet has no protection from meteoroids, and all of them collide with the surface as meteorites and create craters.\nFor planets with a significant atmosphere, most meteoroids burn up as meteors before hitting a planet\'s surface. When meteoroids do impact, the effects are often erased by the action of wind.\nWind erosion is a significant factor in shaping the terrain of rocky planets with atmospheres, and over time can erase the effects of both craters and volcanoes. In addition, since liquids cannot exist without pressure, an atmosphere allows liquid to be present at the surface, resulting in lakes, rivers and oceans. Earth and Titan are known to have liquids at their surface and terrain on the planet suggests that Mars had liquid on its surface in the past.\n=== Outside the Solar System ===\nAtmosphere of HD 209458 b\n== Circulation ==\nThe circulation of the atmosphere occurs due to thermal differences when convection becomes a more efficient transporter of heat than thermal radiation. On planets where the primary heat source is solar radiation, excess heat in the tropics is transported to higher latitudes. When a planet generates a significant amount of heat internally, such as is the case for Jupiter, convection in the atmosphere can transport thermal energy from the higher temperature interior up to the surface.\n== Importance ==', 'Stellar evolution is the process by which a star changes over the course of time. Depending on the mass of the star, its lifetime can range from a few million years for the most massive to trillions of years for the least massive, which is considerably longer than the current age of the universe. The table shows the lifetimes of stars as a function of their masses. All stars are formed from collapsing clouds of gas and dust, often called nebulae or molecular clouds.  Over the course of millions of years, these protostars settle down into a state of equilibrium, becoming what is known as a main-sequence star.\nNuclear fusion powers a star for most of its existence. Initially the energy is generated by the fusion of hydrogen atoms at the core of the main-sequence star. Later, as the preponderance of atoms at the core becomes helium, stars like the Sun begin to fuse hydrogen along a spherical shell surrounding the core. This process causes the star to gradually grow in size, passing through the subgiant stage until it reaches the red-giant phase. Stars with at least half the mass of the Sun can also begin to generate energy through the fusion of helium at their core, whereas more-massive stars can fuse heavier elements along a series of concentric shells. Once a star like the Sun has exhausted its nuclear fuel, its core collapses into a dense white dwarf and the outer layers are expelled as a planetary nebula. Stars with around ten or more times the mass of the Sun can explode in a supernova as their inert iron cores collapse into an extremely dense neutron star or black hole. Although the universe is not old enough for any of the smallest red dwarfs to have reached the end of their existence, stellar models suggest they will slowly become brighter and hotter before running out of hydrogen fuel and becoming low-mass white dwarfs.\nStellar evolution is not studied by observing the life of a single star, as most stellar changes occur too slowly to be detected, even over many centuries. Instead, astrophysicists come to understand how stars evolve by observing numerous stars at various points in their lifetime, and by simulating stellar structure using computer models.\n== Star formation ==\n=== Protostar ===\nStellar evolution starts with the gravitational collapse of a giant molecular cloud. Typical giant molecular clouds are roughly 100 light-years (9.5×1014 km) across and contain up to 6,000,000 solar masses (1.2×1037 kg). As it collapses, a giant molecular cloud breaks into smaller and smaller pieces. In each of these fragments, the collapsing gas releases gravitational potential energy as heat. As its temperature and pressure increase, a fragment condenses into a rotating ball of superhot gas known as a protostar.  Filamentary structures are truly ubiquitous in the molecular cloud. Dense molecular filaments will fragment into gravitationally bound cores, which are the precursors of stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed fragmentation manner of the filaments. In supercritical filaments, observations have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded two protostars with gas outflows.\nA protostar continues to grow by accretion of gas and dust from the molecular cloud, becoming a pre-main-sequence star as it reaches its final mass. Further development is determined by its mass. Mass is typically compared to the mass of the Sun: 1.0 M☉ (2.0×1030 kg) means 1 solar mass.\nProtostars are encompassed in dust, and are thus more readily visible at infrared wavelengths.\nObservations from the Wide-field Infrared Survey Explorer (WISE) have been especially important for unveiling numerous galactic protostars and their parent star clusters.\n=== Brown dwarfs and sub-stellar objects ===\nProtostars with masses less than roughly 0.08 M☉ (1.6×1029 kg) never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (MJ), 2.5 × 1028 kg, or 0.0125 M☉). Objects smaller than 13 MJ are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets). Both types, deuterium-burning and not, shine dimly and fade away slowly, cooling gradually over hundreds of millions of years.\n=== Main sequence stellar mass objects ===\nFor a more-massive protostar, the core temperature will eventually reach 10 million kelvin, initiating the proton–proton chain reaction and allowing hydrogen to fuse, first to deuterium and then to helium. In stars of slightly over 1 M☉ (2.0×1030 kg), the carbon–nitrogen–oxygen fusion reaction (CNO cycle) contributes a large portion of the energy generation. The onset of nuclear fusion leads relatively quickly to a hydrostatic equilibrium in which energy released by the core maintains a high gas pressure, balancing the weight of the star\'s matter and preventing further gravitational collapse. The star thus evolves rapidly to a stable state, beginning the main-sequence phase of its evolution.\nA new star will sit at a specific point on the main sequence of the Hertzsprung–Russell diagram, with the main-sequence spectral type depending upon the mass of the star. Small, relatively cold, low-mass red dwarfs fuse hydrogen slowly and will remain on the main sequence for hundreds of billions of years or longer, whereas massive, hot O-type stars will leave the main sequence after just a few million years. A mid-sized yellow dwarf star, like the Sun, will remain on the main sequence for about 10 billion years. The Sun is thought to be in the middle of its main sequence lifespan.\n=== Planetary system ===\nA star may gain a protoplanetary disk, which furthermore can develop into a planetary system.\n== Mature stars ==\nEventually the star\'s core exhausts its supply of hydrogen and the star begins to evolve off the main sequence. Without the outward radiation pressure generated by the fusion of hydrogen to counteract the force of gravity, the core contracts until either electron degeneracy pressure becomes sufficient to oppose gravity or the core becomes hot enough (around 100 MK) for helium fusion to begin. Which of these happens first depends upon the star\'s mass.\n=== Low-mass stars ===\nWhat happens after a low-mass star ceases to produce energy through fusion has not been directly observed; the universe is around 13.8 billion years old, which is less time (by several orders of magnitude, in some cases) than it takes for fusion to cease in such stars.\nRecent astrophysical models suggest that red dwarfs of 0.1 M☉ may stay on the main sequence for some six to twelve trillion years, gradually increasing in both temperature and luminosity, and take several hundred billion years more to collapse, slowly, into a white dwarf.  Such stars will not become red giants as the whole star is a convection zone and it will not develop a degenerate helium core with a shell burning hydrogen.  Instead, hydrogen fusion will proceed until almost the whole star is helium.\nSlightly more massive stars do expand into red giants, but their helium cores are not massive enough to reach the temperatures required for helium fusion so they never reach the tip of the red-giant branch.  When hydrogen shell burning finishes, these stars move directly off the red-giant branch like a post-asymptotic-giant-branch (AGB) star, but at lower luminosity, to become a white dwarf.  A star with an initial mass about 0.6 M☉ will be able to reach temperatures high enough to fuse helium, and these "mid-sized" stars go on to further stages of evolution beyond the red-giant branch.\n=== Mid-sized stars ===\nStars of roughly 0.6–10 M☉ become red giants, which are large non-main-sequence stars of stellar classification K or M. Red giants lie along the right edge of the Hertzsprung–Russell diagram due to their red color and large luminosity. Examples include Aldebaran in the constellation Taurus and Arcturus in the constellation of Boötes.\nMid-sized stars are red giants during two different phases of their post-main-sequence evolution: red-giant-branch stars, with inert cores made of helium and hydrogen-burning shells, and asymptotic-giant-branch stars, with inert cores made of carbon and helium-burning shells inside the hydrogen-burning shells.  Between these two phases, stars spend a period on the horizontal branch with a helium-fusing core.  Many of these helium-fusing stars cluster towards the cool end of the horizontal branch as K-type giants and are referred to as red clump giants.\n==== Subgiant phase ====\nWhen a star exhausts the hydrogen in its core, it leaves the main sequence and begins to fuse hydrogen in a shell outside the core.  The core increases in mass as the shell produces more helium.  Depending on the mass of the helium core, this continues for several million to one or two billion years, with the star expanding and cooling at a similar or slightly lower luminosity to its main sequence state.  Eventually either the core becomes degenerate, in stars around the mass of the sun, or the outer layers cool sufficiently to become opaque, in more massive stars.  Either of these changes cause the hydrogen shell to increase in temperature and the luminosity of the star to increase, at which point the star expands onto the red-giant branch.\n==== Red-giant-branch phase ====']

Question: What happens to excess base metal as a solution cools from the upper transformation temperature towards an insoluble state?

Choices:
Choice A) The excess base metal will often solidify, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.
Choice B) The excess base metal will often crystallize-out, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.
Choice C) The excess base metal will often dissolve, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.
Choice D) The excess base metal will often liquefy, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.
Choice E) The excess base metal will often evaporate, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Quantum number', 'In response to the so-called "naturalness crisis" in the Minimal Supersymmetric Standard Model, some researchers have abandoned naturalness and the original motivation to solve the hierarchy problem naturally with supersymmetry, while other researchers have moved on to other supersymmetric models such as split supersymmetry. Still others have moved to string theory as a result of the naturalness crisis. Former enthusiastic supporter Mikhail Shifman went as far as urging the theoretical community to search for new ideas and accept that supersymmetry was a failed theory in particle physics. However, some researchers suggested that this "naturalness" crisis was premature because various calculations were too optimistic about the limits of masses which would allow a supersymmetric extension of the Standard Model as a solution.\n== General supersymmetry ==\nSupersymmetry appears in many related contexts of theoretical physics. It is possible to have multiple supersymmetries and also have supersymmetric extra dimensions.\n=== Extended supersymmetry ===\nIt is possible to have more than one kind of supersymmetry transformation. Theories with more than one supersymmetry transformation are known as extended supersymmetric theories. The more supersymmetry a theory has, the more constrained are the field content and interactions. Typically the number of copies of a supersymmetry is a power of 2 (1, 2, 4, 8...). In four dimensions, a spinor has four degrees of freedom and thus the minimal number of supersymmetry generators is four in four dimensions and having eight copies of supersymmetry means that there are 32 supersymmetry generators.\nThe maximal number of supersymmetry generators possible is 32. Theories with more than 32 supersymmetry generators automatically have massless fields with spin greater than 2. It is not known how to make massless fields with spin greater than two interact, so the maximal number of supersymmetry generators considered is 32. This is due to the Weinberg–Witten theorem. This corresponds to an N = 8 supersymmetry theory. Theories with 32 supersymmetries automatically have a graviton.\nFor four dimensions there are the following theories, with the corresponding multiplets (CPT adds a copy, whenever they are not invariant under such symmetry):\n=== Supersymmetry in alternate numbers of dimensions ===\nIt is possible to have supersymmetry in dimensions other than four. Because the properties of spinors change drastically between different dimensions, each dimension has its characteristic. In d dimensions, the size of spinors is approximately 2d/2 or 2(d − 1)/2. Since the maximum number of supersymmetries is 32, the greatest number of dimensions in which a supersymmetric theory can exist is eleven.\n=== Fractional supersymmetry ===\nFractional supersymmetry is a generalization of the notion of supersymmetry in which the minimal positive amount of spin does not have to be \u20601/2\u2060 but can be an arbitrary \u20601/N\u2060 for integer value of N. Such a generalization is possible in two or fewer spacetime dimensions.\n== See also ==\n== References ==\n== Further reading ==\n=== Theoretical introductions, free and online ===\n=== Monographs ===\n=== On experiments ===\n== External links ==\nSupersymmetry – European Organization for Nuclear Research (CERN)\nThe status of supersymmetry – Symmetry Magazine (Fermilab/SLAC), January 12, 2021\nAs Supersymmetry Fails Tests, Physicists Seek New Ideas – Quanta Magazine, November 20, 2012\nWhat is Supersymmetry? – Fermilab, May 21, 2013\nWhy Supersymmetry? – Fermilab, May 31, 2013\nThe Standard Model and Supersymmetry – World Science Festival, March 4, 2015\nSUSY running out of hiding places – BBC, December 11, 2012', '{\\displaystyle L_{z}=m_{\\ell }\\hbar }\nThe values of mℓ range from −ℓ to ℓ, with integer intervals.\nThe s subshell (ℓ = 0) contains only one orbital, and therefore the mℓ of an electron in an s orbital will always be 0. The p subshell (ℓ = 1) contains three orbitals, so the mℓ of an electron in a p orbital will be −1, 0, or 1. The d subshell (ℓ = 2) contains five orbitals, with mℓ values of −2, −1, 0, 1, and 2.\n=== Spin magnetic quantum number ===\nThe spin magnetic quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis:\n{\\displaystyle S_{z}=m_{s}\\hbar }\nIn general, the values of ms range from −s to s, where s is the spin quantum number, associated with the magnitude of particle\'s intrinsic spin angular momentum:\n{\\displaystyle m_{s}=-s,-s+1,-s+2,\\cdots ,s-2,s-1,s}\nAn electron state has spin number s = \u20601/2\u2060, consequently ms will be +\u20601/2\u2060 ("spin up") or −\u20601/2\u2060 "spin down" states. Since electron are fermions they obey the Pauli exclusion principle: each electron state must have different quantum numbers.  Therefore, every orbital will be occupied with at most two electrons, one for each spin state.\n=== The Aufbau principle and Hund\'s Rules ===\nA multi-electron atom can be modeled qualitatively as a hydrogen like atom with higher nuclear charge and correspondingly more electrons. The occupation of the electron states in such an atom can be predicted by the Aufbau principle and Hund\'s empirical rules for the quantum numbers.  The Aufbau principle fills orbitals based on their principal and azimuthal quantum numbers (lowest n + l first, with lowest n breaking ties; Hund\'s rule favors unpaired electrons in the outermost orbital). These rules are empirical but they can be related to electron physics.:\u200a10\u200a:\u200a260\n== Spin-orbit coupled systems ==\nWhen one takes the spin–orbit interaction into consideration, the L and S operators no longer commute with the Hamiltonian, and the eigenstates of the system no longer have well-defined orbital angular momentum and spin. Thus another set of quantum numbers should be used. This set includes\nThe total angular momentum quantum number:\n{\\displaystyle j=|\\ell \\pm s|,}\nwhich gives the total angular momentum through the relation\n{\\displaystyle J^{2}=\\hbar ^{2}j(j+1).}\nThe projection of the total angular momentum along a specified axis:\n{\\displaystyle m_{j}=-j,-j+1,-j+2,\\cdots ,j-2,j-1,j}\nanalogous to the above and satisfies both\n{\\displaystyle m_{j}=m_{\\ell }+m_{s},}\nand\n{\\displaystyle |m_{\\ell }+m_{s}|\\leq j.}\nParityThis is the eigenvalue under reflection: positive (+1) for states which came from even ℓ and negative (−1) for states which came from odd ℓ. The former is also known as even parity and the latter as odd parity, and is given by\n{\\displaystyle P=(-1)^{\\ell }.}\nFor example, consider the following 8 states, defined by their quantum numbers:\nThe quantum states in the system can be described as linear combination of these 8 states. However, in the presence of spin–orbit interaction, if one wants to describe the same system by 8 states that are eigenvectors of the Hamiltonian (i.e. each represents a state that does not mix with others over time), we should consider the following 8 states:\n== Atomic nuclei ==\nIn nuclei, the entire assembly of protons and neutrons (nucleons) has a resultant angular momentum due to the angular momenta of each nucleon, usually denoted I. If the total angular momentum of a neutron is jn = ℓ + s and for a proton is jp = ℓ + s (where s for protons and neutrons happens to be \u20601/2\u2060 again (see note)), then the nuclear angular momentum quantum numbers I are given by:\n{\\displaystyle I=|j_{n}-j_{p}|,|j_{n}-j_{p}|+1,|j_{n}-j_{p}|+2,\\cdots ,(j_{n}+j_{p})-2,(j_{n}+j_{p})-1,(j_{n}+j_{p})}\nNote: The orbital angular momenta of the nuclear (and atomic) states are all integer multiples of ħ while the intrinsic angular momentum of the neutron and  proton are half-integer multiples.  It should be immediately apparent that the combination of the intrinsic spins of the nucleons with their orbital motion will always give half-integer values for the total spin, I, of any odd-A nucleus and integer values for any even-A nucleus.\nParity with the number I is used to label nuclear angular momentum states, examples for some isotopes of hydrogen (H), carbon (C), and sodium (Na) are;\nThe reason for the unusual fluctuations in I, even by differences of just one nucleon, are due to the odd and even numbers of protons and neutrons – pairs of nucleons have a total angular momentum of zero (just like electrons in orbitals), leaving an odd or even number of unpaired nucleons. The property of nuclear spin is an important factor for the operation of NMR spectroscopy in organic chemistry, and MRI in nuclear medicine, due to the nuclear magnetic moment interacting with an external magnetic field.\n== Elementary particles ==\nElementary particles contain many quantum numbers which are usually said to be intrinsic to them. However, it should be understood that the elementary particles are quantum states of the standard model of particle physics, and hence the quantum numbers of these particles bear the same relation to the Hamiltonian of this model as the quantum numbers of the Bohr atom does to its Hamiltonian. In other words, each quantum number denotes a symmetry of the problem. It is more useful in quantum field theory to distinguish between spacetime and internal symmetries.\nTypical quantum numbers related to spacetime symmetries are spin (related to rotational symmetry), the parity, C-parity and T-parity (related to the Poincaré symmetry of spacetime). Typical internal symmetries are lepton number and baryon number or the electric charge. (For a full list of quantum numbers of this kind see the article on flavour.)\n== Multiplicative quantum numbers ==\nMost conserved quantum numbers are additive, so in an elementary particle reaction, the sum of the quantum numbers should be the same before and after the reaction. However, some, usually called a parity, are multiplicative; i.e., their product is conserved. All multiplicative quantum numbers belong to a symmetry (like parity) in which applying the symmetry transformation twice is equivalent to doing nothing (involution).\n== See also ==\nElectron configuration\n== References ==\n== Further reading ==\nDirac, Paul A. M. (1982). Principles of Quantum Mechanics. Oxford University Press. ISBN 0-19-852011-5.\nGriffiths, David J. (2004). Introduction to Quantum Mechanics (2nd ed.). Prentice Hall. ISBN 0-13-805326-X.\nHalzen, Francis & Martin, Alan D. (1984). Quarks and Leptons: An Introductory Course in Modern Particle Physics. John Wiley & Sons. ISBN 0-471-88741-2.\nEisberg, Robert Martin; Resnick, Robert (1985). Quantum Physics of Atoms, Molecules, Solids, Nuclei and Particles (2nd ed.). John Wiley & Sons. ISBN 978-0-471-87373-0 – via Internet Archive.', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Fermi level', 'While supersymmetry has not been discovered at high energy, see Section Supersymmetry in particle physics, supersymmetry was found to be effectively realized at the intermediate energy of hadronic physics where baryons and mesons are superpartners. An exception is the pion that appears as a zero mode in the mass spectrum and thus protected by the supersymmetry: It has no baryonic partner. The realization of this effective supersymmetry is readily explained in quark–diquark models: Because two different color charges close together (e.g., blue and red) appear under coarse resolution as the corresponding anti-color (e.g. anti-green), a diquark cluster viewed with coarse resolution (i.e., at the energy-momentum scale used to study hadron structure) effectively appears as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves likes a meson.\n=== Supersymmetry in condensed matter physics ===\nSUSY concepts have provided useful extensions to the WKB approximation. Additionally, SUSY has been applied to disorder averaged systems both quantum and non-quantum (through statistical mechanics), the Fokker–Planck equation being an example of a non-quantum theory. The \'supersymmetry\' in all these systems arises from the fact that one is modelling one particle and as such the \'statistics\' do not matter. The use of the supersymmetry method provides a mathematical rigorous alternative to the replica trick, but only in non-interacting systems, which attempts to address the so-called \'problem of the denominator\' under disorder averaging. For more on the applications of supersymmetry in condensed matter physics see Efetov (1997).\nIn 2021, a group of researchers showed that, in theory,\n{\\displaystyle N=(0,1)}\nSUSY could be realised at the edge of a Moore–Read quantum Hall state. However, to date, no experiments have been done yet to realise it at an edge of a Moore–Read state. In 2022, a different group of researchers created a computer simulation of atoms in 1 dimensions that had supersymmetric topological quasiparticles.\n=== Supersymmetry in optics ===\nIn 2013, integrated optics was found to provide a fertile ground on which certain ramifications of SUSY can be explored in readily-accessible laboratory settings. Making use of the analogous mathematical structure of the quantum-mechanical Schrödinger equation and the wave equation governing the evolution of light in one-dimensional settings, one may interpret the refractive index distribution of a structure as a potential landscape in which optical wave packets propagate. In this manner, a new class of functional optical structures with possible applications in phase matching, mode conversion and space-division multiplexing becomes possible. SUSY transformations have been also proposed as a way to address inverse scattering problems in optics and as a one-dimensional transformation optics.\n=== Supersymmetry in dynamical systems ===\nAll stochastic (partial) differential equations, the models for all types of continuous time dynamical systems, possess topological supersymmetry. In the operator representation of stochastic evolution, the topological supersymmetry is the exterior derivative which is commutative with the stochastic evolution operator defined as the stochastically averaged pullback induced on differential forms by SDE-defined diffeomorphisms of the phase space. The topological sector of the so-emerging supersymmetric theory of stochastic dynamics can be recognized as the Witten-type topological field theory.\nThe meaning of the topological supersymmetry in dynamical systems is the preservation of the phase space continuity—infinitely close points will remain close during continuous time evolution even in the presence of noise. When the topological supersymmetry is broken spontaneously, this property is violated in the limit of the infinitely long temporal evolution and the model can be said to exhibit (the stochastic generalization of) the butterfly effect. From a more general perspective, spontaneous breakdown of the topological supersymmetry is the theoretical essence of the ubiquitous dynamical phenomenon variously known as chaos, turbulence, self-organized criticality etc. The Goldstone theorem explains the associated emergence of the long-range dynamical behavior that manifests itself as \u20601/f\u2060 noise, butterfly effect, and the scale-free statistics of sudden (instantonic) processes, such as earthquakes, neuroavalanches, and solar flares, known as the Zipf\'s law and the Richter scale.\n=== Supersymmetry in mathematics ===\nSUSY is also sometimes studied mathematically for its intrinsic properties. This is because it describes complex fields satisfying a property known as holomorphy, which allows holomorphic quantities to be exactly computed. This makes supersymmetric models useful "toy models" of more realistic theories. A prime example of this has been the demonstration of S-duality in four-dimensional gauge theories that interchanges particles and monopoles.\nThe proof of the Atiyah–Singer index theorem is much simplified by the use of supersymmetric quantum mechanics.\n=== Supersymmetry in string theory ===\nSupersymmetry is an integral part of string theory, a possible theory of everything. There are two types of string theory, supersymmetric string theory or superstring theory, and non-supersymmetric string theory. By definition of superstring theory, supersymmetry is required in superstring theory at some level. However, even in non-supersymmetric string theory, a type of supersymmetry called misaligned supersymmetry is still required in the theory in order to ensure no physical tachyons appear. Any string theories without some kind of supersymmetry, such as bosonic string theory and the\n{\\displaystyle E_{7}\\times E_{7}}\n16\n{\\displaystyle SU(16)}\n, and\n{\\displaystyle E_{8}}\nheterotic string theories, will have a tachyon and therefore the spacetime vacuum itself would be unstable and would decay into some tachyon-free string theory usually in a lower spacetime dimension. There is no experimental evidence that either supersymmetry or misaligned supersymmetry holds in our universe, and many physicists have moved on from supersymmetry and string theory entirely due to the non-detection of supersymmetry at the LHC.\nDespite the null results for supersymmetry at the LHC so far, some particle physicists have nevertheless moved to string theory in order to resolve the naturalness crisis for certain supersymmetric extensions of the Standard Model. According to the particle physicists, there exists a concept of "stringy naturalness" in string theory, where the string theory landscape could have a power law statistical pull on soft SUSY breaking terms to large values (depending on the number of hidden sector SUSY breaking fields contributing to the soft terms). If this is coupled with an anthropic requirement that contributions to the weak scale not exceed a factor between 2 and 5 from its measured value (as argued by Agrawal et al.), then the Higgs mass is pulled up to the vicinity of 125 GeV while most sparticles are pulled to values beyond the current reach of LHC. (The Higgs was determined to have a mass of 125 GeV ±0.15 GeV in 2022.) An exception occurs for higgsinos which gain mass not from SUSY breaking but rather from whatever mechanism solves the SUSY mu problem. Light higgsino pair production in association with hard initial state jet radiation leads to a soft opposite-sign dilepton plus jet plus missing transverse energy signal.\n== Supersymmetry in particle physics ==\nIn particle physics, a supersymmetric extension of the Standard Model is a possible candidate for undiscovered particle physics, and seen by some physicists as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to be incomplete and where limitations of current theories are well established. In particular, one supersymmetric extension of the Standard Model, the Minimal Supersymmetric Standard Model (MSSM), became popular in theoretical particle physics, as the Minimal Supersymmetric Standard Model is the simplest supersymmetric extension of the Standard Model that could resolve major hierarchy problems within the Standard Model, by guaranteeing that quadratic divergences of all orders will cancel out in perturbation theory. If a supersymmetric extension of the Standard Model is correct, superpartners of the existing elementary particles would be new and undiscovered particles and supersymmetry is expected to be spontaneously broken.\nThere is no experimental evidence that a supersymmetric extension to the Standard Model is correct, or whether or not other extensions to current models might be more accurate. It is only since around 2010 that particle accelerators specifically designed to study physics beyond the Standard Model have become operational (i.e. the Large Hadron Collider (LHC)), and it is not known where exactly to look, nor the energies required for a successful search. However, the negative results from the LHC since 2010 have already ruled out some supersymmetric extensions to the Standard Model, and many physicists believe that the Minimal Supersymmetric Standard Model, while not ruled out, is no longer able to fully resolve the hierarchy problem.\n=== Supersymmetric extensions of the Standard Model ===', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  ħωk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, θk†, θk, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the θk, and the Fock space is built by applying all products of creation operators θk† to |0⟩.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at φ = 0, so that the vacuum minimizing the Hamiltonian satisfies ⟨φ⟩ = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  φ = v .  This occurs for example, if V(φ) = gφ4 − 2m2φ2 with g > 0 and m2 > 0, for which the minimum energy is found at v = ±m/√g. The value of v in one of these vacua may be considered as condensate of the field φ. Canonical quantization then can be carried out for the shifted field  φ(x,t) − v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator  [A, B]  expressed in the phase space formulation is iħ{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  ħ-deformations are highly nonunique—quantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  ħ → 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an ħ-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional Schrödinger Hilbert space. If the chosen variables are complex, we get something like the Segal–Bargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', 'Supersymmetry', "Departments:\nEducational Sciences\nPsychological Sciences\nArt Education\n=== College of Engineering ===\nThe College of Engineering was established in 1980, and has become one of the largest at Qatar University. The college offers both undergraduate and graduate courses.\nThe college's previous dean, Dr. Alfadala, was also the founder and former chairman of the university's Gas Processing Center (GPC) research facility. The current Dean is Dr. Khalifa Al-Khalifa.\n==== Programs ====\nThe College of Engineering offers the following undergraduate programs: Bachelor of Architecture, Bachelor of Science in Chemical Engineering, Bachelor of Science in Civil Engineering, Bachelor of Science in Computer Engineering, Bachelor of Science in Computer Science, Bachelor of Science in Electrical Engineering, Bachelor of Science in Industrial and Systems Engineering, Bachelor of Science in Mechanical Engineering.\nThe College of Engineering offers the following graduate programs: Masters of Science in Computing, Masters of Urban Planning & Design, Masters of Science in Engineering Management, Masters of Science in Environmental Engineering, Master of Science in Civil Engineering, Master of Science in Mechanical Engineering, Master of Science in Electrical Engineering.\nThe College of Engineering offers the following Doctor of Philosophy (PhD) programs: Doctor of Philosophy in Architecture, Doctor of Philosophy in Urban Planning, Doctor of Philosophy Chemical Engineering, Doctor of Philosophy in Computer Science, Doctor of Philosophy in Computer Engineering, Doctor of Philosophy in Civil Engineering, Doctor of Philosophy in Electrical Engineering, Doctor of Philosophy in Industrial and Systems Engineering, Doctor of Philosophy in Mechanical Engineering, Doctor of Philosophy in Engineering Management, Doctor of Philosophy in Environmental Engineering, Doctor of Philosophy in Material Science and Engineering.\n==== Departments ====\nDepartments are:\nArchitectural Engineering\nChemical Engineering\n(ABET Substantial Equivalency accredited)\nCivil Engineering\n(ABET Substantial Equivalency accredited)\nComputer Engineering\n(ABET Substantial Equivalency accredited)\nComputer Science\n(ABET Substantial Equivalency accredited)\nElectrical Engineering (former chairman: Soliman Abdel-hady Soliman)\n(ABET Substantial Equivalency accredited)\nMechanical Engineering\n(ABET Substantial Equivalency accredited)\nIndustrial & System Engineering\n(ABET Substantial Equivalency accredited)\n==== Deans ====\nProf. Galal Shawky (1980-1990)\nProf. Ismael Abdulrahman Taj (1990-1999)\nDr. Mohammed Al-Hammadi (2000-2001)\nProf. Adnan Nayfah (2003-2005)\nDr. Nabil Al-Salem (2005-2006)\nDr. Hassan Al-Fadala (2006-2008)\nProf. Mazen Hasna (2008-2013)\nProf. Rashid Alammari (2013-2016)\nProf. Khalifa Nasser Al-Khalifa (2016-2018)\nProf. Abdelmagid Hamouda (2018–2019)\nDr. Khalid Kamal Naji (2019–Present)\n==== Notable alumni ====\nNotable alumni include:\nEng. Fawaz Al-Baker executive managing director Qatar Power Company\nNasser Al-Khelaifi\nIbrahim bin Yousuf Al-Fakhro\nSaad Ahmed Al Mohannadi\nIlham Al-Qaradawi\nDr. Yousef Alhorr\n==== Research Centers ====\nGas Processing Center\nThe center addresses the problems, challenges and opportunities facing the state of Qatar's gas processing industry. The center is focused on two main themes which are Asset Management/Process Optimization and Sustainable Development.\nThe services provided by the center have been designed to address the necessities and challenges of both Qatar University and the Qatari Industry. These services include: Applied Research Projects/Consulting, Professional Training and Seminars, Bi-annual Gas Processing Symposium, Information Management/Library.\nKindi Lab for Computing Research\nQatar Transportation and Traffic Safety Center\nRoad traffic accidents have major societal, health, environmental and economic impacts on Qatar's economy. Moreover, the accidents cause significant delays and traffic congestions. The expected increase in population and special events that occur in Qatar on regular basis as well as the above impacts have prompted the College of Engineering, Qatar University to establish Qatar Transportation and Traffic Safety Center (QTTSC) to conduct extensive studies and analysis of accidents data and information in order to significantly reduce such road accidents.\nThe studies will include studies related to patterns of accidents, factors that contribute to road accidents, drivers' attributes and others as well as recommendations for approaches that result in an improved road safety.\nThere are three major areas of concern: Road User Behavioral Change, Vehicle Safety and Biomechanics and Road Engineering and Environment. The center will be housed and managed by the College of Engineering and its funding will be obtained from different sources including Qatar University, companies and government agencies.\nChemE Car Competition\nThe ChemE competition's aim is to encourage creativity and innovation among students and also to reach out high school students to motivate them towards studies in chemical engineering.\nThe competition requires participants to design and race a small car that is operated through chemical processes and carrying a load of water (0-500g) for a distance of between 15 and 30 meters.\n=== College of Law ===\nIn 2004, Qatar University instated a College of Law by separating the law department of the existing College of Sharia. 10%-15% of its undergraduate program is instructed in English. In 2008, it asked the ABA to conduct a full-scale, on-site evaluation of all aspects of the school's objectives, programs, and administration. Many of the recommendations made by the ABA were subsequently implemented including the introduction of the first legal writing and research skills program (taught in English) established in any law school in the Middle East. The legal skills program was recognized at the 13th Global Legal Skills Conference held in Melbourne, Australia in December 2018.\nThe College of Law has the highest percentage of Qatari students of any college in Qatar University.\nThe College of Law is accredited by the High Council for the Evaluation of Research and Higher Education (HCERES) and the British Accreditation Council (BAC). The College of Law also partners with leading law schools, and welcomes visiting Fulbright fellows and exchange students.\nThe International Review of Law is an international legal periodical that is published by the College of Law (through Qatar University Press) biannually. It is an internationally peer reviewed multi-lingual law journal that seeks to articulate contemporary legal discourse across cultures and borders. The journal is open to doctrinal, context based, reformative or comparative work, in all fields of law. The journal accepts submissions in English, Arabic and French and provides abstract translations for all publications. The chief editor of the journal is currently Dr. Sonia Malak.\nDr. Mohamed Abdulaziz Al Khulaifi was appointed dean in 2014.\nIn November 2018, the College of Law hosted the Annual Conference of the International Association of Law Schools.\nIn April 2020, the College of Law will move into its new purpose build facility currently under construction.\nPrograms:\nUndergraduate Law\nLLM in Public Law\nLLM in Private Law\nGraduate Certificate in Legal Studies\n=== College of Pharmacy ===\nThe College of Pharmacy at Qatar University was founded as a college in 2008. It is the first pharmacy college to be established in Qatar. It began as a program in 2006, and saw its first student intake in 2007. The year 2008 also marked the college's accreditation by the CCAPP (Canada) and became the first international pharmacy program to receive accreditation by that organization. Peter Jewesson was the founding College Dean, and had also been the director of the previous Pharmacy program.\nThe College of Pharmacy offers three degrees:\n5-year program Bachelor of Science in Pharmacy – BSc (Pharm)\nAccredited by CCAPP\n6-year Doctor of Pharmacy - PharmD\nThe PharmD degree is targeted for select graduates pursuing advanced clinical training. It is offered as a full-time study plan for QU graduates, and a part-time study plan for BSc (Pharm) pharmacists practicing in Qatar\n2-year Master of Science in Pharmacy - MSc (Pharm)\nThe MSc (Pharm) degree is designed to build on the undergraduate degree experience.\n=== College of Sharia and Islamic Studies ===\nCollege of Sharia and Islamic Studies was among the first founded at Qatar University when it was established in 1977. In recent years, it added new major and minor programs in subjects such as “Da’wa and Media” and “Banking and Insurance”. Dr. Aisha Yousuf Al-Mannai is the current dean.\nDepartments:\nIslamic Jurisprudence\nIslamic Culture & Preaching\nFoundations of Islam\n=== College of Medicine ===\nIn December 2014, Qatar University announced the establishment of the College of Medicine which was planned to take the 1st batch in by Fall 2015 (September) and graduate them by 2021.\n=== College of Health Science ===\nThe College of Health Science (CHS) was founded in 2016 following 30 years as a department within various colleges at Qatar University. CHS is the national provider of health sciences education and research in Qatar and offers undergraduate (BSc) programs in biomedical sciences, human nutrition and public health, as well as graduate programs; (MSc) in biomedical science and master of public health (MPH).\n=== Sport Science Program ===\nThe Sport Science Program was opened to students in the fall 2009 semester. The program was constructed as a joint project sponsored by Aspire Academy.\nWhile QU's Sport Science program is not an independent college, it has been formed with autonomy from the other colleges, much as the College of Pharmacy began.\nThe Program offers a Bachelor of Science degree which allows for one of 3 concentrations:\nSport Management\nExercise and Fitness\nPhysical Education\n== Honors Program ==", 'Neutron']

Question: What is the spin quantum number?

Choices:
Choice A) The spin quantum number is a measure of the distance between an elementary particle and the nucleus of an atom.
Choice B) The spin quantum number is a measure of the size of an elementary particle.
Choice C) The spin quantum number is a measure of the charge of an elementary particle.
Choice D) The spin quantum number is a measure of the speed of an elementary particle's rotation around some axis.
Choice E) The spin quantum number is a dimensionless quantity obtained by dividing the spin angular momentum by the reduced Planck constant ħ, which has the same dimensions as angular momentum.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Neutron', '==== Type II ====\nStars with initial masses less than about 8 M☉ never develop a core large enough to collapse and they eventually lose their atmospheres to become white dwarfs. Stars with at least 9 M☉ (possibly as much as 12 M☉) evolve in a complex fashion, progressively burning heavier elements at hotter temperatures in their cores. The star becomes layered like an onion, with the burning of more easily fused elements occurring in larger shells. Although popularly described as an onion with an iron core, the least massive supernova progenitors only have oxygen-neon(-magnesium) cores. These super-AGB stars may form the majority of core collapse supernovae, although less luminous and so less commonly observed than those from more massive progenitors.\nIf core collapse occurs during a supergiant phase when the star still has a hydrogen envelope, the result is a type II supernova. The rate of mass loss for luminous stars depends on the metallicity and luminosity. Extremely luminous stars at near solar metallicity will lose all their hydrogen before they reach core collapse and so will not form a supernova of type II. At low metallicity, all stars will reach core collapse with a hydrogen envelope but sufficiently massive stars collapse directly to a black hole without producing a visible supernova.\nStars with an initial mass up to about 90 times the Sun, or a little less at high metallicity, result in a type II-P supernova, which is the most commonly observed type. At moderate to high metallicity, stars near the upper end of that mass range will have lost most of their hydrogen when core collapse occurs and the result will be a type II-L supernova. At very low metallicity, stars of around 140–250 M☉ will reach core collapse by pair instability while they still have a hydrogen atmosphere and an oxygen core and the result will be a supernova with type II characteristics but a very large mass of ejected 56Ni and high luminosity.\n==== Type Ib and Ic ====\nThese supernovae, like those of type II, are massive stars that undergo core collapse. Unlike the progenitors of type II supernovae, the stars which become types Ib and Ic supernovae have lost most of their outer (hydrogen) envelopes due to strong stellar winds or else from interaction with a companion. These stars are known as Wolf–Rayet stars, and they occur at moderate to high metallicity where continuum driven winds cause sufficiently high mass-loss rates. Observations of type Ib/c supernova do not match the observed or expected occurrence of Wolf–Rayet stars. Alternate explanations for this type of core collapse supernova involve stars stripped of their hydrogen by binary interactions. Binary models provide a better match for the observed supernovae, with the proviso that no suitable binary helium stars have ever been observed.\nType Ib supernovae are the more common and result from Wolf–Rayet stars of type WC which still have helium in their atmospheres. For a narrow range of masses, stars evolve further before reaching core collapse to become WO stars with very little helium remaining, and these are the progenitors of type Ic supernovae.\nA few percent of the type Ic supernovae are associated with gamma-ray bursts (GRB), though it is also believed that any hydrogen-stripped type Ib or Ic supernova could produce a GRB, depending on the circumstances of the geometry. The mechanism for producing this type of GRB is the jets produced by the magnetic field of the rapidly spinning magnetar formed at the collapsing core of the star. The jets would also transfer energy into the expanding outer shell, producing a super-luminous supernova.\nUltra-stripped supernovae occur when the exploding star has been stripped (almost) all the way to the metal core, via mass transfer in a close binary. As a result, very little material is ejected from the exploding star (c. 0.1 M☉). In the most extreme cases, ultra-stripped supernovae can occur in naked metal cores, barely above the Chandrasekhar mass limit. SN 2005ek might be the first observational example of an ultra-stripped supernova, giving rise to a relatively dim and fast decaying light curve. The nature of ultra-stripped supernovae can be both iron core-collapse and electron capture supernovae, depending on the mass of the collapsing core. Ultra-stripped supernovae are believed to be associated with the second supernova explosion in a binary system, producing for example a tight double neutron star system.\nIn 2022 a team of astronomers led by researchers from the Weizmann Institute of Science reported the first supernova explosion showing direct evidence for a Wolf-Rayet progenitor star. SN 2019hgp was a type Icn supernova and is also the first in which the element neon has been detected.\n==== Electron-capture supernovae ====\nIn 1980, a "third type" of supernova was predicted by Ken\'ichi Nomoto of the University of Tokyo, called an electron-capture supernova. It would arise when a star "in the transitional range (~8 to 10 solar masses) between white dwarf formation and iron core-collapse supernovae", and with a degenerate O+Ne+Mg core, imploded after its core ran out of nuclear fuel, causing gravity to compress the electrons in the star\'s core into their atomic nuclei, leading to a supernova explosion and leaving behind a neutron star. In June 2021, a paper in the journal Nature Astronomy reported that the 2018 supernova SN 2018zd (in the galaxy NGC 2146, about 31 million light-years from Earth) appeared to be the first observation of an electron-capture supernova. The 1054 supernova explosion that created the Crab Nebula in our galaxy had been thought to be the best candidate for an electron-capture supernova, and the 2021 paper makes it more likely that this was correct.\n=== Failed supernovae ===\nThe core collapse of some massive stars may not result in a visible supernova. This happens if the initial core collapse cannot be reversed by the mechanism that produces an explosion, usually because the core is too massive. These events are difficult to detect, but large surveys have detected possible candidates. The red supergiant N6946-BH1 in NGC 6946 underwent a modest outburst in March 2009, before fading from view. Only a faint infrared source remains at the star\'s location.\n=== Light curves ===\nThe ejecta gases would dim quickly without some energy input to keep them hot. The source of this energy—which can maintain the optical supernova glow for months—was, at first, a puzzle. Some considered rotational energy from the central pulsar as a source. Although the energy that initially powers each type of supernovae is delivered promptly, the light curves are dominated by subsequent radioactive heating of the rapidly expanding ejecta. The intensely radioactive nature of the ejecta gases was first calculated on sound nucleosynthesis grounds in the late 1960s, and this has since been demonstrated as correct for most supernovae. It was not until SN 1987A that direct observation of gamma-ray lines unambiguously identified the major radioactive nuclei.\nIt is now known by direct observation that much of the light curve (the graph of luminosity as a function of time) after the occurrence of a type II Supernova, such as SN 1987A, is explained by those predicted radioactive decays. Although the luminous emission consists of optical photons, it is the radioactive power absorbed by the ejected gases that keeps the remnant hot enough to radiate light. The radioactive decay of 56Ni through its daughters 56Co to 56Fe produces gamma-ray photons, primarily with energies of 847 keV and 1,238 keV, that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of 56Ni to 56Co (half-life 6 days) while energy for the later light curve in particular fit very closely with the 77.3-day half-life of 56Co decaying to 56Fe. Later measurements by space gamma-ray telescopes of the small fraction of the 56Co and 57Co gamma rays that escaped the SN 1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power sources.\nThe late-time decay phase of visual light curves for different supernova types all depend on radioactive heating, but they vary in shape and amplitude because of the underlying mechanisms, the way that visible radiation is produced, the epoch of its observation, and the transparency of the ejected material. The light curves can be significantly different at other wavelengths. For example, at ultraviolet wavelengths there is an early extremely luminous peak lasting only a few hours corresponding to the breakout of the shock launched by the initial event, but that breakout is hardly detectable optically.', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'Fusion powers stars and produces most elements lighter than cobalt in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 616 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.645% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and the result is fusion; this is an exothermic process.\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is 13.6 eV—less than one-millionth of the 17.6 MeV released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though individual fission reactions are generally much more energetic than individual fusion ones, which are themselves millions of times more energetic than chemical reactions. Via the mass–energy equivalence, fusion yields a 0.7% efficiency of reactant mass into energy. This can be only be exceeded by the extreme cases of the accretion process involving neutron stars or black holes, approaching 40% efficiency, and antimatter annihilation at 100% efficiency. (The complete conversion of one gram of matter would expel 9×1013 joules of energy.)\n== In astrophysics ==\nFusion is responsible for the astrophysical production of the majority of elements lighter than iron. This includes most types of Big Bang nucleosynthesis and stellar nucleosynthesis. Non-fusion processes that contribute include the s-process and r-process in neutron merger and supernova nucleosynthesis, responsible for elements heavier than iron.\n=== Stars ===\nAn important fusion process is the stellar nucleosynthesis that powers stars, including the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounts for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was unknown; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein\'s equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy had not yet been discovered, nor even that stars are largely composed of hydrogen (see metallicity). Eddington\'s paper reasoned that:\nThe leading theory of stellar energy, the contraction hypothesis, should cause the rotation of a star to visibly speed up due to conservation of angular momentum. But observations of Cepheid variable stars showed this was not happening.\nThe only other known plausible source of energy was conversion of matter to energy; Einstein had shown some years earlier that a small amount of matter was equivalent to a large amount of energy.\nFrancis Aston had also recently shown that the mass of a helium atom was about 0.8% less than the mass of the four hydrogen atoms which would, combined, form a helium atom (according to the then-prevailing theory of atomic structure which held atomic weight to be the distinguishing property between elements; work by Henry Moseley and Antonius van den Broek would later show that nucleic charge was the distinguishing property and that a helium nucleus, therefore, consisted of two hydrogen nuclei plus additional mass). This suggested that if such a combination could happen, it would release considerable energy as a byproduct.\nIf a star contained just 5% of fusible hydrogen, it would suffice to explain how stars got their energy. (It is now known that most \'ordinary\' stars are usually made of around 70% to 75% hydrogen)\nFurther elements might also be fused, and other scientists had speculated that stars were the "crucible" in which light elements combined to create heavy elements, but without more accurate measurements of their atomic masses nothing more could be said at the time.\nAll of these speculations were proven correct in the following decades.\nThe primary source of solar energy, and that of similar size stars, is the fusion of hydrogen to form helium (the proton–proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to fuse heavier elements. In massive cores, silicon-burning is the final fusion cycle, leading to a build-up of iron and nickel nuclei.\nNuclear binding energy makes the production of elements heavier than nickel via fusion energetically unfavorable. These elements are produced in non-fusion processes: the s-process, r-process, and the variety of processes that can produce p-nuclei. Such processes occur in giant star shells, or supernovae, or neutron star mergers.\n=== Brown dwarfs ===\nBrown dwarfs fuse deuterium and in very high mass cases also fuse lithium.\n=== White dwarfs ===\nCarbon-oxygen white dwarfs, which accrete matter either from an active stellar companion or white dwarf merger, approach the Chandrasekhar limit of 1.44 solar masses. Immediately prior, carbon burning fusion begins, destroying the Earth-sized dwarf within one second, in a Type Ia supernova.\nMuch more rarely, helium white dwarfs may merge, which does not cause an explosion but begins helium burning in an extreme type of helium star.\n=== Neutron stars ===\nSome neutron stars accrete hydrogen and helium from an active stellar companion. Periodically, the helium accretion reaches a critical level, and a thermonuclear burn wave propagates across the surface, on the timescale of one second.\n=== Black hole accretion disks ===\nSimilar to stellar fusion, extreme conditions within black hole accretion disks can allow fusion reactions. Calculations show the most energetic reactions occur around lower stellar mass black holes, below 10 solar masses, compared to those above 100. Beyond five Schwarzschild radii, carbon-burning and fusion of helium-3 dominates the reactions. Within this distance, around lower mass black holes, fusion of nitrogen, oxygen, neon, and magnesium can occur. In the extreme limit, the silicon-burning process can begin with the fusion of silicon and selenium nuclei.\n=== Big Bang ===\nFrom the period approximately 10 seconds to 20 minutes after the Big Bang, the universe cooled from over 100 keV to 1 keV. This allowed the combination of protons and neutrons in deuterium nuclei, and beginning a rapid fusion chain into tritium and helium-3 and ending in predominantly helium-4, with a minimal fraction of lithium, beryllium, and boron nuclei.\n== Requirements ==\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbors due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface-area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.', 'The neutron is a subatomic particle, symbol n or n0, that has no electric charge, and a mass slightly greater than that of a proton. The neutron was discovered by James Chadwick in 1932, leading to the discovery of nuclear fission in 1938, the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).\nNeutrons are found, together with a similar number of protons in the nuclei of atoms. Atoms of a chemical element that differ only in neutron number are called isotopes. Free neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes. Neutron stars, formed from massive collapsing stars, consist of neutrons at the density of atomic nuclei but a total mass more than the Sun.\nNeutron properties and interactions are described by nuclear physics.  Neutrons are not elementary particles; each is composed of three quarks. A free neutron spontaneously decays to a proton, an electron, and an antineutrino, with a mean lifetime of about 15 minutes.\nThe neutron is essential to the production of nuclear power.\nDedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments.  Free neutrons do not directly ionize atoms, but they do indirectly cause ionizing radiation, so they can be a biological hazard, depending on dose. A small natural "neutron background" flux of free neutrons exists on Earth, caused by cosmic rays, and by the natural radioactivity of spontaneously fissionable elements in the Earth\'s crust.\n== Discovery ==\nThe story of the discovery of the neutron and its properties is central to the extraordinary developments in atomic physics that occurred in the first half of the 20th century, leading ultimately to the atomic bomb in 1945. The name derives from the Latin root for neutralis (neuter) and the Greek suffix -on (a suffix used in the names of subatomic particles, e.g. electron and proton)\nand references to the word neutron can be found in the literature as early as 1899 in connection with discussion on the nature of the atom.\nIn the 1911 Rutherford model, the atom consisted of a small positively charged massive nucleus surrounded by a much larger cloud of negatively charged electrons. In 1920, Ernest Rutherford suggested that the nucleus consisted of positive protons and neutrally charged particles, suggested to be a proton and an electron bound in some way. Electrons were assumed to reside within the nucleus because it was known that beta radiation consisted of electrons emitted from the nucleus. About the time Rutherford suggested the neutral proton-electron composite, several other publications appeared making similar suggestions, and in 1921 the American chemist W. D. Harkins first named the hypothetical particle a "neutron".\nThroughout the 1920s, physicists assumed that the atomic nucleus was composed of protons and "nuclear electrons". Beginning in 1928, it became clear that this model was inconsistent with the then-new quantum theory. Confined to a volume the size of an nucleus, an electron consistent with the Heisenberg uncertainty relation of quantum mechanics would have an energy exceeding the binding energy of the nucleus. The energy was so large that according to the Klein paradox, discovered by Oskar Klein in 1928, an electron would escape the confinement of a nucleus. Furthermore, the observed properties of atoms and molecules were inconsistent with the nuclear spin expected from the proton–electron hypothesis. Protons and electrons both carry an intrinsic spin of \u20601/2\u2060ħ, and the isotopes of the same species were found to have either integer or fractional spin. By the hypothesis, isotopes would be composed of the same number of protons, but differing numbers of neutral bound proton+electron "particles". This physical picture was a contradiction, since there is no way to arrange the spins of an electron and a proton in a bound state to get a fractional spin.\nIn 1931, Walther Bothe and Herbert Becker found that if alpha particle radiation from polonium fell on beryllium, boron, or lithium, an unusually penetrating radiation was produced. The radiation was not influenced by an electric field, so Bothe and Becker assumed it was gamma radiation. The following year Irène Joliot-Curie and Frédéric Joliot-Curie in Paris showed that if this "gamma" radiation fell on paraffin, or any other hydrogen-containing compound, it ejected protons of very high energy. Neither Rutherford nor James Chadwick at the Cavendish Laboratory in Cambridge were convinced by the gamma ray interpretation. Chadwick quickly performed a series of experiments that showed that the new radiation consisted of uncharged particles with about the same mass as the proton. These properties matched Rutherford\'s hypothesized neutron. Chadwick won the 1935 Nobel Prize in Physics for this discovery.\nModels for an atomic nucleus consisting of protons and neutrons were quickly developed by Werner Heisenberg and others. The proton–neutron model explained the puzzle of nuclear spins. The origins of beta radiation were explained by Enrico Fermi in 1934 by the process of beta decay, in which the neutron decays to a proton by creating an electron and a then-undiscovered neutrino. In 1935, Chadwick and his doctoral student Maurice Goldhaber reported the first accurate measurement of the mass of the neutron.\nBy 1934, Fermi had bombarded heavier elements with neutrons to induce radioactivity in elements of high atomic number. In 1938, Fermi received the Nobel Prize in Physics "for his demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons". In December 1938 Otto Hahn, Lise Meitner, and Fritz Strassmann discovered nuclear fission, or the fractionation of uranium nuclei into lighter elements, induced by neutron bombardment. In 1945 Hahn received the 1944 Nobel Prize in Chemistry "for his discovery of the fission of heavy atomic nuclei".\nThe discovery of nuclear fission would lead to the development of nuclear power and the atomic bomb by the end of World War II. It was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, in a cascade known as a nuclear chain reaction.:\u200a460–461\u200a These events and findings led Fermi to construct the Chicago Pile-1 at the University of Chicago in 1942, the first self-sustaining nuclear reactor. Just three years later the Manhattan Project was able to test the first atomic bomb, the Trinity nuclear test in July 1945.\n== Occurrence ==\n=== Atomic nucleus ===\nAn atomic nucleus is formed by a number of protons, Z (the atomic number), and a number of neutrons, N (the neutron number), bound together by the nuclear force. Protons and neutrons each have a mass of approximately one dalton. The atomic number determines the chemical properties of the atom, and the neutron number determines the isotope or nuclide.:\u200a4\u200a The terms isotope and nuclide are often used synonymously, but they refer to chemical and nuclear properties, respectively.:\u200a4\u200a Isotopes are nuclides with the same atomic number, but different neutron number. Nuclides with the same neutron number, but different atomic number, are called isotones. The atomic mass number, A, is equal to the sum of atomic and neutron numbers. Nuclides with the same atomic mass number, but different atomic and neutron numbers, are called isobars.  The mass of a nucleus is always slightly less than the sum of its proton and neutron masses: the difference in mass represents the mass equivalent to nuclear binding energy, the energy which would need to be added to take the nucleus apart.:\u200a822\nThe nucleus of the most common isotope of the hydrogen atom (with the chemical symbol 1H) is a lone proton.:\u200a20\u200a The nuclei of the heavy hydrogen isotopes deuterium (D or 2H) and tritium (T or 3H) contain one proton bound to one and two neutrons, respectively.:\u200a20\u200a All other types of atomic nuclei are composed of two or more protons and various numbers of neutrons. The most common nuclide of the common chemical element lead, 208Pb, has 82 protons and 126 neutrons, for example. The table of nuclides comprises all the known nuclides. Even though it is not a chemical element, the neutron is included in this table.\nProtons and neutrons behave almost identically under the influence of the nuclear force within the nucleus. They are therefore both referred to collectively as nucleons.  The concept of isospin, in which the proton and neutron are viewed as two quantum states of the same particle, is used to model the interactions of nucleons by the nuclear or weak forces.:\u200a141\nNeutrons are a necessary constituent of any atomic nucleus that contains more than one proton. As a result of their positive charges, interacting protons have a mutual electromagnetic repulsion that is stronger than their attractive nuclear interaction, so proton-only nuclei are unstable (see diproton and neutron–proton ratio). Neutrons bind with protons and one another in the nucleus via the nuclear force, effectively moderating the repulsive forces between the protons and stabilizing the nucleus.:\u200a461\u200a Heavy nuclei carry a large positive charge, hence they require "extra" neutrons to be stable.:\u200a461', "==== Red-giant-branch phase ====\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star.  For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower 12C/13C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\nThe helium core continues to grow on the red-giant branch.  It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase.  The star increases in luminosity towards the tip of the red-giant branch.  Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n==== Horizontal branch ====\nIn the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 108 times the luminosity of the Sun for a few days and 1011 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning.  These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n==== Asymptotic-giant-branch phase ====\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time).  Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star.  Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically.  This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface.  This is known as the second dredge up, and in some stars there may even be a third dredge up.  In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra.  A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\nAnother well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths.  These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity.  These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n==== Post-AGB ====\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n=== Massive stars ===\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants.  Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.\n==== Supergiant evolution ====\nExtremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\nThe core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core.  In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process.  At the end of helium fusion, the core of a star consists primarily of carbon and oxygen.  In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium.  Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.\nThe exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉.  After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse.  Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning.  For a range of stars of approximately 8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova.", 'Nuclear fusion', "In more massive stars, the fusion of neon proceeds without a runaway deflagration.  This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements.  Surrounding the core are shells of lighter elements still undergoing fusion.  The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged.  The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope.  The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars.  Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself.  The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.\n==== Supernova ====\nWhen the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole.  Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\n== Stellar remnants ==\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n=== White and black dwarfs ===\nFor a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years.\nThe chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n=== Neutron stars ===\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n=== Black holes ===\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉.", 'The light curves for type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. Their optical energy output is driven by radioactive decay of ejected nickel-56 (half-life 6 days), which then decays to radioactive cobalt-56 (half-life 77 days). These radioisotopes excite the surrounding material to incandescence. Modern studies of cosmology rely on 56Ni radioactivity providing the energy for the optical brightness of supernovae of type Ia, which are the "standard candles" of cosmology but whose diagnostic 847 keV and 1,238 keV gamma rays were first detected only in 2014. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half-life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission from the remaining cobalt-56 becomes dominant, although this portion of the light curve has been little-studied.\nType Ib and Ic light curves are similar to type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of the created nickel-56. The peak luminosity varies considerably and there are even occasional type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increased peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.\nThe light curves for type II supernovae are characterised by a much slower decline than type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial destruction this hydrogen becomes heated and ionised. The majority of type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in type I supernovae, due to the efficiency of conversion into light by all the hydrogen.\nIn type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a type I supernova and the hydrogen even disappears from the spectrum after several weeks.\nType IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a superluminous supernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.\nLarge numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.\nNotes:\n=== Asymmetry ===\nA long-standing puzzle surrounding type II supernovae is why the remaining compact object receives a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high peculiar velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500 km/s or greater. This indicates an expansion asymmetry, but the mechanism by which momentum is transferred to the compact object remains a puzzle. Proposed explanations for this kick include convection in the collapsing star, asymmetric ejection of matter during neutron star formation, and asymmetrical neutrino emissions.\nOne possible explanation for this asymmetry is large-scale convection above the core. The convection can create radial variations in density giving rise to variations in the amount of energy absorbed from neutrino outflow. However analysis of this mechanism predicts only modest momentum transfer. Another possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova. (A similar model is used for explaining long gamma-ray bursts.) The dominant mechanism may depend upon the mass of the progenitor star.\nInitial asymmetries have also been confirmed in type Ia supernovae through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the expansion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarisation of the emitted light.\n=== Energy output ===\nAlthough supernovae are primarily known as luminous events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.\nThere is a fundamental difference between the balance of energy production in the different types of supernova. In type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.\nStandard type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is 56Ni generated from silicon burning. 56Ni is radioactive and decays into 56Co by beta plus decay (with a half life of six days) and gamma rays. 56Co itself decays by the beta plus (positron) path with a half life of 77 days into stable 56Fe. These two processes are responsible for the electromagnetic radiation from type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.\nCore collapse supernovae are on average visually fainter than type Ia supernovae, but the total energy released is far higher, as outlined in the following table.\nIn some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high-luminosity supernovae and is thought to be the cause of type Ic hypernovae and long-duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low-luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.\nWhen a supernova occurs inside a small dense cloud of circumstellar material, it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause type IIn hypernovae.\nAlthough pair-instability supernovae are core collapse supernovae with spectra and light curves similar to type II-P, the nature after core collapse is more like that of a giant type Ia with runaway fusion of carbon, oxygen and silicon. The total energy released by the highest-mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.\n=== Progenitor ===', 'Brumfiel, Geoff (22 May 2006). "Chaos could keep fusion under control". Nature. doi:10.1038/news060522-2. ISSN 0028-0836. S2CID 62598131.\nBussard, Robert (8 October 2007) [9 November 2006]. Should Google Go Nuclear? Clean, cheap, nuclear power... (Video). Google TechTalks – via YouTube.\nWenisch, Antonia; Kromp, Richard; Reinberger, David (November 2007). "Science or Fiction: Is there a Future for Nuclear?" (PDF). Austrian Institute of Ecology. Archived from the original (PDF) on 26 January 2021. Retrieved 8 October 2008.\nKikuchi, Mitsuru; Lackner, Karl & Tran, M. Q. (2012). Fusion physics. Publication / Division of Scientific and Technical Information, International Atomic Energy Agency. Vienna: International Atomic Energy Agency. p. 22. ISBN 978-92-0-130410-0. Archived from the original on 8 December 2015. Retrieved 8 December 2015.\nJanev, R. K., ed. (1995). Atomic and Molecular Processes in Fusion Edge Plasmas. Boston, MA: Springer US. doi:10.1007/978-1-4757-9319-2. ISBN 978-1-4757-9321-5. Archived from the original on 16 January 2023. Retrieved 16 January 2023.\nIliadis, Christian (2015). Nuclear Physics of Stars, 2nd ed. Weinheim: Wiley-VCH. doi:10.1002/9783527692668. ISBN 9783527692668.\n== External links ==\nNuclearFiles.org – A repository of documents related to nuclear power.\nAnnotated bibliography for nuclear fusion from the Alsos Digital Library for Nuclear Issues\nNRL Fusion Formulary Archived 26 October 2020 at the Wayback Machine\n"FusionWiki".']

Question: What is the decay energy for the free neutron decay process?

Choices:
Choice A) 0.013343 MeV
Choice B) 0.013 MeV
Choice C) 1,000 MeV
Choice D) 0.782 MeV
Choice E) 0.782343 MeV

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', 'Crystallography', 'Point group\n\nIn geometry, a point group is a mathematical group of symmetry operations (isometries in a Euclidean space) that have a fixed point in common. The coordinate origin of the Euclidean space is conventionally taken to be a fixed point, and every point group in dimension d is then a subgroup of the orthogonal group O(d). Point groups are used to describe the symmetries of geometric figures and physical objects such as molecules.\nEach point group can be represented as sets of orthogonal matrices M that transform point x into point y according to y = Mx. Each element of a point group is either a rotation (determinant of M = 1), or it is a reflection or improper rotation (determinant of M = −1).\nThe geometric symmetries of crystals are described by space groups, which allow translations and contain point groups as subgroups. Discrete point groups in more than one dimension come in infinite families, but from the crystallographic restriction theorem and one of Bieberbach\'s theorems, each number of dimensions has only a finite number of point groups that are symmetric over some lattice or grid with that number of dimensions. These are the crystallographic point groups.\n== Chiral and achiral point groups, reflection groups ==\nPoint groups can be classified into chiral (or purely rotational) groups and achiral groups.\nThe chiral groups are subgroups of the special orthogonal group SO(d): they contain only orientation-preserving orthogonal transformations, i.e., those of determinant +1. The achiral groups contain also transformations of determinant −1. In an achiral group, the orientation-preserving transformations form a (chiral) subgroup of index 2.\nFinite Coxeter groups or reflection groups are those point groups that are generated purely by a set of reflectional mirrors passing through the same point. A rank n Coxeter group has n mirrors and is represented by a Coxeter–Dynkin diagram. Coxeter notation offers a bracketed notation equivalent to the Coxeter diagram, with markup symbols for rotational and other subsymmetry point groups. Reflection groups are necessarily achiral (except for the trivial group containing only the identity element).\n== List of point groups ==\n=== One dimension ===\nThere are only two one-dimensional point groups, the identity group and the reflection group.\n=== Two dimensions ===\nPoint groups in two dimensions, sometimes called rosette groups.\nThey come in two infinite families:\nCyclic groups Cn of n-fold rotation groups\nDihedral groups Dn of n-fold rotation and reflection groups\nApplying the crystallographic restriction theorem restricts n to values 1, 2, 3, 4, and 6 for both families, yielding 10 groups.\nThe subset of pure reflectional point groups, defined by 1 or 2 mirrors, can also be given by their Coxeter group and related polygons. These include 5 crystallographic groups. The symmetry of the reflectional groups can be doubled by an isomorphism, mapping both mirrors onto each other by a bisecting mirror, doubling the symmetry order.\n=== Three dimensions ===\nPoint groups in three dimensions, sometimes called molecular point groups after their wide use in studying symmetries of molecules.\nThey come in 7 infinite families of axial groups (also called prismatic), and 7 additional polyhedral groups (also called Platonic). In Schoenflies notation,\nAxial groups: Cn,  S2n, Cnh, Cnv, Dn, Dnd, Dnh\nPolyhedral groups: T, Td, Th, O, Oh, I, Ih\nApplying the crystallographic restriction theorem to these groups yields the 32 crystallographic point groups.\n==== Reflection groups ====\nThe reflection point groups, defined by 1 to 3 mirror planes, can also be given by their Coxeter group and related polyhedra. The [3,3] group can be doubled, written as [[3,3]], mapping the first and last mirrors onto each other, doubling the symmetry to 48, and isomorphic to the [4,3] group.\n=== Four dimensions ===\nThe four-dimensional point groups (chiral as well as achiral) are listed in Conway and Smith, Section 4, Tables 4.1–4.3.\nThe following list gives the four-dimensional reflection groups (excluding those that leave a subspace fixed and that are therefore lower-dimensional reflection groups). Each group is specified as a Coxeter group, and like the polyhedral groups of 3D, it can be named by its related convex regular 4-polytope. Related pure rotational groups exist for each with half the order, and can be represented by the bracket Coxeter notation with a \'+\' exponent, for example [3,3,3]+ has three 3-fold gyration points and symmetry order 60. Front-back symmetric groups like [3,3,3] and [3,4,3] can be doubled, shown as double brackets in Coxeter\'s notation, for example [[3,3,3]] with its order doubled to 240.\n=== Five dimensions ===\nThe following table gives the five-dimensional reflection groups (excluding those that are lower-dimensional reflection groups), by listing them as Coxeter groups. Related chiral groups exist for each with half the order, and can be represented by the bracket Coxeter notation with a \'+\' exponent, for example [3,3,3,3]+ has four 3-fold gyration points and symmetry order 360.\n=== Six dimensions ===\nThe following table gives the six-dimensional reflection groups (excluding those that are lower-dimensional reflection groups), by listing them as Coxeter groups. Related pure rotational groups exist for each with half the order, and can be represented by the bracket Coxeter notation with a \'+\' exponent, for example [3,3,3,3,3]+ has five 3-fold gyration points and symmetry order 2520.\n=== Seven dimensions ===\nThe following table gives the seven-dimensional reflection groups (excluding those that are lower-dimensional reflection groups), by listing them as Coxeter groups. Related chiral groups exist for each with half the order, defined by an even number of reflections, and can be represented by the bracket Coxeter notation with a \'+\' exponent, for example [3,3,3,3,3,3]+ has six 3-fold gyration points and symmetry order 20160.\n=== Eight dimensions ===\nThe following table gives the eight-dimensional reflection groups (excluding those that are lower-dimensional reflection groups), by listing them as Coxeter groups. Related chiral groups exist for each with half the order, defined by an even number of reflections, and can be represented by the bracket Coxeter notation with a \'+\' exponent, for example [3,3,3,3,3,3,3]+ has seven 3-fold gyration points and symmetry order 181440.\n== See also ==\nPoint groups in two dimensions\nPoint groups in three dimensions\nPoint groups in four dimensions\nCrystallography\nCrystallographic point group\nMolecular symmetry\nSpace group\nX-ray diffraction\nBravais lattice\nInfrared spectroscopy of metal carbonyls\n== References ==\n== Further reading ==\nH. S. M. Coxeter (1995), F. Arthur Sherk; Peter McMullen; Anthony C. Thompson; Asia Ivic Weiss (eds.), Kaleidoscopes: Selected Writings of H. S. M. Coxeter, Wiley-Interscience Publication, ISBN 978-0-471-01003-6\n(Paper 23) H. S. M. Coxeter, Regular and Semi-Regular Polytopes II, [Math. Zeit. 188 (1985) 559–591]\nH. S. M. Coxeter; W. O. J. Moser (1980), Generators and Relations for Discrete Groups (4th ed.), New York: Springer-Verlag\nN. W. Johnson (2018), "Chapter 11: Finite symmetry groups", Geometries and Transformations\n== External links ==\nWeb-based point group tutorial (needs Java and Flash)\nSubgroup enumeration (needs Java)\nThe Geometry Center: 2.1 Formulas for Symmetries in Cartesian Coordinates (two dimensions)\nThe Geometry Center: 10.1 Formulas for Symmetries in Cartesian Coordinates (three dimensions)', 'Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word κρύσταλλος (krústallos; "clear ice, rock-crystal"), and γράφειν (gráphein; "to write"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well new developments in these fields.\n== History and timeline ==\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the Davisson–Germer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n== Methodology ==\nCrystallographic methods depend mainly on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. Crystallographers often explicitly state the type of beam used, as in the terms X-ray diffraction, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.\nX-rays interact with the spatial distribution of electrons in the sample.\nNeutrons are scattered by the atomic nuclei through the strong nuclear forces, but in addition the magnetic moment of neutrons is non-zero, so they are also scattered by magnetic fields. When neutrons are scattered from hydrogen-containing materials, they produce diffraction patterns with high noise levels, which can sometimes be resolved by substituting deuterium for hydrogen.\nElectrons are charged particles and therefore interact with the total charge distribution of both the atomic nuclei and the electrons of the sample.:\u200aChpt 4\nIt is hard to focus x-rays or neutrons, but since electrons are charged they can be focused and are used in electron microscope to produce magnified images. There are many ways that transmission electron microscopy and related techniques such as scanning transmission electron microscopy, high-resolution electron microscopy can be used to obtain images with in many cases atomic resolution from which crystallographic information can be obtained. There are also other methods such as low-energy electron diffraction, low-energy electron microscopy and reflection high-energy electron diffraction which can be used to obtain crystallographic information about surfaces.\n== Applications in various areas ==\n=== Materials science ===\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Most materials do not occur as a single crystal, but are poly-crystalline in nature (they exist as an aggregate of small crystals with different orientations). As such, powder diffraction techniques, which take diffraction patterns of samples with a large number of crystals, play an important role in structural determination.\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements. Crystallographic studies help elucidate the relationship between a material\'s structure and its properties, aiding in developing new materials with tailored characteristics. This understanding is crucial in various fields, including metallurgy, geology, and materials science. Advancements in crystallographic techniques, such as electron diffraction and X-ray crystallography, continue to expand our understanding of material behavior at the atomic level.\nIn another example, iron transforms from a body-centered cubic (bcc) structure called ferrite to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material\'s properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which structures are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory.\n=== Biology ===\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. The double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol, Pymol or VMD can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron diffraction has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n== Notation ==\nCoordinates in square brackets such as [100] denote a direction vector (in real space).\nCoordinates in angle brackets or chevrons such as <100> denote a family of directions which are related by symmetry operations. In the cubic crystal system for example, <100> would mean [100], [010], [001] or the negative of any of those directions.\nMiller indices in parentheses such as (100) denote a plane of the crystal structure, and regular repetitions of that plane with a particular spacing. In the cubic system, the normal to the (hkl) plane is the direction [hkl], but in lower-symmetry cases, the normal to (hkl) is not parallel to [hkl].\nIndices in curly brackets or braces such as {100} denote a family of planes and their normals. In cubic materials the symmetry makes them equivalent, just as the way angle brackets denote a family of directions. In non-cubic materials, <hkl> is not necessarily perpendicular to {hkl}.\n== Reference literature ==\nThe International Tables for Crystallography is an eight-book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure through x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and do not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:', 'An early use of the piezoelectricity of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. The quartz clock is a familiar device using the mineral. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.\nAlmost all the industrial demand for quartz crystal (used primarily in electronics) is met with synthetic quartz produced by the hydrothermal process. However, synthetic crystals are less prized for use as gemstones. The popularity of crystal healing has increased the demand for natural quartz crystals, which are now often mined in developing countries using primitive mining methods, sometimes involving child labor.\n== See also ==\nFused quartz\nList of minerals\nQuartz fiber\nQuartz reef mining\nQuartzolite\nShocked quartz\n== References ==\n== External links ==\nQuartz varieties, properties, crystal morphology. Photos and illustrations\nGilbert Hart, "Nomenclature of Silica", American Mineralogist, Volume 12, pp. 383–395. 1927\n"The Quartz Watch – Inventors". The Lemelson Center, National Museum of American History, Smithsonian Institution. Archived from the original on 7 January 2009.\nTerminology used to describe the characteristics of quartz crystals when used as oscillators\nQuartz use as prehistoric stone tool raw material', 'for convex polyhedra to higher-dimensional polytopes:\n{\\displaystyle \\sum \\varphi =(-1)^{d-1}}\n== Generalisations of a polytope ==\n=== Infinite polytopes ===\nNot all manifolds are finite. Where a polytope is understood as a tiling or decomposition of a manifold,  this idea may be extended to infinite manifolds. plane tilings, space-filling (honeycombs) and hyperbolic tilings are in this sense polytopes, and are sometimes called apeirotopes because they have infinitely many cells.\nAmong these, there are regular forms including the regular skew polyhedra and the infinite series of tilings represented by the regular apeirogon, square tiling, cubic honeycomb, and so on.\n=== Abstract polytopes ===\nThe theory of abstract polytopes attempts to detach polytopes from the space containing them, considering their purely combinatorial properties. This allows the definition of the term to be extended to include objects for which it is difficult to define an intuitive underlying space, such as the 11-cell.\nAn abstract polytope is a partially ordered set of elements or members, which obeys certain rules. It is a purely algebraic structure, and the theory was developed in order to avoid some of the issues which make it difficult to reconcile the various geometric classes within a consistent mathematical framework. A geometric polytope is said to be a realization in some real space of the associated abstract polytope.\n=== Complex polytopes ===\nStructures analogous to polytopes exist in complex Hilbert spaces\n{\\displaystyle \\mathbb {C} ^{n}}\nwhere n real dimensions are accompanied by n imaginary ones. Regular complex polytopes are more appropriately treated as configurations.\n== Duality ==\nEvery n-polytope has a dual structure, obtained by interchanging its vertices for facets, edges for ridges, and so on generally interchanging its (j − 1)-dimensional elements for (n − j)-dimensional elements (for j = 1 to n − 1), while retaining the connectivity or incidence between elements.\nFor an abstract polytope, this simply reverses the ordering of the set. This reversal is seen in the Schläfli symbols for regular polytopes, where the symbol for the dual polytope is simply the reverse of the original. For example, {4, 3, 3} is dual to {3, 3, 4}.\nIn the case of a geometric polytope, some geometric rule for dualising is necessary, see for example the rules described for dual polyhedra. Depending on circumstance, the dual figure may or may not be another geometric polytope.\nIf the dual is reversed, then the original polytope is recovered. Thus, polytopes exist in dual pairs.\n=== Self-dual polytopes ===\nIf a polytope has the same number of vertices as facets, of edges as ridges, and so forth, and the same connectivities, then the dual figure will be similar to the original and the polytope is self-dual.\nSome common self-dual polytopes include:\nEvery regular n-simplex, in any number of dimensions, with Schläfli symbol {3n}. These include the equilateral triangle {3}, regular tetrahedron {3,3}, and 5-cell  {3,3,3}.\nEvery hypercubic honeycomb, in any number of dimensions. These include the apeirogon {∞}, square tiling {4,4} and cubic honeycomb {4,3,4}.\nNumerous compact, paracompact and noncompact hyperbolic tilings, such as the icosahedral honeycomb {3,5,3}, and order-5 pentagonal tiling {5,5}.\nIn 2 dimensions, all regular polygons (regular 2-polytopes)\nIn 3 dimensions, the canonical polygonal pyramids and elongated pyramids, and tetrahedrally diminished dodecahedron.\nIn 4 dimensions, the 24-cell, with Schläfli symbol {3,4,3}. Also the great 120-cell {5,5/2,5} and grand stellated 120-cell {5/2,5,5/2}.\n== History ==\nPolygons and polyhedra have been known since ancient times.\nAn early hint of higher dimensions came in 1827 when August Ferdinand Möbius discovered that two mirror-image solids can be superimposed by rotating one of them through a fourth mathematical dimension. By the 1850s, a handful of other mathematicians such as Arthur Cayley and Hermann Grassmann had also considered higher dimensions.\nLudwig Schläfli was the first to consider analogues of polygons and polyhedra in these higher spaces. He described the six convex regular 4-polytopes in 1852 but his work was not published until 1901, six years after his death. By 1854, Bernhard Riemann\'s Habilitationsschrift had firmly established the geometry of higher dimensions, and thus the concept of n-dimensional polytopes was made acceptable. Schläfli\'s polytopes were rediscovered many times in the following decades, even during his lifetime.\nIn 1882 Reinhold Hoppe, writing in German, coined the word polytop to refer to this more general concept of polygons and polyhedra. In due course Alicia Boole Stott, daughter of logician George Boole, introduced the anglicised polytope into the English language.:\u200avi\nIn 1895, Thorold Gosset not only rediscovered Schläfli\'s regular polytopes but also investigated the ideas of semiregular polytopes and space-filling tessellations in higher dimensions. Polytopes also began to be studied in non-Euclidean spaces such as hyperbolic space.\nAn important milestone was reached in 1948 with H. S. M. Coxeter\'s book Regular Polytopes, summarizing work to date and adding new findings of his own.\nMeanwhile, the French mathematician Henri Poincaré had developed the topological idea of a polytope as the piecewise decomposition (e.g. CW-complex) of a manifold. Branko Grünbaum published his influential work on Convex Polytopes in 1967.\nIn 1952 Geoffrey Colin Shephard generalised the idea as complex polytopes in complex space, where each real dimension has an imaginary one associated with it. Coxeter developed the theory further.\nThe conceptual issues raised by complex polytopes, non-convexity, duality and other phenomena led Grünbaum and others to the more general study of abstract combinatorial properties relating vertices, edges, faces and so on. A related idea was that of incidence complexes, which studied the incidence or connection of the various elements with one another. These developments led eventually to the theory of abstract polytopes as partially ordered sets, or posets, of such elements. Peter McMullen and Egon Schulte published their book Abstract Regular Polytopes in 2002.\nEnumerating the uniform polytopes, convex and nonconvex, in four or more dimensions remains an outstanding problem. The convex uniform 4-polytopes were fully enumerated by John Conway and Michael Guy using a computer in 1965; in higher dimensions this problem was still open as of 1997. The full enumeration for nonconvex uniform polytopes is not known in dimensions four and higher as of 2008.\nIn modern times, polytopes and related concepts have found many important applications in fields as diverse as computer graphics, optimization, search engines, cosmology, quantum mechanics and numerous other fields. In 2013 the amplituhedron was discovered as a simplifying construct in certain calculations of theoretical physics.\n== Applications ==\nIn the field of optimization, linear programming studies the maxima and minima of linear functions; these maxima and minima occur on the boundary of an n-dimensional polytope. In linear programming, polytopes occur in the use of generalized barycentric coordinates and slack variables.\nIn  twistor theory, a branch of theoretical physics, a polytope called the amplituhedron is used in to calculate the scattering amplitudes of subatomic particles when they collide. The construct is purely theoretical with no known physical manifestation, but is said to greatly simplify certain calculations.\n== See also ==\n== References ==\n=== Citations ===\n=== Bibliography ===\n== External links ==\nWeisstein, Eric W. "Polytope". MathWorld.\n"Math will rock your world" – application of polytopes to a database of articles used to support custom news feeds via the Internet – (Business Week Online)\nRegular and semi-regular convex polytopes a short historical overview:', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'Polytope', 'In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', "In more massive stars, the fusion of neon proceeds without a runaway deflagration.  This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements.  Surrounding the core are shells of lighter elements still undergoing fusion.  The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged.  The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope.  The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars.  Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself.  The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.\n==== Supernova ====\nWhen the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole.  Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\n== Stellar remnants ==\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n=== White and black dwarfs ===\nFor a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years.\nThe chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n=== Neutron stars ===\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n=== Black holes ===\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉."]

Question: How many crystallographic point groups are there in three-dimensional space?

Choices:
Choice A) 7
Choice B) 32
Choice C) 14
Choice D) 5
Choice E) 27

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['In physics, chemistry, and other related fields like biology, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.\n== Types of phase transition ==\n=== States of matter ===\nPhase transitions commonly refer to when a substance transforms between one of the four states of matter to another. At the phase transition point for a substance, for instance the boiling point, the two phases involved - liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the boiling point the gaseous form is the more stable.\nCommon transitions between the solid, liquid, and gaseous phases of a single component, due to the effects of temperature and/or pressure are identified in the following table:\nFor a single component, the most stable phase at different temperatures and pressures can be shown on a phase diagram. Such a diagram usually depicts states in equilibrium. A phase transition usually occurs when the pressure or temperature changes and the system crosses from one region to another, like water turning from liquid to solid as soon as the temperature drops below the freezing point. In exception to the usual case, it is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating and supercooling, for example. Metastable states do not appear on usual phase diagrams.\n=== Structural ===\nPhase transitions can also occur when a solid changes to a different structure without changing its chemical makeup. In elements, this is known as allotropy, whereas in compounds it is known as polymorphism. The change from one crystal structure to another, from a crystalline solid to an amorphous solid, or from one amorphous structure to another (polyamorphs) are all examples of solid to solid phase transitions.\nThe martensitic transformation occurs as one of the many phase transformations in carbon steel and stands as a model for displacive phase transformations. Order-disorder transitions such as in alpha-titanium aluminides. As with states of matter, there is also a metastable to equilibrium phase transformation for structural phase transitions. A metastable polymorph which forms rapidly due to lower surface energy will transform to an equilibrium phase given sufficient thermal input to overcome an energetic barrier.\n=== Magnetic ===\nPhase transitions can also describe the change between different kinds of magnetic ordering. The most well-known is the transition between the ferromagnetic and paramagnetic phases of magnetic materials, which occurs at what is called the Curie point. Another example is the transition between differently ordered, commensurate or incommensurate, magnetic structures, such as in cerium antimonide. A simplified but highly useful model of magnetic phase transitions is provided by the Ising model.\n=== Mixtures ===\nPhase transitions involving solutions and mixtures are more complicated than transitions involving a single compound. While chemically pure compounds exhibit a single temperature melting point between solid and liquid phases, mixtures can either have a single melting point, known as congruent melting, or they have different liquidus and solidus temperatures resulting in a temperature span where solid and liquid coexist in equilibrium. This is often the case in solid solutions, where the two components are isostructural.\nThere are also a number of phase transitions involving three phases: a eutectic transformation, in which a two-component single-phase liquid is cooled and transforms into two solid phases. The same process, but beginning with a solid instead of a liquid is called a eutectoid transformation. A peritectic transformation, in which a two-component single-phase solid is heated and transforms into a solid phase and a liquid phase. A peritectoid reaction is a peritectoid reaction, except involving only solid phases. A monotectic reaction consists of change from a liquid and to a combination of a solid and a second liquid, where the two liquids display a miscibility gap.\nSeparation into multiple phases can occur via spinodal decomposition, in which a single phase is cooled and separates into two different compositions.\nNon-equilibrium mixtures can occur, such as in supersaturation.\n=== Other examples ===\nOther phase changes include:\nTransition to a mesophase between solid and liquid, such as one of the "liquid crystal" phases.\nThe dependence of the adsorption geometry on coverage and temperature, such as for hydrogen on iron (110).\nThe emergence of superconductivity in certain metals and ceramics when cooled below a critical temperature.\nThe emergence of metamaterial properties in artificial photonic media as their parameters are varied.\nQuantum condensation of bosonic fluids (Bose–Einstein condensation). The superfluid transition in liquid helium is an example of this.\nThe breaking of symmetries in the laws of physics during the early history of the universe as its temperature cooled.\nIsotope fractionation occurs during a phase transition, the ratio of light to heavy isotopes in the involved molecules changes. When water vapor condenses (an equilibrium fractionation), the heavier water isotopes (18O and 2H) become enriched in the liquid phase while the lighter isotopes (16O and 1H) tend toward the vapor phase.\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are small. Phase transitions can occur for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n== Classifications ==\n=== Ehrenfest classification ===\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions. For example, the Gross–Witten–Wadia phase transition in 2-d lattice quantum chromodynamics is a third-order phase transition, and the Tracy–Widom distribution can be interpreted as a third-order transition. The Curie points of many ferromagnetics is also a third-order transition, as shown by their specific heat having a sudden change in slope.\nIn practice, only the first- and second-order phase transitions are typically observed. The second-order phase transition was for a while controversial, as it seems to require two sheets of the Gibbs free energy to osculate exactly, which is so unlikely as to never occur in practice. Cornelis Gorter replied the criticism by pointing out that the Gibbs free energy surface might have two sheets on one side, but only one sheet on the other side, creating a forked appearance. ( pp. 146--150)\nThe Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.', 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Classical mechanics', 'Fusion powers stars and produces most elements lighter than cobalt in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 616 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.645% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and the result is fusion; this is an exothermic process.\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is 13.6 eV—less than one-millionth of the 17.6 MeV released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though individual fission reactions are generally much more energetic than individual fusion ones, which are themselves millions of times more energetic than chemical reactions. Via the mass–energy equivalence, fusion yields a 0.7% efficiency of reactant mass into energy. This can be only be exceeded by the extreme cases of the accretion process involving neutron stars or black holes, approaching 40% efficiency, and antimatter annihilation at 100% efficiency. (The complete conversion of one gram of matter would expel 9×1013 joules of energy.)\n== In astrophysics ==\nFusion is responsible for the astrophysical production of the majority of elements lighter than iron. This includes most types of Big Bang nucleosynthesis and stellar nucleosynthesis. Non-fusion processes that contribute include the s-process and r-process in neutron merger and supernova nucleosynthesis, responsible for elements heavier than iron.\n=== Stars ===\nAn important fusion process is the stellar nucleosynthesis that powers stars, including the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounts for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was unknown; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein\'s equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy had not yet been discovered, nor even that stars are largely composed of hydrogen (see metallicity). Eddington\'s paper reasoned that:\nThe leading theory of stellar energy, the contraction hypothesis, should cause the rotation of a star to visibly speed up due to conservation of angular momentum. But observations of Cepheid variable stars showed this was not happening.\nThe only other known plausible source of energy was conversion of matter to energy; Einstein had shown some years earlier that a small amount of matter was equivalent to a large amount of energy.\nFrancis Aston had also recently shown that the mass of a helium atom was about 0.8% less than the mass of the four hydrogen atoms which would, combined, form a helium atom (according to the then-prevailing theory of atomic structure which held atomic weight to be the distinguishing property between elements; work by Henry Moseley and Antonius van den Broek would later show that nucleic charge was the distinguishing property and that a helium nucleus, therefore, consisted of two hydrogen nuclei plus additional mass). This suggested that if such a combination could happen, it would release considerable energy as a byproduct.\nIf a star contained just 5% of fusible hydrogen, it would suffice to explain how stars got their energy. (It is now known that most \'ordinary\' stars are usually made of around 70% to 75% hydrogen)\nFurther elements might also be fused, and other scientists had speculated that stars were the "crucible" in which light elements combined to create heavy elements, but without more accurate measurements of their atomic masses nothing more could be said at the time.\nAll of these speculations were proven correct in the following decades.\nThe primary source of solar energy, and that of similar size stars, is the fusion of hydrogen to form helium (the proton–proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to fuse heavier elements. In massive cores, silicon-burning is the final fusion cycle, leading to a build-up of iron and nickel nuclei.\nNuclear binding energy makes the production of elements heavier than nickel via fusion energetically unfavorable. These elements are produced in non-fusion processes: the s-process, r-process, and the variety of processes that can produce p-nuclei. Such processes occur in giant star shells, or supernovae, or neutron star mergers.\n=== Brown dwarfs ===\nBrown dwarfs fuse deuterium and in very high mass cases also fuse lithium.\n=== White dwarfs ===\nCarbon-oxygen white dwarfs, which accrete matter either from an active stellar companion or white dwarf merger, approach the Chandrasekhar limit of 1.44 solar masses. Immediately prior, carbon burning fusion begins, destroying the Earth-sized dwarf within one second, in a Type Ia supernova.\nMuch more rarely, helium white dwarfs may merge, which does not cause an explosion but begins helium burning in an extreme type of helium star.\n=== Neutron stars ===\nSome neutron stars accrete hydrogen and helium from an active stellar companion. Periodically, the helium accretion reaches a critical level, and a thermonuclear burn wave propagates across the surface, on the timescale of one second.\n=== Black hole accretion disks ===\nSimilar to stellar fusion, extreme conditions within black hole accretion disks can allow fusion reactions. Calculations show the most energetic reactions occur around lower stellar mass black holes, below 10 solar masses, compared to those above 100. Beyond five Schwarzschild radii, carbon-burning and fusion of helium-3 dominates the reactions. Within this distance, around lower mass black holes, fusion of nitrogen, oxygen, neon, and magnesium can occur. In the extreme limit, the silicon-burning process can begin with the fusion of silicon and selenium nuclei.\n=== Big Bang ===\nFrom the period approximately 10 seconds to 20 minutes after the Big Bang, the universe cooled from over 100 keV to 1 keV. This allowed the combination of protons and neutrons in deuterium nuclei, and beginning a rapid fusion chain into tritium and helium-3 and ending in predominantly helium-4, with a minimal fraction of lithium, beryllium, and boron nuclei.\n== Requirements ==\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbors due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface-area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.', 'Shower-curtain effect\n\nThe shower-curtain effect in physics describes the phenomenon of a shower curtain being blown inward when a shower is running. The problem of identifying the cause of this effect has been featured in Scientific American magazine, with several theories given to explain the phenomenon but no definite conclusion.\nThe shower-curtain effect may also be used to describe the observation of how nearby phase front distortions of an optical wave are more severe than remote distortions of the same amplitude.\n== Hypotheses ==\n=== Buoyancy hypothesis ===\nAlso called chimney effect or stack effect, observes that warm air (from the hot shower) rises out over the shower curtain as cooler air (near the floor) pushes in under the curtain to replace the rising air.  By pushing the curtain in towards the shower, the (short range) vortex and Coandă effects become more significant. However, the shower-curtain effect persists when cold water is used, implying that this is not the sole mechanism.\n=== Bernoulli effect hypothesis ===\nThe most popular explanation given for the shower-curtain effect is Bernoulli\'s principle.  Bernoulli\'s principle states that an increase in velocity results in a decrease in pressure.  This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water.  This movement would be parallel to the plane of the shower curtain.  If air is moving across the inside surface of the shower curtain, Bernoulli\'s principle says the air pressure there will drop.  This would result in a pressure differential between the inside and outside, causing the curtain to move inward.  It would be strongest when the gap between the bather and the curtain is smallest, resulting in the curtain attaching to the bather.\n=== Horizontal vortex hypothesis ===\nA computer simulation of a typical bathroom found that none of the above theories pan out in their analysis, but instead found that the spray from the shower-head drives a horizontal vortex. This vortex has a low-pressure zone in the centre, which sucks the curtain.\nDavid Schmidt of the University of Massachusetts was awarded the 2001 Ig Nobel Prize in Physics for his partial solution to the question of why shower curtains billow inwards. He used a computational fluid dynamics code to achieve the results.  Professor Schmidt is adamant that this was done "for fun" in his own free time without the use of grants.\n=== Coandă effect ===\nThe Coandă effect, also known as "boundary layer attachment", is the tendency of a moving fluid to adhere to an adjacent wall.\n=== Condensation ===\nA hot shower will produce steam that condenses on the shower side of the curtain, lowering the pressure there.  In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.\n=== Air pressure ===\nColder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.\n== Solutions ==\nMany shower curtains come with features to reduce the shower-curtain effect. They may have adhesive suction cups on the bottom edges of the curtain, which are then pushed onto the sides of the shower when in use. Others may have magnets at the bottom, though these are not effective on acrylic or fiberglass tubs.\nIt is possible to use a telescopic shower curtain rod to block the curtain on its lower part and to prevent it from sucking inside.\nHanging the curtain rod higher or lower, or especially further away from the shower head, can reduce the effect. A convex shower rod can also be used to hold the curtain against the inside wall of a tub.\nA weight can be attached to a long string and the string attached to the curtain rod in the middle of the curtain (on the inside). Hanging the weight low against the curtain just above the rim of the shower pan or tub makes it an effective billowing deterrent without allowing the weight to hit the pan or tub and damage it.\nThere are a few alternative solutions that either attach to the shower curtain directly, attach to the shower rod or attach to the wall.\n== References ==\n== External links ==\nScientific American: Why does the shower curtain move toward the water?\nWhy does the shower curtain blow up and in instead of down and out?\nVideo demonstration of how this phenomenon could be solved.\nThe Straight Dope: Why does the shower curtain blow in despite the water pushing it out (revisited)?\n2001 Ig Nobel Prize Winners\nFluent NEWS: Shower Curtain Grabs Scientist – But He Lives to Tell Why\nArggh, Why Does the Shower Curtain Attack Me? by Joe Palca. All Things Considered, National Public Radio.  November 4, 2006. (audio)\nExperimental Investigation of the Influence of the Relative Position of the Scattering Layer on Image Quality: the Shower Curtain Effect\nThe shower curtain effect; ESA', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', "Planck's law"]

Question: What is the relation between the three moment theorem and the bending moments at three successive supports of a continuous beam?

Choices:
Choice A) The three moment theorem expresses the relation between the deflection of two points on a beam relative to the point of intersection between tangent at those two points and the vertical through the first point.
Choice B) The three moment theorem is used to calculate the maximum allowable bending moment of a beam, which is determined by the weight distribution of each segment of the beam.
Choice C) The three moment theorem describes the relationship between bending moments at three successive supports of a continuous beam, subject to a loading on two adjacent spans with or without settlement of the supports.
Choice D) The three moment theorem is used to calculate the weight distribution of each segment of a beam, which is required to apply Mohr's theorem.
Choice E) The three moment theorem is used to derive the change in slope of a deflection curve between two points of a beam, which is equal to the area of the M/EI diagram between those two points.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==', 'These pigments are embedded in plants and algae in complexes called antenna proteins. In such proteins, the pigments are arranged to work together. Such a combination of proteins is also called a light-harvesting complex.\nAlthough all cells in the green parts of a plant have chloroplasts, the majority of those are found in specially adapted structures called leaves. Certain species adapted to conditions of strong sunlight and aridity, such as many Euphorbia and cactus species, have their main photosynthetic organs in their stems. The cells in the interior tissues of a leaf, called the mesophyll, can contain between 450,000 and 800,000 chloroplasts for every square millimeter of leaf. The surface of the leaf is coated with a water-resistant waxy cuticle that protects the leaf from excessive evaporation of water and decreases the absorption of ultraviolet or blue light to minimize heating. The transparent epidermis layer allows light to pass through to the palisade mesophyll cells where most of the photosynthesis takes place.\n== Light-dependent reactions ==\nIn the light-dependent reactions, one molecule of the pigment chlorophyll absorbs one photon and loses one electron. This electron is taken up by a modified form of chlorophyll called pheophytin, which passes the electron to a quinone molecule, starting the flow of electrons down an electron transport chain that leads to the ultimate reduction of NADP to NADPH. In addition, this creates a proton gradient (energy gradient) across the chloroplast membrane, which is used by ATP synthase in the synthesis of ATP. The chlorophyll molecule ultimately regains the electron it lost when a water molecule is split in a process called photolysis, which releases oxygen.\nThe overall equation for the light-dependent reactions under the conditions of non-cyclic electron flow in green plants is:\nNot all wavelengths of light can support photosynthesis. The photosynthetic action spectrum depends on the type of accessory pigments present. For example, in green plants, the action spectrum resembles the absorption spectrum for chlorophylls and carotenoids with absorption peaks in violet-blue and red light. In red algae, the action spectrum is blue-green light, which allows these algae to use the blue end of the spectrum to grow in the deeper waters that filter out the longer wavelengths (red light) used by above-ground green plants. The non-absorbed part of the light spectrum is what gives photosynthetic organisms their color (e.g., green plants, red algae, purple bacteria) and is the least effective for photosynthesis in the respective organisms.\n=== Z scheme ===\nIn plants, light-dependent reactions occur in the thylakoid membranes of the chloroplasts where they drive the synthesis of ATP and NADPH. The light-dependent reactions are of two forms: cyclic and non-cyclic.\nIn the non-cyclic reaction, the photons are captured in the light-harvesting antenna complexes of photosystem II by chlorophyll and other accessory pigments (see diagram "Z-scheme"). The absorption of a photon by the antenna complex loosens an electron by a process called photoinduced charge separation. The antenna system is at the core of the chlorophyll molecule of the photosystem II reaction center. That loosened electron is taken up by the primary electron-acceptor molecule, pheophytin. As the electrons are shuttled through an electron transport chain (the so-called Z-scheme shown in the diagram), a chemiosmotic potential is generated by pumping proton cations (H+) across the membrane and into the thylakoid space. An ATP synthase enzyme uses that chemiosmotic potential to make ATP during photophosphorylation, whereas NADPH is a product of the terminal redox reaction in the Z-scheme. The electron enters a chlorophyll molecule in Photosystem I. There it is further excited by the light absorbed by that photosystem. The electron is then passed along a chain of electron acceptors to which it transfers some of its energy. The energy delivered to the electron acceptors is used to move hydrogen ions across the thylakoid membrane into the lumen. The electron is eventually used to reduce the coenzyme NADP with an H+ to NADPH (which has functions in the light-independent reaction); at that point, the path of that electron ends.\nThe cyclic reaction is similar to that of the non-cyclic but differs in that it generates only ATP, and no reduced NADP (NADPH) is created. The cyclic reaction takes place only at photosystem I. Once the electron is displaced from the photosystem, the electron is passed down the electron acceptor molecules and returns to photosystem I, from where it was emitted, hence the name cyclic reaction.\n=== Water photolysis ===\nLinear electron transport through a photosystem will leave the reaction center of that photosystem oxidized. Elevating another electron will first require re-reduction of the reaction center. The excited electrons lost from the reaction center (P700) of photosystem I are replaced by transfer from plastocyanin, whose electrons come from electron transport through photosystem II. Photosystem II, as the first step of the Z-scheme, requires an external source of electrons to reduce its oxidized chlorophyll a reaction center. The source of electrons for photosynthesis in green plants and cyanobacteria is water. Two water molecules are oxidized by the energy of four successive charge-separation reactions of photosystem II to yield a molecule of diatomic oxygen and four hydrogen ions. The electrons yielded are transferred to a redox-active tyrosine residue that is oxidized by the energy of P680+. This resets the ability of P680 to absorb another photon and release another photo-dissociated electron. The oxidation of water is catalyzed in photosystem II by a redox-active structure that contains four manganese ions and a calcium ion; this oxygen-evolving complex binds two water molecules and contains the four oxidizing equivalents that are used to drive the water-oxidizing reaction (Kok\'s S-state diagrams). The hydrogen ions are released in the thylakoid lumen and therefore contribute to the transmembrane chemiosmotic potential that leads to ATP synthesis. Oxygen is a waste product of light-dependent reactions, but the majority of organisms on Earth use oxygen and its energy for cellular respiration, including photosynthetic organisms.\n== Light-independent reactions ==\n=== Calvin cycle ===\nIn the light-independent (or "dark") reactions, the enzyme RuBisCO captures CO2 from the atmosphere and, in a process called the Calvin cycle, uses the newly formed NADPH and releases three-carbon sugars, which are later combined to form sucrose and starch. The overall equation for the light-independent reactions in green plants is:\u200a128\nCarbon fixation produces the three-carbon sugar intermediate, which is then converted into the final carbohydrate products. The simple carbon sugars photosynthesis produces are then used to form other organic compounds, such as the building material cellulose, the precursors for lipid and amino acid biosynthesis, or as a fuel in cellular respiration. The latter occurs not only in plants but also in animals when the carbon and energy from plants is passed through a food chain.\nThe fixation or reduction of carbon dioxide is a process in which carbon dioxide combines with a five-carbon sugar, ribulose 1,5-bisphosphate, to yield two molecules of a three-carbon compound, glycerate 3-phosphate, also known as 3-phosphoglycerate. Glycerate 3-phosphate, in the presence of ATP and NADPH produced during the light-dependent stages, is reduced to glyceraldehyde 3-phosphate. This product is also referred to as 3-phosphoglyceraldehyde (PGAL) or, more generically, as triose phosphate. Most (five out of six molecules) of the glyceraldehyde 3-phosphate produced are used to regenerate ribulose 1,5-bisphosphate so the process can continue. The triose phosphates not thus "recycled" often condense to form hexose phosphates, which ultimately yield sucrose, starch, and cellulose, as well as glucose and fructose. The sugars produced during carbon metabolism yield carbon skeletons that can be used for other metabolic reactions like the production of amino acids and lipids.\n=== Carbon concentrating mechanisms ===\n==== On land ====\nIn hot and dry conditions, plants close their stomata to prevent water loss. Under these conditions, CO2 will decrease and oxygen gas, produced by the light reactions of photosynthesis, will increase, causing an increase of photorespiration by the oxygenase activity of ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) and decrease in carbon fixation. Some plants have evolved mechanisms to increase the CO2 concentration in the leaves under these conditions.', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', "==== Red-giant-branch phase ====\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star.  For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower 12C/13C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\nThe helium core continues to grow on the red-giant branch.  It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase.  The star increases in luminosity towards the tip of the red-giant branch.  Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n==== Horizontal branch ====\nIn the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 108 times the luminosity of the Sun for a few days and 1011 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning.  These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n==== Asymptotic-giant-branch phase ====\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time).  Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star.  Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically.  This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface.  This is known as the second dredge up, and in some stars there may even be a third dredge up.  In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra.  A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\nAnother well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths.  These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity.  These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n==== Post-AGB ====\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n=== Massive stars ===\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants.  Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.\n==== Supergiant evolution ====\nExtremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\nThe core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core.  In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process.  At the end of helium fusion, the core of a star consists primarily of carbon and oxygen.  In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium.  Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.\nThe exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉.  After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse.  Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning.  For a range of stars of approximately 8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova.", 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', '=== Progenitor ===\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic type Ic-BL events that are associated with long-duration gamma-ray bursts.\n== External impact ==\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\n=== Source of heavy elements ===\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\n=== Role in stellar evolution ===\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n=== Cosmic rays ===\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\n=== Gravitational waves ===', 'Photosynthesis', "In more massive stars, the fusion of neon proceeds without a runaway deflagration.  This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements.  Surrounding the core are shells of lighter elements still undergoing fusion.  The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged.  The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope.  The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars.  Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself.  The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.\n==== Supernova ====\nWhen the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole.  Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\n== Stellar remnants ==\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n=== White and black dwarfs ===\nFor a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years.\nThe chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n=== Neutron stars ===\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n=== Black holes ===\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉.", 'Accretion of material onto the protostar continues partially from the newly formed circumstellar disc. When the density and temperature are high enough, deuterium fusion begins, and the outward pressure of the resultant radiation slows (but does not stop) the collapse. Material comprising the cloud continues to "rain" onto the protostar. In this stage bipolar jets are produced called Herbig–Haro objects. This is probably the means by which excess angular momentum of the infalling material is expelled, allowing the star to continue to form.\nWhen the surrounding gas and dust envelope disperses and accretion process stops, the star is considered a pre-main-sequence star (PMS star). The energy source of these objects is (gravitational contraction)Kelvin–Helmholtz mechanism, as opposed to hydrogen burning in main sequence stars. The PMS star follows a Hayashi track on the Hertzsprung–Russell (H–R) diagram. The contraction will proceed until the Hayashi limit is reached, and thereafter contraction will continue on a Kelvin–Helmholtz timescale with the temperature remaining stable. Stars with less than 0.5 M☉ thereafter join the main sequence. For more massive PMS stars, at the end of the Hayashi track they will slowly collapse in near hydrostatic equilibrium, following the Henyey track.\nFinally, hydrogen begins to fuse in the core of the star, and the rest of the enveloping material is cleared away. This ends the protostellar phase and begins the star\'s main sequence phase on the H–R diagram.\nThe stages of the process are well defined in stars with masses around 1 M☉ or less. In high mass stars, the length of the star formation process is comparable to the other timescales of their evolution, much shorter, and the process is not so well defined. The later evolution of stars is studied in stellar evolution.\n== Observations ==\nKey elements of star formation are only available by observing in wavelengths other than the optical. The protostellar stage of stellar existence is almost invariably hidden away deep inside dense clouds of gas and dust left over from the GMC. Often, these star-forming cocoons known as Bok globules, can be seen in silhouette against bright emission from surrounding gas. Early stages of a star\'s life can be seen in infrared light, which penetrates the dust more easily than visible light.\nObservations from the Wide-field Infrared Survey Explorer (WISE) have thus been especially important for unveiling numerous galactic protostars and their parent star clusters.  Examples of such embedded star clusters are FSR 1184, FSR 1190, Camargo 14, Camargo 74, Majaess 64, and Majaess 98.\nThe structure of the molecular cloud and the effects of the protostar can be observed in near-IR extinction maps (where the number of stars are counted per unit area and compared to a nearby zero extinction area of sky), continuum dust emission and rotational transitions of CO and other molecules; these last two are observed in the millimeter and submillimeter range. The radiation from the protostar and early star has to be observed in infrared astronomy wavelengths, as the extinction caused by the rest of the cloud in which the star is forming is usually too big to allow us to observe it in the visual part of the spectrum. This presents considerable difficulties as the Earth\'s atmosphere is almost entirely opaque from 20μm to 850μm, with narrow windows at 200μm and 450μm. Even outside this range, atmospheric subtraction techniques must be used.\nX-ray observations have proven useful for studying young stars, since X-ray emission from these objects is about 100–100,000 times stronger than X-ray emission from main-sequence stars. The earliest detections of X-rays from T Tauri stars were made by the Einstein X-ray Observatory. For low-mass stars X-rays are generated by the heating of the stellar corona through magnetic reconnection, while for high-mass O and early B-type stars X-rays are generated through supersonic shocks in the stellar winds. Photons in the soft X-ray energy range covered by the Chandra X-ray Observatory and XMM-Newton may penetrate the interstellar medium with only moderate absorption due to gas, making the X-ray a useful wavelength for seeing the stellar populations within molecular clouds. X-ray emission as evidence of stellar youth makes this band particularly useful for performing censuses of stars in star-forming regions, given that not all young stars have infrared excesses. X-ray observations have provided near-complete censuses of all stellar-mass objects in the Orion Nebula Cluster and Taurus Molecular Cloud.\nThe formation of individual stars can only be directly observed in the Milky Way Galaxy, but in distant galaxies star formation has been detected through its unique spectral signature.\nInitial research indicates star-forming clumps start as giant, dense areas in turbulent gas-rich matter in young galaxies, live about 500 million years, and may migrate to the center of a galaxy, creating the central bulge of a galaxy.\nOn February 21, 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\nIn February 2018, astronomers reported, for the first time, a signal of the reionization epoch, an indirect detection of light from the earliest stars formed - about 180 million years after the Big Bang.\nAn article published on October 22, 2019, reported on the detection of 3MM-1, a massive star-forming galaxy about 12.5 billion light-years away that is obscured by clouds of dust. At a mass of about 1010.8 solar masses, it showed a star formation rate about 100 times as high as in the Milky Way.\n=== Notable pathfinder objects ===\nMWC 349 was first discovered in 1978, and is estimated to be only 1,000 years old.\nVLA 1623 – The first exemplar Class 0 protostar, a type of embedded protostar that has yet to accrete the majority of its mass. Found in 1993, is possibly younger than 10,000 years.\nL1014 – An extremely faint embedded object representative of a new class of sources that are only now being detected with the newest telescopes. Their status is still undetermined, they could be the youngest low-mass Class 0 protostars yet seen or even very low-mass evolved objects (like brown dwarfs or even rogue planets).\nGCIRS 8* – The youngest known main sequence star in the Galactic Center region, discovered in August 2006. It is estimated to be 3.5 million years old.\n== Low mass and high mass star formation ==\nStars of different masses are thought to form by slightly different mechanisms.  The theory of low-mass star formation, which is well-supported by observation, suggests that low-mass stars form by the gravitational collapse of rotating density enhancements within molecular clouds.  As described above, the collapse of a rotating cloud of gas and dust leads to the formation of an accretion disk through which matter is channeled onto a central protostar.  For stars with masses higher than about 8 M☉, however, the mechanism of star formation is not well understood.\nMassive stars emit copious quantities of radiation which pushes against infalling material.  In the past, it was thought that this radiation pressure might be substantial enough to halt accretion onto the massive protostar and prevent the formation of stars with masses more than a few tens of solar masses. Recent theoretical work has shown that the production of a jet and outflow clears a cavity through which much of the radiation from a massive protostar can escape without hindering accretion through the disk and onto the protostar. Present thinking is that massive stars may therefore be able to form by a mechanism similar to that by which low mass stars form.\nThere is mounting evidence that at least some massive protostars are indeed surrounded by accretion disks.  Disk accretion in high-mass protostars, similar to their low-mass counterparts, is expected to exhibit bursts of episodic accretion as a result of a gravitationally instability leading to clumpy and in-continuous accretion rates. Recent evidence of accretion bursts in high-mass protostars has indeed been confirmed observationally. Several other theories of massive star formation remain to be tested observationally.  Of these, perhaps the most prominent is the theory of competitive accretion, which suggests that massive protostars are "seeded" by low-mass protostars which compete with other protostars to draw in matter from the entire parent molecular cloud, instead of simply from a small local region.\nAnother theory of massive star formation suggests that massive stars may form by the coalescence of two or more stars of lower mass.\n== Filamentary nature of star formation ==\nRecent studies have emphasized the role of filamentary structures in molecular clouds as the initial conditions for star formation. Findings from the Herschel Space Observatory highlight the ubiquitous nature of these filaments in the cold interstellar medium (ISM). The spatial relationship between cores and filaments indicates that the majority of prestellar cores are located within 0.1 pc of supercritical filaments. This supports the hypothesis that filamentary structures act as pathways for the accumulation of gas and dust, leading to core formation.', 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups']

Question: What is the function of the fibrous cardiac skeleton?

Choices:
Choice A) The fibrous cardiac skeleton is a system of blood vessels that supplies oxygen and nutrients to the heart muscle.
Choice B) The fibrous cardiac skeleton is responsible for the pumping action of the heart, regulating the flow of blood through the atria and ventricles.
Choice C) The fibrous cardiac skeleton provides structure to the heart, forming the atrioventricular septum that separates the atria from the ventricles, and the fibrous rings that serve as bases for the four heart valves.
Choice D) The fibrous cardiac skeleton is a network of nerves that controls the heartbeat and rhythm of the heart.
Choice E) The fibrous cardiac skeleton is a protective layer that surrounds the heart, shielding it from external damage.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', "Planck's law", 'Vol A -  Space Group Symmetry,\nVol A1 - Symmetry Relations Between Space Groups,\nVol B -  Reciprocal Space,\nVol C - Mathematical, Physical, and Chemical Tables,\nVol D - Physical Properties of Crystals,\nVol E - Subperiodic Groups,\nVol F - Crystallography of Biological Macromolecules, and\nVol G - Definition and Exchange of Crystallographic Data.\n== Notable scientists ==\n== See also ==\n== References ==\n== External links ==\nFree book, Geometry of Crystals, Polycrystals and Phase Transformations\nAmerican Crystallographic Association\nLearning Crystallography\nWeb Course on Crystallography\nCrystallographic Space Groups', 'Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word κρύσταλλος (krústallos; "clear ice, rock-crystal"), and γράφειν (gráphein; "to write"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well new developments in these fields.\n== History and timeline ==\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the Davisson–Germer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n== Methodology ==\nCrystallographic methods depend mainly on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. Crystallographers often explicitly state the type of beam used, as in the terms X-ray diffraction, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.\nX-rays interact with the spatial distribution of electrons in the sample.\nNeutrons are scattered by the atomic nuclei through the strong nuclear forces, but in addition the magnetic moment of neutrons is non-zero, so they are also scattered by magnetic fields. When neutrons are scattered from hydrogen-containing materials, they produce diffraction patterns with high noise levels, which can sometimes be resolved by substituting deuterium for hydrogen.\nElectrons are charged particles and therefore interact with the total charge distribution of both the atomic nuclei and the electrons of the sample.:\u200aChpt 4\nIt is hard to focus x-rays or neutrons, but since electrons are charged they can be focused and are used in electron microscope to produce magnified images. There are many ways that transmission electron microscopy and related techniques such as scanning transmission electron microscopy, high-resolution electron microscopy can be used to obtain images with in many cases atomic resolution from which crystallographic information can be obtained. There are also other methods such as low-energy electron diffraction, low-energy electron microscopy and reflection high-energy electron diffraction which can be used to obtain crystallographic information about surfaces.\n== Applications in various areas ==\n=== Materials science ===\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Most materials do not occur as a single crystal, but are poly-crystalline in nature (they exist as an aggregate of small crystals with different orientations). As such, powder diffraction techniques, which take diffraction patterns of samples with a large number of crystals, play an important role in structural determination.\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements. Crystallographic studies help elucidate the relationship between a material\'s structure and its properties, aiding in developing new materials with tailored characteristics. This understanding is crucial in various fields, including metallurgy, geology, and materials science. Advancements in crystallographic techniques, such as electron diffraction and X-ray crystallography, continue to expand our understanding of material behavior at the atomic level.\nIn another example, iron transforms from a body-centered cubic (bcc) structure called ferrite to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material\'s properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which structures are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory.\n=== Biology ===\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. The double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol, Pymol or VMD can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron diffraction has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n== Notation ==\nCoordinates in square brackets such as [100] denote a direction vector (in real space).\nCoordinates in angle brackets or chevrons such as <100> denote a family of directions which are related by symmetry operations. In the cubic crystal system for example, <100> would mean [100], [010], [001] or the negative of any of those directions.\nMiller indices in parentheses such as (100) denote a plane of the crystal structure, and regular repetitions of that plane with a particular spacing. In the cubic system, the normal to the (hkl) plane is the direction [hkl], but in lower-symmetry cases, the normal to (hkl) is not parallel to [hkl].\nIndices in curly brackets or braces such as {100} denote a family of planes and their normals. In cubic materials the symmetry makes them equivalent, just as the way angle brackets denote a family of directions. In non-cubic materials, <hkl> is not necessarily perpendicular to {hkl}.\n== Reference literature ==\nThe International Tables for Crystallography is an eight-book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure through x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and do not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:', 'Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),\nWhen the temperature is not constant within the device (as in thermocouples),\nWhen the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of μ and T to different bands (conduction band vs. valence band). Even then, the values of μ and T may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n== Technicalities ==\n=== Nomenclature ===\nThe term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, μ − ϵC, called ζ above.\nIt is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ϵC due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, chemical potential and electrochemical potential.\nIt is also important to note that Fermi level is not necessarily the same thing as Fermi energy.\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas.\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.\n=== Fermi level referencing and the location of zero Fermi level ===\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its μ is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n==== Why it is not advisable to use "the energy in vacuum" as a reference zero ====\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where the vacuum is. The problem is that not all points in the vacuum are equivalent.\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n=== Discrete charging effects in small systems ===\nIn cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nWhen the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential μ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):\n{\\displaystyle \\mu (\\left\\langle N\\right\\rangle ,T)=\\left({\\frac {\\partial F}{\\partial \\left\\langle N\\right\\rangle }}\\right)_{T},}\nwhere\n{\\displaystyle F(N,T)=\\Omega (N,T)+\\mu N}\nis the Helmholtz free energy of the grand canonical ensemble.\nIf the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,\n{\\displaystyle \\mu \'(N,T)=F(N+1,T)-F(N,T),}\nwhere F(N, T) is the free energy function of the canonical ensemble, alternatively,\n{\\displaystyle \\mu \'\'(N,T)=F(N,T)-F(N-1,T)=\\mu \'(N-1,T).}\nThese chemical potentials are not equivalent, μ ≠ μ′ ≠ μ″, except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade, but technically affects large sized semiconductors at zero temperature, at least ideally.\nThe parameter, μ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n== Notes ==\n== References ==', 'In response to the so-called "naturalness crisis" in the Minimal Supersymmetric Standard Model, some researchers have abandoned naturalness and the original motivation to solve the hierarchy problem naturally with supersymmetry, while other researchers have moved on to other supersymmetric models such as split supersymmetry. Still others have moved to string theory as a result of the naturalness crisis. Former enthusiastic supporter Mikhail Shifman went as far as urging the theoretical community to search for new ideas and accept that supersymmetry was a failed theory in particle physics. However, some researchers suggested that this "naturalness" crisis was premature because various calculations were too optimistic about the limits of masses which would allow a supersymmetric extension of the Standard Model as a solution.\n== General supersymmetry ==\nSupersymmetry appears in many related contexts of theoretical physics. It is possible to have multiple supersymmetries and also have supersymmetric extra dimensions.\n=== Extended supersymmetry ===\nIt is possible to have more than one kind of supersymmetry transformation. Theories with more than one supersymmetry transformation are known as extended supersymmetric theories. The more supersymmetry a theory has, the more constrained are the field content and interactions. Typically the number of copies of a supersymmetry is a power of 2 (1, 2, 4, 8...). In four dimensions, a spinor has four degrees of freedom and thus the minimal number of supersymmetry generators is four in four dimensions and having eight copies of supersymmetry means that there are 32 supersymmetry generators.\nThe maximal number of supersymmetry generators possible is 32. Theories with more than 32 supersymmetry generators automatically have massless fields with spin greater than 2. It is not known how to make massless fields with spin greater than two interact, so the maximal number of supersymmetry generators considered is 32. This is due to the Weinberg–Witten theorem. This corresponds to an N = 8 supersymmetry theory. Theories with 32 supersymmetries automatically have a graviton.\nFor four dimensions there are the following theories, with the corresponding multiplets (CPT adds a copy, whenever they are not invariant under such symmetry):\n=== Supersymmetry in alternate numbers of dimensions ===\nIt is possible to have supersymmetry in dimensions other than four. Because the properties of spinors change drastically between different dimensions, each dimension has its characteristic. In d dimensions, the size of spinors is approximately 2d/2 or 2(d − 1)/2. Since the maximum number of supersymmetries is 32, the greatest number of dimensions in which a supersymmetric theory can exist is eleven.\n=== Fractional supersymmetry ===\nFractional supersymmetry is a generalization of the notion of supersymmetry in which the minimal positive amount of spin does not have to be \u20601/2\u2060 but can be an arbitrary \u20601/N\u2060 for integer value of N. Such a generalization is possible in two or fewer spacetime dimensions.\n== See also ==\n== References ==\n== Further reading ==\n=== Theoretical introductions, free and online ===\n=== Monographs ===\n=== On experiments ===\n== External links ==\nSupersymmetry – European Organization for Nuclear Research (CERN)\nThe status of supersymmetry – Symmetry Magazine (Fermilab/SLAC), January 12, 2021\nAs Supersymmetry Fails Tests, Physicists Seek New Ideas – Quanta Magazine, November 20, 2012\nWhat is Supersymmetry? – Fermilab, May 21, 2013\nWhy Supersymmetry? – Fermilab, May 31, 2013\nThe Standard Model and Supersymmetry – World Science Festival, March 4, 2015\nSUSY running out of hiding places – BBC, December 11, 2012', 'In the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein\'s proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal. This has at times been called Planck\'s "second theory".\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his \'third theory\', that both emission and absorption of light were quantal.\nThe colourful term "ultraviolet catastrophe" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck\'s thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of "catastrophe". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called \'photons\', and by Rayleigh and by Jeans.\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν. In contrast to Planck\'s and Einstein\'s formulas, Bohr\'s formula referred explicitly and categorically to energy levels of atoms. Bohr\'s formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck\'s model, the frequency\n{\\displaystyle \\nu }\nhas no immediate relation to frequencies that might describe those quantum states themselves.\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck\'s law. The actual word \'photon\' was invented still later, by G.N. Lewis in 1926, who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word \'photon\' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein\'s 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\nUltimately, Planck\'s law of black-body radiation contributed to Einstein\'s concept of quanta of light carrying linear momentum, which became the fundamental basis for the development of quantum mechanics.\nThe above-mentioned linearity of Planck\'s mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg\'s original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg\'s theory accounted for Bohr\'s above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states. At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg\'s paper and recognized the matrix character of Heisenberg\'s theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg\'s original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics. Heisenberg\'s explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck\'s oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = \u2060h/2π\u2060, and ω = 2πν denotes angular frequency, and less often the equivalent formula E = hν. This statement about a really existing and propagating light quantum, based on Einstein\'s, has a physical meaning different from that of Planck\'s above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\nAn article by Helge Kragh published in Physics World gives an account of this history.\n== See also ==\nEmissivity\nRadiance\nSakuma–Hattori equation\n== References ==\n=== Bibliography ===\n== External links ==\nSummary of Radiation\nRadiation of a Blackbody – interactive simulation to play with Planck\'s law\nScienceworld entry on Planck\'s Law', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  ħωk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, θk†, θk, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the θk, and the Fock space is built by applying all products of creation operators θk† to |0⟩.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at φ = 0, so that the vacuum minimizing the Hamiltonian satisfies ⟨φ⟩ = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  φ = v .  This occurs for example, if V(φ) = gφ4 − 2m2φ2 with g > 0 and m2 > 0, for which the minimum energy is found at v = ±m/√g. The value of v in one of these vacua may be considered as condensate of the field φ. Canonical quantization then can be carried out for the shifted field  φ(x,t) − v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator  [A, B]  expressed in the phase space formulation is iħ{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  ħ-deformations are highly nonunique—quantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  ħ → 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an ħ-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional Schrödinger Hilbert space. If the chosen variables are complex, we get something like the Segal–Bargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', '{\\displaystyle L_{z}=m_{\\ell }\\hbar }\nThe values of mℓ range from −ℓ to ℓ, with integer intervals.\nThe s subshell (ℓ = 0) contains only one orbital, and therefore the mℓ of an electron in an s orbital will always be 0. The p subshell (ℓ = 1) contains three orbitals, so the mℓ of an electron in a p orbital will be −1, 0, or 1. The d subshell (ℓ = 2) contains five orbitals, with mℓ values of −2, −1, 0, 1, and 2.\n=== Spin magnetic quantum number ===\nThe spin magnetic quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis:\n{\\displaystyle S_{z}=m_{s}\\hbar }\nIn general, the values of ms range from −s to s, where s is the spin quantum number, associated with the magnitude of particle\'s intrinsic spin angular momentum:\n{\\displaystyle m_{s}=-s,-s+1,-s+2,\\cdots ,s-2,s-1,s}\nAn electron state has spin number s = \u20601/2\u2060, consequently ms will be +\u20601/2\u2060 ("spin up") or −\u20601/2\u2060 "spin down" states. Since electron are fermions they obey the Pauli exclusion principle: each electron state must have different quantum numbers.  Therefore, every orbital will be occupied with at most two electrons, one for each spin state.\n=== The Aufbau principle and Hund\'s Rules ===\nA multi-electron atom can be modeled qualitatively as a hydrogen like atom with higher nuclear charge and correspondingly more electrons. The occupation of the electron states in such an atom can be predicted by the Aufbau principle and Hund\'s empirical rules for the quantum numbers.  The Aufbau principle fills orbitals based on their principal and azimuthal quantum numbers (lowest n + l first, with lowest n breaking ties; Hund\'s rule favors unpaired electrons in the outermost orbital). These rules are empirical but they can be related to electron physics.:\u200a10\u200a:\u200a260\n== Spin-orbit coupled systems ==\nWhen one takes the spin–orbit interaction into consideration, the L and S operators no longer commute with the Hamiltonian, and the eigenstates of the system no longer have well-defined orbital angular momentum and spin. Thus another set of quantum numbers should be used. This set includes\nThe total angular momentum quantum number:\n{\\displaystyle j=|\\ell \\pm s|,}\nwhich gives the total angular momentum through the relation\n{\\displaystyle J^{2}=\\hbar ^{2}j(j+1).}\nThe projection of the total angular momentum along a specified axis:\n{\\displaystyle m_{j}=-j,-j+1,-j+2,\\cdots ,j-2,j-1,j}\nanalogous to the above and satisfies both\n{\\displaystyle m_{j}=m_{\\ell }+m_{s},}\nand\n{\\displaystyle |m_{\\ell }+m_{s}|\\leq j.}\nParityThis is the eigenvalue under reflection: positive (+1) for states which came from even ℓ and negative (−1) for states which came from odd ℓ. The former is also known as even parity and the latter as odd parity, and is given by\n{\\displaystyle P=(-1)^{\\ell }.}\nFor example, consider the following 8 states, defined by their quantum numbers:\nThe quantum states in the system can be described as linear combination of these 8 states. However, in the presence of spin–orbit interaction, if one wants to describe the same system by 8 states that are eigenvectors of the Hamiltonian (i.e. each represents a state that does not mix with others over time), we should consider the following 8 states:\n== Atomic nuclei ==\nIn nuclei, the entire assembly of protons and neutrons (nucleons) has a resultant angular momentum due to the angular momenta of each nucleon, usually denoted I. If the total angular momentum of a neutron is jn = ℓ + s and for a proton is jp = ℓ + s (where s for protons and neutrons happens to be \u20601/2\u2060 again (see note)), then the nuclear angular momentum quantum numbers I are given by:\n{\\displaystyle I=|j_{n}-j_{p}|,|j_{n}-j_{p}|+1,|j_{n}-j_{p}|+2,\\cdots ,(j_{n}+j_{p})-2,(j_{n}+j_{p})-1,(j_{n}+j_{p})}\nNote: The orbital angular momenta of the nuclear (and atomic) states are all integer multiples of ħ while the intrinsic angular momentum of the neutron and  proton are half-integer multiples.  It should be immediately apparent that the combination of the intrinsic spins of the nucleons with their orbital motion will always give half-integer values for the total spin, I, of any odd-A nucleus and integer values for any even-A nucleus.\nParity with the number I is used to label nuclear angular momentum states, examples for some isotopes of hydrogen (H), carbon (C), and sodium (Na) are;\nThe reason for the unusual fluctuations in I, even by differences of just one nucleon, are due to the odd and even numbers of protons and neutrons – pairs of nucleons have a total angular momentum of zero (just like electrons in orbitals), leaving an odd or even number of unpaired nucleons. The property of nuclear spin is an important factor for the operation of NMR spectroscopy in organic chemistry, and MRI in nuclear medicine, due to the nuclear magnetic moment interacting with an external magnetic field.\n== Elementary particles ==\nElementary particles contain many quantum numbers which are usually said to be intrinsic to them. However, it should be understood that the elementary particles are quantum states of the standard model of particle physics, and hence the quantum numbers of these particles bear the same relation to the Hamiltonian of this model as the quantum numbers of the Bohr atom does to its Hamiltonian. In other words, each quantum number denotes a symmetry of the problem. It is more useful in quantum field theory to distinguish between spacetime and internal symmetries.\nTypical quantum numbers related to spacetime symmetries are spin (related to rotational symmetry), the parity, C-parity and T-parity (related to the Poincaré symmetry of spacetime). Typical internal symmetries are lepton number and baryon number or the electric charge. (For a full list of quantum numbers of this kind see the article on flavour.)\n== Multiplicative quantum numbers ==\nMost conserved quantum numbers are additive, so in an elementary particle reaction, the sum of the quantum numbers should be the same before and after the reaction. However, some, usually called a parity, are multiplicative; i.e., their product is conserved. All multiplicative quantum numbers belong to a symmetry (like parity) in which applying the symmetry transformation twice is equivalent to doing nothing (involution).\n== See also ==\nElectron configuration\n== References ==\n== Further reading ==\nDirac, Paul A. M. (1982). Principles of Quantum Mechanics. Oxford University Press. ISBN 0-19-852011-5.\nGriffiths, David J. (2004). Introduction to Quantum Mechanics (2nd ed.). Prentice Hall. ISBN 0-13-805326-X.\nHalzen, Francis & Martin, Alan D. (1984). Quarks and Leptons: An Introductory Course in Modern Particle Physics. John Wiley & Sons. ISBN 0-471-88741-2.\nEisberg, Robert Martin; Resnick, Robert (1985). Quantum Physics of Atoms, Molecules, Solids, Nuclei and Particles (2nd ed.). John Wiley & Sons. ISBN 978-0-471-87373-0 – via Internet Archive.', 'is known as the Stefan–Boltzmann constant.\n=== Radiative transfer ===\nThe equation of radiative transfer describes the way in which radiation is affected as it travels through a material medium. For the special case in which the material medium is in thermodynamic equilibrium in the neighborhood of a point in the medium, Planck\'s law is of special importance.\nFor simplicity, we can consider the linear steady state, without scattering. The equation of radiative transfer states that for a beam of light going through a small distance ds, energy is conserved: The change in the (spectral) radiance of that beam (Iν) is equal to the amount removed by the material medium plus the amount gained from the material medium. If the radiation field is in equilibrium with the material medium, these two contributions will be equal. The material medium will have a certain emission coefficient and absorption coefficient.\nThe absorption coefficient α is the fractional change in the intensity of the light beam as it travels the distance ds, and has units of length−1. It is composed of two parts, the decrease due to absorption and the increase due to stimulated emission. Stimulated emission is emission by the material body which is caused by and is proportional to the incoming radiation. It is included in the absorption term because, like absorption, it is proportional to the intensity of the incoming radiation. Since the amount of absorption will generally vary linearly as the density ρ of the material, we may define a "mass absorption coefficient" κν = \u2060α/ρ\u2060 which is a property of the material itself. The change in intensity of a light beam due to absorption as it traverses a small distance ds will then be\n{\\displaystyle dI_{\\nu }=-\\kappa _{\\nu }\\rho I_{\\nu }\\,ds}\nThe "mass emission coefficient" jν is equal to the radiance per unit volume of a small volume element divided by its mass (since, as for the mass absorption coefficient, the emission is proportional to the emitting mass) and has units of power⋅solid angle−1⋅frequency−1⋅density−1. Like the mass absorption coefficient, it too is a property of the material itself. The change in a light beam as it traverses a small distance ds will then be\n{\\displaystyle dI_{\\nu }=j_{\\nu }\\rho \\,ds}\nThe equation of radiative transfer will then be the sum of these two contributions:\n{\\displaystyle {\\frac {dI_{\\nu }}{ds}}=j_{\\nu }\\rho -\\kappa _{\\nu }\\rho I_{\\nu }.}\nIf the radiation field is in equilibrium with the material medium, then the radiation will be homogeneous (independent of position) so that dIν = 0 and:\n{\\displaystyle \\kappa _{\\nu }B_{\\nu }=j_{\\nu }}\nwhich is another statement of Kirchhoff\'s law, relating two material properties of the medium, and which yields the radiative transfer equation at a point around which the medium is in thermodynamic equilibrium:\n{\\displaystyle {\\frac {dI_{\\nu }}{ds}}=\\kappa _{\\nu }\\rho (B_{\\nu }-I_{\\nu }).}\n=== Einstein coefficients ===\nThe principle of detailed balance states that, at thermodynamic equilibrium, each elementary process is in equilibrium with its reverse process.\nIn 1916, Albert Einstein applied this principle on an atomic level to the case of an atom radiating and absorbing radiation due to transitions between two particular energy levels, giving a deeper insight into the equation of radiative transfer and Kirchhoff\'s law for this type of radiation. If level 1 is the lower energy level with energy E1, and level 2 is the upper energy level with energy E2, then the frequency ν of the radiation radiated or absorbed will be determined by Bohr\'s frequency condition:\n{\\displaystyle E_{2}-E_{1}=h\\nu .}\nIf n1 and n2 are the number densities of the atom in states 1 and 2 respectively, then the rate of change of these densities in time will be due to three processes:\nSpontaneous emission\n21\n{\\displaystyle \\left({\\frac {dn_{1}}{dt}}\\right)_{\\mathrm {spon} }=A_{21}n_{2}}\nStimulated emission\n21\n{\\displaystyle \\left({\\frac {dn_{1}}{dt}}\\right)_{\\mathrm {stim} }=B_{21}n_{2}u_{\\nu }}\nPhoto-absorption\n12\n{\\displaystyle \\left({\\frac {dn_{2}}{dt}}\\right)_{\\mathrm {abs} }=B_{12}n_{1}u_{\\nu }}\nwhere uν is the spectral energy density of the radiation field. The three parameters A21, B21 and B12, known as the Einstein coefficients, are associated with the photon frequency ν produced by the transition between two energy levels (states). As a result, each line in a spectrum has its own set of associated coefficients. When the atoms and the radiation field are in equilibrium, the radiance will be given by Planck\'s law and, by the principle of detailed balance, the sum of these rates must be zero:\n21\n21\n12\n{\\displaystyle 0=A_{21}n_{2}+B_{21}n_{2}{\\frac {4\\pi }{c}}B_{\\nu }(T)-B_{12}n_{1}{\\frac {4\\pi }{c}}B_{\\nu }(T)}\nSince the atoms are also in equilibrium, the populations of the two levels are related by the Boltzmann factor:\n{\\displaystyle {\\frac {n_{2}}{n_{1}}}={\\frac {g_{2}}{g_{1}}}e^{-h\\nu /k_{\\mathrm {B} }T}}\nwhere g1 and g2 are the multiplicities of the respective energy levels. Combining the above two equations with the requirement that they be valid at any temperature yields two relationships between the Einstein coefficients:\n21\n21\n{\\displaystyle {\\frac {A_{21}}{B_{21}}}={\\frac {8\\pi h\\nu ^{3}}{c^{3}}}}\n21\n12\n{\\displaystyle {\\frac {B_{21}}{B_{12}}}={\\frac {g_{1}}{g_{2}}}}\nso that knowledge of one coefficient will yield the other two.\nFor the case of isotropic absorption and emission, the emission coefficient (jν) and absorption coefficient (κν) defined in the radiative transfer section above, can be expressed in terms of the Einstein coefficients. The relationships between the Einstein coefficients will yield the expression of Kirchhoff\'s law expressed in the Radiative transfer section above, namely that\n{\\displaystyle j_{\\nu }=\\kappa _{\\nu }B_{\\nu }.}\nThese coefficients apply to both atoms and molecules.\n== Properties ==\n=== Peaks ===\nThe distributions Bν, Bω, Bν̃ and Bk peak at a photon energy of\n2.821\n{\\displaystyle E=\\left[3+W\\left(-3e^{-3}\\right)\\right]k_{\\mathrm {B} }T\\approx 2.821\\ k_{\\mathrm {B} }T,}\nwhere W is the Lambert W function and e is Euler\'s number.\nHowever, the distribution Bλ peaks at a different energy\n4.965\n{\\displaystyle E=\\left[5+W\\left(-5e^{-5}\\right)\\right]k_{\\mathrm {B} }T\\approx 4.965\\ k_{\\mathrm {B} }T,}\nThe reason for this is that, as mentioned above, one cannot go from (for example) Bν to Bλ simply by substituting ν by λ. In addition, one must also multiply by\n{\\textstyle \\left|{d\\nu }/{d\\lambda }\\right|=c/{\\lambda ^{2}}}\n, which shifts the peak of the distribution to higher energies. These peaks are the mode energy of a photon, when binned using equal-size bins of frequency or wavelength, respectively. Dividing hc (14387.770 μm·K) by these energy expression gives the wavelength of the peak.\nThe spectral radiance at these peaks is given by:\nmax\n1.896\n10\n19\n{\\displaystyle {\\begin{aligned}B_{\\nu ,{\\text{max}}}(T)&={\\frac {2k_{\\mathrm {B} }^{3}T^{3}x^{3}}{h^{2}c^{2}}}{\\frac {1}{e^{x}-1}}\\\\&\\approx 1.896\\times 10^{-19}{\\frac {\\mathrm {W} }{\\mathrm {m^{2}\\cdot Hz\\cdot sr} }}\\times (T/\\mathrm {K} )^{3}\\\\\\end{aligned}}}\nwith\n{\\displaystyle x=3+W(-3e^{-3}),}\nand\nmax\n4.096\n10\nsr\n{\\displaystyle {\\begin{aligned}B_{\\lambda ,{\\text{max}}}(T)&={\\frac {2k_{\\mathrm {B} }^{5}T^{5}x^{5}}{h^{4}c^{3}}}{\\frac {1}{e^{x}-1}}\\\\&\\approx 4.096\\times 10^{-6}{\\frac {\\text{W}}{{\\text{m}}^{2}\\cdot {\\text{sr}}}}\\times ~(T/{\\text{K}})^{5}\\end{aligned}}}\nwith\n{\\displaystyle x=5+W(-5e^{-5}).}\nMeanwhile, the average energy of a photon from a blackbody is\n30\n2.701\n{\\displaystyle E=\\left[{\\frac {\\pi ^{4}}{30\\ \\zeta (3)}}\\right]k_{\\mathrm {B} }T\\approx 2.701\\ k_{\\mathrm {B} }T,}\nwhere\n{\\displaystyle \\zeta }\nis the Riemann zeta function.\n=== Approximations ===\nIn the limit of low frequencies (i.e. long wavelengths), Planck\'s law becomes the Rayleigh–Jeans law\n{\\displaystyle B_{\\nu }(T)\\approx {\\frac {2\\nu ^{2}}{c^{2}}}k_{\\mathrm {B} }T}\nor\n{\\displaystyle B_{\\lambda }(T)\\approx {\\frac {2c}{\\lambda ^{4}}}k_{\\mathrm {B} }T}\nThe radiance increases as the square of the frequency, illustrating the ultraviolet catastrophe. In the limit of high frequencies (i.e. small wavelengths) Planck\'s law tends to the Wien approximation:\n{\\displaystyle B_{\\nu }(T)\\approx {\\frac {2h\\nu ^{3}}{c^{2}}}e^{-{\\frac {h\\nu }{k_{\\mathrm {B} }T}}}}\nor\n{\\displaystyle B_{\\lambda }(T)\\approx {\\frac {2hc^{2}}{\\lambda ^{5}}}e^{-{\\frac {hc}{\\lambda k_{\\mathrm {B} }T}}}.}\n=== Percentiles ===\nWien\'s displacement law in its stronger form states that the shape of Planck\'s law is independent of temperature. It is therefore possible to list the percentile points of the total radiation as well as the peaks for wavelength and frequency, in a form which gives the wavelength λ when divided by temperature T. The second column of the following table lists the corresponding values of λT, that is, those values of x for which the wavelength λ is \u2060x/T\u2060 micrometers at the radiance percentile point given by the corresponding entry in the first column.\nThat is, 0.01% of the radiation is at a wavelength below \u2060910/T\u2060 μm, 20% below \u20602676/T\u2060 μm, etc. The wavelength and frequency peaks are in bold and occur at 25.0% and 64.6% respectively. The 41.8% point is the wavelength-frequency-neutral peak (i.e. the peak in power per unit change in logarithm of wavelength or frequency). These are the points at which the respective Planck-law functions \u20601/λ5\u2060, ν3 and \u2060ν2/λ2\u2060, respectively, divided by exp(\u2060hν/kBT\u2060) − 1 attain their maxima. The much smaller gap in ratio of wavelengths between 0.1% and 0.01% (1110 is 22% more than 910) than between 99.9% and 99.99% (113374 is 120% more than 51613) reflects the exponential decay of energy at short wavelengths (left end) and polynomial decay at long.']

Question: What is reciprocal length or inverse length?

Choices:
Choice A) Reciprocal length or inverse length is a quantity or measurement used in physics and chemistry. It is the reciprocal of time, and common units used for this measurement include the reciprocal second or inverse second (symbol: s−1), the reciprocal minute or inverse minute (symbol: min−1).
Choice B) Reciprocal length or inverse length is a quantity or measurement used in geography and geology. It is the reciprocal of area, and common units used for this measurement include the reciprocal square metre or inverse square metre (symbol: m−2), the reciprocal square kilometre or inverse square kilometre (symbol: km−2).
Choice C) Reciprocal length or inverse length is a quantity or measurement used in biology and medicine. It is the reciprocal of mass, and common units used for this measurement include the reciprocal gram or inverse gram (symbol: g−1), the reciprocal kilogram or inverse kilogram (symbol: kg−1).
Choice D) Reciprocal length or inverse length is a quantity or measurement used in economics and finance. It is the reciprocal of interest rate, and common units used for this measurement include the reciprocal percent or inverse percent (symbol: %−1), the reciprocal basis point or inverse basis point (symbol: bp−1).
Choice E) Reciprocal length or inverse length is a quantity or measurement used in several branches of science and mathematics. It is the reciprocal of length, and common units used for this measurement include the reciprocal metre or inverse metre (symbol: m−1), the reciprocal centimetre or inverse centimetre (symbol: cm−1).

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['Fischer–Tropsch process', 'The Fischer–Tropsch process (FT) is a collection of chemical reactions that converts a mixture of carbon monoxide and hydrogen, known as syngas, into liquid hydrocarbons. These reactions occur in the presence of metal catalysts, typically at temperatures of 150–300 °C (302–572 °F) and pressures of one to several tens of atmospheres. The Fischer–Tropsch process is an important reaction in both coal liquefaction and gas to liquids technology for producing liquid hydrocarbons.\nIn the usual implementation, carbon monoxide and hydrogen, the feedstocks for FT, are produced from coal, natural gas, or biomass in a process known as gasification. The process then converts these gases into synthetic lubrication oil and synthetic fuel. This process has received intermittent attention as a source of low-sulfur diesel fuel and to address the supply or cost of petroleum-derived hydrocarbons. Fischer–Tropsch process is discussed as a step of producing carbon-neutral liquid hydrocarbon fuels from CO2 and hydrogen.\nThe process was first developed by Franz Fischer and Hans Tropsch at the Kaiser Wilhelm Institute for Coal Research in Mülheim an der Ruhr, Germany, in 1925.\n== Reaction mechanism ==\nThe Fischer–Tropsch process involves a series of chemical reactions that produce a variety of hydrocarbons, ideally having the formula (CnH2n+2). The more useful reactions produce alkanes as follows:\n(2n + 1) H2 + n CO → CnH2n+2 + n H2O\nwhere n is typically 10–20, resulting mostly in the formation of higher alkanes. The formation of methane (n = 1) is unwanted. Most of the alkanes produced tend to be straight-chain, suitable as diesel fuel. In addition to alkane formation, competing reactions give small amounts of alkenes, as well as alcohols and other oxygenated hydrocarbons.\nThe reaction is a highly exothermic reaction due to a standard reaction enthalpy (ΔH) of −165\u202fkJ/mol CO combined.\n=== Fischer–Tropsch intermediates and elemental reactions ===\nConverting a mixture of H2 and CO into aliphatic products is a multi-step reaction with several intermediate compounds. The growth of the hydrocarbon chain may be visualized as involving a repeated sequence in which hydrogen atoms are added to carbon and oxygen, the C–O bond is split and a new C–C bond is formed.\nFor one –CH2– group produced by CO + 2 H2 → (CH2) + H2O, several reactions are necessary:\nAssociative adsorption of CO\nSplitting of the C–O bond\nDissociative adsorption of 2 H2\nTransfer of 2 H to the oxygen to yield H2O\nDesorption of H2O\nTransfer of 2 H to the carbon to yield CH2\nThe conversion of CO to alkanes involves hydrogenation of CO, the hydrogenolysis (cleavage with H2) of C–O bonds, and the formation of C–C bonds. Such reactions are assumed to proceed via initial formation of surface-bound metal carbonyls. The CO ligand is speculated to undergo dissociation, possibly into oxide and carbide ligands. Other potential intermediates are various C1 fragments including formyl (CHO), hydroxycarbene (HCOH), hydroxymethyl (CH2OH), methyl (CH3), methylene (CH2), methylidyne (CH), and hydroxymethylidyne (COH). Furthermore, and critical to the production of liquid fuels, are reactions that form C–C bonds, such as migratory insertion. Many related stoichiometric reactions have been simulated on discrete metal clusters, but homogeneous Fischer–Tropsch catalysts are of no commercial importance.\nAddition of isotopically labelled alcohol to the feed stream results in incorporation of alcohols into product. This observation establishes the facility of C–O bond scission. Using 14C-labelled ethylene and propene over cobalt catalysts results in incorporation of these olefins into the growing chain. Chain growth reaction thus appears to involve both \'olefin insertion\' as well as \'CO-insertion\'.\nCO\n17\n18\n{\\displaystyle {\\ce {8 CO + 17 H2 -> C8H18 + 8 H2O}}}\n== Feedstocks: gasification ==\nFischer–Tropsch plants associated with biomass or coal or related solid feedstocks (sources of carbon) must first convert the solid fuel into gases. These gases include CO, H2, and alkanes. This conversion is called gasification. Synthesis gas ("syngas") is obtained from biomass/coal gasification is a mixture of hydrogen and carbon monoxide. The H2:CO ratio is adjusted using the water-gas shift reaction. Coal-based FT plants produce varying amounts of CO2, depending upon the energy source of the gasification process. However, most coal-based plants rely on the feed coal to supply all the energy requirements of the process.\n=== Feedstocks: GTL ===\nCarbon monoxide for FT catalysis is derived from hydrocarbons. In gas to liquids (GTL) technology, the hydrocarbons are low molecular weight materials that often would be discarded or flared. Stranded gas provides relatively cheap gas. For GTL to be commercially viable, gas must remain relatively cheaper than oil.\nSeveral reactions are required to obtain the gaseous reactants required for FT catalysis. First, reactant gases entering a reactor must be desulfurized. Otherwise, sulfur-containing impurities deactivate ("poison") the catalysts required for FT reactions.\nSeveral reactions are employed to adjust the H2:CO ratio. Most important is the water-gas shift reaction, which provides a source of hydrogen at the expense of carbon monoxide:\nCO\nCO\n{\\displaystyle {\\ce {H2O + CO -> H2 + CO2}}}\nFor FT plants that use methane as the feedstock, another important reaction is dry reforming, which converts the methane into CO and H2:\nCH\nCO\nCO\n{\\displaystyle {\\ce {CH4 + CO2 -> 2CO + 2H2}}}\n=== Process conditions ===\nGenerally, the Fischer–Tropsch process is operated in the temperature range of 150–300 °C (302–572 °F). Higher temperatures lead to faster reactions and higher conversion rates but also tend to favor methane production. For this reason, the temperature is usually maintained at the low to middle part of the range. Increasing the pressure leads to higher conversion rates and also favors the formation of long-chained alkanes, both of which are desirable. Typical pressures range from one to several tens of atmospheres. Even higher pressures would be favorable, but the benefits may not justify the additional costs of high-pressure equipment, and higher pressures can lead to catalyst deactivation via coke formation.\nA variety of synthesis-gas compositions can be used. For cobalt-based catalysts the optimal H2:CO ratio is around 1.8–2.1. Iron-based catalysts can tolerate lower ratios, due to their intrinsic water-gas shift reaction activity. This reactivity can be important for synthesis gas derived from coal or biomass, which tend to have relatively low H2:CO ratios (< 1).\n=== Design of the Fischer–Tropsch process reactor ===\nEfficient removal of heat from the reactor is the basic need of FT reactors since these reactions are characterized by high exothermicity. Four types of reactors are discussed:\n==== Multi tubular fixed-bed reactor ====\nThis type of reactor contains several tubes with small diameters. These tubes contain catalysts and are surrounded by cooling water which removes the heat of the reaction. A fixed-bed reactor is suitable for operation at low temperatures and has an upper-temperature limit of 257 °C (530 K). Excess temperature leads to carbon deposition and hence blockage of the reactor. Since large amounts of the products formed are in liquid state, this type of reactor can also be referred to as a trickle flow reactor system.\n==== Entrained flow reactor ====\nThis type of reactor contains two banks of heat exchangers which remove heat; the remainder of which is removed by the products and recycled in the system. The formation of heavy waxes should be avoided, since they condense on the catalyst and form agglomerations. This leads to fluidization. Hence, risers are operated over 297 °C (570 K).\n==== Slurry reactors ====\nHeat removal is done by internal cooling coils. The synthesis gas is bubbled through the waxy products and finely-divided catalyst which is suspended in the liquid medium. This also provides agitation of the contents of the reactor. The catalyst particle size reduces diffusional heat and mass transfer limitations. A lower temperature in the reactor leads to a more viscous product and a higher temperature (> 297 °C, 570 K) gives an undesirable product spectrum. Also, separation of the product from the catalyst is a problem.\n==== Fluid-bed and circulating catalyst (riser) reactors ====', 'Photosynthesis', '=== Progenitor ===\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic type Ic-BL events that are associated with long-duration gamma-ray bursts.\n== External impact ==\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\n=== Source of heavy elements ===\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\n=== Role in stellar evolution ===\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\n=== Cosmic rays ===\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\n=== Gravitational waves ===', 'These pigments are embedded in plants and algae in complexes called antenna proteins. In such proteins, the pigments are arranged to work together. Such a combination of proteins is also called a light-harvesting complex.\nAlthough all cells in the green parts of a plant have chloroplasts, the majority of those are found in specially adapted structures called leaves. Certain species adapted to conditions of strong sunlight and aridity, such as many Euphorbia and cactus species, have their main photosynthetic organs in their stems. The cells in the interior tissues of a leaf, called the mesophyll, can contain between 450,000 and 800,000 chloroplasts for every square millimeter of leaf. The surface of the leaf is coated with a water-resistant waxy cuticle that protects the leaf from excessive evaporation of water and decreases the absorption of ultraviolet or blue light to minimize heating. The transparent epidermis layer allows light to pass through to the palisade mesophyll cells where most of the photosynthesis takes place.\n== Light-dependent reactions ==\nIn the light-dependent reactions, one molecule of the pigment chlorophyll absorbs one photon and loses one electron. This electron is taken up by a modified form of chlorophyll called pheophytin, which passes the electron to a quinone molecule, starting the flow of electrons down an electron transport chain that leads to the ultimate reduction of NADP to NADPH. In addition, this creates a proton gradient (energy gradient) across the chloroplast membrane, which is used by ATP synthase in the synthesis of ATP. The chlorophyll molecule ultimately regains the electron it lost when a water molecule is split in a process called photolysis, which releases oxygen.\nThe overall equation for the light-dependent reactions under the conditions of non-cyclic electron flow in green plants is:\nNot all wavelengths of light can support photosynthesis. The photosynthetic action spectrum depends on the type of accessory pigments present. For example, in green plants, the action spectrum resembles the absorption spectrum for chlorophylls and carotenoids with absorption peaks in violet-blue and red light. In red algae, the action spectrum is blue-green light, which allows these algae to use the blue end of the spectrum to grow in the deeper waters that filter out the longer wavelengths (red light) used by above-ground green plants. The non-absorbed part of the light spectrum is what gives photosynthetic organisms their color (e.g., green plants, red algae, purple bacteria) and is the least effective for photosynthesis in the respective organisms.\n=== Z scheme ===\nIn plants, light-dependent reactions occur in the thylakoid membranes of the chloroplasts where they drive the synthesis of ATP and NADPH. The light-dependent reactions are of two forms: cyclic and non-cyclic.\nIn the non-cyclic reaction, the photons are captured in the light-harvesting antenna complexes of photosystem II by chlorophyll and other accessory pigments (see diagram "Z-scheme"). The absorption of a photon by the antenna complex loosens an electron by a process called photoinduced charge separation. The antenna system is at the core of the chlorophyll molecule of the photosystem II reaction center. That loosened electron is taken up by the primary electron-acceptor molecule, pheophytin. As the electrons are shuttled through an electron transport chain (the so-called Z-scheme shown in the diagram), a chemiosmotic potential is generated by pumping proton cations (H+) across the membrane and into the thylakoid space. An ATP synthase enzyme uses that chemiosmotic potential to make ATP during photophosphorylation, whereas NADPH is a product of the terminal redox reaction in the Z-scheme. The electron enters a chlorophyll molecule in Photosystem I. There it is further excited by the light absorbed by that photosystem. The electron is then passed along a chain of electron acceptors to which it transfers some of its energy. The energy delivered to the electron acceptors is used to move hydrogen ions across the thylakoid membrane into the lumen. The electron is eventually used to reduce the coenzyme NADP with an H+ to NADPH (which has functions in the light-independent reaction); at that point, the path of that electron ends.\nThe cyclic reaction is similar to that of the non-cyclic but differs in that it generates only ATP, and no reduced NADP (NADPH) is created. The cyclic reaction takes place only at photosystem I. Once the electron is displaced from the photosystem, the electron is passed down the electron acceptor molecules and returns to photosystem I, from where it was emitted, hence the name cyclic reaction.\n=== Water photolysis ===\nLinear electron transport through a photosystem will leave the reaction center of that photosystem oxidized. Elevating another electron will first require re-reduction of the reaction center. The excited electrons lost from the reaction center (P700) of photosystem I are replaced by transfer from plastocyanin, whose electrons come from electron transport through photosystem II. Photosystem II, as the first step of the Z-scheme, requires an external source of electrons to reduce its oxidized chlorophyll a reaction center. The source of electrons for photosynthesis in green plants and cyanobacteria is water. Two water molecules are oxidized by the energy of four successive charge-separation reactions of photosystem II to yield a molecule of diatomic oxygen and four hydrogen ions. The electrons yielded are transferred to a redox-active tyrosine residue that is oxidized by the energy of P680+. This resets the ability of P680 to absorb another photon and release another photo-dissociated electron. The oxidation of water is catalyzed in photosystem II by a redox-active structure that contains four manganese ions and a calcium ion; this oxygen-evolving complex binds two water molecules and contains the four oxidizing equivalents that are used to drive the water-oxidizing reaction (Kok\'s S-state diagrams). The hydrogen ions are released in the thylakoid lumen and therefore contribute to the transmembrane chemiosmotic potential that leads to ATP synthesis. Oxygen is a waste product of light-dependent reactions, but the majority of organisms on Earth use oxygen and its energy for cellular respiration, including photosynthetic organisms.\n== Light-independent reactions ==\n=== Calvin cycle ===\nIn the light-independent (or "dark") reactions, the enzyme RuBisCO captures CO2 from the atmosphere and, in a process called the Calvin cycle, uses the newly formed NADPH and releases three-carbon sugars, which are later combined to form sucrose and starch. The overall equation for the light-independent reactions in green plants is:\u200a128\nCarbon fixation produces the three-carbon sugar intermediate, which is then converted into the final carbohydrate products. The simple carbon sugars photosynthesis produces are then used to form other organic compounds, such as the building material cellulose, the precursors for lipid and amino acid biosynthesis, or as a fuel in cellular respiration. The latter occurs not only in plants but also in animals when the carbon and energy from plants is passed through a food chain.\nThe fixation or reduction of carbon dioxide is a process in which carbon dioxide combines with a five-carbon sugar, ribulose 1,5-bisphosphate, to yield two molecules of a three-carbon compound, glycerate 3-phosphate, also known as 3-phosphoglycerate. Glycerate 3-phosphate, in the presence of ATP and NADPH produced during the light-dependent stages, is reduced to glyceraldehyde 3-phosphate. This product is also referred to as 3-phosphoglyceraldehyde (PGAL) or, more generically, as triose phosphate. Most (five out of six molecules) of the glyceraldehyde 3-phosphate produced are used to regenerate ribulose 1,5-bisphosphate so the process can continue. The triose phosphates not thus "recycled" often condense to form hexose phosphates, which ultimately yield sucrose, starch, and cellulose, as well as glucose and fructose. The sugars produced during carbon metabolism yield carbon skeletons that can be used for other metabolic reactions like the production of amino acids and lipids.\n=== Carbon concentrating mechanisms ===\n==== On land ====\nIn hot and dry conditions, plants close their stomata to prevent water loss. Under these conditions, CO2 will decrease and oxygen gas, produced by the light reactions of photosynthesis, will increase, causing an increase of photorespiration by the oxygenase activity of ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) and decrease in carbon fixation. Some plants have evolved mechanisms to increase the CO2 concentration in the leaves under these conditions.', 'Star formation is the process by which dense regions within molecular clouds in interstellar space—sometimes referred to as "stellar nurseries" or "star-forming regions"—collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products.  It is closely related to planet formation, another branch of astronomy.  Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred  as star clusters or stellar associations.\n== First stars ==\nStar formation is divided into three groups called "Populations". Population III stars formed from primordial hydrogen after the Big Bang. These stars are poorly understood but should contain only hydrogen and helium. Population II stars formed from the debris of the first stars and they in turn created more higher atomic number chemical elements. Population I stars are young metal-rich (contain elements other than hydrogen and helium) stars like our Sun.\nThe initial star formation was driven by gravitational attraction of hydrogen local areas of higher gravity called dark matter halos. As the hydrogen lost energy through atomic or molecular energy transitions, the temperature of local clumps fell allowing more gravitational condensation. Eventually the process leads to collapse in to a start. Details of the dynamics of the Population III stars is now believe to be as complex as star formation today.\n== Stellar nurseries ==\n=== Interstellar clouds ===\nSpiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae, where star formation takes place. In contrast to spiral galaxies, elliptical galaxies lose the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through  mergers with other galaxies.\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds. The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.\nObservations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth\'s sun. The average interior temperature is 10 K (−441.7 °F).\nAbout half the total mass of the Milky Way\'s galactic ISM is found in molecular clouds and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉. The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away. However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light-year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\n=== Cloud collapse ===\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that,  to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. During cloud collapse dozens to tens of thousands of stars form more or less simultaneously which is observable in so-called embedded clusters. The end product of a core collapse is an  open cluster of stars.\nIn triggered star formation, one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.)  Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy.  As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n== Protostar ==\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of 60–100 K, and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about 10−13 g / cm3. A core region, called the first hydrostatic core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\nWhen the core temperature reaches about 2000 K, the thermal energy dissociates the H2 molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10−8 g / cm3, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.', 'Stellar evolution is the process by which a star changes over the course of time. Depending on the mass of the star, its lifetime can range from a few million years for the most massive to trillions of years for the least massive, which is considerably longer than the current age of the universe. The table shows the lifetimes of stars as a function of their masses. All stars are formed from collapsing clouds of gas and dust, often called nebulae or molecular clouds.  Over the course of millions of years, these protostars settle down into a state of equilibrium, becoming what is known as a main-sequence star.\nNuclear fusion powers a star for most of its existence. Initially the energy is generated by the fusion of hydrogen atoms at the core of the main-sequence star. Later, as the preponderance of atoms at the core becomes helium, stars like the Sun begin to fuse hydrogen along a spherical shell surrounding the core. This process causes the star to gradually grow in size, passing through the subgiant stage until it reaches the red-giant phase. Stars with at least half the mass of the Sun can also begin to generate energy through the fusion of helium at their core, whereas more-massive stars can fuse heavier elements along a series of concentric shells. Once a star like the Sun has exhausted its nuclear fuel, its core collapses into a dense white dwarf and the outer layers are expelled as a planetary nebula. Stars with around ten or more times the mass of the Sun can explode in a supernova as their inert iron cores collapse into an extremely dense neutron star or black hole. Although the universe is not old enough for any of the smallest red dwarfs to have reached the end of their existence, stellar models suggest they will slowly become brighter and hotter before running out of hydrogen fuel and becoming low-mass white dwarfs.\nStellar evolution is not studied by observing the life of a single star, as most stellar changes occur too slowly to be detected, even over many centuries. Instead, astrophysicists come to understand how stars evolve by observing numerous stars at various points in their lifetime, and by simulating stellar structure using computer models.\n== Star formation ==\n=== Protostar ===\nStellar evolution starts with the gravitational collapse of a giant molecular cloud. Typical giant molecular clouds are roughly 100 light-years (9.5×1014 km) across and contain up to 6,000,000 solar masses (1.2×1037 kg). As it collapses, a giant molecular cloud breaks into smaller and smaller pieces. In each of these fragments, the collapsing gas releases gravitational potential energy as heat. As its temperature and pressure increase, a fragment condenses into a rotating ball of superhot gas known as a protostar.  Filamentary structures are truly ubiquitous in the molecular cloud. Dense molecular filaments will fragment into gravitationally bound cores, which are the precursors of stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed fragmentation manner of the filaments. In supercritical filaments, observations have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded two protostars with gas outflows.\nA protostar continues to grow by accretion of gas and dust from the molecular cloud, becoming a pre-main-sequence star as it reaches its final mass. Further development is determined by its mass. Mass is typically compared to the mass of the Sun: 1.0 M☉ (2.0×1030 kg) means 1 solar mass.\nProtostars are encompassed in dust, and are thus more readily visible at infrared wavelengths.\nObservations from the Wide-field Infrared Survey Explorer (WISE) have been especially important for unveiling numerous galactic protostars and their parent star clusters.\n=== Brown dwarfs and sub-stellar objects ===\nProtostars with masses less than roughly 0.08 M☉ (1.6×1029 kg) never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (MJ), 2.5 × 1028 kg, or 0.0125 M☉). Objects smaller than 13 MJ are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets). Both types, deuterium-burning and not, shine dimly and fade away slowly, cooling gradually over hundreds of millions of years.\n=== Main sequence stellar mass objects ===\nFor a more-massive protostar, the core temperature will eventually reach 10 million kelvin, initiating the proton–proton chain reaction and allowing hydrogen to fuse, first to deuterium and then to helium. In stars of slightly over 1 M☉ (2.0×1030 kg), the carbon–nitrogen–oxygen fusion reaction (CNO cycle) contributes a large portion of the energy generation. The onset of nuclear fusion leads relatively quickly to a hydrostatic equilibrium in which energy released by the core maintains a high gas pressure, balancing the weight of the star\'s matter and preventing further gravitational collapse. The star thus evolves rapidly to a stable state, beginning the main-sequence phase of its evolution.\nA new star will sit at a specific point on the main sequence of the Hertzsprung–Russell diagram, with the main-sequence spectral type depending upon the mass of the star. Small, relatively cold, low-mass red dwarfs fuse hydrogen slowly and will remain on the main sequence for hundreds of billions of years or longer, whereas massive, hot O-type stars will leave the main sequence after just a few million years. A mid-sized yellow dwarf star, like the Sun, will remain on the main sequence for about 10 billion years. The Sun is thought to be in the middle of its main sequence lifespan.\n=== Planetary system ===\nA star may gain a protoplanetary disk, which furthermore can develop into a planetary system.\n== Mature stars ==\nEventually the star\'s core exhausts its supply of hydrogen and the star begins to evolve off the main sequence. Without the outward radiation pressure generated by the fusion of hydrogen to counteract the force of gravity, the core contracts until either electron degeneracy pressure becomes sufficient to oppose gravity or the core becomes hot enough (around 100 MK) for helium fusion to begin. Which of these happens first depends upon the star\'s mass.\n=== Low-mass stars ===\nWhat happens after a low-mass star ceases to produce energy through fusion has not been directly observed; the universe is around 13.8 billion years old, which is less time (by several orders of magnitude, in some cases) than it takes for fusion to cease in such stars.\nRecent astrophysical models suggest that red dwarfs of 0.1 M☉ may stay on the main sequence for some six to twelve trillion years, gradually increasing in both temperature and luminosity, and take several hundred billion years more to collapse, slowly, into a white dwarf.  Such stars will not become red giants as the whole star is a convection zone and it will not develop a degenerate helium core with a shell burning hydrogen.  Instead, hydrogen fusion will proceed until almost the whole star is helium.\nSlightly more massive stars do expand into red giants, but their helium cores are not massive enough to reach the temperatures required for helium fusion so they never reach the tip of the red-giant branch.  When hydrogen shell burning finishes, these stars move directly off the red-giant branch like a post-asymptotic-giant-branch (AGB) star, but at lower luminosity, to become a white dwarf.  A star with an initial mass about 0.6 M☉ will be able to reach temperatures high enough to fuse helium, and these "mid-sized" stars go on to further stages of evolution beyond the red-giant branch.\n=== Mid-sized stars ===\nStars of roughly 0.6–10 M☉ become red giants, which are large non-main-sequence stars of stellar classification K or M. Red giants lie along the right edge of the Hertzsprung–Russell diagram due to their red color and large luminosity. Examples include Aldebaran in the constellation Taurus and Arcturus in the constellation of Boötes.\nMid-sized stars are red giants during two different phases of their post-main-sequence evolution: red-giant-branch stars, with inert cores made of helium and hydrogen-burning shells, and asymptotic-giant-branch stars, with inert cores made of carbon and helium-burning shells inside the hydrogen-burning shells.  Between these two phases, stars spend a period on the horizontal branch with a helium-fusing core.  Many of these helium-fusing stars cluster towards the cool end of the horizontal branch as K-type giants and are referred to as red clump giants.\n==== Subgiant phase ====\nWhen a star exhausts the hydrogen in its core, it leaves the main sequence and begins to fuse hydrogen in a shell outside the core.  The core increases in mass as the shell produces more helium.  Depending on the mass of the helium core, this continues for several million to one or two billion years, with the star expanding and cooling at a similar or slightly lower luminosity to its main sequence state.  Eventually either the core becomes degenerate, in stars around the mass of the sun, or the outer layers cool sufficiently to become opaque, in more massive stars.  Either of these changes cause the hydrogen shell to increase in temperature and the luminosity of the star to increase, at which point the star expands onto the red-giant branch.\n==== Red-giant-branch phase ====', "== Importance ==\nFrom the perspective of a planetary geologist, the atmosphere acts to shape a planetary surface. Wind picks up dust and other particles which, when they collide with the terrain, erode the relief and leave deposits (eolian processes). Frost and precipitations, which depend on the atmospheric composition, also influence the relief. Climate changes can influence a planet's geological history. Conversely, studying the surface of the Earth leads to an understanding of the atmosphere and climate of other planets.\nFor a meteorologist, the composition of the Earth's atmosphere is a factor affecting the climate and its variations.\nFor a biologist or paleontologist, the Earth's atmospheric composition is closely dependent on the appearance of life and its evolution.\n== See also ==\nAtmometer (evaporimeter)\nAtmospheric pressure\nInternational Standard Atmosphere\nKármán line\nSky\n== References ==\n== Further reading ==\nSanchez-Lavega, Agustin (2010). An Introduction to Planetary Atmospheres. Taylor & Francis. ISBN 978-1420067323.\n== External links ==\nProperties of atmospheric strata – The flight environment of the atmosphere\nAtmosphere – Everything you need to know", 'Maxwell\'s demon is a thought experiment that appears to disprove the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867. In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".\nIn the thought experiment, a demon controls a door between two chambers containing gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon\'s actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, seemingly without applying any work, thereby violating the second law of thermodynamics.\nThe concept of Maxwell\'s demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no device can violate the second law in this way. Other researchers have implemented forms of Maxwell\'s demon in experiments, though they all differ from the thought experiment to some extent and none has been shown to violate the second law.\n== Origin and history of the idea ==\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell\'s 1872 book on thermodynamics titled Theory of Heat.\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a "finite being". Being a deeply religious man, he never used the word "demon". Instead, William Thomson (Lord Kelvin) was the first to use it for Maxwell\'s concept, in the journal Nature in 1874, and implied that he intended the Greek mythology interpretation of a daemon, a supernatural being working in the background, rather than a malevolent being.\n== Original thought experiment ==\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.In other words, Maxwell imagines one container divided into two parts, A and B. Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics.  A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.\n== Criticism and development ==\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must "generate" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between A and B than the amount of energy gained by the difference of temperature caused by doing so.\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin.  Szilárd pointed out that a real-life Maxwell\'s demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\nIn 1960, Rolf Landauer raised an exception to this argument.  He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these "reversible" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between entropy in thermodynamics and information theory, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy, but the demon cannot store it indefinitely. In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard\'s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons.  Regarding Landauer\'s principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz et al. in 2012. Furthermore, Lutz et al. confirmed that in order to approach the Landauer\'s limit, the system must asymptotically approach zero processing speed.\nRecently, Landauer\'s principle has also been invoked to resolve an apparently unrelated paradox of statistical physics, Loschmidt’s paradox.\nJohn Earman and John D. Norton have argued that Szilárd and Landauer\'s explanations of Maxwell\'s demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton\'s argument, while maintaining that Landauer\'s principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n== Recent progress ==', 'Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.\nWith the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away.\n== Designations ==\nThe concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were given names, particularly with Arabic or Latin designations.\nAs well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)\nCirca 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star\'s right ascension was invented and added to John Flamsteed\'s star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering.\nThe internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice.\n== Units of measurement ==\nAlthough stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:\nThe solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass\ntogether (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:\nThe nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.\nLarge lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.\n== Formation and evolution ==\nStars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.\nAll stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:\nVery low mass stars, with masses below 0.5 M☉, are fully convective and distribute helium evenly throughout the whole star while on the main sequence. Therefore, they never undergo shell burning and never become red giants. After exhausting their hydrogen they become helium white dwarfs and slowly cool. As the lifetime of 0.5 M☉ stars is longer than the age of the universe, no such star has yet reached the white dwarf stage.\nLow mass stars (including the Sun), with a mass between 0.5 M☉ and ~2.25 M☉ depending on composition, do become red giants as their core hydrogen is depleted and they begin to burn helium in core in a helium flash; they develop a degenerate carbon-oxygen core later on the asymptotic giant branch; they finally blow off their outer shell as a planetary nebula and leave behind their core in the form of a white dwarf.\nIntermediate-mass stars, between ~2.25 M☉ and ~8 M☉, pass through evolutionary stages similar to low mass stars, but after a relatively short period on the red-giant branch they ignite helium without a flash and spend an extended period in the red clump before forming a degenerate carbon-oxygen core.\nMassive stars generally have a minimum mass of ~8 M☉. After exhausting the hydrogen at the core these stars become supergiants and go on to fuse elements heavier than helium. Many end their lives when their cores collapse and they explode as supernovae.\n=== Star formation ===\nThe formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.\nAs the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf.\nEarly stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.\nThese jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.\nEarly in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.']

Question: What is the role of methane in Fischer-Tropsch processes?

Choices:
Choice A) Methane is partially converted to carbon monoxide for utilization in Fischer-Tropsch processes.
Choice B) Methane is used as a catalyst in Fischer-Tropsch processes.
Choice C) Methane is not used in Fischer-Tropsch processes.
Choice D) Methane is fully converted to carbon monoxide for utilization in Fischer-Tropsch processes.
Choice E) Methane is a byproduct of Fischer-Tropsch processes.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Given the following question, context, and choices, carefully analyze each choice to determine which one is correct. Break down the problem step by step:

Context: ['{\\displaystyle L_{z}=m_{\\ell }\\hbar }\nThe values of mℓ range from −ℓ to ℓ, with integer intervals.\nThe s subshell (ℓ = 0) contains only one orbital, and therefore the mℓ of an electron in an s orbital will always be 0. The p subshell (ℓ = 1) contains three orbitals, so the mℓ of an electron in a p orbital will be −1, 0, or 1. The d subshell (ℓ = 2) contains five orbitals, with mℓ values of −2, −1, 0, 1, and 2.\n=== Spin magnetic quantum number ===\nThe spin magnetic quantum number describes the intrinsic spin angular momentum of the electron within each orbital and gives the projection of the spin angular momentum S along the specified axis:\n{\\displaystyle S_{z}=m_{s}\\hbar }\nIn general, the values of ms range from −s to s, where s is the spin quantum number, associated with the magnitude of particle\'s intrinsic spin angular momentum:\n{\\displaystyle m_{s}=-s,-s+1,-s+2,\\cdots ,s-2,s-1,s}\nAn electron state has spin number s = \u20601/2\u2060, consequently ms will be +\u20601/2\u2060 ("spin up") or −\u20601/2\u2060 "spin down" states. Since electron are fermions they obey the Pauli exclusion principle: each electron state must have different quantum numbers.  Therefore, every orbital will be occupied with at most two electrons, one for each spin state.\n=== The Aufbau principle and Hund\'s Rules ===\nA multi-electron atom can be modeled qualitatively as a hydrogen like atom with higher nuclear charge and correspondingly more electrons. The occupation of the electron states in such an atom can be predicted by the Aufbau principle and Hund\'s empirical rules for the quantum numbers.  The Aufbau principle fills orbitals based on their principal and azimuthal quantum numbers (lowest n + l first, with lowest n breaking ties; Hund\'s rule favors unpaired electrons in the outermost orbital). These rules are empirical but they can be related to electron physics.:\u200a10\u200a:\u200a260\n== Spin-orbit coupled systems ==\nWhen one takes the spin–orbit interaction into consideration, the L and S operators no longer commute with the Hamiltonian, and the eigenstates of the system no longer have well-defined orbital angular momentum and spin. Thus another set of quantum numbers should be used. This set includes\nThe total angular momentum quantum number:\n{\\displaystyle j=|\\ell \\pm s|,}\nwhich gives the total angular momentum through the relation\n{\\displaystyle J^{2}=\\hbar ^{2}j(j+1).}\nThe projection of the total angular momentum along a specified axis:\n{\\displaystyle m_{j}=-j,-j+1,-j+2,\\cdots ,j-2,j-1,j}\nanalogous to the above and satisfies both\n{\\displaystyle m_{j}=m_{\\ell }+m_{s},}\nand\n{\\displaystyle |m_{\\ell }+m_{s}|\\leq j.}\nParityThis is the eigenvalue under reflection: positive (+1) for states which came from even ℓ and negative (−1) for states which came from odd ℓ. The former is also known as even parity and the latter as odd parity, and is given by\n{\\displaystyle P=(-1)^{\\ell }.}\nFor example, consider the following 8 states, defined by their quantum numbers:\nThe quantum states in the system can be described as linear combination of these 8 states. However, in the presence of spin–orbit interaction, if one wants to describe the same system by 8 states that are eigenvectors of the Hamiltonian (i.e. each represents a state that does not mix with others over time), we should consider the following 8 states:\n== Atomic nuclei ==\nIn nuclei, the entire assembly of protons and neutrons (nucleons) has a resultant angular momentum due to the angular momenta of each nucleon, usually denoted I. If the total angular momentum of a neutron is jn = ℓ + s and for a proton is jp = ℓ + s (where s for protons and neutrons happens to be \u20601/2\u2060 again (see note)), then the nuclear angular momentum quantum numbers I are given by:\n{\\displaystyle I=|j_{n}-j_{p}|,|j_{n}-j_{p}|+1,|j_{n}-j_{p}|+2,\\cdots ,(j_{n}+j_{p})-2,(j_{n}+j_{p})-1,(j_{n}+j_{p})}\nNote: The orbital angular momenta of the nuclear (and atomic) states are all integer multiples of ħ while the intrinsic angular momentum of the neutron and  proton are half-integer multiples.  It should be immediately apparent that the combination of the intrinsic spins of the nucleons with their orbital motion will always give half-integer values for the total spin, I, of any odd-A nucleus and integer values for any even-A nucleus.\nParity with the number I is used to label nuclear angular momentum states, examples for some isotopes of hydrogen (H), carbon (C), and sodium (Na) are;\nThe reason for the unusual fluctuations in I, even by differences of just one nucleon, are due to the odd and even numbers of protons and neutrons – pairs of nucleons have a total angular momentum of zero (just like electrons in orbitals), leaving an odd or even number of unpaired nucleons. The property of nuclear spin is an important factor for the operation of NMR spectroscopy in organic chemistry, and MRI in nuclear medicine, due to the nuclear magnetic moment interacting with an external magnetic field.\n== Elementary particles ==\nElementary particles contain many quantum numbers which are usually said to be intrinsic to them. However, it should be understood that the elementary particles are quantum states of the standard model of particle physics, and hence the quantum numbers of these particles bear the same relation to the Hamiltonian of this model as the quantum numbers of the Bohr atom does to its Hamiltonian. In other words, each quantum number denotes a symmetry of the problem. It is more useful in quantum field theory to distinguish between spacetime and internal symmetries.\nTypical quantum numbers related to spacetime symmetries are spin (related to rotational symmetry), the parity, C-parity and T-parity (related to the Poincaré symmetry of spacetime). Typical internal symmetries are lepton number and baryon number or the electric charge. (For a full list of quantum numbers of this kind see the article on flavour.)\n== Multiplicative quantum numbers ==\nMost conserved quantum numbers are additive, so in an elementary particle reaction, the sum of the quantum numbers should be the same before and after the reaction. However, some, usually called a parity, are multiplicative; i.e., their product is conserved. All multiplicative quantum numbers belong to a symmetry (like parity) in which applying the symmetry transformation twice is equivalent to doing nothing (involution).\n== See also ==\nElectron configuration\n== References ==\n== Further reading ==\nDirac, Paul A. M. (1982). Principles of Quantum Mechanics. Oxford University Press. ISBN 0-19-852011-5.\nGriffiths, David J. (2004). Introduction to Quantum Mechanics (2nd ed.). Prentice Hall. ISBN 0-13-805326-X.\nHalzen, Francis & Martin, Alan D. (1984). Quarks and Leptons: An Introductory Course in Modern Particle Physics. John Wiley & Sons. ISBN 0-471-88741-2.\nEisberg, Robert Martin; Resnick, Robert (1985). Quantum Physics of Atoms, Molecules, Solids, Nuclei and Particles (2nd ed.). John Wiley & Sons. ISBN 978-0-471-87373-0 – via Internet Archive.', 'A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a "brief oscillation". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\nFor example, a wavelet could be created to have a frequency of middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. "Complementary" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.\n== Etymology ==\nThe word wavelet has been used for decades in digital signal processing and exploration geophysics. The equivalent French word ondelette meaning "small wave" was used by Jean Morlet and Alex Grossmann in the early 1980s.\n== Wavelet theory ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the shift and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the Lp function space L2(R) ). For instance the signal may be represented on every frequency band of the form [f, 2f] for all positive frequencies f > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in L2(R), the mother wavelet. For the example of the scale one frequency band [1, 2] this function is\nsinc\nsinc\nsin\nsin\n{\\displaystyle \\psi (t)=2\\,\\operatorname {sinc} (2t)-\\,\\operatorname {sinc} (t)={\\frac {\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}}}\nwith the (normalized) sinc function. That, Meyer\'s, and two other examples of mother wavelets are:\nThe subspace of scale a or frequency band [1/a, 2/a] is generated by the functions (sometimes called child wavelets)\n{\\displaystyle \\psi _{a,b}(t)={\\frac {1}{\\sqrt {a}}}\\psi \\left({\\frac {t-b}{a}}\\right),}\nwhere a is positive and defines the scale and b is any real number and defines the shift. The pair (a, b) defines a point in the right halfplane R+ × R.\nThe projection of a function x onto the subspace of scale a then has the form\n{\\displaystyle x_{a}(t)=\\int _{\\mathbb {R} }WT_{\\psi }\\{x\\}(a,b)\\cdot \\psi _{a,b}(t)\\,db}\nwith wavelet coefficients\n{\\displaystyle WT_{\\psi }\\{x\\}(a,b)=\\langle x,\\psi _{a,b}\\rangle =\\int _{\\mathbb {R} }x(t){\\psi _{a,b}(t)}\\,dt.}\nFor the analysis of the signal x, one can assemble the wavelet coefficients into a scaleogram of the signal.\nSee a list of some Continuous wavelets.\n=== Discrete wavelet transforms (discrete shift and scale parameters, continuous in time) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters a > 1, b > 0. The corresponding discrete subset of the halfplane consists of all the points (am, nb am) with m, n in Z. The corresponding child wavelets are now given as\n{\\displaystyle \\psi _{m,n}(t)={\\frac {1}{\\sqrt {a^{m}}}}\\psi \\left({\\frac {t-nba^{m}}{a^{m}}}\\right).}\nA sufficient condition for the reconstruction of any signal x of finite energy by the formula\n{\\displaystyle x(t)=\\sum _{m\\in \\mathbb {Z} }\\sum _{n\\in \\mathbb {Z} }\\langle x,\\,\\psi _{m,n}\\rangle \\cdot \\psi _{m,n}(t)}\nis that the functions\n{\\displaystyle \\{\\psi _{m,n}:m,n\\in \\mathbb {Z} \\}}\nform an orthonormal basis of L2(R).\n=== Multiresolution based discrete wavelet transforms (continuous in time) ===\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\nFrom the mother and father wavelets one constructs the subspaces\nspan\nwhere\n{\\displaystyle V_{m}=\\operatorname {span} (\\phi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\phi _{m,n}(t)=2^{-m/2}\\phi (2^{-m}t-n)}\nspan\nwhere\n{\\displaystyle W_{m}=\\operatorname {span} (\\psi _{m,n}:n\\in \\mathbb {Z} ),{\\text{ where }}\\psi _{m,n}(t)=2^{-m/2}\\psi (2^{-m}t-n).}\nThe father wavelet\n{\\displaystyle V_{i}}\nkeeps the time domain properties, while the mother wavelets\n{\\displaystyle W_{i}}\nkeeps the frequency domain properties.\nFrom these it is required that the sequence\n{\\displaystyle \\{0\\}\\subset \\dots \\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset \\dots \\subset L^{2}(\\mathbb {R} )}\nforms a multiresolution analysis of L2 and that the subspaces\n{\\displaystyle \\dots ,W_{1},W_{0},W_{-1},\\dots }\nare the orthogonal "differences" of the above sequence, that is, Wm is the orthogonal complement of Vm inside the subspace Vm−1,\n{\\displaystyle V_{m}\\oplus W_{m}=V_{m-1}.}\nIn analogy to the sampling theorem one may conclude that the space Vm with sampling distance 2m more or less covers the frequency baseband from 0 to 1/2m-1. As orthogonal complement, Wm roughly covers the band [1/2m−1, 1/2m].\nFrom those inclusions and orthogonality relations, especially\n{\\displaystyle V_{0}\\oplus W_{0}=V_{-1}}\n, follows the existence of sequences\n{\\displaystyle h=\\{h_{n}\\}_{n\\in \\mathbb {Z} }}\nand\n{\\displaystyle g=\\{g_{n}\\}_{n\\in \\mathbb {Z} }}\nthat satisfy the identities\n{\\displaystyle g_{n}=\\langle \\phi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\phi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }g_{n}\\phi (2t-n),}\nand\n{\\displaystyle h_{n}=\\langle \\psi _{0,0},\\,\\phi _{-1,n}\\rangle }\nso that\n{\\textstyle \\psi (t)={\\sqrt {2}}\\sum _{n\\in \\mathbb {Z} }h_{n}\\phi (2t-n).}\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\nFrom the multiresolution analysis derives the orthogonal decomposition of the space L2 as\n{\\displaystyle L^{2}=V_{j_{0}}\\oplus W_{j_{0}}\\oplus W_{j_{0}-1}\\oplus W_{j_{0}-2}\\oplus W_{j_{0}-3}\\oplus \\cdots }\nFor any signal or function\n{\\displaystyle S\\in L^{2}}\nthis gives a representation in basis functions of the corresponding subspaces as', 'Classical mechanics', 'While supersymmetry has not been discovered at high energy, see Section Supersymmetry in particle physics, supersymmetry was found to be effectively realized at the intermediate energy of hadronic physics where baryons and mesons are superpartners. An exception is the pion that appears as a zero mode in the mass spectrum and thus protected by the supersymmetry: It has no baryonic partner. The realization of this effective supersymmetry is readily explained in quark–diquark models: Because two different color charges close together (e.g., blue and red) appear under coarse resolution as the corresponding anti-color (e.g. anti-green), a diquark cluster viewed with coarse resolution (i.e., at the energy-momentum scale used to study hadron structure) effectively appears as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves likes a meson.\n=== Supersymmetry in condensed matter physics ===\nSUSY concepts have provided useful extensions to the WKB approximation. Additionally, SUSY has been applied to disorder averaged systems both quantum and non-quantum (through statistical mechanics), the Fokker–Planck equation being an example of a non-quantum theory. The \'supersymmetry\' in all these systems arises from the fact that one is modelling one particle and as such the \'statistics\' do not matter. The use of the supersymmetry method provides a mathematical rigorous alternative to the replica trick, but only in non-interacting systems, which attempts to address the so-called \'problem of the denominator\' under disorder averaging. For more on the applications of supersymmetry in condensed matter physics see Efetov (1997).\nIn 2021, a group of researchers showed that, in theory,\n{\\displaystyle N=(0,1)}\nSUSY could be realised at the edge of a Moore–Read quantum Hall state. However, to date, no experiments have been done yet to realise it at an edge of a Moore–Read state. In 2022, a different group of researchers created a computer simulation of atoms in 1 dimensions that had supersymmetric topological quasiparticles.\n=== Supersymmetry in optics ===\nIn 2013, integrated optics was found to provide a fertile ground on which certain ramifications of SUSY can be explored in readily-accessible laboratory settings. Making use of the analogous mathematical structure of the quantum-mechanical Schrödinger equation and the wave equation governing the evolution of light in one-dimensional settings, one may interpret the refractive index distribution of a structure as a potential landscape in which optical wave packets propagate. In this manner, a new class of functional optical structures with possible applications in phase matching, mode conversion and space-division multiplexing becomes possible. SUSY transformations have been also proposed as a way to address inverse scattering problems in optics and as a one-dimensional transformation optics.\n=== Supersymmetry in dynamical systems ===\nAll stochastic (partial) differential equations, the models for all types of continuous time dynamical systems, possess topological supersymmetry. In the operator representation of stochastic evolution, the topological supersymmetry is the exterior derivative which is commutative with the stochastic evolution operator defined as the stochastically averaged pullback induced on differential forms by SDE-defined diffeomorphisms of the phase space. The topological sector of the so-emerging supersymmetric theory of stochastic dynamics can be recognized as the Witten-type topological field theory.\nThe meaning of the topological supersymmetry in dynamical systems is the preservation of the phase space continuity—infinitely close points will remain close during continuous time evolution even in the presence of noise. When the topological supersymmetry is broken spontaneously, this property is violated in the limit of the infinitely long temporal evolution and the model can be said to exhibit (the stochastic generalization of) the butterfly effect. From a more general perspective, spontaneous breakdown of the topological supersymmetry is the theoretical essence of the ubiquitous dynamical phenomenon variously known as chaos, turbulence, self-organized criticality etc. The Goldstone theorem explains the associated emergence of the long-range dynamical behavior that manifests itself as \u20601/f\u2060 noise, butterfly effect, and the scale-free statistics of sudden (instantonic) processes, such as earthquakes, neuroavalanches, and solar flares, known as the Zipf\'s law and the Richter scale.\n=== Supersymmetry in mathematics ===\nSUSY is also sometimes studied mathematically for its intrinsic properties. This is because it describes complex fields satisfying a property known as holomorphy, which allows holomorphic quantities to be exactly computed. This makes supersymmetric models useful "toy models" of more realistic theories. A prime example of this has been the demonstration of S-duality in four-dimensional gauge theories that interchanges particles and monopoles.\nThe proof of the Atiyah–Singer index theorem is much simplified by the use of supersymmetric quantum mechanics.\n=== Supersymmetry in string theory ===\nSupersymmetry is an integral part of string theory, a possible theory of everything. There are two types of string theory, supersymmetric string theory or superstring theory, and non-supersymmetric string theory. By definition of superstring theory, supersymmetry is required in superstring theory at some level. However, even in non-supersymmetric string theory, a type of supersymmetry called misaligned supersymmetry is still required in the theory in order to ensure no physical tachyons appear. Any string theories without some kind of supersymmetry, such as bosonic string theory and the\n{\\displaystyle E_{7}\\times E_{7}}\n16\n{\\displaystyle SU(16)}\n, and\n{\\displaystyle E_{8}}\nheterotic string theories, will have a tachyon and therefore the spacetime vacuum itself would be unstable and would decay into some tachyon-free string theory usually in a lower spacetime dimension. There is no experimental evidence that either supersymmetry or misaligned supersymmetry holds in our universe, and many physicists have moved on from supersymmetry and string theory entirely due to the non-detection of supersymmetry at the LHC.\nDespite the null results for supersymmetry at the LHC so far, some particle physicists have nevertheless moved to string theory in order to resolve the naturalness crisis for certain supersymmetric extensions of the Standard Model. According to the particle physicists, there exists a concept of "stringy naturalness" in string theory, where the string theory landscape could have a power law statistical pull on soft SUSY breaking terms to large values (depending on the number of hidden sector SUSY breaking fields contributing to the soft terms). If this is coupled with an anthropic requirement that contributions to the weak scale not exceed a factor between 2 and 5 from its measured value (as argued by Agrawal et al.), then the Higgs mass is pulled up to the vicinity of 125 GeV while most sparticles are pulled to values beyond the current reach of LHC. (The Higgs was determined to have a mass of 125 GeV ±0.15 GeV in 2022.) An exception occurs for higgsinos which gain mass not from SUSY breaking but rather from whatever mechanism solves the SUSY mu problem. Light higgsino pair production in association with hard initial state jet radiation leads to a soft opposite-sign dilepton plus jet plus missing transverse energy signal.\n== Supersymmetry in particle physics ==\nIn particle physics, a supersymmetric extension of the Standard Model is a possible candidate for undiscovered particle physics, and seen by some physicists as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to be incomplete and where limitations of current theories are well established. In particular, one supersymmetric extension of the Standard Model, the Minimal Supersymmetric Standard Model (MSSM), became popular in theoretical particle physics, as the Minimal Supersymmetric Standard Model is the simplest supersymmetric extension of the Standard Model that could resolve major hierarchy problems within the Standard Model, by guaranteeing that quadratic divergences of all orders will cancel out in perturbation theory. If a supersymmetric extension of the Standard Model is correct, superpartners of the existing elementary particles would be new and undiscovered particles and supersymmetry is expected to be spontaneously broken.\nThere is no experimental evidence that a supersymmetric extension to the Standard Model is correct, or whether or not other extensions to current models might be more accurate. It is only since around 2010 that particle accelerators specifically designed to study physics beyond the Standard Model have become operational (i.e. the Large Hadron Collider (LHC)), and it is not known where exactly to look, nor the energies required for a successful search. However, the negative results from the LHC since 2010 have already ruled out some supersymmetric extensions to the Standard Model, and many physicists believe that the Minimal Supersymmetric Standard Model, while not ruled out, is no longer able to fully resolve the hierarchy problem.\n=== Supersymmetric extensions of the Standard Model ===', 'The Ehrenfest classification implicitly allows for continuous phase transformations, where the bonding character of a material changes, but there is no discontinuity in any free energy derivative. An example of this occurs at the supercritical liquid–gas boundaries.\nThe first example of a phase transition which did not fit into the Ehrenfest classification was the exact solution of the Ising model, discovered in 1944 by Lars Onsager. The exact specific heat differed from the earlier mean-field approximations, which had predicted that it has a simple discontinuity at critical temperature. Instead, the exact specific heat had a logarithmic divergence at the critical temperature. In the following decades, the Ehrenfest classification was replaced by a simplified classification scheme that is able to incorporate such transitions.\n=== Modern classifications ===\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not.\nFamiliar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Yoseph Imry and Michael Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\nSecond-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state–mixed-state and mixed-state–superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements.  Lev Landau gave a phenomenological theory of second-order phase transitions.\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\nSeveral transitions are known as infinite-order phase transitions.\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a quenched disorder state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n== Characteristic properties ==\n=== Phase coexistence ===\nA disorder-broadened  first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n=== Critical points ===\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n=== Symmetry ===\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n=== Order parameters ===\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.\nSome phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n=== Relevance in cosmology ===\nSymmetry-breaking phase transitions play an important role in cosmology. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to explain the asymmetry between the amount of matter and antimatter in the present-day universe, according to  electroweak baryogenesis theory.\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer.\nSee also relational order theories and order and disorder.\n=== Critical exponents and universality classes ===', 'In the second edition of his monograph, in 1912, Planck sustained his dissent from Einstein\'s proposal of light quanta. He proposed in some detail that absorption of light by his virtual material resonators might be continuous, occurring at a constant rate in equilibrium, as distinct from quantal absorption. Only emission was quantal. This has at times been called Planck\'s "second theory".\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his \'third theory\', that both emission and absorption of light were quantal.\nThe colourful term "ultraviolet catastrophe" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck\'s thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of "catastrophe". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called \'photons\', and by Rayleigh and by Jeans.\nIn 1913, Bohr gave another formula with a further different physical meaning to the quantity hν. In contrast to Planck\'s and Einstein\'s formulas, Bohr\'s formula referred explicitly and categorically to energy levels of atoms. Bohr\'s formula was Wτ2 − Wτ1 = hν where Wτ2 and Wτ1 denote the energy levels of quantum states of an atom, with quantum numbers τ2 and τ1. The symbol ν denotes the frequency of a quantum of radiation that can be emitted or absorbed as the atom passes between those two quantum states. In contrast to Planck\'s model, the frequency\n{\\displaystyle \\nu }\nhas no immediate relation to frequencies that might describe those quantum states themselves.\nLater, in 1924, Satyendra Nath Bose developed the theory of the statistical mechanics of photons, which allowed a theoretical derivation of Planck\'s law. The actual word \'photon\' was invented still later, by G.N. Lewis in 1926, who mistakenly believed that photons were conserved, contrary to Bose–Einstein statistics; nevertheless the word \'photon\' was adopted to express the Einstein postulate of the packet nature of light propagation. In an electromagnetic field isolated in a vacuum in a vessel with perfectly reflective walls, such as was considered by Planck, indeed the photons would be conserved according to Einstein\'s 1905 model, but Lewis was referring to a field of photons considered as a system closed with respect to ponderable matter but open to exchange of electromagnetic energy with a surrounding system of ponderable matter, and he mistakenly imagined that still the photons were conserved, being stored inside atoms.\nUltimately, Planck\'s law of black-body radiation contributed to Einstein\'s concept of quanta of light carrying linear momentum, which became the fundamental basis for the development of quantum mechanics.\nThe above-mentioned linearity of Planck\'s mechanical assumptions, not allowing for energetic interactions between frequency components, was superseded in 1925 by Heisenberg\'s original quantum mechanics. In his paper submitted on 29 July 1925, Heisenberg\'s theory accounted for Bohr\'s above-mentioned formula of 1913. It admitted non-linear oscillators as models of atomic quantum states, allowing energetic interaction between their own multiple internal discrete Fourier frequency components, on the occasions of emission or absorption of quanta of radiation. The frequency of a quantum of radiation was that of a definite coupling between internal atomic meta-stable oscillatory quantum states. At that time, Heisenberg knew nothing of matrix algebra, but Max Born read the manuscript of Heisenberg\'s paper and recognized the matrix character of Heisenberg\'s theory. Then Born and Jordan published an explicitly matrix theory of quantum mechanics, based on, but in form distinctly different from, Heisenberg\'s original quantum mechanics; it is the Born and Jordan matrix theory that is today called matrix mechanics. Heisenberg\'s explanation of the Planck oscillators, as non-linear effects apparent as Fourier modes of transient processes of emission or absorption of radiation, showed why Planck\'s oscillators, viewed as enduring physical objects such as might be envisaged by classical physics, did not give an adequate explanation of the phenomena.\nNowadays, as a statement of the energy of a light quantum, often one finds the formula E = ħω, where ħ = \u2060h/2π\u2060, and ω = 2πν denotes angular frequency, and less often the equivalent formula E = hν. This statement about a really existing and propagating light quantum, based on Einstein\'s, has a physical meaning different from that of Planck\'s above statement ϵ = hν about the abstract energy units to be distributed amongst his hypothetical resonant material oscillators.\nAn article by Helge Kragh published in Physics World gives an account of this history.\n== See also ==\nEmissivity\nRadiance\nSakuma–Hattori equation\n== References ==\n=== Bibliography ===\n== External links ==\nSummary of Radiation\nRadiation of a Blackbody – interactive simulation to play with Planck\'s law\nScienceworld entry on Planck\'s Law', '{\\displaystyle |0\\rangle }\n.  This Hilbert space is called Fock space.  For each  k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to\n{\\displaystyle H=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}a_{k}^{\\dagger }a_{k}=\\sum _{k=-\\infty }^{\\infty }\\hbar \\omega _{k}N_{k},}\nwhere Nk may be interpreted as the number operator giving the number of particles in a state with momentum k.\nThis Hamiltonian differs from the previous expression by the subtraction of the zero-point energy  ħωk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation.  This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering.\n==== Other fields ====\nAll other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed.\nIt turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed.  These are defined by {A, B} = AB + BA.\nWhen quantizing fermions, the fields are expanded in creation and annihilation operators, θk†, θk, which satisfy\n0.\n{\\displaystyle \\{\\theta _{k},\\theta _{l}^{\\dagger }\\}=\\delta _{kl},\\ \\ \\{\\theta _{k},\\theta _{l}\\}=0,\\ \\ \\{\\theta _{k}^{\\dagger },\\theta _{l}^{\\dagger }\\}=0.}\nThe states are constructed on a vacuum\n{\\displaystyle |0\\rangle }\nannihilated by the θk, and the Fock space is built by applying all products of creation operators θk† to |0⟩.  Pauli\'s exclusion principle is satisfied, because\n{\\displaystyle (\\theta _{k}^{\\dagger })^{2}|0\\rangle =0}\n, by virtue of the anti-commutation relations.\n=== Condensates ===\nThe construction of the scalar field states above assumed that the potential was minimized at φ = 0, so that the vacuum minimizing the Hamiltonian satisfies ⟨φ⟩ = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value  φ = v .  This occurs for example, if V(φ) = gφ4 − 2m2φ2 with g > 0 and m2 > 0, for which the minimum energy is found at v = ±m/√g. The value of v in one of these vacua may be considered as condensate of the field φ. Canonical quantization then can be carried out for the shifted field  φ(x,t) − v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field.  This construction is utilized in the Higgs mechanism in the standard model of particle physics.\n== Mathematical quantization ==\n=== Deformation quantization ===\nThe classical theory is described using a spacelike  foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator  [A, B]  expressed in the phase space formulation is iħ{A, B} .  (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved,\nand providing the arguments of such brackets,  ħ-deformations are highly nonunique—quantization is an "art", and is specified by the physical context.\n(Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit,  ħ → 0.)\nNow, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian.\nA further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an ħ-deformation of the corresponding Poisson algebra or even Poisson supermanifolds.\n=== Geometric quantization ===\nIn contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold\n{\\displaystyle M}\n, one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over\n{\\displaystyle M}\n. On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of\n{\\displaystyle M}\nOne then proceeds by choosing a polarization, that is (roughly), a choice of\n{\\displaystyle n}\nvariables on the\n{\\displaystyle 2n}\n-dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the\n{\\displaystyle n}\nchosen variables, in the sense that they are covariantly constant in the other\n{\\displaystyle n}\ndirections. If the chosen variables are real, we get something like the traditional Schrödinger Hilbert space. If the chosen variables are complex, we get something like the Segal–Bargmann space.\n== See also ==\nCorrespondence principle\nCreation and annihilation operators\nDirac bracket\nMoyal bracket\nPhase space formulation (of quantum mechanics)\nGeometric quantization\n== References ==\n=== Historical References ===\nSilvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7\n=== General Technical References ===\nAlexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3\nJames D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964\nHall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158.\nAn introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2\nFranz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8\n== External links ==\nPedagogic Aides to Quantum Field Theory  Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2.', 'In response to the so-called "naturalness crisis" in the Minimal Supersymmetric Standard Model, some researchers have abandoned naturalness and the original motivation to solve the hierarchy problem naturally with supersymmetry, while other researchers have moved on to other supersymmetric models such as split supersymmetry. Still others have moved to string theory as a result of the naturalness crisis. Former enthusiastic supporter Mikhail Shifman went as far as urging the theoretical community to search for new ideas and accept that supersymmetry was a failed theory in particle physics. However, some researchers suggested that this "naturalness" crisis was premature because various calculations were too optimistic about the limits of masses which would allow a supersymmetric extension of the Standard Model as a solution.\n== General supersymmetry ==\nSupersymmetry appears in many related contexts of theoretical physics. It is possible to have multiple supersymmetries and also have supersymmetric extra dimensions.\n=== Extended supersymmetry ===\nIt is possible to have more than one kind of supersymmetry transformation. Theories with more than one supersymmetry transformation are known as extended supersymmetric theories. The more supersymmetry a theory has, the more constrained are the field content and interactions. Typically the number of copies of a supersymmetry is a power of 2 (1, 2, 4, 8...). In four dimensions, a spinor has four degrees of freedom and thus the minimal number of supersymmetry generators is four in four dimensions and having eight copies of supersymmetry means that there are 32 supersymmetry generators.\nThe maximal number of supersymmetry generators possible is 32. Theories with more than 32 supersymmetry generators automatically have massless fields with spin greater than 2. It is not known how to make massless fields with spin greater than two interact, so the maximal number of supersymmetry generators considered is 32. This is due to the Weinberg–Witten theorem. This corresponds to an N = 8 supersymmetry theory. Theories with 32 supersymmetries automatically have a graviton.\nFor four dimensions there are the following theories, with the corresponding multiplets (CPT adds a copy, whenever they are not invariant under such symmetry):\n=== Supersymmetry in alternate numbers of dimensions ===\nIt is possible to have supersymmetry in dimensions other than four. Because the properties of spinors change drastically between different dimensions, each dimension has its characteristic. In d dimensions, the size of spinors is approximately 2d/2 or 2(d − 1)/2. Since the maximum number of supersymmetries is 32, the greatest number of dimensions in which a supersymmetric theory can exist is eleven.\n=== Fractional supersymmetry ===\nFractional supersymmetry is a generalization of the notion of supersymmetry in which the minimal positive amount of spin does not have to be \u20601/2\u2060 but can be an arbitrary \u20601/N\u2060 for integer value of N. Such a generalization is possible in two or fewer spacetime dimensions.\n== See also ==\n== References ==\n== Further reading ==\n=== Theoretical introductions, free and online ===\n=== Monographs ===\n=== On experiments ===\n== External links ==\nSupersymmetry – European Organization for Nuclear Research (CERN)\nThe status of supersymmetry – Symmetry Magazine (Fermilab/SLAC), January 12, 2021\nAs Supersymmetry Fails Tests, Physicists Seek New Ideas – Quanta Magazine, November 20, 2012\nWhat is Supersymmetry? – Fermilab, May 21, 2013\nWhy Supersymmetry? – Fermilab, May 31, 2013\nThe Standard Model and Supersymmetry – World Science Festival, March 4, 2015\nSUSY running out of hiding places – BBC, December 11, 2012', 'Both the core mass function (CMF) and filament line mass function (FLMF) observed in the California GMC follow power-law distributions at the high-mass end, consistent with the Salpeter initial mass function (IMF). Current results strongly support the existence of a connection between the FLMF and the CMF/IMF, demonstrating that this connection holds at the level of an individual cloud, specifically the California GMC. The FLMF presented is a distribution of local line masses for a complete, homogeneous sample of filaments within the same cloud. It is the local line mass of a filament that defines its ability to fragment at a particular location along its spine, not the average line mass of the filament. This connection is more direct and provides tighter constraints on the origin of the CMF/IMF.\n== See also ==\nAccretion – Accumulation of particles into a massive object by gravitationally attracting more matter\nChampagne flow model\nChronology of the universe – History and future of the universe\nFormation and evolution of the Solar System\nGalaxy formation and evolution – Subfield of cosmology\nList of star-forming regions in the Local Group – Regions in the Milky Way galaxy and Local Group where new stars are forming\nPea galaxy – Possible type of luminous blue compact galaxy\nStar evolution – Changes to stars over their lifespansPages displaying short descriptions of redirect targets\n== References ==', '=== Rotation curves ===\nIn addition to demonstrating that rotation curves in MOND are flat, equation 2 provides a concrete relation between a galaxy\'s total baryonic mass (the sum of its mass in stars and gas) and its asymptotic rotation velocity. This predicted relation was called the mass-asymptotic speed relation (MASSR) by Milgrom; its observational manifestation is known as the baryonic Tully–Fisher relation (BTFR), and is found to conform quite closely to the MOND prediction. This relation is derived from the Deep-MOND limit as follows:\nMilgrom\'s law fully specifies the rotation curve of a galaxy given only the distribution of its baryonic mass. In particular, MOND predicts a far stronger correlation between features in the baryonic mass distribution and features in the rotation curve than does the dark matter hypothesis (since dark matter dominates the galaxy\'s mass budget and is conventionally assumed not to closely track the distribution of baryons). Such a tight correlation is claimed to be observed in several spiral galaxies, a fact which has been referred to as "Renzo\'s rule".\nSince MOND modifies Newtonian dynamics in an acceleration-dependent way, it predicts a specific relationship between the acceleration of a star at any radius from the centre of a galaxy and the amount of unseen (dark matter) mass within that radius that would be inferred in a Newtonian analysis. This is known as the mass discrepancy-acceleration relation, and has been measured observationally. One aspect of the MOND prediction is that the mass of the inferred dark matter goes to zero when the stellar centripetal acceleration becomes greater than a0, where MOND reverts to Newtonian mechanics. In a dark matter hypothesis, it is a challenge to understand why this mass should correlate so closely with acceleration, and why there appears to be a critical acceleration above which dark matter is not required.\nParticularly massive galaxies are within the Newtonian regime (a > a0) out to radii enclosing the vast majority of their baryonic mass. At these radii, MOND predicts that the rotation curve should fall as 1/r, in accordance with Kepler\'s Laws. In contrast, from a dark matter perspective one would expect the halo to significantly boost the rotation velocity and cause it to asymptote to a constant value, as in less massive galaxies. Observations of high-mass ellipticals bear out the MOND prediction.\nIn 2020, a group of astronomers analyzing data from the Spitzer Photometry and Accurate Rotation Curves (SPARC) sample together with estimates of the large-scale external gravitational field from an all-sky galaxy catalog, concluded that there was highly statistically significant evidence of violations of the strong equivalence principle in weak gravitational fields in the vicinity of rotationally supported galaxies. They observed an effect consistent with the external field effect of modified Newtonian dynamics and inconsistent with tidal effects in the Lambda-CDM model paradigm commonly known as the Standard Model of Cosmology.\nIn 2023, a study claimed that cold dark matter cannot explain galactic rotation curves, while MOND can.\n=== Dwarf galaxies ===\nRecent work has shown that many of the dwarf galaxies around the Milky Way and Andromeda are located preferentially in a single plane and have correlated motions. This suggests that they may have formed during a close encounter with another galaxy and hence are tidal dwarf galaxies. If so, the presence of mass discrepancies in these systems constitutes evidence for MOND. In addition, it has been claimed that a gravitational force stronger than Newton\'s (such as Milgrom\'s) is required for these galaxies to retain their orbits over time. Centaurus A has a similar plane of dwarf galaxies around it which is challenging for LCDM which expects uniform halos of dwarf galaxies.\nIn MOND, all isolated gravitationally bound objects with a < a0 that are in equilibrium – regardless of their origin – should exhibit a mass discrepancy when analyzed using Newtonian mechanics, and should lie on the BTFR. Under the dark matter hypothesis, objects formed from baryonic material ejected during the merger or tidal interaction of two galaxies ("tidal dwarf galaxies") are expected to be devoid of dark matter and hence show no mass discrepancy. Three objects unambiguously identified as tidal dwarf galaxies appear to have mass discrepancies in agreement with the MOND prediction.\nIn a 2022 published survey of dwarf galaxies from the Fornax Deep Survey (FDS) catalogue, a group of astronomers and physicists conclude that \'observed deformations of dwarf galaxies in the Fornax Cluster and the lack of low surface brightness dwarfs towards its centre are incompatible with ΛCDM expectations but well consistent with MOND.\'\n=== Gravitational lensing ===\nWeak gravitational lensing around isolated spiral and elliptical galaxies confirms the gravitational field of such galaxies follows Milgrom\'s law. This corresponds to flat rotation curves out to distances of 1 Mpc.\nStrong gravitational lensing using Einstein rings also seems to confirm the MOND expectation for the mass discrepancy-acceleration relation.\n=== Other ===\nBoth MOND and dark matter halos stabilize disk galaxies, helping them retain their rotation-supported structure and preventing their transformation into elliptical galaxies. In MOND, this added stability is only available for regions of galaxies within the deep-MOND regime (i.e., with a < a0), suggesting that spirals with a > a0 in their central regions should be prone to instabilities and hence less likely to survive to the present day. This may explain the "Freeman limit" to the observed central surface mass density of spiral galaxies, which is roughly a0/G. This scale must be put in by hand in dark matter-based galaxy formation models.\nGalactic bars in barred galaxies are in tension with dark matter simulations as they are too pronounced and rotate too fast, yet do match MOND based calculations.\nIn 2022, Kroupa et al. published a study of open star clusters, arguing that asymmetry in the population of leading and trailing tidal tails, and the observed lifetime of these clusters, are inconsistent with Newtonian dynamics but consistent with MOND.\nIn 2023, a study measured the acceleration of 26,615 wide binaries within 200 parsecs. The study showed that those binaries with accelerations less than 1 nm/s2 systematically deviate from Newtonian dynamics, but conform to MOND predictions, specifically to AQUAL. The results are disputed, with some authors arguing that the detection is caused by poor quality controls, while the original authors claimed that the added quality controls do not significantly affect the results.\nIn 2024, a study claimed that the universe\'s earliest galaxies formed and grew too quickly for the Lambda-CDM model to explain, but such rapid growth is predicted in MOND.\n== Responses and criticism ==\n=== Dark matter explanation ===\nWhile acknowledging that Milgrom\'s law provides a succinct and accurate description of a range of galactic phenomena, many physicists reject the idea that classical dynamics itself needs to be modified and attempt instead to explain the law\'s success by reference to the behavior of dark matter. Some effort has gone towards establishing the presence of a characteristic acceleration scale as a natural consequence of the behavior of cold dark matter halos, although Milgrom has argued that such arguments explain only a small subset of MOND phenomena. An alternative proposal is to ad hoc modify the properties of dark matter (e.g., to make it interact strongly with itself or baryons) in order to induce the tight coupling between the baryonic and dark matter mass that the observations point to. Finally, some researchers suggest that explaining the empirical success of Milgrom\'s law requires a more radical break with conventional assumptions about the nature of dark matter. One idea (dubbed "dipolar dark matter") is to make dark matter gravitationally polarizable by ordinary matter and have this polarization enhance the gravitational attraction between baryons.\n=== Outstanding problems for MOND ===\nSome ultra diffuse galaxies, such as NGC 1052-DF2, originally appeared to be free of dark matter. Were this the case, it would have posed a problem for MOND because it cannot explain the rotation curves. However, further research showed that the galaxies were at a different distance than previously thought, leaving the galaxies with plenty of room for dark matter. The idea that a single value of a0 can fit all the different galaxies\' rotation curves has also been criticized, although this finding is disputed. It has also been claimed that MOND offers a poor fit to both the HI column density and size of Lyα absorbers. Modified inertia versions of MOND have long suffered from poor theoretical compatibility with cherished physical principles such as conservation laws. Researchers working on MOND generally do not interpret it as a modification of inertia, with only very limited work done on this area.\n==== Solar system ====']

Question: What is the formalism that angular momentum is associated with in rotational invariance?

Choices:
Choice A) Angular momentum is the 1-form Noether charge associated with rotational invariance.
Choice B) Angular momentum is the 3-form Noether charge associated with rotational invariance.
Choice C) Angular momentum is the 5-form Noether charge associated with rotational invariance.
Choice D) Angular momentum is the 2-form Noether charge associated with rotational invariance.
Choice E) Angular momentum is the 4-form Noether charge associated with rotational invariance.

Very important notes:
- Start by reading the question carefully. analyze what the question is asking for yourself before going through the choices.
- Carefully consider the relationship or concept described in the question and context.
- Evaluate each choice against the context and the question, one by one.
- Check if each choice is correct, partially correct, or incorrect based on the context and the information provided.
- Provide a final answer after considering all choices.

It's ok to talk and think about the problem but the last line of your answer should be the string 'Answer:' followed by either A, B, C, D, or E.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
